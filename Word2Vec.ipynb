{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56ba3ebc-2725-445f-9b98-bc4740981009",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:04<00:00, 21.61s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from functools import partial \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from functools import partial\n",
    "\n",
    "import time\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"data/bbc-text.csv\")\n",
    "\n",
    "\n",
    "df[\"text\"]=df[\"text\"].apply(lambda x: re.sub(r'[^\\w\\s]', ' ',x) ) \n",
    "\n",
    "\n",
    "def log_time(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()  # Record start time\n",
    "        result = func(*args, **kwargs)  # Execute the wrapped function\n",
    "        end_time = time.time()  # Record end time\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Function '{func.__name__}' executed in {elapsed_time:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    " \n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Disable unnecessary pipeline components\n",
    "nlp.disable_pipes([\"parser\", \"ner\"])\n",
    "\n",
    "# Custom tokenizer to speed up processing\n",
    "def custom_tokenizer(text):\n",
    "    return Doc(nlp.vocab, words=text.split())\n",
    "\n",
    "nlp.tokenizer = custom_tokenizer\n",
    "\n",
    "# Preprocess function\n",
    "def preprocess_text(text, nlp, max_words=150):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc if not token.is_stop and token.text.strip()]\n",
    "    return tokens[:max_words] + ['<PAD>'] * (max_words - len(tokens))\n",
    "# Vectorize function\n",
    "def vectorize_text(tokens, nlp):\n",
    "    return np.array([nlp.vocab[token].vector for token in tokens])\n",
    "\n",
    "# Apply preprocessing in batches\n",
    "batch_size = 1000\n",
    "preprocessed_texts = []\n",
    "\n",
    "for i in tqdm(range(0, len(df), batch_size)):\n",
    "    batch = df['text'].iloc[i:i+batch_size]\n",
    "    preprocessed_batch = [preprocess_text(text, nlp) for text in nlp.pipe(batch, batch_size=64)]\n",
    "    preprocessed_texts.extend(preprocessed_batch)\n",
    "\n",
    "df['processed_text'] = preprocessed_texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57c77e80-494e-4d47-913d-132c67760217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcef1351-731c-4305-ac16-e209d225c4ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5908f9e9-27cb-4b2b-a7bc-2a0fc2d43979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [02:18<00:00, 46.31s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>[tv, future, hands, viewers, home, theatre, sy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>[worldcom, boss, left, books, worldcom, boss, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>[tigers, wary, farrell, gamble, leicester, rus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>[yeading, face, newcastle, fa, cup, premiershi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>[ocean, s, raids, box, office, ocean, s, crime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>business</td>\n",
       "      <td>cars pull down us retail figures us retail sal...</td>\n",
       "      <td>[cars, pull, retail, figures, retail, sales, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>politics</td>\n",
       "      <td>kilroy unveils immigration policy ex chatshow ...</td>\n",
       "      <td>[kilroy, unveils, immigration, policy, ex, cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>rem announce new glasgow concert us band rem h...</td>\n",
       "      <td>[rem, announce, new, glasgow, concert, band, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>politics</td>\n",
       "      <td>how political squabbles snowball it s become c...</td>\n",
       "      <td>[political, squabbles, snowball, s, commonplac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>sport</td>\n",
       "      <td>souness delight at euro progress boss graeme s...</td>\n",
       "      <td>[souness, delight, euro, progress, boss, graem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           category                                               text  \\\n",
       "0              tech  tv future in the hands of viewers with home th...   \n",
       "1          business  worldcom boss  left books alone  former worldc...   \n",
       "2             sport  tigers wary of farrell  gamble  leicester say ...   \n",
       "3             sport  yeading face newcastle in fa cup premiership s...   \n",
       "4     entertainment  ocean s twelve raids box office ocean s twelve...   \n",
       "...             ...                                                ...   \n",
       "2220       business  cars pull down us retail figures us retail sal...   \n",
       "2221       politics  kilroy unveils immigration policy ex chatshow ...   \n",
       "2222  entertainment  rem announce new glasgow concert us band rem h...   \n",
       "2223       politics  how political squabbles snowball it s become c...   \n",
       "2224          sport  souness delight at euro progress boss graeme s...   \n",
       "\n",
       "                                         processed_text  \n",
       "0     [tv, future, hands, viewers, home, theatre, sy...  \n",
       "1     [worldcom, boss, left, books, worldcom, boss, ...  \n",
       "2     [tigers, wary, farrell, gamble, leicester, rus...  \n",
       "3     [yeading, face, newcastle, fa, cup, premiershi...  \n",
       "4     [ocean, s, raids, box, office, ocean, s, crime...  \n",
       "...                                                 ...  \n",
       "2220  [cars, pull, retail, figures, retail, sales, f...  \n",
       "2221  [kilroy, unveils, immigration, policy, ex, cha...  \n",
       "2222  [rem, announce, new, glasgow, concert, band, r...  \n",
       "2223  [political, squabbles, snowball, s, commonplac...  \n",
       "2224  [souness, delight, euro, progress, boss, graem...  \n",
       "\n",
       "[2225 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb0ae15-dbd4-4fe8-b31d-ff1ec6495dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d9d0a7-5c24-46c8-9927-15f21145fcfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a274d63b-1a24-460a-9dae-fdcc22fbb21c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75934a5e-6b6c-4e45-b81d-48bbf9a345e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import cupy as cp \n",
    "import pickle\n",
    "class Word2Vec:\n",
    "\n",
    "    def __init__(self, embedding_size, semi_context_window, complete_text,optimizer,learning_rate=0.01,flattening_strategy=\"concat\",\n",
    "                 momentum_beta=0.9,\n",
    "                 rmsprop_beta=0.98,\n",
    "                 epsilon=0.00000001,\n",
    "                 load_pretrained=False):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.side_window_size = semi_context_window\n",
    "        self.complete_text = complete_text\n",
    "        self.vocabulary = self.create_vocabulary(complete_text)\n",
    "        self.flattening_strategy = flattening_strategy\n",
    "        \n",
    "        if load_pretrained == False:\n",
    "            self.words_len_embedding_layer = cp.random.rand(len(self.vocabulary), embedding_size)\n",
    "            self.words_len_embedding_bias = cp.random.rand(embedding_size)\n",
    "            \n",
    "    \n",
    "            self.outlayer_maps_vocab_average = cp.random.rand(embedding_size, len(self.vocabulary))\n",
    "            self.out_bias_maps_vocab_average = cp.random.rand(len(self.vocabulary))\n",
    "    \n",
    "            self.outlayer_maps_vocab_concat = cp.random.rand(semi_context_window * 2 * embedding_size, len(self.vocabulary))\n",
    "            self.out_bias_maps_vocab_concat = cp.random.rand(len(self.vocabulary))\n",
    "        else:\n",
    "            self.load_weights()\n",
    "            \n",
    "        self.optimizer = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.RMSPDW2 = 0\n",
    "        self.RMSPb2 = 0 \n",
    "        self.RMSPDW1 = 0\n",
    "        self.RMSPb1 = 0 \n",
    "        self.MomDW2 = 0\n",
    "        self.Momb2  = 0\n",
    "        self.MomDW1 = 0\n",
    "        self.Momb1 = 0 \n",
    "        self.momentum_beta=momentum_beta\n",
    "        self.rmsprop_beta=rmsprop_beta\n",
    "        self.epsilon=epsilon\n",
    "        self.iterations=0\n",
    "        self.total_loss = 0\n",
    "\n",
    "    def cross_entropy_loss(self, predictions, target):\n",
    "        # Cross-entropy loss for a batch of predictions and targets\n",
    "        batch_loss = -cp.sum(target * cp.log(predictions + 1e-9), axis=1)\n",
    "        return cp.mean(batch_loss)\n",
    "\n",
    "    def softmax(self, x, axis=-1):\n",
    "        x = cp.clip(x, -1e4, 1e4)  # Clip for numerical stability\n",
    "        e_x = cp.exp(x - cp.max(x, axis=axis, keepdims=True))\n",
    "        return e_x / cp.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "    def input_one_hot_vectors(self,words,vocabulary):\n",
    "     \n",
    "        inputs=np.zeros((len(words),len(vocabulary)),int)\n",
    "         \n",
    "        for i in range(len(words)): \n",
    "            inputs[i][vocabulary[words[i]]]=1\n",
    "        \n",
    "        return inputs \n",
    "\n",
    "    def inputs_window_words(self, input_text, vocabulary):\n",
    "        sequence = re.sub(r'[^\\w\\s]', ' ', input_text).split()\n",
    "        training_samples = []\n",
    "        for i in range(self.side_window_size, len(sequence) - self.side_window_size):\n",
    "            words_before = sequence[i - self.side_window_size:i]\n",
    "            words_after = sequence[i + 1:i + 1 + self.side_window_size]\n",
    "            # print(words_before)\n",
    "            # print(words_after)\n",
    "            words_input = words_before + words_after\n",
    "            X_i = self.input_one_hot_vectors(words_input, vocabulary)\n",
    "            y_i = self.input_one_hot_vectors([sequence[i]], vocabulary)\n",
    "            # print(words_input,sequence[i])\n",
    "            training_sample = [X_i, y_i]\n",
    "            training_samples.append(training_sample)\n",
    "        yield training_samples\n",
    "\n",
    "    def forward(self, Input):\n",
    "        sigma_zout_one = self.softmax(\n",
    "            cp.matmul(cp.array(Input), self.words_len_embedding_layer) + self.words_len_embedding_bias)\n",
    "        \n",
    "        if self.flattening_strategy == \"concatenate\":\n",
    "            sigma_zout_one = sigma_zout_one.reshape(1, sigma_zout_one.shape[0] * self.embedding_size)\n",
    "            sigma_zout_output = self.softmax(\n",
    "                cp.matmul(sigma_zout_one, self.outlayer_maps_vocab_concat) + self.out_bias_maps_vocab_concat)\n",
    "\n",
    "\n",
    "        elif self.flattening_strategy == \"average\":\n",
    "            sigma_zout_one = cp.mean(sigma_zout_one, axis=0).reshape(1, sigma_zout_one.shape[1])\n",
    "            sigma_zout_output = self.softmax(\n",
    "                cp.matmul(sigma_zout_one, self.outlayer_maps_vocab_average) + self.out_bias_maps_vocab_average)\n",
    "\n",
    "        return [sigma_zout_one, sigma_zout_output]\n",
    "\n",
    "    def training_validation(self, X_validation, y_validation, current_loss):\n",
    "        # Pad validation sequences\n",
    "        X_validation = self.pad_sequences(X_validation, self.words_per_phrase)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = self.predict(X_validation)\n",
    "\n",
    "        # Convert predictions to class labels\n",
    "        y_pred = cupy.argmax(predictions, axis=1)\n",
    "\n",
    "        # Convert one-hot encoded true labels to class labels\n",
    "        y_true = cupy.argmax(y_validation, axis=1)\n",
    "\n",
    "        #print(\"y_true\",np.array(y_true.get()))\n",
    "        # Calculate F1 score\n",
    "        f1 = f1_score(y_true.get(), y_pred.get(), average=\"weighted\")\n",
    "        print(\"Validation F1 score: \", f1)\n",
    "\n",
    "        \n",
    "    def backpropagation(self, dLoss_dZ2, sigma_Z_1, sigma_Z_2, Xi):\n",
    "        # print(\"dLoss_dZ2\", dLoss_dZ2.shape)\n",
    "        # print(\"W2\", self.outlayer_maps_vocab_concat.shape)\n",
    "        # print(\"b2\", self.out_bias_maps_vocab_concat.shape)\n",
    "        # print(\"sigma_Z_1\", sigma_Z_1.shape)\n",
    "        # print(\"sigma_Z_2\", sigma_Z_2.shape)\n",
    "        # print(\"W1\", self.words_len_embedding_layer.shape)\n",
    "        # print(\"b1\", self.words_len_embedding_bias.shape)\n",
    "        # print(\"Xi\", Xi.shape)\n",
    "\n",
    "        if self.flattening_strategy == \"concatenate\":\n",
    "\n",
    "\n",
    "\n",
    "            #Gradient Embedding Weights C\n",
    "            dL_dSigmaZ1=cp.matmul(dLoss_dZ2,self.outlayer_maps_vocab_concat.T)\n",
    "            dL_dZ1=dL_dSigmaZ1*sigma_Z_1*(1-sigma_Z_1)\n",
    "            dL_dZ1=dL_dZ1.reshape(2*self.side_window_size,self.embedding_size)\n",
    "            dLoss_dW1=cp.matmul(dL_dZ1.T,Xi).T \n",
    "        \n",
    "\n",
    "            #Gradient Embedding Bias C\n",
    "            dLoss_db1=cp.sum(dL_dZ1,axis=0).T\n",
    "           \n",
    "            #Gradient Embedding W2C\n",
    "            dLoss_dW2=cp.matmul(dLoss_dZ2.T,sigma_Z_1).T \n",
    "          \n",
    "\n",
    "            #Gradient Embedding B2C\n",
    "            dLoss_db2=dLoss_dZ2.T \n",
    "            \n",
    "            # print(\"dLoss_dW2\",dLoss_dW2.shape,\"W2\",self.outlayer_maps_vocab_concat.shape)\n",
    "            # print(\"dLoss_db2\",dLoss_db2.shape,\"B2\",self.out_bias_maps_vocab_concat.shape)\n",
    "            # print(\"dLoss_dW1\",dLoss_dW1.shape,\"W1\",self.words_len_embedding_layer.shape)\n",
    "            # print(\"dLoss_db1\",dLoss_db1.shape,\"B1\",self.words_len_embedding_bias.shape)\n",
    "            # Update weights and biases\n",
    "            self.update_params(dLoss_dW2,dLoss_db2,dLoss_dW1,dLoss_db1)\n",
    "\n",
    "             \n",
    "        elif self.flattening_strategy == \"average\":\n",
    "            \n",
    "            #Gradient Embedding Weights\n",
    "            dL_dSigmaZ1=cp.matmul(dLoss_dZ2,self.outlayer_maps_vocab_average.T)\n",
    "            dL_dSigmaZ1=dL_dSigmaZ1.repeat(2*self.side_window_size,axis=0)\n",
    "            dL_dZ1=dL_dSigmaZ1*sigma_Z_1*(1-sigma_Z_1)\n",
    "            dLoss_dW1=cp.matmul(dL_dZ1.T,Xi).T\n",
    "           \n",
    "\n",
    "            #Gradient Embedding Bias\n",
    "            dLoss_db1=cp.sum(dL_dZ1,axis=0).T\n",
    "            \n",
    "            #Gradient Embedding W2\n",
    "            dLoss_dW2=cp.matmul(dLoss_dZ2.T,sigma_Z_1).T\n",
    "           \n",
    "            \n",
    "            #Gradient Embedding B2\n",
    "            dLoss_db2=dLoss_dZ2.T\n",
    "            \n",
    "            # print(\"dLoss_dW2\",dLoss_dW2.shape,\"W2\",self.outlayer_maps_vocab_average.shape)\n",
    "            # print(\"dLoss_db2\",dLoss_db2.shape,\"B2\",self.out_bias_maps_vocab_average.shape)\n",
    "            # print(\"dLoss_dW1\",dLoss_dW1.shape,\"W1\",self.words_len_embedding_layer.shape)\n",
    "            # print(\"dLoss_db1\",dLoss_db1.shape,\"B1\",self.words_len_embedding_bias.shape)\n",
    "            # Update weights and biases\n",
    "            self.update_params(dLoss_dW2,dLoss_db2,dLoss_dW1,dLoss_db1)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            print(\"Please select valid flattening strategy!\")\n",
    "         \n",
    "\n",
    "    \n",
    "    def create_vocabulary(self,complete_text):\n",
    "        \n",
    "        text = re.sub(r'[^\\w\\s]', ' ', complete_text).split()\n",
    "        vocabulary = list(set(text)) \n",
    "        vocabulary = {word: idx for idx, word in enumerate(vocabulary)} \n",
    "        print(\"Vocabulary size: \", len(vocabulary))\n",
    "        with open('data/vocabulary.pkl', 'wb') as handle:\n",
    "            pickle.dump(vocabulary, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    \n",
    "        return vocabulary\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "     \n",
    "        with open('data/words_len_embedding_layer.pkl', 'wb') as handle:\n",
    "            pickle.dump(self.words_len_embedding_layer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "        with open('data/words_len_embedding_bias.pkl', 'wb') as handle:\n",
    "            pickle.dump(self.words_len_embedding_bias, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "            \n",
    "        if self.flattening_strategy == \"concatenate\":\n",
    "            with open('data/outlayer_maps_vocab_concat.pkl', 'wb') as handle:\n",
    "                pickle.dump(self.outlayer_maps_vocab_concat, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "            with open('data/out_bias_maps_vocab_concat.pkl', 'wb') as handle:\n",
    "                pickle.dump(self.out_bias_maps_vocab_concat, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                \n",
    "    \n",
    "        if self.flattening_strategy == \"average\":\n",
    "            with open('data/outlayer_maps_vocab_average.pkl', 'wb') as handle:\n",
    "                pickle.dump(self.outlayer_maps_vocab_average, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "            with open('data/out_bias_maps_vocab_average.pkl', 'wb') as handle:\n",
    "                pickle.dump(self.out_bias_maps_vocab_average, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def load_weights(self):\n",
    "\n",
    "        with open('data/words_len_embedding_layer.pkl', 'rb') as f:\n",
    "            self.words_len_embedding_layer = pickle.load(f)\n",
    "\n",
    "        with open('data/words_len_embedding_bias.pkl', 'rb') as f:\n",
    "            self.words_len_embedding_bias = pickle.load(f)\n",
    "\n",
    "        if self.flattening_strategy == \"average\":\n",
    "            \n",
    "            with open('data/outlayer_maps_vocab_average.pkl', 'rb') as f:\n",
    "                self.outlayer_maps_vocab_average = pickle.load(f)\n",
    "    \n",
    "            with open('data/out_bias_maps_vocab_average.pkl', 'rb') as f:\n",
    "                self.out_bias_maps_vocab_average = pickle.load(f)\n",
    "            \n",
    "            \n",
    "        if self.flattening_strategy == \"concatenate\":\n",
    "            with open('data/outlayer_maps_vocab_concat.pkl', 'rb') as f:\n",
    "                self.outlayer_maps_vocab_concat = pickle.load(f)\n",
    "    \n",
    "            with open('data/out_bias_maps_vocab_concat.pkl', 'rb') as f:\n",
    "                self.out_bias_maps_vocab_concat = pickle.load(f)\n",
    "\n",
    "    def calculate_momentum_terms(self,dLoss_dW2,dLoss_db2,dLoss_dW1,dLoss_db1):\n",
    "        # Update biased first moments (momentum)\n",
    "        self.MomDW2 = (self.momentum_beta * self.MomDW2 + (1 - self.momentum_beta) * dLoss_dW2)\n",
    "        self.Momb2 = (self.momentum_beta * self.Momb2 + (1 - self.momentum_beta) * dLoss_db2)\n",
    "        self.MomDW1 = (self.momentum_beta * self.MomDW1 + (1 - self.momentum_beta) * dLoss_dW1)\n",
    "        self.Momb1 = (self.momentum_beta * self.Momb1 + (1 - self.momentum_beta) * dLoss_db1)\n",
    "        self.MomDW2_corrected = self.MomDW2 / (1 - self.momentum_beta ** self.iterations)\n",
    "        self.Momb2_corrected = self.Momb2 / (1 - self.momentum_beta ** self.iterations)\n",
    "        self.MomDW1_corrected = self.MomDW1 / (1 - self.momentum_beta ** self.iterations)\n",
    "        self.Momb1_corrected = self.Momb1 / (1 - self.momentum_beta ** self.iterations)\n",
    "\n",
    "    def calculate_rmsprom_terms(self,dLoss_dW2,dLoss_db2,dLoss_dW1,dLoss_db1):\n",
    "        # Update biased second moments (variance)\n",
    "        self.RMSPDW2 = (self.rmsprop_beta * self.RMSPDW2 + (1 - self.rmsprop_beta) * dLoss_dW2 * dLoss_dW2)\n",
    "        self.RMSPb2 = (self.rmsprop_beta * self.RMSPb2 + (1 - self.rmsprop_beta) * dLoss_db2 * dLoss_db2)\n",
    "        self.RMSPDW1 = (self.rmsprop_beta * self.RMSPDW1 + (1 - self.rmsprop_beta) * dLoss_dW1 * dLoss_dW1)\n",
    "        self.RMSPb1 = (self.rmsprop_beta * self.RMSPb1 + (1 - self.rmsprop_beta) * dLoss_db1 * dLoss_db1)\n",
    "        # Bias correction for second moments\n",
    "        self.RMSPDW2_corrected = self.RMSPDW2 / (1 - self.rmsprop_beta ** self.iterations)\n",
    "        self.RMSPb2_corrected = self.RMSPb2 / (1 - self.rmsprop_beta ** self.iterations)\n",
    "        self.RMSPDW1_corrected = self.RMSPDW1 / (1 - self.rmsprop_beta ** self.iterations)\n",
    "        self.RMSPb1_corrected = self.RMSPb1 / (1 - self.rmsprop_beta ** self.iterations)\n",
    "\n",
    "     \n",
    "    def update_params(self,dLoss_dW2,dLoss_db2,dLoss_dW1,dLoss_db1):\n",
    "         \n",
    "        self.iterations += 1\n",
    "        \n",
    "        # # Linear learning rate decay\n",
    "        if self.iterations>150000:\n",
    "            self.learning_rate=0.05\n",
    "        if self.iterations>500000:\n",
    "            self.learning_rate=0.01\n",
    "        if self.iterations>750000:\n",
    "            self.learning_rate=0.005\n",
    "        if self.iterations>1000000:\n",
    "            self.learning_rate=0.001\n",
    "        if self.iterations>1250000:\n",
    "            self.learning_rate=0.0001    \n",
    "        # lr = self.learning_rate * (1 - self.iterations / self.total_iterations)\n",
    "        # self.learning_rate = max(lr, 0.001)  # Prevent learning rate from becoming too small\n",
    "        \n",
    "        \n",
    "        if self.optimizer==\"adam\":\n",
    "            \n",
    "            self.calculate_momentum_terms(dLoss_dW2,dLoss_db2,dLoss_dW1,dLoss_db1)\n",
    "            self.calculate_rmsprom_terms(dLoss_dW2,dLoss_db2,dLoss_dW1,dLoss_db1) \n",
    "\n",
    "            \n",
    "            # Update parameters using bias-corrected moments\n",
    "            \n",
    "            self.words_len_embedding_layer -= self.learning_rate * self.MomDW1_corrected / (cp.sqrt(self.RMSPDW1_corrected) + self.epsilon)\n",
    "            self.words_len_embedding_bias -= self.learning_rate * self.Momb1_corrected / (cp.sqrt(self.RMSPb1_corrected) + self.epsilon)\n",
    "            \n",
    "            if self.flattening_strategy == \"concatenate\":\n",
    "                self.outlayer_maps_vocab_concat -= self.learning_rate * self.MomDW2_corrected / (cp.sqrt(self.RMSPDW2_corrected) + self.epsilon)\n",
    "                self.out_bias_maps_vocab_concat -= (self.learning_rate * self.Momb2_corrected / (cp.sqrt(self.RMSPb2_corrected) + self.epsilon)).reshape(dLoss_db2.shape[0])\n",
    "                \n",
    "            if self.flattening_strategy == \"average\":\n",
    "                self.outlayer_maps_vocab_average -= self.learning_rate * self.MomDW2_corrected / (cp.sqrt(self.RMSPDW2_corrected) + self.epsilon)\n",
    "                self.out_bias_maps_vocab_average -= (self.learning_rate * self.Momb2_corrected / (cp.sqrt(self.RMSPb2_corrected) + self.epsilon)).reshape(dLoss_db2.shape[0])\n",
    "            \n",
    "        if self.optimizer==\"momentum\": \n",
    "            \n",
    "            self.calculate_momentum_terms(dLoss_dW2,dLoss_db2,dLoss_dW1,dLoss_db1)\n",
    "\n",
    "            self.words_len_embedding_layer -= self.learning_rate * self.MomDW1_corrected  \n",
    "            self.words_len_embedding_bias -= self.learning_rate * self.Momb1_corrected\n",
    "\n",
    "            \n",
    "            if self.flattening_strategy == \"concatenate\":\n",
    "                self.outlayer_maps_vocab_concat -= self.learning_rate * self.MomDW2_corrected  \n",
    "                self.out_bias_maps_vocab_concat -= self.learning_rate * self.Momb2_corrected \n",
    "                \n",
    "            if self.flattening_strategy == \"average\":\n",
    "                self.outlayer_maps_vocab_average -= self.learning_rate * self.MomDW2_corrected  \n",
    "                self.out_bias_maps_vocab_average -= self.learning_rate * self.Momb2_corrected  \n",
    "                \n",
    "                \n",
    "            \n",
    "        if self.optimizer==None: \n",
    "              \n",
    "            self.words_len_embedding_layer -= self.learning_rate * dLoss_dW1  \n",
    "            self.words_len_embedding_bias -= self.learning_rate * dLoss_db1  \n",
    "            \n",
    "            if self.flattening_strategy == \"concatenate\":\n",
    "                self.outlayer_maps_vocab_concat -= self.learning_rate * dLoss_dW2\n",
    "                self.out_bias_maps_vocab_concat -= self.learning_rate * dLoss_db2.reshape(dLoss_db2.shape[0])\n",
    "                \n",
    "            if self.flattening_strategy == \"average\":\n",
    "                self.outlayer_maps_vocab_average -= self.learning_rate * dLoss_dW2 \n",
    "                self.out_bias_maps_vocab_average -= self.learning_rate * dLoss_db2.reshape(dLoss_db2.shape[0])\n",
    "         \n",
    "\n",
    " \n",
    "    @log_time\n",
    "    def trianiteration(self,training_samples):\n",
    "        for sample in training_samples:\n",
    "            \n",
    "            Xi = cp.array(sample[0]) \n",
    "            \n",
    "            yi = cp.array(sample[1])\n",
    "            \n",
    "            sigmaz_out_one, sigma_zout_two = self.forward(Xi)\n",
    "            \n",
    "            Loss = self.cross_entropy_loss(sigma_zout_two, yi)\n",
    "            \n",
    "            self.total_loss += Loss\n",
    "            \n",
    "            dLoss_dZout = sigma_zout_two - yi\n",
    "    \n",
    "            self.backpropagation(dLoss_dZout, sigmaz_out_one, sigma_zout_two, Xi)\n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        sigma_zout_one = self.softmax(\n",
    "            cp.matmul(cp.array(X), self.words_len_embedding_layer) + self.words_len_embedding_bias)\n",
    "        return sigma_zout_one\n",
    "\n",
    "    def train(self, X_train):\n",
    "        \n",
    "        # if self.validation_split > 0:\n",
    "        #     X_train, X_train_validation, y_train, y_train_validation = train_test_split(\n",
    "        #         X_train, y_train, test_size=self.validation_split, random_state=42)\n",
    "        #     print(\"Training starting:\")\n",
    "        #     print(\"X_train samples: \", X_train.shape[0])\n",
    "        #     print(\"X_train_validation samples\", X_train_validation.shape[0], \"\\n\")\n",
    "\n",
    "        \n",
    "        for trainingepoch in range(50):\n",
    "            self.iterations=0\n",
    "            print(\"Epoch: \",trainingepoch)\n",
    "            for i in range(len(X_train)):\n",
    "                gen = self.inputs_window_words(X_train[i], self.vocabulary)\n",
    "                for training_samples in gen:\n",
    "\n",
    "                    print(f\"training samples from row {i}: {len(training_samples)},iterations {self.iterations},lr {self.learning_rate}\")\n",
    "\n",
    "                    self.trianiteration(training_samples)\n",
    "                        \n",
    "                        \n",
    "                    print(f\"Loss sample: {self.total_loss / len(training_samples)}\")\n",
    "                    self.total_loss = 0\n",
    "            self.save_checkpoint()\n",
    "\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3761a96-8533-4ebe-a11a-642cb3c898a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'complete_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcomplete_text\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'complete_text' is not defined"
     ]
    }
   ],
   "source": [
    "complete_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da7e056f-6ee1-43e4-a7ad-3a47c3fe94f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  24040\n",
      "Epoch:  0\n",
      "training samples from row 0: 140,iterations 0,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.5048 seconds\n",
      "Loss sample: 10.132353232091553\n",
      "training samples from row 1: 140,iterations 140,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3785 seconds\n",
      "Loss sample: 10.105676746761674\n",
      "training samples from row 2: 140,iterations 280,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3862 seconds\n",
      "Loss sample: 10.149971715417308\n",
      "training samples from row 3: 140,iterations 420,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3725 seconds\n",
      "Loss sample: 10.112176459786149\n",
      "training samples from row 4: 140,iterations 560,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3652 seconds\n",
      "Loss sample: 10.139584025734402\n",
      "training samples from row 5: 140,iterations 700,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3717 seconds\n",
      "Loss sample: 10.104262737717214\n",
      "training samples from row 6: 140,iterations 840,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3892 seconds\n",
      "Loss sample: 10.150759986659567\n",
      "training samples from row 7: 140,iterations 980,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3812 seconds\n",
      "Loss sample: 10.13236530567916\n",
      "training samples from row 8: 140,iterations 1120,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3666 seconds\n",
      "Loss sample: 10.14361276211026\n",
      "training samples from row 9: 140,iterations 1260,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3681 seconds\n",
      "Loss sample: 10.168208873300948\n",
      "training samples from row 10: 140,iterations 1400,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3815 seconds\n",
      "Loss sample: 10.078232070686743\n",
      "training samples from row 11: 140,iterations 1540,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3786 seconds\n",
      "Loss sample: 10.116728957357925\n",
      "training samples from row 12: 140,iterations 1680,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3755 seconds\n",
      "Loss sample: 10.084659202676505\n",
      "training samples from row 13: 140,iterations 1820,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3689 seconds\n",
      "Loss sample: 10.140578736674257\n",
      "training samples from row 14: 140,iterations 1960,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3899 seconds\n",
      "Loss sample: 10.045352962139706\n",
      "training samples from row 15: 140,iterations 2100,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3786 seconds\n",
      "Loss sample: 10.0972965080078\n",
      "training samples from row 16: 140,iterations 2240,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3866 seconds\n",
      "Loss sample: 10.070140679705323\n",
      "training samples from row 17: 140,iterations 2380,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3800 seconds\n",
      "Loss sample: 10.1222603516877\n",
      "training samples from row 18: 140,iterations 2520,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3718 seconds\n",
      "Loss sample: 10.074908586068958\n",
      "training samples from row 19: 140,iterations 2660,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3732 seconds\n",
      "Loss sample: 10.106816261107646\n",
      "training samples from row 20: 140,iterations 2800,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.4011 seconds\n",
      "Loss sample: 10.059989701987464\n",
      "training samples from row 21: 140,iterations 2940,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3832 seconds\n",
      "Loss sample: 10.15073526650303\n",
      "training samples from row 22: 140,iterations 3080,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3731 seconds\n",
      "Loss sample: 10.167357154215049\n",
      "training samples from row 23: 140,iterations 3220,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3787 seconds\n",
      "Loss sample: 10.150980440034541\n",
      "training samples from row 24: 140,iterations 3360,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3850 seconds\n",
      "Loss sample: 10.150715781464378\n",
      "training samples from row 25: 140,iterations 3500,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3928 seconds\n",
      "Loss sample: 10.07383045117328\n",
      "training samples from row 26: 140,iterations 3640,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3958 seconds\n",
      "Loss sample: 10.060966984157591\n",
      "training samples from row 27: 140,iterations 3780,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3725 seconds\n",
      "Loss sample: 10.077306147338737\n",
      "training samples from row 28: 140,iterations 3920,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3662 seconds\n",
      "Loss sample: 10.085265023536708\n",
      "training samples from row 29: 140,iterations 4060,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3735 seconds\n",
      "Loss sample: 10.111732593916818\n",
      "training samples from row 30: 140,iterations 4200,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3701 seconds\n",
      "Loss sample: 10.080374261648931\n",
      "training samples from row 31: 140,iterations 4340,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3830 seconds\n",
      "Loss sample: 10.093328620536647\n",
      "training samples from row 32: 140,iterations 4480,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3790 seconds\n",
      "Loss sample: 10.160813221492337\n",
      "training samples from row 33: 140,iterations 4620,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3811 seconds\n",
      "Loss sample: 10.06315235844401\n",
      "training samples from row 34: 140,iterations 4760,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3931 seconds\n",
      "Loss sample: 10.109263110407765\n",
      "training samples from row 35: 140,iterations 4900,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3795 seconds\n",
      "Loss sample: 10.103819498176417\n",
      "training samples from row 36: 140,iterations 5040,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3715 seconds\n",
      "Loss sample: 10.122894566865366\n",
      "training samples from row 37: 140,iterations 5180,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3750 seconds\n",
      "Loss sample: 10.139675976640179\n",
      "training samples from row 38: 140,iterations 5320,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3752 seconds\n",
      "Loss sample: 10.057671729739933\n",
      "training samples from row 39: 140,iterations 5460,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3907 seconds\n",
      "Loss sample: 10.119523657669292\n",
      "training samples from row 40: 140,iterations 5600,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3750 seconds\n",
      "Loss sample: 10.056614601231418\n",
      "training samples from row 41: 140,iterations 5740,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3749 seconds\n",
      "Loss sample: 10.115064146806594\n",
      "training samples from row 42: 140,iterations 5880,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3751 seconds\n",
      "Loss sample: 10.078414023745148\n",
      "training samples from row 43: 140,iterations 6020,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3761 seconds\n",
      "Loss sample: 10.097893169865241\n",
      "training samples from row 44: 140,iterations 6160,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3912 seconds\n",
      "Loss sample: 10.165191107558423\n",
      "training samples from row 45: 140,iterations 6300,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3767 seconds\n",
      "Loss sample: 10.09732520938584\n",
      "training samples from row 46: 140,iterations 6440,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3755 seconds\n",
      "Loss sample: 10.141412339531362\n",
      "training samples from row 47: 140,iterations 6580,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3845 seconds\n",
      "Loss sample: 10.0636176460179\n",
      "training samples from row 48: 140,iterations 6720,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3751 seconds\n",
      "Loss sample: 10.105127991751864\n",
      "training samples from row 49: 140,iterations 6860,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3856 seconds\n",
      "Loss sample: 10.096029688987727\n",
      "training samples from row 50: 140,iterations 7000,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3760 seconds\n",
      "Loss sample: 10.068414509627823\n",
      "training samples from row 51: 140,iterations 7140,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3906 seconds\n",
      "Loss sample: 10.089713057419567\n",
      "training samples from row 52: 140,iterations 7280,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3771 seconds\n",
      "Loss sample: 10.048603246210208\n",
      "training samples from row 53: 140,iterations 7420,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3736 seconds\n",
      "Loss sample: 10.088718428946773\n",
      "training samples from row 54: 140,iterations 7560,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3779 seconds\n",
      "Loss sample: 10.155939119965463\n",
      "training samples from row 55: 140,iterations 7700,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3783 seconds\n",
      "Loss sample: 10.091937106345757\n",
      "training samples from row 56: 140,iterations 7840,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3746 seconds\n",
      "Loss sample: 10.10019392296614\n",
      "training samples from row 57: 140,iterations 7980,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3749 seconds\n",
      "Loss sample: 10.119593311253267\n",
      "training samples from row 58: 140,iterations 8120,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3987 seconds\n",
      "Loss sample: 10.155082693511488\n",
      "training samples from row 59: 140,iterations 8260,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3693 seconds\n",
      "Loss sample: 10.131070977485544\n",
      "training samples from row 60: 140,iterations 8400,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3726 seconds\n",
      "Loss sample: 10.112387003203143\n",
      "training samples from row 61: 140,iterations 8540,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3819 seconds\n",
      "Loss sample: 10.11394882345186\n",
      "training samples from row 62: 140,iterations 8680,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3782 seconds\n",
      "Loss sample: 10.082156556387938\n",
      "training samples from row 63: 140,iterations 8820,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3804 seconds\n",
      "Loss sample: 10.091355734380887\n",
      "training samples from row 64: 140,iterations 8960,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3867 seconds\n",
      "Loss sample: 10.03742431299199\n",
      "training samples from row 65: 140,iterations 9100,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3787 seconds\n",
      "Loss sample: 10.145807403668705\n",
      "training samples from row 66: 140,iterations 9240,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3890 seconds\n",
      "Loss sample: 10.098884012285529\n",
      "training samples from row 67: 140,iterations 9380,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3793 seconds\n",
      "Loss sample: 10.13845490525702\n",
      "training samples from row 68: 140,iterations 9520,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3694 seconds\n",
      "Loss sample: 10.099014444577454\n",
      "training samples from row 69: 140,iterations 9660,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3865 seconds\n",
      "Loss sample: 10.127428869474924\n",
      "training samples from row 70: 140,iterations 9800,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3790 seconds\n",
      "Loss sample: 10.074486115640044\n",
      "training samples from row 71: 140,iterations 9940,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3596 seconds\n",
      "Loss sample: 10.068631227505309\n",
      "training samples from row 72: 140,iterations 10080,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3675 seconds\n",
      "Loss sample: 10.07255308820313\n",
      "training samples from row 73: 140,iterations 10220,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3621 seconds\n",
      "Loss sample: 10.071994070920175\n",
      "training samples from row 74: 140,iterations 10360,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3850 seconds\n",
      "Loss sample: 10.067498312157948\n",
      "training samples from row 75: 140,iterations 10500,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3700 seconds\n",
      "Loss sample: 10.163160124931201\n",
      "training samples from row 76: 140,iterations 10640,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3861 seconds\n",
      "Loss sample: 10.149699828436564\n",
      "training samples from row 77: 140,iterations 10780,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3979 seconds\n",
      "Loss sample: 10.11598364744076\n",
      "training samples from row 78: 140,iterations 10920,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3939 seconds\n",
      "Loss sample: 10.155228029657174\n",
      "training samples from row 79: 140,iterations 11060,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3751 seconds\n",
      "Loss sample: 10.129340224713742\n",
      "training samples from row 80: 140,iterations 11200,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3841 seconds\n",
      "Loss sample: 10.132279239384397\n",
      "training samples from row 81: 140,iterations 11340,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3838 seconds\n",
      "Loss sample: 10.064657959840302\n",
      "training samples from row 82: 140,iterations 11480,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3928 seconds\n",
      "Loss sample: 10.158965247668585\n",
      "training samples from row 83: 140,iterations 11620,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.4073 seconds\n",
      "Loss sample: 10.109176485500258\n",
      "training samples from row 84: 140,iterations 11760,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3859 seconds\n",
      "Loss sample: 10.08133074890092\n",
      "training samples from row 85: 140,iterations 11900,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.4020 seconds\n",
      "Loss sample: 10.129121938532357\n",
      "training samples from row 86: 140,iterations 12040,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.4003 seconds\n",
      "Loss sample: 10.089599819632225\n",
      "training samples from row 87: 140,iterations 12180,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3798 seconds\n",
      "Loss sample: 10.090052398128742\n",
      "training samples from row 88: 140,iterations 12320,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3761 seconds\n",
      "Loss sample: 10.058985688944137\n",
      "training samples from row 89: 140,iterations 12460,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3803 seconds\n",
      "Loss sample: 10.0347286487653\n",
      "training samples from row 90: 140,iterations 12600,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3776 seconds\n",
      "Loss sample: 10.146988663743674\n",
      "training samples from row 91: 140,iterations 12740,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3759 seconds\n",
      "Loss sample: 9.998715048236305\n",
      "training samples from row 92: 140,iterations 12880,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3766 seconds\n",
      "Loss sample: 10.13038231305901\n",
      "training samples from row 93: 140,iterations 13020,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3788 seconds\n",
      "Loss sample: 10.119848836405547\n",
      "training samples from row 94: 140,iterations 13160,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3770 seconds\n",
      "Loss sample: 10.15897963668747\n",
      "training samples from row 95: 140,iterations 13300,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3933 seconds\n",
      "Loss sample: 9.950930882782185\n",
      "training samples from row 96: 140,iterations 13440,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3774 seconds\n",
      "Loss sample: 10.05127662918496\n",
      "training samples from row 97: 140,iterations 13580,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3762 seconds\n",
      "Loss sample: 10.068987717657862\n",
      "training samples from row 98: 140,iterations 13720,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3905 seconds\n",
      "Loss sample: 10.034195404317243\n",
      "training samples from row 99: 140,iterations 13860,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3702 seconds\n",
      "Loss sample: 10.067220841371116\n",
      "training samples from row 100: 140,iterations 14000,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3750 seconds\n",
      "Loss sample: 10.053312749722746\n",
      "training samples from row 101: 140,iterations 14140,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3762 seconds\n",
      "Loss sample: 10.121835044871077\n",
      "training samples from row 102: 140,iterations 14280,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3594 seconds\n",
      "Loss sample: 10.10432655131678\n",
      "training samples from row 103: 140,iterations 14420,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3906 seconds\n",
      "Loss sample: 10.148036595673195\n",
      "training samples from row 104: 140,iterations 14560,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3769 seconds\n",
      "Loss sample: 10.098847925152452\n",
      "training samples from row 105: 140,iterations 14700,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3907 seconds\n",
      "Loss sample: 10.048292922991518\n",
      "training samples from row 106: 140,iterations 14840,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3749 seconds\n",
      "Loss sample: 9.943151495814247\n",
      "training samples from row 107: 140,iterations 14980,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3910 seconds\n",
      "Loss sample: 10.052996660087338\n",
      "training samples from row 108: 140,iterations 15120,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3907 seconds\n",
      "Loss sample: 10.13701014860028\n",
      "training samples from row 109: 140,iterations 15260,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3920 seconds\n",
      "Loss sample: 10.176314648663586\n",
      "training samples from row 110: 140,iterations 15400,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3906 seconds\n",
      "Loss sample: 10.1043078269336\n",
      "training samples from row 111: 140,iterations 15540,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3749 seconds\n",
      "Loss sample: 10.114363911712104\n",
      "training samples from row 112: 140,iterations 15680,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3749 seconds\n",
      "Loss sample: 10.029560388023878\n",
      "training samples from row 113: 140,iterations 15820,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3749 seconds\n",
      "Loss sample: 10.10962370745944\n",
      "training samples from row 114: 140,iterations 15960,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3770 seconds\n",
      "Loss sample: 9.897979209202111\n",
      "training samples from row 115: 140,iterations 16100,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3906 seconds\n",
      "Loss sample: 10.106994298389465\n",
      "training samples from row 116: 140,iterations 16240,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3758 seconds\n",
      "Loss sample: 9.944670696231125\n",
      "training samples from row 117: 140,iterations 16380,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3751 seconds\n",
      "Loss sample: 10.129532280014446\n",
      "training samples from row 118: 140,iterations 16520,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3749 seconds\n",
      "Loss sample: 10.109625308090214\n",
      "training samples from row 119: 140,iterations 16660,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3765 seconds\n",
      "Loss sample: 10.145682320040585\n",
      "training samples from row 120: 140,iterations 16800,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3906 seconds\n",
      "Loss sample: 9.930950637623337\n",
      "training samples from row 121: 140,iterations 16940,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3763 seconds\n",
      "Loss sample: 9.885606312581531\n",
      "training samples from row 122: 140,iterations 17080,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3749 seconds\n",
      "Loss sample: 10.090604288340453\n",
      "training samples from row 123: 140,iterations 17220,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3911 seconds\n",
      "Loss sample: 10.072487946426605\n",
      "training samples from row 124: 140,iterations 17360,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.4068 seconds\n",
      "Loss sample: 10.055067554987428\n",
      "training samples from row 125: 140,iterations 17500,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3762 seconds\n",
      "Loss sample: 10.123809019019145\n",
      "training samples from row 126: 140,iterations 17640,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3769 seconds\n",
      "Loss sample: 10.101718575913242\n",
      "training samples from row 127: 140,iterations 17780,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3755 seconds\n",
      "Loss sample: 9.89491279639083\n",
      "training samples from row 128: 140,iterations 17920,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3767 seconds\n",
      "Loss sample: 10.067381350317897\n",
      "training samples from row 129: 140,iterations 18060,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3750 seconds\n",
      "Loss sample: 10.075490604823168\n",
      "training samples from row 130: 140,iterations 18200,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3750 seconds\n",
      "Loss sample: 10.105048797742677\n",
      "training samples from row 131: 140,iterations 18340,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3758 seconds\n",
      "Loss sample: 10.08856722568771\n",
      "training samples from row 132: 140,iterations 18480,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3906 seconds\n",
      "Loss sample: 10.09857194233422\n",
      "training samples from row 133: 140,iterations 18620,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3757 seconds\n",
      "Loss sample: 10.093941081122177\n",
      "training samples from row 134: 140,iterations 18760,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3749 seconds\n",
      "Loss sample: 10.105895811019627\n",
      "training samples from row 135: 140,iterations 18900,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3750 seconds\n",
      "Loss sample: 10.15303495025879\n",
      "training samples from row 136: 140,iterations 19040,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3764 seconds\n",
      "Loss sample: 9.897469855094087\n",
      "training samples from row 137: 140,iterations 19180,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3905 seconds\n",
      "Loss sample: 10.093034309002775\n",
      "training samples from row 138: 140,iterations 19320,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3750 seconds\n",
      "Loss sample: 10.144088293109828\n",
      "training samples from row 139: 140,iterations 19460,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3607 seconds\n",
      "Loss sample: 10.024673265496455\n",
      "training samples from row 140: 140,iterations 19600,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3905 seconds\n",
      "Loss sample: 10.093144227623707\n",
      "training samples from row 141: 140,iterations 19740,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3906 seconds\n",
      "Loss sample: 10.090526966789884\n",
      "training samples from row 142: 140,iterations 19880,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3751 seconds\n",
      "Loss sample: 10.088118923780335\n",
      "training samples from row 143: 140,iterations 20020,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3750 seconds\n",
      "Loss sample: 10.104749051080015\n",
      "training samples from row 144: 140,iterations 20160,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3767 seconds\n",
      "Loss sample: 10.084737009425822\n",
      "training samples from row 145: 140,iterations 20300,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3906 seconds\n",
      "Loss sample: 10.082108423813722\n",
      "training samples from row 146: 140,iterations 20440,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3749 seconds\n",
      "Loss sample: 10.093391407147543\n",
      "training samples from row 147: 140,iterations 20580,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3765 seconds\n",
      "Loss sample: 10.03477259439126\n",
      "training samples from row 148: 140,iterations 20720,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3796 seconds\n",
      "Loss sample: 10.027596108002767\n",
      "training samples from row 149: 140,iterations 20860,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3795 seconds\n",
      "Loss sample: 10.069402067954844\n",
      "training samples from row 150: 140,iterations 21000,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3789 seconds\n",
      "Loss sample: 10.100234465675262\n",
      "training samples from row 151: 140,iterations 21140,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3767 seconds\n",
      "Loss sample: 10.156313077724548\n",
      "training samples from row 152: 140,iterations 21280,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3593 seconds\n",
      "Loss sample: 10.037962066056126\n",
      "training samples from row 153: 140,iterations 21420,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3910 seconds\n",
      "Loss sample: 10.085698626850577\n",
      "training samples from row 154: 140,iterations 21560,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3876 seconds\n",
      "Loss sample: 10.06860388534234\n",
      "training samples from row 155: 140,iterations 21700,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3905 seconds\n",
      "Loss sample: 10.119715574629316\n",
      "training samples from row 156: 140,iterations 21840,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3769 seconds\n",
      "Loss sample: 10.111590373165553\n",
      "training samples from row 157: 140,iterations 21980,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3749 seconds\n",
      "Loss sample: 10.071922018912405\n",
      "training samples from row 158: 140,iterations 22120,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3928 seconds\n",
      "Loss sample: 10.084080024967417\n",
      "training samples from row 159: 140,iterations 22260,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3906 seconds\n",
      "Loss sample: 10.143212652541525\n",
      "training samples from row 160: 140,iterations 22400,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3750 seconds\n",
      "Loss sample: 10.085788416392111\n",
      "training samples from row 161: 140,iterations 22540,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3767 seconds\n",
      "Loss sample: 10.094484265409085\n",
      "training samples from row 162: 140,iterations 22680,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3749 seconds\n",
      "Loss sample: 10.076546846935004\n",
      "training samples from row 163: 140,iterations 22820,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3938 seconds\n",
      "Loss sample: 9.94369336448854\n",
      "training samples from row 164: 140,iterations 22960,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3755 seconds\n",
      "Loss sample: 10.08472366306579\n",
      "training samples from row 165: 140,iterations 23100,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3925 seconds\n",
      "Loss sample: 10.105005069789529\n",
      "training samples from row 166: 140,iterations 23240,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3770 seconds\n",
      "Loss sample: 10.03601626152277\n",
      "training samples from row 167: 140,iterations 23380,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3787 seconds\n",
      "Loss sample: 10.14696391934792\n",
      "training samples from row 168: 140,iterations 23520,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3786 seconds\n",
      "Loss sample: 10.052575409602875\n",
      "training samples from row 169: 140,iterations 23660,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3687 seconds\n",
      "Loss sample: 10.07026348438066\n",
      "training samples from row 170: 140,iterations 23800,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3942 seconds\n",
      "Loss sample: 10.06163267236194\n",
      "training samples from row 171: 140,iterations 23940,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3938 seconds\n",
      "Loss sample: 10.090455782627787\n",
      "training samples from row 172: 140,iterations 24080,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3779 seconds\n",
      "Loss sample: 10.101851722231796\n",
      "training samples from row 173: 140,iterations 24220,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3923 seconds\n",
      "Loss sample: 10.035406893635198\n",
      "training samples from row 174: 140,iterations 24360,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3779 seconds\n",
      "Loss sample: 10.12194587203161\n",
      "training samples from row 175: 140,iterations 24500,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3761 seconds\n",
      "Loss sample: 10.072043191861775\n",
      "training samples from row 176: 140,iterations 24640,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3936 seconds\n",
      "Loss sample: 10.11962724669948\n",
      "training samples from row 177: 140,iterations 24780,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3754 seconds\n",
      "Loss sample: 9.982853026006792\n",
      "training samples from row 178: 140,iterations 24920,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3915 seconds\n",
      "Loss sample: 10.144291003661417\n",
      "training samples from row 179: 140,iterations 25060,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3778 seconds\n",
      "Loss sample: 10.082490500190946\n",
      "training samples from row 180: 140,iterations 25200,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3629 seconds\n",
      "Loss sample: 10.1220708907939\n",
      "training samples from row 181: 140,iterations 25340,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3925 seconds\n",
      "Loss sample: 10.177322946224601\n",
      "training samples from row 182: 140,iterations 25480,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3764 seconds\n",
      "Loss sample: 9.895949925417188\n",
      "training samples from row 183: 140,iterations 25620,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3762 seconds\n",
      "Loss sample: 10.049205970414457\n",
      "training samples from row 184: 140,iterations 25760,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3926 seconds\n",
      "Loss sample: 10.063816655071815\n",
      "training samples from row 185: 140,iterations 25900,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3910 seconds\n",
      "Loss sample: 9.837081161933423\n",
      "training samples from row 186: 140,iterations 26040,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3927 seconds\n",
      "Loss sample: 10.094858869610599\n",
      "training samples from row 187: 140,iterations 26180,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3771 seconds\n",
      "Loss sample: 9.746568530465728\n",
      "training samples from row 188: 140,iterations 26320,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3748 seconds\n",
      "Loss sample: 10.077025271089784\n",
      "training samples from row 189: 140,iterations 26460,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3793 seconds\n",
      "Loss sample: 10.095763170752763\n",
      "training samples from row 190: 140,iterations 26600,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3755 seconds\n",
      "Loss sample: 9.937474734958313\n",
      "training samples from row 191: 140,iterations 26740,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3932 seconds\n",
      "Loss sample: 10.09651545058926\n",
      "training samples from row 192: 140,iterations 26880,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3912 seconds\n",
      "Loss sample: 10.023614619970965\n",
      "training samples from row 193: 140,iterations 27020,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3777 seconds\n",
      "Loss sample: 10.131645738593408\n",
      "training samples from row 194: 140,iterations 27160,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3927 seconds\n",
      "Loss sample: 10.126408119428126\n",
      "training samples from row 195: 140,iterations 27300,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3771 seconds\n",
      "Loss sample: 10.135608862309585\n",
      "training samples from row 196: 140,iterations 27440,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3927 seconds\n",
      "Loss sample: 10.043140324990219\n",
      "training samples from row 197: 140,iterations 27580,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3767 seconds\n",
      "Loss sample: 9.73793531488829\n",
      "training samples from row 198: 140,iterations 27720,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3917 seconds\n",
      "Loss sample: 10.119586664391017\n",
      "training samples from row 199: 140,iterations 27860,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3844 seconds\n",
      "Loss sample: 10.162391452897428\n",
      "training samples from row 200: 140,iterations 28000,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3756 seconds\n",
      "Loss sample: 9.867492194483303\n",
      "training samples from row 201: 140,iterations 28140,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3788 seconds\n",
      "Loss sample: 10.031732129578847\n",
      "training samples from row 202: 140,iterations 28280,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3759 seconds\n",
      "Loss sample: 10.125500578007662\n",
      "training samples from row 203: 140,iterations 28420,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3913 seconds\n",
      "Loss sample: 10.142454103998631\n",
      "training samples from row 204: 140,iterations 28560,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3803 seconds\n",
      "Loss sample: 10.029128225290883\n",
      "training samples from row 205: 140,iterations 28700,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3781 seconds\n",
      "Loss sample: 10.100228055173947\n",
      "training samples from row 206: 140,iterations 28840,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3801 seconds\n",
      "Loss sample: 10.097040020262709\n",
      "training samples from row 207: 140,iterations 28980,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3914 seconds\n",
      "Loss sample: 9.955760712257165\n",
      "training samples from row 208: 140,iterations 29120,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3938 seconds\n",
      "Loss sample: 9.99230455213197\n",
      "training samples from row 209: 140,iterations 29260,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3765 seconds\n",
      "Loss sample: 10.059458520965746\n",
      "training samples from row 210: 140,iterations 29400,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3746 seconds\n",
      "Loss sample: 9.973846619038756\n",
      "training samples from row 211: 140,iterations 29540,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3921 seconds\n",
      "Loss sample: 10.101404701991871\n",
      "training samples from row 212: 140,iterations 29680,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3929 seconds\n",
      "Loss sample: 9.815048812683793\n",
      "training samples from row 213: 140,iterations 29820,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3773 seconds\n",
      "Loss sample: 10.047176984632118\n",
      "training samples from row 214: 140,iterations 29960,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3829 seconds\n",
      "Loss sample: 10.089891648605118\n",
      "training samples from row 215: 140,iterations 30100,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3917 seconds\n",
      "Loss sample: 9.875956777867895\n",
      "training samples from row 216: 140,iterations 30240,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3908 seconds\n",
      "Loss sample: 10.07905259861936\n",
      "training samples from row 217: 140,iterations 30380,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3936 seconds\n",
      "Loss sample: 10.012359934045493\n",
      "training samples from row 218: 140,iterations 30520,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3934 seconds\n",
      "Loss sample: 10.082069162997772\n",
      "training samples from row 219: 140,iterations 30660,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3797 seconds\n",
      "Loss sample: 9.87739411363719\n",
      "training samples from row 220: 140,iterations 30800,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3753 seconds\n",
      "Loss sample: 9.988979760530533\n",
      "training samples from row 221: 140,iterations 30940,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3924 seconds\n",
      "Loss sample: 10.136435026858708\n",
      "training samples from row 222: 140,iterations 31080,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3765 seconds\n",
      "Loss sample: 10.06852256904735\n",
      "training samples from row 223: 140,iterations 31220,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3770 seconds\n",
      "Loss sample: 10.056890152127345\n",
      "training samples from row 224: 140,iterations 31360,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3822 seconds\n",
      "Loss sample: 10.107091045317677\n",
      "training samples from row 225: 140,iterations 31500,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3747 seconds\n",
      "Loss sample: 10.086442861777243\n",
      "training samples from row 226: 140,iterations 31640,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3907 seconds\n",
      "Loss sample: 10.125370375719404\n",
      "training samples from row 227: 140,iterations 31780,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3639 seconds\n",
      "Loss sample: 10.066839404487931\n",
      "training samples from row 228: 140,iterations 31920,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3750 seconds\n",
      "Loss sample: 10.132732243153985\n",
      "training samples from row 229: 140,iterations 32060,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3767 seconds\n",
      "Loss sample: 10.06870075687406\n",
      "training samples from row 230: 140,iterations 32200,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3907 seconds\n",
      "Loss sample: 10.057715573453084\n",
      "training samples from row 231: 140,iterations 32340,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3921 seconds\n",
      "Loss sample: 10.062057532837873\n",
      "training samples from row 232: 140,iterations 32480,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3749 seconds\n",
      "Loss sample: 10.087413334071224\n",
      "training samples from row 233: 140,iterations 32620,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3749 seconds\n",
      "Loss sample: 9.921435557403525\n",
      "training samples from row 234: 140,iterations 32760,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3922 seconds\n",
      "Loss sample: 10.13260884267831\n",
      "training samples from row 235: 140,iterations 32900,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3906 seconds\n",
      "Loss sample: 10.065149940963337\n",
      "training samples from row 236: 140,iterations 33040,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3797 seconds\n",
      "Loss sample: 10.096119316849915\n",
      "training samples from row 237: 140,iterations 33180,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3906 seconds\n",
      "Loss sample: 10.054081091751003\n",
      "training samples from row 238: 140,iterations 33320,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3750 seconds\n",
      "Loss sample: 10.07428121998666\n",
      "training samples from row 239: 140,iterations 33460,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3919 seconds\n",
      "Loss sample: 10.093056338536686\n",
      "training samples from row 240: 140,iterations 33600,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3907 seconds\n",
      "Loss sample: 9.883645724833078\n",
      "training samples from row 241: 140,iterations 33740,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3735 seconds\n",
      "Loss sample: 10.082080044499302\n",
      "training samples from row 242: 140,iterations 33880,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3765 seconds\n",
      "Loss sample: 10.064689518916987\n",
      "training samples from row 243: 140,iterations 34020,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3905 seconds\n",
      "Loss sample: 9.857946868969792\n",
      "training samples from row 244: 140,iterations 34160,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3750 seconds\n",
      "Loss sample: 10.077523134069182\n",
      "training samples from row 245: 140,iterations 34300,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3907 seconds\n",
      "Loss sample: 10.070341901340951\n",
      "training samples from row 246: 140,iterations 34440,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.4062 seconds\n",
      "Loss sample: 10.056408854593197\n",
      "training samples from row 247: 140,iterations 34580,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3773 seconds\n",
      "Loss sample: 10.090997823627916\n",
      "training samples from row 248: 140,iterations 34720,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3750 seconds\n",
      "Loss sample: 10.124345929931543\n",
      "training samples from row 249: 140,iterations 34860,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3750 seconds\n",
      "Loss sample: 9.925608420424739\n",
      "training samples from row 250: 140,iterations 35000,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3924 seconds\n",
      "Loss sample: 9.695078182056749\n",
      "training samples from row 251: 140,iterations 35140,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3750 seconds\n",
      "Loss sample: 10.117023291061903\n",
      "training samples from row 252: 140,iterations 35280,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3614 seconds\n",
      "Loss sample: 10.075439109748825\n",
      "training samples from row 253: 140,iterations 35420,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3749 seconds\n",
      "Loss sample: 10.158982343308013\n",
      "training samples from row 254: 140,iterations 35560,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3906 seconds\n",
      "Loss sample: 9.818847471113143\n",
      "training samples from row 255: 140,iterations 35700,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3701 seconds\n",
      "Loss sample: 10.092566036951133\n",
      "training samples from row 256: 140,iterations 35840,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3762 seconds\n",
      "Loss sample: 9.67951397641953\n",
      "training samples from row 257: 140,iterations 35980,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3768 seconds\n",
      "Loss sample: 10.115964114065855\n",
      "training samples from row 258: 140,iterations 36120,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3750 seconds\n",
      "Loss sample: 10.119734435743224\n",
      "training samples from row 259: 140,iterations 36260,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3749 seconds\n",
      "Loss sample: 10.127914035144437\n",
      "training samples from row 260: 140,iterations 36400,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3755 seconds\n",
      "Loss sample: 9.671663025176551\n",
      "training samples from row 261: 140,iterations 36540,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3905 seconds\n",
      "Loss sample: 9.755752782938147\n",
      "training samples from row 262: 140,iterations 36680,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3920 seconds\n",
      "Loss sample: 10.114189980234046\n",
      "training samples from row 263: 140,iterations 36820,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3750 seconds\n",
      "Loss sample: 10.129498832921238\n",
      "training samples from row 264: 140,iterations 36960,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3937 seconds\n",
      "Loss sample: 10.086424651432742\n",
      "training samples from row 265: 140,iterations 37100,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3749 seconds\n",
      "Loss sample: 10.037204092289803\n",
      "training samples from row 266: 140,iterations 37240,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3749 seconds\n",
      "Loss sample: 10.064804755920559\n",
      "training samples from row 267: 140,iterations 37380,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3763 seconds\n",
      "Loss sample: 9.765693666110508\n",
      "training samples from row 268: 140,iterations 37520,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3905 seconds\n",
      "Loss sample: 10.105698176082416\n",
      "training samples from row 269: 140,iterations 37660,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3762 seconds\n",
      "Loss sample: 10.105550447611842\n",
      "training samples from row 270: 140,iterations 37800,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3907 seconds\n",
      "Loss sample: 10.042946214379949\n",
      "training samples from row 271: 140,iterations 37940,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3750 seconds\n",
      "Loss sample: 10.097500307865639\n",
      "training samples from row 272: 140,iterations 38080,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3922 seconds\n",
      "Loss sample: 9.655267337628176\n",
      "training samples from row 273: 140,iterations 38220,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3831 seconds\n",
      "Loss sample: 10.039116793653541\n",
      "training samples from row 274: 140,iterations 38360,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3845 seconds\n",
      "Loss sample: 9.74597860530701\n",
      "training samples from row 275: 140,iterations 38500,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3750 seconds\n",
      "Loss sample: 10.052684783563443\n",
      "training samples from row 276: 140,iterations 38640,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3906 seconds\n",
      "Loss sample: 10.142145227419684\n",
      "training samples from row 277: 140,iterations 38780,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3922 seconds\n",
      "Loss sample: 10.056573861019565\n",
      "training samples from row 278: 140,iterations 38920,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3750 seconds\n",
      "Loss sample: 10.07609276715979\n",
      "training samples from row 279: 140,iterations 39060,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3906 seconds\n",
      "Loss sample: 10.133691036286846\n",
      "training samples from row 280: 140,iterations 39200,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3918 seconds\n",
      "Loss sample: 10.065150982491591\n",
      "training samples from row 281: 140,iterations 39340,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3906 seconds\n",
      "Loss sample: 10.114913774577214\n",
      "training samples from row 282: 140,iterations 39480,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3752 seconds\n",
      "Loss sample: 10.096478858270482\n",
      "training samples from row 283: 140,iterations 39620,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3788 seconds\n",
      "Loss sample: 10.074850520339123\n",
      "training samples from row 284: 140,iterations 39760,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3750 seconds\n",
      "Loss sample: 10.086690554034192\n",
      "training samples from row 285: 140,iterations 39900,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3768 seconds\n",
      "Loss sample: 10.153454243774426\n",
      "training samples from row 286: 140,iterations 40040,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3906 seconds\n",
      "Loss sample: 9.60965951890636\n",
      "training samples from row 287: 140,iterations 40180,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3749 seconds\n",
      "Loss sample: 10.069624282401376\n",
      "training samples from row 288: 140,iterations 40320,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3907 seconds\n",
      "Loss sample: 10.115595835872094\n",
      "training samples from row 289: 140,iterations 40460,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.4065 seconds\n",
      "Loss sample: 10.074055850531728\n",
      "training samples from row 290: 140,iterations 40600,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3927 seconds\n",
      "Loss sample: 10.147683635910823\n",
      "training samples from row 291: 140,iterations 40740,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3764 seconds\n",
      "Loss sample: 10.099766833571271\n",
      "training samples from row 292: 140,iterations 40880,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3854 seconds\n",
      "Loss sample: 10.057140503361246\n",
      "training samples from row 293: 140,iterations 41020,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3912 seconds\n",
      "Loss sample: 9.83086044538642\n",
      "training samples from row 294: 140,iterations 41160,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.4104 seconds\n",
      "Loss sample: 10.064271566340276\n",
      "training samples from row 295: 140,iterations 41300,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3774 seconds\n",
      "Loss sample: 10.13487157844206\n",
      "training samples from row 296: 140,iterations 41440,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3932 seconds\n",
      "Loss sample: 9.823637434719473\n",
      "training samples from row 297: 140,iterations 41580,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3952 seconds\n",
      "Loss sample: 9.549949068076742\n",
      "training samples from row 298: 140,iterations 41720,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3766 seconds\n",
      "Loss sample: 10.037646148056035\n",
      "training samples from row 299: 140,iterations 41860,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.4099 seconds\n",
      "Loss sample: 10.143409534396397\n",
      "training samples from row 300: 140,iterations 42000,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3884 seconds\n",
      "Loss sample: 10.092811384698452\n",
      "training samples from row 301: 140,iterations 42140,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3919 seconds\n",
      "Loss sample: 10.048367758969981\n",
      "training samples from row 302: 140,iterations 42280,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.4102 seconds\n",
      "Loss sample: 9.705031597321295\n",
      "training samples from row 303: 140,iterations 42420,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3910 seconds\n",
      "Loss sample: 9.882198432503714\n",
      "training samples from row 304: 140,iterations 42560,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3924 seconds\n",
      "Loss sample: 9.69169427467787\n",
      "training samples from row 305: 140,iterations 42700,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3782 seconds\n",
      "Loss sample: 10.096548411108074\n",
      "training samples from row 306: 140,iterations 42840,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3751 seconds\n",
      "Loss sample: 10.164373665973553\n",
      "training samples from row 307: 140,iterations 42980,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3945 seconds\n",
      "Loss sample: 9.919974783974025\n",
      "training samples from row 308: 140,iterations 43120,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3779 seconds\n",
      "Loss sample: 10.081878680879477\n",
      "training samples from row 309: 140,iterations 43260,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3758 seconds\n",
      "Loss sample: 10.104606364246983\n",
      "training samples from row 310: 140,iterations 43400,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3894 seconds\n",
      "Loss sample: 9.5128321480383\n",
      "training samples from row 311: 140,iterations 43540,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3913 seconds\n",
      "Loss sample: 9.593690620649163\n",
      "training samples from row 312: 140,iterations 43680,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3927 seconds\n",
      "Loss sample: 9.842280819242859\n",
      "training samples from row 313: 140,iterations 43820,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.4083 seconds\n",
      "Loss sample: 10.130516103798342\n",
      "training samples from row 314: 140,iterations 43960,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3916 seconds\n",
      "Loss sample: 9.663015846781994\n",
      "training samples from row 315: 140,iterations 44100,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3949 seconds\n",
      "Loss sample: 10.031585304303572\n",
      "training samples from row 316: 140,iterations 44240,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3880 seconds\n",
      "Loss sample: 10.073343985936216\n",
      "training samples from row 317: 140,iterations 44380,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3765 seconds\n",
      "Loss sample: 10.06190048505116\n",
      "training samples from row 318: 140,iterations 44520,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3999 seconds\n",
      "Loss sample: 10.023360007942383\n",
      "training samples from row 319: 140,iterations 44660,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3952 seconds\n",
      "Loss sample: 10.112614279098153\n",
      "training samples from row 320: 140,iterations 44800,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3777 seconds\n",
      "Loss sample: 9.408768569275008\n",
      "training samples from row 321: 140,iterations 44940,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3953 seconds\n",
      "Loss sample: 10.060819381614824\n",
      "training samples from row 322: 140,iterations 45080,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3762 seconds\n",
      "Loss sample: 10.059869195939958\n",
      "training samples from row 323: 140,iterations 45220,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3865 seconds\n",
      "Loss sample: 9.098446506463574\n",
      "training samples from row 324: 140,iterations 45360,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3932 seconds\n",
      "Loss sample: 10.117391558501794\n",
      "training samples from row 325: 140,iterations 45500,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3787 seconds\n",
      "Loss sample: 9.952591821939507\n",
      "training samples from row 326: 140,iterations 45640,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3963 seconds\n",
      "Loss sample: 10.106107012326845\n",
      "training samples from row 327: 140,iterations 45780,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3821 seconds\n",
      "Loss sample: 10.05575208154483\n",
      "training samples from row 328: 140,iterations 45920,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3919 seconds\n",
      "Loss sample: 10.075233973627657\n",
      "training samples from row 329: 140,iterations 46060,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3796 seconds\n",
      "Loss sample: 10.018047207362564\n",
      "training samples from row 330: 140,iterations 46200,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3924 seconds\n",
      "Loss sample: 10.051148531552862\n",
      "training samples from row 331: 140,iterations 46340,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3923 seconds\n",
      "Loss sample: 10.092569911016914\n",
      "training samples from row 332: 140,iterations 46480,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3941 seconds\n",
      "Loss sample: 10.032657179156384\n",
      "training samples from row 333: 140,iterations 46620,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3931 seconds\n",
      "Loss sample: 10.098404505058653\n",
      "training samples from row 334: 140,iterations 46760,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3728 seconds\n",
      "Loss sample: 9.925709939338894\n",
      "training samples from row 335: 140,iterations 46900,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3939 seconds\n",
      "Loss sample: 10.024863339638495\n",
      "training samples from row 336: 140,iterations 47040,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3761 seconds\n",
      "Loss sample: 9.800752221875777\n",
      "training samples from row 337: 140,iterations 47180,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3941 seconds\n",
      "Loss sample: 10.1176596717863\n",
      "training samples from row 338: 140,iterations 47320,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3930 seconds\n",
      "Loss sample: 10.048430266811343\n",
      "training samples from row 339: 140,iterations 47460,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3757 seconds\n",
      "Loss sample: 10.0911311536965\n",
      "training samples from row 340: 140,iterations 47600,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3934 seconds\n",
      "Loss sample: 10.069184888756629\n",
      "training samples from row 341: 140,iterations 47740,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3931 seconds\n",
      "Loss sample: 10.136875457606479\n",
      "training samples from row 342: 140,iterations 47880,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3844 seconds\n",
      "Loss sample: 10.083011450731602\n",
      "training samples from row 343: 140,iterations 48020,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3774 seconds\n",
      "Loss sample: 10.051870981186761\n",
      "training samples from row 344: 140,iterations 48160,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.4075 seconds\n",
      "Loss sample: 10.066946781362\n",
      "training samples from row 345: 140,iterations 48300,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3784 seconds\n",
      "Loss sample: 9.76745984086495\n",
      "training samples from row 346: 140,iterations 48440,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3938 seconds\n",
      "Loss sample: 10.132345278069279\n",
      "training samples from row 347: 140,iterations 48580,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.4070 seconds\n",
      "Loss sample: 9.948323525134645\n",
      "training samples from row 348: 140,iterations 48720,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3675 seconds\n",
      "Loss sample: 10.043560010039597\n",
      "training samples from row 349: 140,iterations 48860,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3915 seconds\n",
      "Loss sample: 10.093505533761808\n",
      "training samples from row 350: 140,iterations 49000,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3799 seconds\n",
      "Loss sample: 10.06539937593344\n",
      "training samples from row 351: 140,iterations 49140,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3928 seconds\n",
      "Loss sample: 10.090158082015748\n",
      "training samples from row 352: 140,iterations 49280,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3769 seconds\n",
      "Loss sample: 10.133084265087245\n",
      "training samples from row 353: 140,iterations 49420,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3801 seconds\n",
      "Loss sample: 9.727932687709963\n",
      "training samples from row 354: 140,iterations 49560,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3762 seconds\n",
      "Loss sample: 10.050579556021553\n",
      "training samples from row 355: 140,iterations 49700,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3907 seconds\n",
      "Loss sample: 10.011979408369685\n",
      "training samples from row 356: 140,iterations 49840,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3781 seconds\n",
      "Loss sample: 10.07626435097319\n",
      "training samples from row 357: 140,iterations 49980,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3766 seconds\n",
      "Loss sample: 10.082725505965024\n",
      "training samples from row 358: 140,iterations 50120,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3938 seconds\n",
      "Loss sample: 9.643982237013345\n",
      "training samples from row 359: 140,iterations 50260,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3765 seconds\n",
      "Loss sample: 10.102893645951252\n",
      "training samples from row 360: 140,iterations 50400,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3769 seconds\n",
      "Loss sample: 10.003992465062982\n",
      "training samples from row 361: 140,iterations 50540,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3769 seconds\n",
      "Loss sample: 9.967808820375398\n",
      "training samples from row 362: 140,iterations 50680,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3757 seconds\n",
      "Loss sample: 10.102668095728482\n",
      "training samples from row 363: 140,iterations 50820,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3806 seconds\n",
      "Loss sample: 9.967115234054202\n",
      "training samples from row 364: 140,iterations 50960,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3722 seconds\n",
      "Loss sample: 10.014389282931402\n",
      "training samples from row 365: 140,iterations 51100,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3789 seconds\n",
      "Loss sample: 10.036405551915745\n",
      "training samples from row 366: 140,iterations 51240,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3780 seconds\n",
      "Loss sample: 10.12171111801635\n",
      "training samples from row 367: 140,iterations 51380,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3769 seconds\n",
      "Loss sample: 9.704156994252727\n",
      "training samples from row 368: 140,iterations 51520,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3943 seconds\n",
      "Loss sample: 10.075751855055753\n",
      "training samples from row 369: 140,iterations 51660,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3773 seconds\n",
      "Loss sample: 9.769944669556356\n",
      "training samples from row 370: 140,iterations 51800,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3770 seconds\n",
      "Loss sample: 10.128331905300213\n",
      "training samples from row 371: 140,iterations 51940,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3766 seconds\n",
      "Loss sample: 10.113511451480608\n",
      "training samples from row 372: 140,iterations 52080,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3810 seconds\n",
      "Loss sample: 9.718501114845653\n",
      "training samples from row 373: 140,iterations 52220,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3797 seconds\n",
      "Loss sample: 10.01229915894327\n",
      "training samples from row 374: 140,iterations 52360,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3786 seconds\n",
      "Loss sample: 9.816849471179818\n",
      "training samples from row 375: 140,iterations 52500,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3773 seconds\n",
      "Loss sample: 10.028814743724078\n",
      "training samples from row 376: 140,iterations 52640,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3765 seconds\n",
      "Loss sample: 10.111110909092538\n",
      "training samples from row 377: 140,iterations 52780,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3787 seconds\n",
      "Loss sample: 10.048369330229265\n",
      "training samples from row 378: 140,iterations 52920,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3784 seconds\n",
      "Loss sample: 10.095498775585481\n",
      "training samples from row 379: 140,iterations 53060,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3766 seconds\n",
      "Loss sample: 10.103528186866386\n",
      "training samples from row 380: 140,iterations 53200,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3737 seconds\n",
      "Loss sample: 10.130520717745227\n",
      "training samples from row 381: 140,iterations 53340,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3923 seconds\n",
      "Loss sample: 10.083063076151072\n",
      "training samples from row 382: 140,iterations 53480,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.4224 seconds\n",
      "Loss sample: 9.924069891447232\n",
      "training samples from row 383: 140,iterations 53620,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3757 seconds\n",
      "Loss sample: 10.067693839542908\n",
      "training samples from row 384: 140,iterations 53760,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3641 seconds\n",
      "Loss sample: 10.051691488453724\n",
      "training samples from row 385: 140,iterations 53900,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3753 seconds\n",
      "Loss sample: 10.072423935909686\n",
      "training samples from row 386: 140,iterations 54040,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3912 seconds\n",
      "Loss sample: 9.777424007431382\n",
      "training samples from row 387: 140,iterations 54180,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3597 seconds\n",
      "Loss sample: 10.07509837075796\n",
      "training samples from row 388: 140,iterations 54320,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3758 seconds\n",
      "Loss sample: 9.579828514862799\n",
      "training samples from row 389: 140,iterations 54460,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3620 seconds\n",
      "Loss sample: 10.045886951841762\n",
      "training samples from row 390: 140,iterations 54600,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3751 seconds\n",
      "Loss sample: 10.03790638788088\n",
      "training samples from row 391: 140,iterations 54740,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3856 seconds\n",
      "Loss sample: 10.098939126152393\n",
      "training samples from row 392: 140,iterations 54880,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3770 seconds\n",
      "Loss sample: 9.893897469233707\n",
      "training samples from row 393: 140,iterations 55020,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3755 seconds\n",
      "Loss sample: 10.109103866306564\n",
      "training samples from row 394: 140,iterations 55160,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3921 seconds\n",
      "Loss sample: 10.068081583217605\n",
      "training samples from row 395: 140,iterations 55300,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3912 seconds\n",
      "Loss sample: 9.79971701929731\n",
      "training samples from row 396: 140,iterations 55440,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3925 seconds\n",
      "Loss sample: 9.878734946213596\n",
      "training samples from row 397: 140,iterations 55580,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3906 seconds\n",
      "Loss sample: 10.09791122892066\n",
      "training samples from row 398: 140,iterations 55720,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3754 seconds\n",
      "Loss sample: 10.073507618665204\n",
      "training samples from row 399: 140,iterations 55860,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3762 seconds\n",
      "Loss sample: 10.082894717492067\n",
      "training samples from row 400: 140,iterations 56000,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3907 seconds\n",
      "Loss sample: 9.897083380320788\n",
      "training samples from row 401: 140,iterations 56140,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3783 seconds\n",
      "Loss sample: 10.046878289910717\n",
      "training samples from row 402: 140,iterations 56280,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3756 seconds\n",
      "Loss sample: 10.164897199742777\n",
      "training samples from row 403: 140,iterations 56420,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3949 seconds\n",
      "Loss sample: 10.120326948569925\n",
      "training samples from row 404: 140,iterations 56560,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3926 seconds\n",
      "Loss sample: 10.09696525757259\n",
      "training samples from row 405: 140,iterations 56700,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3761 seconds\n",
      "Loss sample: 10.093283516519358\n",
      "training samples from row 406: 140,iterations 56840,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3755 seconds\n",
      "Loss sample: 9.799356701920189\n",
      "training samples from row 407: 140,iterations 56980,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3908 seconds\n",
      "Loss sample: 9.802694442810468\n",
      "training samples from row 408: 140,iterations 57120,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3757 seconds\n",
      "Loss sample: 10.184781473740207\n",
      "training samples from row 409: 140,iterations 57260,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3770 seconds\n",
      "Loss sample: 10.088012939034389\n",
      "training samples from row 410: 140,iterations 57400,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3914 seconds\n",
      "Loss sample: 10.079947389640713\n",
      "training samples from row 411: 140,iterations 57540,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3765 seconds\n",
      "Loss sample: 10.0606690368546\n",
      "training samples from row 412: 140,iterations 57680,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3771 seconds\n",
      "Loss sample: 10.096290721448957\n",
      "training samples from row 413: 140,iterations 57820,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3906 seconds\n",
      "Loss sample: 9.711702315557543\n",
      "training samples from row 414: 140,iterations 57960,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3767 seconds\n",
      "Loss sample: 10.028078072604618\n",
      "training samples from row 415: 140,iterations 58100,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3749 seconds\n",
      "Loss sample: 9.964895956843208\n",
      "training samples from row 416: 140,iterations 58240,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3757 seconds\n",
      "Loss sample: 10.164125003296144\n",
      "training samples from row 417: 140,iterations 58380,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3933 seconds\n",
      "Loss sample: 10.055694912008255\n",
      "training samples from row 418: 140,iterations 58520,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3907 seconds\n",
      "Loss sample: 10.077304019805021\n",
      "training samples from row 419: 140,iterations 58660,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3908 seconds\n",
      "Loss sample: 9.490000073483026\n",
      "training samples from row 420: 140,iterations 58800,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3926 seconds\n",
      "Loss sample: 9.940484111341455\n",
      "training samples from row 421: 140,iterations 58940,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3750 seconds\n",
      "Loss sample: 9.119224617957908\n",
      "training samples from row 422: 140,iterations 59080,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3929 seconds\n",
      "Loss sample: 10.095683506607685\n",
      "training samples from row 423: 140,iterations 59220,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3753 seconds\n",
      "Loss sample: 10.089821160400954\n",
      "training samples from row 424: 140,iterations 59360,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3755 seconds\n",
      "Loss sample: 9.999695422689694\n",
      "training samples from row 425: 140,iterations 59500,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3699 seconds\n",
      "Loss sample: 10.07276510588613\n",
      "training samples from row 426: 140,iterations 59640,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3916 seconds\n",
      "Loss sample: 9.547029763372985\n",
      "training samples from row 427: 140,iterations 59780,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3918 seconds\n",
      "Loss sample: 10.048671636223707\n",
      "training samples from row 428: 140,iterations 59920,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3756 seconds\n",
      "Loss sample: 10.034069836205838\n",
      "training samples from row 429: 140,iterations 60060,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3960 seconds\n",
      "Loss sample: 10.089499493249443\n",
      "training samples from row 430: 140,iterations 60200,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3757 seconds\n",
      "Loss sample: 10.092654435027105\n",
      "training samples from row 431: 140,iterations 60340,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3754 seconds\n",
      "Loss sample: 10.027983026374807\n",
      "training samples from row 432: 140,iterations 60480,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3772 seconds\n",
      "Loss sample: 10.11069928295513\n",
      "training samples from row 433: 140,iterations 60620,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3756 seconds\n",
      "Loss sample: 9.205413580901125\n",
      "training samples from row 434: 140,iterations 60760,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3773 seconds\n",
      "Loss sample: 10.077714332683255\n",
      "training samples from row 435: 140,iterations 60900,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3911 seconds\n",
      "Loss sample: 10.127991787134377\n",
      "training samples from row 436: 140,iterations 61040,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3764 seconds\n",
      "Loss sample: 9.502367015664813\n",
      "training samples from row 437: 140,iterations 61180,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3774 seconds\n",
      "Loss sample: 10.070999463979293\n",
      "training samples from row 438: 140,iterations 61320,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3917 seconds\n",
      "Loss sample: 10.077855914376313\n",
      "training samples from row 439: 140,iterations 61460,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3917 seconds\n",
      "Loss sample: 10.01384781857347\n",
      "training samples from row 440: 140,iterations 61600,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3737 seconds\n",
      "Loss sample: 10.092702835032767\n",
      "training samples from row 441: 140,iterations 61740,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3907 seconds\n",
      "Loss sample: 9.979231390154053\n",
      "training samples from row 442: 140,iterations 61880,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3922 seconds\n",
      "Loss sample: 10.104927440400742\n",
      "training samples from row 443: 140,iterations 62020,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3912 seconds\n",
      "Loss sample: 10.057977568909395\n",
      "training samples from row 444: 140,iterations 62160,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3908 seconds\n",
      "Loss sample: 10.061409675486857\n",
      "training samples from row 445: 140,iterations 62300,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3769 seconds\n",
      "Loss sample: 9.47990015513517\n",
      "training samples from row 446: 140,iterations 62440,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3908 seconds\n",
      "Loss sample: 10.077392222932296\n",
      "training samples from row 447: 140,iterations 62580,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3602 seconds\n",
      "Loss sample: 10.126167767000121\n",
      "training samples from row 448: 140,iterations 62720,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3771 seconds\n",
      "Loss sample: 9.066530198926252\n",
      "training samples from row 449: 140,iterations 62860,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3923 seconds\n",
      "Loss sample: 9.640112372239738\n",
      "training samples from row 450: 140,iterations 63000,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3920 seconds\n",
      "Loss sample: 10.043579539589722\n",
      "training samples from row 451: 140,iterations 63140,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3916 seconds\n",
      "Loss sample: 10.0569792711071\n",
      "training samples from row 452: 140,iterations 63280,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3753 seconds\n",
      "Loss sample: 9.50428801387224\n",
      "training samples from row 453: 140,iterations 63420,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3922 seconds\n",
      "Loss sample: 10.07976431054227\n",
      "training samples from row 454: 140,iterations 63560,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3758 seconds\n",
      "Loss sample: 10.108362063954369\n",
      "training samples from row 455: 140,iterations 63700,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3906 seconds\n",
      "Loss sample: 10.029648259212117\n",
      "training samples from row 456: 140,iterations 63840,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.4070 seconds\n",
      "Loss sample: 9.84966780104066\n",
      "training samples from row 457: 140,iterations 63980,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.4074 seconds\n",
      "Loss sample: 10.091611755242303\n",
      "training samples from row 458: 140,iterations 64120,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3777 seconds\n",
      "Loss sample: 10.061598653623436\n",
      "training samples from row 459: 140,iterations 64260,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3935 seconds\n",
      "Loss sample: 10.096360077123954\n",
      "training samples from row 460: 140,iterations 64400,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3780 seconds\n",
      "Loss sample: 10.07358871203623\n",
      "training samples from row 461: 140,iterations 64540,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3762 seconds\n",
      "Loss sample: 9.704659163975194\n",
      "training samples from row 462: 140,iterations 64680,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3905 seconds\n",
      "Loss sample: 10.107147812031883\n",
      "training samples from row 463: 140,iterations 64820,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3932 seconds\n",
      "Loss sample: 10.093067621260067\n",
      "training samples from row 464: 140,iterations 64960,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3753 seconds\n",
      "Loss sample: 9.64774264152073\n",
      "training samples from row 465: 140,iterations 65100,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3964 seconds\n",
      "Loss sample: 9.945156953327063\n",
      "training samples from row 466: 140,iterations 65240,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3912 seconds\n",
      "Loss sample: 10.035337926512785\n",
      "training samples from row 467: 140,iterations 65380,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3906 seconds\n",
      "Loss sample: 10.094071165860305\n",
      "training samples from row 468: 140,iterations 65520,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3777 seconds\n",
      "Loss sample: 10.09681699047289\n",
      "training samples from row 469: 140,iterations 65660,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3754 seconds\n",
      "Loss sample: 10.052060853765354\n",
      "training samples from row 470: 140,iterations 65800,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.4068 seconds\n",
      "Loss sample: 10.086060805223694\n",
      "training samples from row 471: 140,iterations 65940,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3918 seconds\n",
      "Loss sample: 10.046211972606173\n",
      "training samples from row 472: 140,iterations 66080,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3906 seconds\n",
      "Loss sample: 10.163888872829082\n",
      "training samples from row 473: 140,iterations 66220,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.4249 seconds\n",
      "Loss sample: 9.891499208278441\n",
      "training samples from row 474: 140,iterations 66360,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3846 seconds\n",
      "Loss sample: 10.15534005751414\n",
      "training samples from row 475: 140,iterations 66500,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3775 seconds\n",
      "Loss sample: 10.06347148201242\n",
      "training samples from row 476: 140,iterations 66640,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3611 seconds\n",
      "Loss sample: 10.086691770987391\n",
      "training samples from row 477: 140,iterations 66780,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.4220 seconds\n",
      "Loss sample: 10.054508681227121\n",
      "training samples from row 478: 140,iterations 66920,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3906 seconds\n",
      "Loss sample: 10.007510026165873\n",
      "training samples from row 479: 140,iterations 67060,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3829 seconds\n",
      "Loss sample: 9.439402276171373\n",
      "training samples from row 480: 140,iterations 67200,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3906 seconds\n",
      "Loss sample: 9.759253111026315\n",
      "training samples from row 481: 140,iterations 67340,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3758 seconds\n",
      "Loss sample: 10.103037160881607\n",
      "training samples from row 482: 140,iterations 67480,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3905 seconds\n",
      "Loss sample: 10.064161981007754\n",
      "training samples from row 483: 140,iterations 67620,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3907 seconds\n",
      "Loss sample: 10.129798939781868\n",
      "training samples from row 484: 140,iterations 67760,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3776 seconds\n",
      "Loss sample: 9.27509836028832\n",
      "training samples from row 485: 140,iterations 67900,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3913 seconds\n",
      "Loss sample: 10.091610404301482\n",
      "training samples from row 486: 140,iterations 68040,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3913 seconds\n",
      "Loss sample: 10.062598988831732\n",
      "training samples from row 487: 140,iterations 68180,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3907 seconds\n",
      "Loss sample: 9.668133830875586\n",
      "training samples from row 488: 140,iterations 68320,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3912 seconds\n",
      "Loss sample: 9.117956065690597\n",
      "training samples from row 489: 140,iterations 68460,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3919 seconds\n",
      "Loss sample: 10.08799846644293\n",
      "training samples from row 490: 140,iterations 68600,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3755 seconds\n",
      "Loss sample: 10.095348627250067\n",
      "training samples from row 491: 140,iterations 68740,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3809 seconds\n",
      "Loss sample: 10.018673903218579\n",
      "training samples from row 492: 140,iterations 68880,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3902 seconds\n",
      "Loss sample: 10.126933827916682\n",
      "training samples from row 493: 140,iterations 69020,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3756 seconds\n",
      "Loss sample: 9.973875952461803\n",
      "training samples from row 494: 140,iterations 69160,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.4086 seconds\n",
      "Loss sample: 10.123219124553058\n",
      "training samples from row 495: 140,iterations 69300,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3910 seconds\n",
      "Loss sample: 10.064740003452886\n",
      "training samples from row 496: 140,iterations 69440,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3924 seconds\n",
      "Loss sample: 9.884064478302744\n",
      "training samples from row 497: 140,iterations 69580,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.4072 seconds\n",
      "Loss sample: 10.08328410217292\n",
      "training samples from row 498: 140,iterations 69720,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3911 seconds\n",
      "Loss sample: 10.050697521336483\n",
      "training samples from row 499: 140,iterations 69860,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3774 seconds\n",
      "Loss sample: 10.032220055770349\n",
      "training samples from row 500: 140,iterations 70000,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3916 seconds\n",
      "Loss sample: 10.077718858926785\n",
      "training samples from row 501: 140,iterations 70140,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3926 seconds\n",
      "Loss sample: 9.796218635637597\n",
      "training samples from row 502: 140,iterations 70280,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3776 seconds\n",
      "Loss sample: 9.761944205296187\n",
      "training samples from row 503: 140,iterations 70420,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.4079 seconds\n",
      "Loss sample: 10.11362435111547\n",
      "training samples from row 504: 140,iterations 70560,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3801 seconds\n",
      "Loss sample: 10.072143066155158\n",
      "training samples from row 505: 140,iterations 70700,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3753 seconds\n",
      "Loss sample: 10.107158123346682\n",
      "training samples from row 506: 140,iterations 70840,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3913 seconds\n",
      "Loss sample: 10.07075035923623\n",
      "training samples from row 507: 140,iterations 70980,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3773 seconds\n",
      "Loss sample: 10.021349286300874\n",
      "training samples from row 508: 140,iterations 71120,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3913 seconds\n",
      "Loss sample: 10.11262890295979\n",
      "training samples from row 509: 140,iterations 71260,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3752 seconds\n",
      "Loss sample: 10.029682664868588\n",
      "training samples from row 510: 140,iterations 71400,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3767 seconds\n",
      "Loss sample: 9.950692015661092\n",
      "training samples from row 511: 140,iterations 71540,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3770 seconds\n",
      "Loss sample: 10.068356657444655\n",
      "training samples from row 512: 140,iterations 71680,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3761 seconds\n",
      "Loss sample: 10.099616200218689\n",
      "training samples from row 513: 140,iterations 71820,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3712 seconds\n",
      "Loss sample: 10.003938371368864\n",
      "training samples from row 514: 140,iterations 71960,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3781 seconds\n",
      "Loss sample: 10.08029764851185\n",
      "training samples from row 515: 140,iterations 72100,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3933 seconds\n",
      "Loss sample: 9.699564369159752\n",
      "training samples from row 516: 140,iterations 72240,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3926 seconds\n",
      "Loss sample: 9.840324695277204\n",
      "training samples from row 517: 140,iterations 72380,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3919 seconds\n",
      "Loss sample: 10.139090495399605\n",
      "training samples from row 518: 140,iterations 72520,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3860 seconds\n",
      "Loss sample: 10.03263833478129\n",
      "training samples from row 519: 140,iterations 72660,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3919 seconds\n",
      "Loss sample: 10.066774372343561\n",
      "training samples from row 520: 140,iterations 72800,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3759 seconds\n",
      "Loss sample: 10.091089186984485\n",
      "training samples from row 521: 140,iterations 72940,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3758 seconds\n",
      "Loss sample: 10.092118076682782\n",
      "training samples from row 522: 140,iterations 73080,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3917 seconds\n",
      "Loss sample: 10.045004256525145\n",
      "training samples from row 523: 140,iterations 73220,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3761 seconds\n",
      "Loss sample: 10.061379302481422\n",
      "training samples from row 524: 140,iterations 73360,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3911 seconds\n",
      "Loss sample: 10.14922264876053\n",
      "training samples from row 525: 140,iterations 73500,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3775 seconds\n",
      "Loss sample: 9.133494498062174\n",
      "training samples from row 526: 140,iterations 73640,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3909 seconds\n",
      "Loss sample: 9.591672313216398\n",
      "training samples from row 527: 140,iterations 73780,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3773 seconds\n",
      "Loss sample: 10.056706134511188\n",
      "training samples from row 528: 140,iterations 73920,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3755 seconds\n",
      "Loss sample: 9.079040659174378\n",
      "training samples from row 529: 140,iterations 74060,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3752 seconds\n",
      "Loss sample: 8.724990063487347\n",
      "training samples from row 530: 140,iterations 74200,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3788 seconds\n",
      "Loss sample: 10.044240129483955\n",
      "training samples from row 531: 140,iterations 74340,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3924 seconds\n",
      "Loss sample: 10.004600921376237\n",
      "training samples from row 532: 140,iterations 74480,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3777 seconds\n",
      "Loss sample: 9.787173786929747\n",
      "training samples from row 533: 140,iterations 74620,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.4070 seconds\n",
      "Loss sample: 10.075066222836309\n",
      "training samples from row 534: 140,iterations 74760,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3752 seconds\n",
      "Loss sample: 10.050661657419893\n",
      "training samples from row 535: 140,iterations 74900,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3924 seconds\n",
      "Loss sample: 10.028175999315641\n",
      "training samples from row 536: 140,iterations 75040,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3770 seconds\n",
      "Loss sample: 9.502211934287997\n",
      "training samples from row 537: 140,iterations 75180,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3915 seconds\n",
      "Loss sample: 10.089647516152247\n",
      "training samples from row 538: 140,iterations 75320,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3621 seconds\n",
      "Loss sample: 9.345583984450561\n",
      "training samples from row 539: 140,iterations 75460,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3779 seconds\n",
      "Loss sample: 10.054621240261334\n",
      "training samples from row 540: 140,iterations 75600,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3769 seconds\n",
      "Loss sample: 10.011961226806806\n",
      "training samples from row 541: 140,iterations 75740,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3767 seconds\n",
      "Loss sample: 10.027900431990062\n",
      "training samples from row 542: 140,iterations 75880,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3906 seconds\n",
      "Loss sample: 10.015455023876767\n",
      "training samples from row 543: 140,iterations 76020,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3913 seconds\n",
      "Loss sample: 10.078875198162327\n",
      "training samples from row 544: 140,iterations 76160,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3754 seconds\n",
      "Loss sample: 10.07284144803708\n",
      "training samples from row 545: 140,iterations 76300,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3909 seconds\n",
      "Loss sample: 9.053855416369109\n",
      "training samples from row 546: 140,iterations 76440,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3931 seconds\n",
      "Loss sample: 9.587138926527347\n",
      "training samples from row 547: 140,iterations 76580,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3760 seconds\n",
      "Loss sample: 10.135708854195913\n",
      "training samples from row 548: 140,iterations 76720,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3913 seconds\n",
      "Loss sample: 10.090873993837077\n",
      "training samples from row 549: 140,iterations 76860,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3765 seconds\n",
      "Loss sample: 9.910558999597544\n",
      "training samples from row 550: 140,iterations 77000,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3932 seconds\n",
      "Loss sample: 10.075033522856849\n",
      "training samples from row 551: 140,iterations 77140,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3780 seconds\n",
      "Loss sample: 10.04543189341316\n",
      "training samples from row 552: 140,iterations 77280,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3916 seconds\n",
      "Loss sample: 10.018892692416024\n",
      "training samples from row 553: 140,iterations 77420,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3861 seconds\n",
      "Loss sample: 10.125712426882249\n",
      "training samples from row 554: 140,iterations 77560,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3909 seconds\n",
      "Loss sample: 9.741595303692845\n",
      "training samples from row 555: 140,iterations 77700,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3910 seconds\n",
      "Loss sample: 9.846165164717592\n",
      "training samples from row 556: 140,iterations 77840,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3937 seconds\n",
      "Loss sample: 10.110874878167303\n",
      "training samples from row 557: 140,iterations 77980,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3908 seconds\n",
      "Loss sample: 10.049878258635548\n",
      "training samples from row 558: 140,iterations 78120,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3780 seconds\n",
      "Loss sample: 9.758714846934618\n",
      "training samples from row 559: 140,iterations 78260,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3761 seconds\n",
      "Loss sample: 10.100987992229774\n",
      "training samples from row 560: 140,iterations 78400,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3750 seconds\n",
      "Loss sample: 9.671491283411017\n",
      "training samples from row 561: 140,iterations 78540,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3927 seconds\n",
      "Loss sample: 9.222619632379242\n",
      "training samples from row 562: 140,iterations 78680,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3760 seconds\n",
      "Loss sample: 10.097589518234289\n",
      "training samples from row 563: 140,iterations 78820,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3932 seconds\n",
      "Loss sample: 10.038127089805748\n",
      "training samples from row 564: 140,iterations 78960,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3755 seconds\n",
      "Loss sample: 9.203373023680657\n",
      "training samples from row 565: 140,iterations 79100,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3909 seconds\n",
      "Loss sample: 10.044192222754372\n",
      "training samples from row 566: 140,iterations 79240,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3769 seconds\n",
      "Loss sample: 10.133535710580928\n",
      "training samples from row 567: 140,iterations 79380,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3935 seconds\n",
      "Loss sample: 10.059258490674855\n",
      "training samples from row 568: 140,iterations 79520,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3612 seconds\n",
      "Loss sample: 10.06453474675514\n",
      "training samples from row 569: 140,iterations 79660,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3906 seconds\n",
      "Loss sample: 9.849606034671329\n",
      "training samples from row 570: 140,iterations 79800,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.4071 seconds\n",
      "Loss sample: 9.939427839632174\n",
      "training samples from row 571: 140,iterations 79940,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3797 seconds\n",
      "Loss sample: 10.039224869607102\n",
      "training samples from row 572: 140,iterations 80080,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3760 seconds\n",
      "Loss sample: 9.877225981259045\n",
      "training samples from row 573: 140,iterations 80220,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.4089 seconds\n",
      "Loss sample: 10.149988003472743\n",
      "training samples from row 574: 140,iterations 80360,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3906 seconds\n",
      "Loss sample: 10.044618417232348\n",
      "training samples from row 575: 140,iterations 80500,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3757 seconds\n",
      "Loss sample: 10.122269916796785\n",
      "training samples from row 576: 140,iterations 80640,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3935 seconds\n",
      "Loss sample: 10.056341142394192\n",
      "training samples from row 577: 140,iterations 80780,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3758 seconds\n",
      "Loss sample: 10.11560551102885\n",
      "training samples from row 578: 140,iterations 80920,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.4075 seconds\n",
      "Loss sample: 8.91342622156791\n",
      "training samples from row 579: 140,iterations 81060,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3607 seconds\n",
      "Loss sample: 10.14043924278112\n",
      "training samples from row 580: 140,iterations 81200,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3750 seconds\n",
      "Loss sample: 10.096970572414333\n",
      "training samples from row 581: 140,iterations 81340,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3754 seconds\n",
      "Loss sample: 10.015200766096955\n",
      "training samples from row 582: 140,iterations 81480,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3750 seconds\n",
      "Loss sample: 9.849508747553545\n",
      "training samples from row 583: 140,iterations 81620,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3757 seconds\n",
      "Loss sample: 9.999919967668408\n",
      "training samples from row 584: 140,iterations 81760,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3777 seconds\n",
      "Loss sample: 10.071961450886638\n",
      "training samples from row 585: 140,iterations 81900,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3916 seconds\n",
      "Loss sample: 10.076877206583399\n",
      "training samples from row 586: 140,iterations 82040,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3788 seconds\n",
      "Loss sample: 9.957706943123881\n",
      "training samples from row 587: 140,iterations 82180,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3755 seconds\n",
      "Loss sample: 8.993711590870996\n",
      "training samples from row 588: 140,iterations 82320,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3771 seconds\n",
      "Loss sample: 8.987049484683308\n",
      "training samples from row 589: 140,iterations 82460,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3944 seconds\n",
      "Loss sample: 10.081914780629985\n",
      "training samples from row 590: 140,iterations 82600,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3923 seconds\n",
      "Loss sample: 9.20308561975718\n",
      "training samples from row 591: 140,iterations 82740,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3938 seconds\n",
      "Loss sample: 9.761881189915558\n",
      "training samples from row 592: 140,iterations 82880,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3920 seconds\n",
      "Loss sample: 10.056693295893211\n",
      "training samples from row 593: 140,iterations 83020,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3754 seconds\n",
      "Loss sample: 8.938489410571297\n",
      "training samples from row 594: 140,iterations 83160,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3769 seconds\n",
      "Loss sample: 9.502170829042946\n",
      "training samples from row 595: 140,iterations 83300,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3911 seconds\n",
      "Loss sample: 9.719122659145782\n",
      "training samples from row 596: 140,iterations 83440,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3937 seconds\n",
      "Loss sample: 10.03806795469612\n",
      "training samples from row 597: 140,iterations 83580,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3756 seconds\n",
      "Loss sample: 10.13016600835808\n",
      "training samples from row 598: 140,iterations 83720,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3759 seconds\n",
      "Loss sample: 10.083625339692931\n",
      "training samples from row 599: 140,iterations 83860,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3903 seconds\n",
      "Loss sample: 10.062914795246447\n",
      "training samples from row 600: 140,iterations 84000,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3920 seconds\n",
      "Loss sample: 10.038184841792022\n",
      "training samples from row 601: 140,iterations 84140,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3766 seconds\n",
      "Loss sample: 10.065033493438923\n",
      "training samples from row 602: 140,iterations 84280,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3916 seconds\n",
      "Loss sample: 9.834421365628792\n",
      "training samples from row 603: 140,iterations 84420,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3753 seconds\n",
      "Loss sample: 10.094175900094122\n",
      "training samples from row 604: 140,iterations 84560,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3762 seconds\n",
      "Loss sample: 10.068677813034489\n",
      "training samples from row 605: 140,iterations 84700,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3762 seconds\n",
      "Loss sample: 10.03522781847456\n",
      "training samples from row 606: 140,iterations 84840,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3779 seconds\n",
      "Loss sample: 10.111208614382367\n",
      "training samples from row 607: 140,iterations 84980,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3766 seconds\n",
      "Loss sample: 10.158767596677128\n",
      "training samples from row 608: 140,iterations 85120,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3601 seconds\n",
      "Loss sample: 10.045284791769593\n",
      "training samples from row 609: 140,iterations 85260,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3743 seconds\n",
      "Loss sample: 10.106092737697901\n",
      "training samples from row 610: 140,iterations 85400,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3758 seconds\n",
      "Loss sample: 10.079605807445228\n",
      "training samples from row 611: 140,iterations 85540,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.4076 seconds\n",
      "Loss sample: 8.555241582330991\n",
      "training samples from row 612: 140,iterations 85680,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3779 seconds\n",
      "Loss sample: 10.09993390039248\n",
      "training samples from row 613: 140,iterations 85820,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3764 seconds\n",
      "Loss sample: 10.11778362765916\n",
      "training samples from row 614: 140,iterations 85960,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3764 seconds\n",
      "Loss sample: 9.968046130701595\n",
      "training samples from row 615: 140,iterations 86100,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3921 seconds\n",
      "Loss sample: 10.090831350548463\n",
      "training samples from row 616: 140,iterations 86240,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.4068 seconds\n",
      "Loss sample: 10.007036464521642\n",
      "training samples from row 617: 140,iterations 86380,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3774 seconds\n",
      "Loss sample: 10.172759452440769\n",
      "training samples from row 618: 140,iterations 86520,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3813 seconds\n",
      "Loss sample: 10.046566447237192\n",
      "training samples from row 619: 140,iterations 86660,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3779 seconds\n",
      "Loss sample: 10.07979357112775\n",
      "training samples from row 620: 140,iterations 86800,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3659 seconds\n",
      "Loss sample: 9.924911649448548\n",
      "training samples from row 621: 140,iterations 86940,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3770 seconds\n",
      "Loss sample: 10.150883044242041\n",
      "training samples from row 622: 140,iterations 87080,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.4073 seconds\n",
      "Loss sample: 10.117858793544533\n",
      "training samples from row 623: 140,iterations 87220,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3771 seconds\n",
      "Loss sample: 10.055231561675813\n",
      "training samples from row 624: 140,iterations 87360,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3761 seconds\n",
      "Loss sample: 10.049111639009082\n",
      "training samples from row 625: 140,iterations 87500,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3910 seconds\n",
      "Loss sample: 10.08814457384155\n",
      "training samples from row 626: 140,iterations 87640,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3912 seconds\n",
      "Loss sample: 10.124065997820958\n",
      "training samples from row 627: 140,iterations 87780,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3772 seconds\n",
      "Loss sample: 10.05368959896015\n",
      "training samples from row 628: 140,iterations 87920,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.4067 seconds\n",
      "Loss sample: 10.087283831218055\n",
      "training samples from row 629: 140,iterations 88060,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3766 seconds\n",
      "Loss sample: 10.120785922431944\n",
      "training samples from row 630: 140,iterations 88200,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3756 seconds\n",
      "Loss sample: 10.053527905630588\n",
      "training samples from row 631: 140,iterations 88340,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3936 seconds\n",
      "Loss sample: 10.020916952523464\n",
      "training samples from row 632: 140,iterations 88480,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3926 seconds\n",
      "Loss sample: 9.917623387306534\n",
      "training samples from row 633: 140,iterations 88620,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3792 seconds\n",
      "Loss sample: 9.347554489111918\n",
      "training samples from row 634: 140,iterations 88760,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3751 seconds\n",
      "Loss sample: 8.634199617927099\n",
      "training samples from row 635: 140,iterations 88900,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3750 seconds\n",
      "Loss sample: 10.041468951789044\n",
      "training samples from row 636: 140,iterations 89040,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3903 seconds\n",
      "Loss sample: 10.067421670260245\n",
      "training samples from row 637: 140,iterations 89180,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3775 seconds\n",
      "Loss sample: 10.114990227871706\n",
      "training samples from row 638: 140,iterations 89320,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3915 seconds\n",
      "Loss sample: 10.113933170629704\n",
      "training samples from row 639: 140,iterations 89460,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3762 seconds\n",
      "Loss sample: 10.06524119809008\n",
      "training samples from row 640: 140,iterations 89600,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3777 seconds\n",
      "Loss sample: 10.090676193847523\n",
      "training samples from row 641: 140,iterations 89740,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3916 seconds\n",
      "Loss sample: 9.784138715893985\n",
      "training samples from row 642: 140,iterations 89880,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3756 seconds\n",
      "Loss sample: 10.058347110430313\n",
      "training samples from row 643: 140,iterations 90020,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3938 seconds\n",
      "Loss sample: 10.047483137209753\n",
      "training samples from row 644: 140,iterations 90160,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3910 seconds\n",
      "Loss sample: 10.112195583110248\n",
      "training samples from row 645: 140,iterations 90300,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3759 seconds\n",
      "Loss sample: 8.57985918356972\n",
      "training samples from row 646: 140,iterations 90440,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3707 seconds\n",
      "Loss sample: 10.130135230556489\n",
      "training samples from row 647: 140,iterations 90580,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3607 seconds\n",
      "Loss sample: 10.048221923491756\n",
      "training samples from row 648: 140,iterations 90720,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3775 seconds\n",
      "Loss sample: 10.044074462150386\n",
      "training samples from row 649: 140,iterations 90860,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3910 seconds\n",
      "Loss sample: 10.142620856228481\n",
      "training samples from row 650: 140,iterations 91000,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3767 seconds\n",
      "Loss sample: 10.071220651877049\n",
      "training samples from row 651: 140,iterations 91140,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3773 seconds\n",
      "Loss sample: 9.528242027447524\n",
      "training samples from row 652: 140,iterations 91280,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3757 seconds\n",
      "Loss sample: 10.15971289186531\n",
      "training samples from row 653: 140,iterations 91420,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3924 seconds\n",
      "Loss sample: 8.985087763311087\n",
      "training samples from row 654: 140,iterations 91560,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3774 seconds\n",
      "Loss sample: 10.060503517190151\n",
      "training samples from row 655: 140,iterations 91700,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3936 seconds\n",
      "Loss sample: 10.03229989944432\n",
      "training samples from row 656: 140,iterations 91840,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3629 seconds\n",
      "Loss sample: 8.939940477558414\n",
      "training samples from row 657: 140,iterations 91980,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3913 seconds\n",
      "Loss sample: 10.08608857977334\n",
      "training samples from row 658: 140,iterations 92120,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3786 seconds\n",
      "Loss sample: 8.762525133221729\n",
      "training samples from row 659: 140,iterations 92260,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3764 seconds\n",
      "Loss sample: 9.406369860725567\n",
      "training samples from row 660: 140,iterations 92400,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3761 seconds\n",
      "Loss sample: 9.978434064013953\n",
      "training samples from row 661: 140,iterations 92540,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3781 seconds\n",
      "Loss sample: 9.47109462257859\n",
      "training samples from row 662: 140,iterations 92680,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3752 seconds\n",
      "Loss sample: 9.974052024727003\n",
      "training samples from row 663: 140,iterations 92820,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3749 seconds\n",
      "Loss sample: 10.143392674081678\n",
      "training samples from row 664: 140,iterations 92960,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3769 seconds\n",
      "Loss sample: 10.040869239153134\n",
      "training samples from row 665: 140,iterations 93100,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3905 seconds\n",
      "Loss sample: 10.06384982625397\n",
      "training samples from row 666: 140,iterations 93240,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3775 seconds\n",
      "Loss sample: 10.040292273445287\n",
      "training samples from row 667: 140,iterations 93380,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3929 seconds\n",
      "Loss sample: 10.084902954209818\n",
      "training samples from row 668: 140,iterations 93520,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3914 seconds\n",
      "Loss sample: 10.005499923483141\n",
      "training samples from row 669: 140,iterations 93660,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3937 seconds\n",
      "Loss sample: 9.416562611688034\n",
      "training samples from row 670: 140,iterations 93800,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3921 seconds\n",
      "Loss sample: 10.08436450361362\n",
      "training samples from row 671: 140,iterations 93940,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3930 seconds\n",
      "Loss sample: 10.103183764335517\n",
      "training samples from row 672: 140,iterations 94080,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3920 seconds\n",
      "Loss sample: 10.044361432088044\n",
      "training samples from row 673: 140,iterations 94220,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3916 seconds\n",
      "Loss sample: 10.155700685545545\n",
      "training samples from row 674: 140,iterations 94360,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3781 seconds\n",
      "Loss sample: 10.092522285595694\n",
      "training samples from row 675: 140,iterations 94500,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3912 seconds\n",
      "Loss sample: 9.973989198986768\n",
      "training samples from row 676: 140,iterations 94640,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3757 seconds\n",
      "Loss sample: 10.098513886670945\n",
      "training samples from row 677: 140,iterations 94780,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3737 seconds\n",
      "Loss sample: 10.165486471310947\n",
      "training samples from row 678: 140,iterations 94920,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3765 seconds\n",
      "Loss sample: 9.853229826622782\n",
      "training samples from row 679: 140,iterations 95060,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3776 seconds\n",
      "Loss sample: 8.843400102991549\n",
      "training samples from row 680: 140,iterations 95200,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3940 seconds\n",
      "Loss sample: 9.49864534593332\n",
      "training samples from row 681: 140,iterations 95340,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3765 seconds\n",
      "Loss sample: 10.023032392591592\n",
      "training samples from row 682: 140,iterations 95480,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3778 seconds\n",
      "Loss sample: 10.01721670061491\n",
      "training samples from row 683: 140,iterations 95620,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3750 seconds\n",
      "Loss sample: 8.944298117807355\n",
      "training samples from row 684: 140,iterations 95760,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3774 seconds\n",
      "Loss sample: 10.07511991421899\n",
      "training samples from row 685: 140,iterations 95900,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3771 seconds\n",
      "Loss sample: 10.03503442075074\n",
      "training samples from row 686: 140,iterations 96040,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3598 seconds\n",
      "Loss sample: 9.386697701430126\n",
      "training samples from row 687: 140,iterations 96180,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3617 seconds\n",
      "Loss sample: 10.126534020184152\n",
      "training samples from row 688: 140,iterations 96320,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3609 seconds\n",
      "Loss sample: 10.0238157646155\n",
      "training samples from row 689: 140,iterations 96460,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3778 seconds\n",
      "Loss sample: 10.057575131886422\n",
      "training samples from row 690: 140,iterations 96600,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3767 seconds\n",
      "Loss sample: 9.042986171262376\n",
      "training samples from row 691: 140,iterations 96740,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3776 seconds\n",
      "Loss sample: 9.992287255487375\n",
      "training samples from row 692: 140,iterations 96880,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3942 seconds\n",
      "Loss sample: 9.708014566629021\n",
      "training samples from row 693: 140,iterations 97020,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3791 seconds\n",
      "Loss sample: 9.170168831156689\n",
      "training samples from row 694: 140,iterations 97160,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3789 seconds\n",
      "Loss sample: 10.024275197884212\n",
      "training samples from row 695: 140,iterations 97300,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3794 seconds\n",
      "Loss sample: 8.798952248242156\n",
      "training samples from row 696: 140,iterations 97440,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3777 seconds\n",
      "Loss sample: 10.053508240294374\n",
      "training samples from row 697: 140,iterations 97580,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3701 seconds\n",
      "Loss sample: 10.023692047582395\n",
      "training samples from row 698: 140,iterations 97720,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3797 seconds\n",
      "Loss sample: 10.138965483753504\n",
      "training samples from row 699: 140,iterations 97860,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3925 seconds\n",
      "Loss sample: 10.057237755243289\n",
      "training samples from row 700: 140,iterations 98000,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3748 seconds\n",
      "Loss sample: 10.026854283027946\n",
      "training samples from row 701: 140,iterations 98140,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3602 seconds\n",
      "Loss sample: 10.074814917184817\n",
      "training samples from row 702: 140,iterations 98280,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3774 seconds\n",
      "Loss sample: 10.110984229899307\n",
      "training samples from row 703: 140,iterations 98420,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3851 seconds\n",
      "Loss sample: 9.954348542492967\n",
      "training samples from row 704: 140,iterations 98560,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3798 seconds\n",
      "Loss sample: 10.049258437881967\n",
      "training samples from row 705: 140,iterations 98700,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3951 seconds\n",
      "Loss sample: 9.111160797278075\n",
      "training samples from row 706: 140,iterations 98840,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3978 seconds\n",
      "Loss sample: 10.163754574123057\n",
      "training samples from row 707: 140,iterations 98980,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3701 seconds\n",
      "Loss sample: 9.934544448540144\n",
      "training samples from row 708: 140,iterations 99120,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3635 seconds\n",
      "Loss sample: 10.085808893421252\n",
      "training samples from row 709: 140,iterations 99260,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3841 seconds\n",
      "Loss sample: 10.049862651204466\n",
      "training samples from row 710: 140,iterations 99400,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3688 seconds\n",
      "Loss sample: 9.996592807722388\n",
      "training samples from row 711: 140,iterations 99540,lr 0.0005\n",
      "Function 'trianiteration' executed in 0.3752 seconds\n",
      "Loss sample: 10.059817812127397\n",
      "training samples from row 712: 140,iterations 99680,lr 0.0005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m\n\u001b[0;32m      4\u001b[0m semi_context_window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m \n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m=\u001b[39mWord2Vec(embedding_size, \n\u001b[0;32m      7\u001b[0m                semi_context_window, \n\u001b[0;32m      8\u001b[0m                complete_text,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m                flattening_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m                load_pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprocessed_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 386\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[1;34m(self, X_train)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m training_samples \u001b[38;5;129;01min\u001b[39;00m gen:\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining samples from row \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(training_samples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,iterations \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,lr \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 386\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrianiteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss sample: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(training_samples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[3], line 26\u001b[0m, in \u001b[0;36mlog_time.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     25\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Record start time\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Execute the wrapped function\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Record end time\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     elapsed_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[1;32mIn[4], line 359\u001b[0m, in \u001b[0;36mWord2Vec.trianiteration\u001b[1;34m(self, training_samples)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m Loss\n\u001b[0;32m    357\u001b[0m dLoss_dZout \u001b[38;5;241m=\u001b[39m sigma_zout_two \u001b[38;5;241m-\u001b[39m yi\n\u001b[1;32m--> 359\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackpropagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdLoss_dZout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigmaz_out_one\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_zout_two\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXi\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 159\u001b[0m, in \u001b[0;36mWord2Vec.backpropagation\u001b[1;34m(self, dLoss_dZ2, sigma_Z_1, sigma_Z_2, Xi)\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_params(dLoss_dW2,dLoss_db2,dLoss_dW1,dLoss_db1)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflattening_strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    157\u001b[0m     \n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m#Gradient Embedding Weights\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m     dL_dSigmaZ1\u001b[38;5;241m=\u001b[39m\u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdLoss_dZ2\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutlayer_maps_vocab_average\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m     dL_dSigmaZ1\u001b[38;5;241m=\u001b[39mdL_dSigmaZ1\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mside_window_size,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    161\u001b[0m     dL_dZ1\u001b[38;5;241m=\u001b[39mdL_dSigmaZ1\u001b[38;5;241m*\u001b[39msigma_Z_1\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39msigma_Z_1)\n",
      "File \u001b[1;32m~\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py:683\u001b[0m, in \u001b[0;36m_GUFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    677\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid shape for out \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mouts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    678\u001b[0m                          \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m needs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    680\u001b[0m     _raise_if_invalid_cast(\n\u001b[0;32m    681\u001b[0m         ret_dtype, outs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype, casting, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 683\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_func_to_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimsizess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop_output_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mouts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;66;03m# This code credit goes to Dask\u001b[39;00m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;66;03m# https://github.com/dask/dask/blob/61b578f5a3ad88cbc6a8b9a73ce08c551bd969fa/dask/array/gufunc.py#L462-L503\u001b[39;00m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;66;03m# Treat direct output\u001b[39;00m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py:397\u001b[0m, in \u001b[0;36m_GUFunc._apply_func_to_inputs\u001b[1;34m(self, func, dim, sizes, dims, args, outs)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_supports_out \u001b[38;5;129;01mand\u001b[39;00m outs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    396\u001b[0m     outs \u001b[38;5;241m=\u001b[39m outs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(outs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m outs\n\u001b[1;32m--> 397\u001b[0m     \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mouts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    399\u001b[0m     fouts \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "complete_text = ' '.join(df['processed_text'].apply(lambda x: ' '.join(x)).tolist()) \n",
    "#complete_text = ' '.join(df[\"text\"].tolist()) \n",
    "embedding_size=100\n",
    "semi_context_window=5 \n",
    "\n",
    "model=Word2Vec(embedding_size, \n",
    "               semi_context_window, \n",
    "               complete_text,\n",
    "               optimizer=None,\n",
    "               learning_rate=0.0005,\n",
    "               flattening_strategy=\"average\",\n",
    "               load_pretrained=False)\n",
    "\n",
    "model.train(df['processed_text'].apply(lambda x: ' '.join(x)).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b57e472e-2ecd-43ed-beee-30b161762915",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'<PAD>': 21157,\n",
       "         's': 6199,\n",
       "         'said': 4763,\n",
       "         'mr': 1750,\n",
       "         'year': 1697,\n",
       "         'new': 1350,\n",
       "         'people': 1138,\n",
       "         'time': 862,\n",
       "         'world': 821,\n",
       "         'uk': 801,\n",
       "         't': 762,\n",
       "         'government': 709,\n",
       "         'film': 685,\n",
       "         '1': 677,\n",
       "         'years': 648,\n",
       "         'told': 648,\n",
       "         'best': 647,\n",
       "         '000': 572,\n",
       "         'game': 569,\n",
       "         'number': 561,\n",
       "         '2': 550,\n",
       "         'bbc': 543,\n",
       "         'labour': 531,\n",
       "         'set': 511,\n",
       "         '6': 499,\n",
       "         'added': 485,\n",
       "         'england': 479,\n",
       "         'company': 478,\n",
       "         'firm': 478,\n",
       "         '2004': 478,\n",
       "         'music': 472,\n",
       "         'like': 469,\n",
       "         'win': 460,\n",
       "         'market': 460,\n",
       "         'second': 450,\n",
       "         '3': 445,\n",
       "         'week': 443,\n",
       "         'election': 440,\n",
       "         'won': 439,\n",
       "         'home': 432,\n",
       "         'says': 424,\n",
       "         'way': 419,\n",
       "         '4': 408,\n",
       "         'blair': 407,\n",
       "         'million': 404,\n",
       "         '5': 388,\n",
       "         'party': 388,\n",
       "         'sales': 375,\n",
       "         '10': 375,\n",
       "         'expected': 373,\n",
       "         'play': 372,\n",
       "         'games': 372,\n",
       "         'good': 372,\n",
       "         'work': 371,\n",
       "         'minister': 368,\n",
       "         'high': 367,\n",
       "         'british': 363,\n",
       "         'group': 363,\n",
       "         'london': 357,\n",
       "         'old': 353,\n",
       "         'players': 352,\n",
       "         'news': 349,\n",
       "         'use': 347,\n",
       "         'european': 346,\n",
       "         'going': 344,\n",
       "         'country': 343,\n",
       "         'half': 341,\n",
       "         'growth': 341,\n",
       "         'plans': 339,\n",
       "         'chief': 337,\n",
       "         'end': 336,\n",
       "         'public': 334,\n",
       "         'mobile': 329,\n",
       "         'economy': 323,\n",
       "         'months': 318,\n",
       "         'deal': 314,\n",
       "         'day': 313,\n",
       "         'month': 313,\n",
       "         'tv': 312,\n",
       "         'technology': 311,\n",
       "         'want': 310,\n",
       "         'think': 310,\n",
       "         'director': 307,\n",
       "         'including': 305,\n",
       "         'team': 302,\n",
       "         'international': 302,\n",
       "         'record': 301,\n",
       "         '7': 301,\n",
       "         'firms': 301,\n",
       "         'brown': 301,\n",
       "         'britain': 300,\n",
       "         'come': 299,\n",
       "         '2005': 298,\n",
       "         'according': 295,\n",
       "         'report': 294,\n",
       "         'net': 293,\n",
       "         'place': 286,\n",
       "         'bank': 284,\n",
       "         'based': 284,\n",
       "         '2003': 284,\n",
       "         'open': 283,\n",
       "         'hit': 281,\n",
       "         'club': 278,\n",
       "         'long': 277,\n",
       "         'users': 274,\n",
       "         'wales': 272,\n",
       "         'france': 272,\n",
       "         'final': 269,\n",
       "         'service': 265,\n",
       "         'help': 265,\n",
       "         'companies': 264,\n",
       "         'm': 264,\n",
       "         'star': 264,\n",
       "         'took': 262,\n",
       "         'called': 261,\n",
       "         'economic': 261,\n",
       "         'big': 257,\n",
       "         'general': 254,\n",
       "         'business': 253,\n",
       "         'europe': 253,\n",
       "         'office': 253,\n",
       "         'december': 253,\n",
       "         'howard': 251,\n",
       "         'came': 251,\n",
       "         'found': 251,\n",
       "         'state': 250,\n",
       "         'money': 248,\n",
       "         'phone': 247,\n",
       "         'life': 247,\n",
       "         'industry': 247,\n",
       "         'court': 247,\n",
       "         'don': 245,\n",
       "         'system': 244,\n",
       "         'united': 244,\n",
       "         'decision': 243,\n",
       "         'tax': 243,\n",
       "         'oil': 243,\n",
       "         'software': 242,\n",
       "         'computer': 242,\n",
       "         'future': 241,\n",
       "         'digital': 241,\n",
       "         'cup': 241,\n",
       "         'great': 240,\n",
       "         'services': 239,\n",
       "         'got': 239,\n",
       "         'awards': 236,\n",
       "         'despite': 236,\n",
       "         'january': 235,\n",
       "         'figures': 234,\n",
       "         'match': 233,\n",
       "         'nations': 233,\n",
       "         'biggest': 232,\n",
       "         'executive': 232,\n",
       "         'law': 231,\n",
       "         'radio': 229,\n",
       "         'start': 229,\n",
       "         'need': 229,\n",
       "         'shares': 228,\n",
       "         'president': 227,\n",
       "         'man': 225,\n",
       "         'right': 224,\n",
       "         'saying': 222,\n",
       "         'offer': 222,\n",
       "         'know': 221,\n",
       "         'china': 221,\n",
       "         'information': 221,\n",
       "         'prices': 220,\n",
       "         'video': 219,\n",
       "         'earlier': 219,\n",
       "         'ireland': 219,\n",
       "         'early': 218,\n",
       "         'run': 217,\n",
       "         'action': 217,\n",
       "         'data': 215,\n",
       "         'prime': 213,\n",
       "         '0': 213,\n",
       "         'given': 213,\n",
       "         'rise': 213,\n",
       "         'season': 212,\n",
       "         'internet': 212,\n",
       "         'lost': 210,\n",
       "         'security': 210,\n",
       "         'foreign': 210,\n",
       "         'award': 209,\n",
       "         'playing': 208,\n",
       "         'eu': 208,\n",
       "         've': 207,\n",
       "         'campaign': 207,\n",
       "         'line': 207,\n",
       "         'sunday': 206,\n",
       "         'secretary': 205,\n",
       "         '12': 205,\n",
       "         'times': 205,\n",
       "         'national': 204,\n",
       "         'quarter': 204,\n",
       "         'strong': 204,\n",
       "         'spokesman': 204,\n",
       "         'left': 203,\n",
       "         'michael': 203,\n",
       "         '20': 203,\n",
       "         'ahead': 203,\n",
       "         'played': 202,\n",
       "         'cut': 202,\n",
       "         'player': 202,\n",
       "         'online': 202,\n",
       "         'following': 201,\n",
       "         'return': 199,\n",
       "         'days': 199,\n",
       "         '8': 198,\n",
       "         'making': 197,\n",
       "         'taking': 197,\n",
       "         'anti': 197,\n",
       "         'injury': 196,\n",
       "         'able': 196,\n",
       "         'sold': 196,\n",
       "         'scotland': 195,\n",
       "         'legal': 195,\n",
       "         'later': 194,\n",
       "         'role': 194,\n",
       "         'coach': 194,\n",
       "         'better': 193,\n",
       "         'leader': 193,\n",
       "         'budget': 193,\n",
       "         'research': 193,\n",
       "         'microsoft': 193,\n",
       "         'house': 192,\n",
       "         'real': 192,\n",
       "         'look': 192,\n",
       "         'tory': 190,\n",
       "         'seen': 190,\n",
       "         'face': 189,\n",
       "         'dollar': 189,\n",
       "         'past': 189,\n",
       "         'analysts': 189,\n",
       "         'bill': 188,\n",
       "         'media': 188,\n",
       "         'financial': 187,\n",
       "         'held': 187,\n",
       "         'films': 187,\n",
       "         'went': 187,\n",
       "         'trade': 187,\n",
       "         'far': 186,\n",
       "         'recent': 186,\n",
       "         'series': 186,\n",
       "         'lot': 185,\n",
       "         'spending': 184,\n",
       "         'cost': 183,\n",
       "         'winning': 183,\n",
       "         'latest': 183,\n",
       "         'away': 182,\n",
       "         'lord': 182,\n",
       "         'countries': 182,\n",
       "         'interest': 182,\n",
       "         '30': 181,\n",
       "         'o': 181,\n",
       "         'major': 180,\n",
       "         'likely': 180,\n",
       "         'children': 180,\n",
       "         'statement': 180,\n",
       "         'wednesday': 179,\n",
       "         'south': 179,\n",
       "         'band': 179,\n",
       "         'david': 178,\n",
       "         'announced': 178,\n",
       "         'rugby': 177,\n",
       "         'john': 176,\n",
       "         'thought': 176,\n",
       "         'bid': 175,\n",
       "         'police': 175,\n",
       "         'pay': 174,\n",
       "         '9': 173,\n",
       "         'e': 173,\n",
       "         'include': 173,\n",
       "         'chancellor': 173,\n",
       "         'chelsea': 172,\n",
       "         'tony': 172,\n",
       "         'sale': 172,\n",
       "         'chairman': 172,\n",
       "         'hard': 172,\n",
       "         '11': 172,\n",
       "         'meeting': 172,\n",
       "         'february': 171,\n",
       "         'looking': 170,\n",
       "         'head': 170,\n",
       "         'programme': 168,\n",
       "         'case': 168,\n",
       "         'american': 168,\n",
       "         'manager': 168,\n",
       "         '50': 167,\n",
       "         'website': 167,\n",
       "         'low': 166,\n",
       "         'tuesday': 166,\n",
       "         'issue': 166,\n",
       "         'current': 166,\n",
       "         'increase': 166,\n",
       "         'india': 166,\n",
       "         'tories': 165,\n",
       "         'victory': 165,\n",
       "         'performance': 165,\n",
       "         'change': 165,\n",
       "         'released': 165,\n",
       "         'access': 165,\n",
       "         'share': 165,\n",
       "         'saturday': 164,\n",
       "         '18': 164,\n",
       "         'friday': 164,\n",
       "         'local': 164,\n",
       "         'monday': 163,\n",
       "         'demand': 163,\n",
       "         'support': 163,\n",
       "         'japan': 162,\n",
       "         'march': 162,\n",
       "         'working': 162,\n",
       "         'ms': 162,\n",
       "         'key': 161,\n",
       "         'women': 161,\n",
       "         'free': 161,\n",
       "         'weeks': 160,\n",
       "         'little': 160,\n",
       "         'currently': 160,\n",
       "         'warned': 159,\n",
       "         'title': 159,\n",
       "         'important': 159,\n",
       "         'jobs': 159,\n",
       "         'claims': 159,\n",
       "         'broadband': 158,\n",
       "         'fans': 158,\n",
       "         'ago': 157,\n",
       "         'taken': 157,\n",
       "         'rate': 157,\n",
       "         'union': 156,\n",
       "         'job': 156,\n",
       "         'members': 154,\n",
       "         'french': 154,\n",
       "         '100': 153,\n",
       "         'album': 152,\n",
       "         'having': 151,\n",
       "         'believe': 151,\n",
       "         'control': 151,\n",
       "         'stock': 151,\n",
       "         'mark': 150,\n",
       "         'list': 149,\n",
       "         'beat': 149,\n",
       "         'november': 149,\n",
       "         'power': 149,\n",
       "         'total': 148,\n",
       "         'plan': 148,\n",
       "         'try': 147,\n",
       "         'profits': 147,\n",
       "         'lead': 147,\n",
       "         'league': 146,\n",
       "         'costs': 146,\n",
       "         'problems': 145,\n",
       "         'find': 145,\n",
       "         'christmas': 144,\n",
       "         'reported': 144,\n",
       "         'actor': 144,\n",
       "         'boss': 143,\n",
       "         'asked': 143,\n",
       "         'rose': 143,\n",
       "         'career': 143,\n",
       "         '15': 142,\n",
       "         'chance': 142,\n",
       "         'mail': 141,\n",
       "         'network': 140,\n",
       "         'minutes': 140,\n",
       "         'close': 140,\n",
       "         'wanted': 140,\n",
       "         'possible': 140,\n",
       "         'form': 139,\n",
       "         'needed': 139,\n",
       "         'war': 139,\n",
       "         'yukos': 139,\n",
       "         'centre': 138,\n",
       "         '2001': 138,\n",
       "         '25': 138,\n",
       "         'rights': 138,\n",
       "         'men': 138,\n",
       "         'mps': 138,\n",
       "         'sony': 138,\n",
       "         'clear': 137,\n",
       "         'search': 137,\n",
       "         'annual': 136,\n",
       "         'price': 136,\n",
       "         'australian': 135,\n",
       "         'york': 135,\n",
       "         'continue': 135,\n",
       "         'sport': 135,\n",
       "         'committee': 135,\n",
       "         'sir': 135,\n",
       "         'act': 135,\n",
       "         'consumer': 134,\n",
       "         'giant': 134,\n",
       "         'main': 134,\n",
       "         'race': 134,\n",
       "         'reports': 134,\n",
       "         'thursday': 134,\n",
       "         'event': 134,\n",
       "         'result': 134,\n",
       "         'saw': 133,\n",
       "         'buy': 133,\n",
       "         'football': 133,\n",
       "         'vote': 133,\n",
       "         'singer': 133,\n",
       "         'rates': 133,\n",
       "         'williams': 133,\n",
       "         'previous': 132,\n",
       "         'personal': 131,\n",
       "         'young': 131,\n",
       "         'things': 131,\n",
       "         'coming': 131,\n",
       "         'different': 130,\n",
       "         'forward': 130,\n",
       "         'prize': 130,\n",
       "         'higher': 130,\n",
       "         'term': 130,\n",
       "         'led': 130,\n",
       "         'fourth': 129,\n",
       "         'political': 129,\n",
       "         'running': 129,\n",
       "         'investment': 129,\n",
       "         'policy': 129,\n",
       "         'champion': 129,\n",
       "         'city': 128,\n",
       "         'problem': 128,\n",
       "         'known': 128,\n",
       "         'commission': 128,\n",
       "         'ministers': 128,\n",
       "         'oscar': 127,\n",
       "         'october': 127,\n",
       "         'festival': 127,\n",
       "         'boost': 127,\n",
       "         'development': 127,\n",
       "         'fell': 127,\n",
       "         'global': 127,\n",
       "         'allow': 126,\n",
       "         'weekend': 126,\n",
       "         'chart': 126,\n",
       "         'production': 126,\n",
       "         'site': 126,\n",
       "         'web': 126,\n",
       "         'book': 126,\n",
       "         'apple': 126,\n",
       "         'council': 126,\n",
       "         'release': 125,\n",
       "         'recently': 125,\n",
       "         '17': 125,\n",
       "         'tour': 125,\n",
       "         'available': 125,\n",
       "         'stars': 125,\n",
       "         'denied': 125,\n",
       "         '2002': 124,\n",
       "         'box': 124,\n",
       "         'wants': 124,\n",
       "         'point': 124,\n",
       "         'single': 124,\n",
       "         'phones': 124,\n",
       "         'showed': 124,\n",
       "         'movie': 123,\n",
       "         'cash': 123,\n",
       "         'russian': 123,\n",
       "         'live': 122,\n",
       "         'television': 122,\n",
       "         'failed': 122,\n",
       "         'competition': 121,\n",
       "         'level': 121,\n",
       "         'age': 121,\n",
       "         'rules': 121,\n",
       "         'official': 121,\n",
       "         'irish': 121,\n",
       "         'iraq': 121,\n",
       "         'trying': 120,\n",
       "         'meet': 120,\n",
       "         'aid': 120,\n",
       "         'board': 120,\n",
       "         'grand': 120,\n",
       "         'late': 119,\n",
       "         'september': 119,\n",
       "         'talks': 119,\n",
       "         'success': 119,\n",
       "         'hope': 118,\n",
       "         'agreed': 118,\n",
       "         'test': 118,\n",
       "         'italy': 118,\n",
       "         'arsenal': 117,\n",
       "         'hold': 117,\n",
       "         'comes': 117,\n",
       "         'issues': 117,\n",
       "         'short': 116,\n",
       "         '13': 116,\n",
       "         'health': 116,\n",
       "         'getting': 116,\n",
       "         'points': 116,\n",
       "         'received': 116,\n",
       "         'today': 115,\n",
       "         'accused': 115,\n",
       "         'break': 115,\n",
       "         'family': 115,\n",
       "         'goal': 115,\n",
       "         'virus': 115,\n",
       "         'charles': 115,\n",
       "         'euros': 115,\n",
       "         'evidence': 115,\n",
       "         'olympic': 115,\n",
       "         'networks': 114,\n",
       "         'growing': 114,\n",
       "         'calls': 114,\n",
       "         'huge': 114,\n",
       "         'sent': 114,\n",
       "         'lib': 114,\n",
       "         'means': 113,\n",
       "         'involved': 113,\n",
       "         'liverpool': 113,\n",
       "         'summer': 113,\n",
       "         'version': 113,\n",
       "         'customers': 113,\n",
       "         '24': 113,\n",
       "         'claim': 112,\n",
       "         'round': 112,\n",
       "         'german': 112,\n",
       "         'gordon': 112,\n",
       "         'project': 112,\n",
       "         'instead': 112,\n",
       "         'trial': 111,\n",
       "         'manchester': 111,\n",
       "         'shows': 111,\n",
       "         'launched': 111,\n",
       "         'consumers': 111,\n",
       "         'car': 111,\n",
       "         'products': 110,\n",
       "         'school': 110,\n",
       "         '16': 109,\n",
       "         'street': 109,\n",
       "         'fall': 109,\n",
       "         'association': 109,\n",
       "         'independent': 109,\n",
       "         'pre': 109,\n",
       "         'scottish': 109,\n",
       "         '14': 109,\n",
       "         'position': 108,\n",
       "         'conference': 108,\n",
       "         'opening': 108,\n",
       "         'compared': 108,\n",
       "         'seven': 108,\n",
       "         'launch': 108,\n",
       "         'actress': 108,\n",
       "         'paul': 108,\n",
       "         'dvd': 108,\n",
       "         'robinson': 107,\n",
       "         'co': 107,\n",
       "         'didn': 107,\n",
       "         'date': 106,\n",
       "         'largest': 106,\n",
       "         'exchange': 106,\n",
       "         'named': 106,\n",
       "         'survey': 106,\n",
       "         'special': 105,\n",
       "         'squad': 105,\n",
       "         'stage': 104,\n",
       "         'difficult': 104,\n",
       "         'jones': 104,\n",
       "         'hopes': 104,\n",
       "         'debt': 104,\n",
       "         'germany': 103,\n",
       "         'pressure': 103,\n",
       "         '2000': 103,\n",
       "         'rock': 103,\n",
       "         'changes': 103,\n",
       "         'sites': 103,\n",
       "         'pc': 103,\n",
       "         'leading': 102,\n",
       "         'admitted': 102,\n",
       "         'comedy': 102,\n",
       "         'parliament': 102,\n",
       "         'june': 102,\n",
       "         'stand': 101,\n",
       "         'feel': 101,\n",
       "         'fight': 101,\n",
       "         'popular': 101,\n",
       "         'drugs': 101,\n",
       "         'content': 100,\n",
       "         'confirmed': 100,\n",
       "         'mean': 100,\n",
       "         'order': 100,\n",
       "         'takes': 100,\n",
       "         'tough': 99,\n",
       "         'investors': 99,\n",
       "         'attacks': 99,\n",
       "         'large': 99,\n",
       "         'officials': 99,\n",
       "         'parties': 98,\n",
       "         'started': 98,\n",
       "         'gaming': 98,\n",
       "         'ban': 98,\n",
       "         'potential': 97,\n",
       "         '40': 97,\n",
       "         'agency': 97,\n",
       "         'thing': 97,\n",
       "         'voters': 97,\n",
       "         'row': 96,\n",
       "         'west': 96,\n",
       "         'commons': 96,\n",
       "         'senior': 96,\n",
       "         'night': 96,\n",
       "         'sector': 96,\n",
       "         'results': 96,\n",
       "         'host': 95,\n",
       "         'tsunami': 95,\n",
       "         'madrid': 95,\n",
       "         'period': 95,\n",
       "         'bt': 95,\n",
       "         'choice': 95,\n",
       "         'believes': 94,\n",
       "         'africa': 94,\n",
       "         'helped': 94,\n",
       "         'small': 94,\n",
       "         'paid': 94,\n",
       "         'debut': 94,\n",
       "         'gave': 94,\n",
       "         'outside': 94,\n",
       "         'andy': 93,\n",
       "         'poor': 93,\n",
       "         'needs': 93,\n",
       "         'education': 93,\n",
       "         'energy': 93,\n",
       "         'forced': 93,\n",
       "         'charge': 93,\n",
       "         'force': 93,\n",
       "         'immigration': 93,\n",
       "         'martin': 92,\n",
       "         'ensure': 92,\n",
       "         'euro': 92,\n",
       "         'create': 92,\n",
       "         'speaking': 92,\n",
       "         'claimed': 92,\n",
       "         'included': 92,\n",
       "         'stop': 92,\n",
       "         'll': 92,\n",
       "         'poll': 91,\n",
       "         'worked': 91,\n",
       "         'university': 91,\n",
       "         'charges': 91,\n",
       "         'trading': 91,\n",
       "         'bush': 91,\n",
       "         'concerns': 91,\n",
       "         'brought': 91,\n",
       "         'turn': 91,\n",
       "         'revealed': 91,\n",
       "         'department': 91,\n",
       "         'entertainment': 90,\n",
       "         'newspaper': 90,\n",
       "         'indian': 90,\n",
       "         'press': 90,\n",
       "         'unit': 90,\n",
       "         'fact': 90,\n",
       "         'range': 90,\n",
       "         'appeal': 90,\n",
       "         'sell': 90,\n",
       "         'captain': 89,\n",
       "         'speech': 89,\n",
       "         'training': 89,\n",
       "         'average': 89,\n",
       "         'conservative': 89,\n",
       "         'davis': 89,\n",
       "         'body': 89,\n",
       "         'fund': 89,\n",
       "         'makes': 89,\n",
       "         'shown': 89,\n",
       "         'love': 89,\n",
       "         'insisted': 89,\n",
       "         'idea': 89,\n",
       "         'signed': 89,\n",
       "         'google': 89,\n",
       "         'champions': 89,\n",
       "         'alan': 89,\n",
       "         'reached': 88,\n",
       "         'stay': 88,\n",
       "         'bought': 88,\n",
       "         'song': 88,\n",
       "         'kennedy': 88,\n",
       "         'fraud': 87,\n",
       "         'comments': 87,\n",
       "         'cards': 87,\n",
       "         'bit': 87,\n",
       "         'member': 87,\n",
       "         '22': 87,\n",
       "         'remain': 87,\n",
       "         'human': 87,\n",
       "         'cuts': 87,\n",
       "         'deutsche': 87,\n",
       "         'value': 86,\n",
       "         'let': 86,\n",
       "         'history': 86,\n",
       "         'selling': 86,\n",
       "         'staff': 86,\n",
       "         'similar': 86,\n",
       "         'johnson': 86,\n",
       "         'ball': 86,\n",
       "         'speed': 86,\n",
       "         'double': 86,\n",
       "         'critics': 86,\n",
       "         'mike': 86,\n",
       "         'sure': 85,\n",
       "         'premiership': 85,\n",
       "         'profit': 85,\n",
       "         'musical': 85,\n",
       "         'details': 85,\n",
       "         'story': 85,\n",
       "         'spam': 85,\n",
       "         'increased': 84,\n",
       "         'newcastle': 84,\n",
       "         'soon': 84,\n",
       "         'spain': 84,\n",
       "         'agreement': 84,\n",
       "         'spent': 84,\n",
       "         'battle': 84,\n",
       "         'wrong': 84,\n",
       "         'authorities': 84,\n",
       "         'defence': 83,\n",
       "         'cross': 83,\n",
       "         'april': 83,\n",
       "         'original': 83,\n",
       "         'america': 83,\n",
       "         'winner': 83,\n",
       "         'smith': 83,\n",
       "         'ray': 83,\n",
       "         'turned': 83,\n",
       "         'widely': 83,\n",
       "         'markets': 83,\n",
       "         'hollywood': 83,\n",
       "         'bring': 83,\n",
       "         'gold': 83,\n",
       "         'favourite': 82,\n",
       "         'devices': 82,\n",
       "         'focus': 82,\n",
       "         'previously': 82,\n",
       "         'central': 82,\n",
       "         'opportunity': 82,\n",
       "         'created': 82,\n",
       "         'nominations': 82,\n",
       "         'decided': 82,\n",
       "         'windows': 82,\n",
       "         '19': 81,\n",
       "         'opposition': 81,\n",
       "         'proposals': 81,\n",
       "         '2006': 81,\n",
       "         'rival': 81,\n",
       "         'mp': 81,\n",
       "         'card': 81,\n",
       "         'nominated': 81,\n",
       "         'lords': 81,\n",
       "         'raise': 81,\n",
       "         'longer': 81,\n",
       "         'private': 81,\n",
       "         'robert': 81,\n",
       "         'standard': 81,\n",
       "         'russia': 81,\n",
       "         'athens': 81,\n",
       "         'situation': 80,\n",
       "         'finance': 80,\n",
       "         'defeat': 80,\n",
       "         'richard': 80,\n",
       "         'd': 80,\n",
       "         'pcs': 80,\n",
       "         'ceremony': 80,\n",
       "         'james': 80,\n",
       "         'community': 80,\n",
       "         'provide': 80,\n",
       "         'drive': 80,\n",
       "         'fox': 80,\n",
       "         'post': 80,\n",
       "         'impact': 79,\n",
       "         'analyst': 79,\n",
       "         'bad': 79,\n",
       "         'laws': 79,\n",
       "         'civil': 79,\n",
       "         'nearly': 79,\n",
       "         'exports': 79,\n",
       "         'groups': 79,\n",
       "         'workers': 79,\n",
       "         'liberal': 79,\n",
       "         'taxes': 79,\n",
       "         'non': 78,\n",
       "         'attack': 78,\n",
       "         'seed': 78,\n",
       "         'australia': 78,\n",
       "         'states': 78,\n",
       "         '27': 78,\n",
       "         'credit': 78,\n",
       "         'millions': 78,\n",
       "         'messages': 78,\n",
       "         'experience': 78,\n",
       "         'began': 78,\n",
       "         'positive': 78,\n",
       "         'north': 77,\n",
       "         'debate': 77,\n",
       "         'shot': 77,\n",
       "         'moved': 77,\n",
       "         'challenge': 77,\n",
       "         'websites': 77,\n",
       "         'hand': 77,\n",
       "         'extra': 77,\n",
       "         'area': 77,\n",
       "         'drama': 77,\n",
       "         'air': 77,\n",
       "         'hits': 76,\n",
       "         'followed': 76,\n",
       "         'fuel': 76,\n",
       "         'loss': 76,\n",
       "         'message': 76,\n",
       "         'audience': 76,\n",
       "         'confidence': 76,\n",
       "         'target': 76,\n",
       "         'dems': 76,\n",
       "         '5m': 76,\n",
       "         'contract': 76,\n",
       "         'remains': 76,\n",
       "         'banks': 76,\n",
       "         'particularly': 75,\n",
       "         'step': 75,\n",
       "         'crime': 75,\n",
       "         'happy': 75,\n",
       "         'dropped': 75,\n",
       "         'rising': 75,\n",
       "         'rest': 75,\n",
       "         'lives': 75,\n",
       "         'minute': 75,\n",
       "         'numbers': 75,\n",
       "         'faces': 75,\n",
       "         'dr': 75,\n",
       "         'confident': 75,\n",
       "         'keen': 75,\n",
       "         'winners': 75,\n",
       "         'response': 75,\n",
       "         'process': 74,\n",
       "         'ready': 74,\n",
       "         'white': 74,\n",
       "         'august': 74,\n",
       "         'domestic': 74,\n",
       "         '5bn': 74,\n",
       "         'japanese': 74,\n",
       "         'aviator': 74,\n",
       "         'download': 74,\n",
       "         'conservatives': 74,\n",
       "         'ferguson': 74,\n",
       "         'criticised': 74,\n",
       "         'experts': 74,\n",
       "         'mobiles': 74,\n",
       "         'looked': 73,\n",
       "         'maker': 73,\n",
       "         'singles': 73,\n",
       "         'met': 73,\n",
       "         'concerned': 73,\n",
       "         'generation': 73,\n",
       "         'magazine': 73,\n",
       "         'attempt': 73,\n",
       "         'de': 73,\n",
       "         'offered': 73,\n",
       "         'worth': 73,\n",
       "         'track': 73,\n",
       "         'scheme': 73,\n",
       "         'picture': 73,\n",
       "         'gadget': 73,\n",
       "         'programs': 73,\n",
       "         'guilty': 72,\n",
       "         'course': 72,\n",
       "         'lower': 72,\n",
       "         'politics': 72,\n",
       "         'cabinet': 72,\n",
       "         'reach': 72,\n",
       "         'management': 72,\n",
       "         'program': 72,\n",
       "         'black': 72,\n",
       "         'pair': 72,\n",
       "         'slam': 72,\n",
       "         'deficit': 72,\n",
       "         'view': 72,\n",
       "         'build': 72,\n",
       "         'leave': 72,\n",
       "         'peer': 72,\n",
       "         'screen': 72,\n",
       "         'channel': 71,\n",
       "         'moment': 71,\n",
       "         'believed': 71,\n",
       "         'controversial': 71,\n",
       "         'warning': 71,\n",
       "         'accounts': 71,\n",
       "         'device': 71,\n",
       "         '28': 71,\n",
       "         'facing': 71,\n",
       "         'majority': 71,\n",
       "         'july': 71,\n",
       "         'wins': 71,\n",
       "         'hours': 71,\n",
       "         'spend': 71,\n",
       "         'study': 71,\n",
       "         'blunkett': 71,\n",
       "         'park': 70,\n",
       "         'gone': 70,\n",
       "         'terror': 70,\n",
       "         'death': 70,\n",
       "         'died': 70,\n",
       "         'english': 70,\n",
       "         'thousands': 70,\n",
       "         'trust': 70,\n",
       "         'areas': 70,\n",
       "         'social': 70,\n",
       "         'operating': 70,\n",
       "         'unveiled': 70,\n",
       "         'rejected': 69,\n",
       "         'ask': 69,\n",
       "         'kept': 69,\n",
       "         'airline': 69,\n",
       "         'rule': 69,\n",
       "         'federal': 69,\n",
       "         'tournament': 69,\n",
       "         'ruled': 69,\n",
       "         'producer': 69,\n",
       "         'fast': 69,\n",
       "         'indoor': 69,\n",
       "         'figure': 69,\n",
       "         'joined': 69,\n",
       "         'looks': 69,\n",
       "         'retail': 69,\n",
       "         'pop': 69,\n",
       "         'file': 69,\n",
       "         'written': 69,\n",
       "         'aimed': 69,\n",
       "         'files': 69,\n",
       "         'computers': 69,\n",
       "         'systems': 68,\n",
       "         'starring': 68,\n",
       "         'expect': 68,\n",
       "         'missed': 68,\n",
       "         '21': 68,\n",
       "         'asia': 68,\n",
       "         'bankruptcy': 68,\n",
       "         'argued': 68,\n",
       "         'giving': 68,\n",
       "         'simply': 68,\n",
       "         'organisation': 68,\n",
       "         'campbell': 68,\n",
       "         'sullivan': 67,\n",
       "         'peter': 67,\n",
       "         'fit': 67,\n",
       "         'east': 67,\n",
       "         'society': 67,\n",
       "         'takeover': 67,\n",
       "         'suspended': 67,\n",
       "         'measures': 67,\n",
       "         'stake': 67,\n",
       "         'light': 67,\n",
       "         '26': 67,\n",
       "         'planned': 67,\n",
       "         'highest': 67,\n",
       "         'recovery': 67,\n",
       "         'easy': 67,\n",
       "         'felt': 67,\n",
       "         'gm': 67,\n",
       "         'straw': 67,\n",
       "         'matter': 66,\n",
       "         'planning': 66,\n",
       "         'woman': 66,\n",
       "         'judge': 66,\n",
       "         'cases': 66,\n",
       "         'schools': 66,\n",
       "         'suggested': 66,\n",
       "         'compete': 66,\n",
       "         'present': 66,\n",
       "         'terms': 65,\n",
       "         'ended': 65,\n",
       "         'parents': 65,\n",
       "         'allowed': 65,\n",
       "         'movies': 65,\n",
       "         'talk': 65,\n",
       "         'question': 65,\n",
       "         'tests': 65,\n",
       "         'titles': 65,\n",
       "         'records': 65,\n",
       "         'ex': 65,\n",
       "         'happened': 65,\n",
       "         'hour': 65,\n",
       "         'viewers': 64,\n",
       "         'theatre': 64,\n",
       "         'raised': 64,\n",
       "         'risk': 64,\n",
       "         '60': 64,\n",
       "         '31': 64,\n",
       "         'missing': 64,\n",
       "         'effort': 64,\n",
       "         'kelly': 64,\n",
       "         'receive': 64,\n",
       "         'paris': 64,\n",
       "         'appeared': 64,\n",
       "         'technologies': 63,\n",
       "         'built': 63,\n",
       "         'george': 63,\n",
       "         'edinburgh': 63,\n",
       "         'capital': 63,\n",
       "         'appear': 63,\n",
       "         ...})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    " \n",
    "Counter(' '.join(df['processed_text'].apply(lambda x: ' '.join(x)).tolist()).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b90a4ba-1370-4c3e-bf50-bd50056351c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fc6f95-bfa3-4ca3-bf7c-0cd5f7fbf3be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578b022e-e518-4267-a517-75271a6667af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a737c1d-3452-409f-9440-ca5164d4d6b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
