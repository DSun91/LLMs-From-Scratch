{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9b3f3e9",
   "metadata": {},
   "source": [
    "# Multi-Head Attention from Scratch Using CuPy\n",
    "\n",
    "In this notebook, we will implement the **Multi-Head Attention** mechanism from scratch using **CuPy**, a GPU-accelerated library similar to NumPy. The multi-head attention mechanism is a key component of the Transformer model, enabling it to attend to different parts of the input simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Overview of Multi-Head Attention\n",
    "\n",
    "Multi-head attention allows the model to have multiple attention \"heads,\" each of which focuses on different parts of the input sequence. Each head computes its own attention values, and the results are concatenated and transformed into the final output.\n",
    "\n",
    "The steps involved in multi-head attention:\n",
    "- **Linear transformations**: Apply learned weight matrices to the queries (Q), keys (K), and values (V).\n",
    "- **Scaled Dot-Product Attention**: For each head, compute the attention scores and apply them to the values.\n",
    "- **Concatenation**: Concatenate the outputs from all heads.\n",
    "- **Final Linear Transformation**: Apply a final linear transformation to the concatenated output.\n",
    "\n",
    "Mathematically, the output of the multi-head attention mechanism can be written as:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h)W^O\n",
    "$$\n",
    "where each attention head is computed as:\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$$\n",
    "---\n",
    "\n",
    "## 2. CuPy Setup\n",
    "\n",
    "Before we begin, make sure you have **CuPy** installed. You can install it via:\n",
    "\n",
    "```bash\n",
    "!pip install cupy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "db372a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 7, 10), (10, 10), (10, 10), (10, 10))"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "import numpy as np\n",
    "import re\n",
    "import cupy as cp\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np \n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jax\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "np.set_printoptions(edgeitems=30, linewidth=100000, formatter=dict(float=lambda x: \"%.3g\" % x)) \n",
    "def softmax(x, axis=-1):\n",
    "    # Subtract the max value for numerical stability\n",
    "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "num_classes=2\n",
    "word2vec_len = 10\n",
    "num_phrases = 3\n",
    "words_per_phrase = 7 \n",
    "dk = dv = 10\n",
    " \n",
    "num_heads=5\n",
    " \n",
    " \n",
    "inputs = np.random.rand(num_phrases,words_per_phrase, word2vec_len)\n",
    "target = softmax(np.random.rand(num_phrases,num_classes))\n",
    "\n",
    "Q = np.random.rand(word2vec_len, dk) / jnp.sqrt(word2vec_len)\n",
    "K = np.random.rand(word2vec_len, dk) / jnp.sqrt(word2vec_len)\n",
    "V = np.random.rand(word2vec_len, dv) / jnp.sqrt(word2vec_len)\n",
    "inputs.shape,Q.shape,K.shape,V.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26957032",
   "metadata": {},
   "source": [
    "### Consideriamo input formato da 3 frasi composte da 7 parole ciasuna ed ogni parola avente rappresentazione vettoriale di dimensione 10. Mentre vogliamo dopo l'attentione che ogni parola abbia rappresentazione vettoriale di 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "25dd2618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.218, 0.563, 0.196, 0.275, 0.568, 0.155, 0.426, 0.0049, 0.0325, 0.463],\n",
       "        [0.762, 0.183, 0.711, 0.974, 0.514, 0.945, 0.548, 0.534, 0.942, 0.313],\n",
       "        [0.999, 0.963, 0.784, 0.742, 0.302, 0.408, 0.382, 0.0891, 0.324, 0.414],\n",
       "        [0.35, 0.131, 0.818, 0.348, 0.508, 0.908, 0.41, 0.463, 0.907, 0.862],\n",
       "        [0.224, 0.312, 0.908, 0.26, 0.129, 0.0262, 0.0881, 0.778, 0.285, 0.946],\n",
       "        [0.611, 0.243, 0.21, 0.748, 0.684, 0.506, 0.42, 0.107, 0.7, 0.41],\n",
       "        [0.552, 0.159, 0.151, 0.51, 0.923, 0.934, 0.609, 0.809, 0.26, 0.0536]],\n",
       "\n",
       "       [[0.993, 0.179, 0.283, 0.982, 0.213, 0.898, 0.028, 0.295, 0.166, 0.795],\n",
       "        [0.297, 0.493, 0.0927, 0.986, 0.307, 0.0541, 0.404, 0.766, 0.0191, 0.981],\n",
       "        [0.24, 0.603, 0.928, 0.829, 0.719, 0.502, 0.742, 0.161, 0.611, 0.717],\n",
       "        [0.26, 0.547, 0.817, 0.995, 0.393, 0.628, 0.731, 0.446, 0.959, 0.918],\n",
       "        [0.641, 0.0695, 0.112, 0.784, 0.725, 0.718, 0.537, 0.0113, 0.916, 0.151],\n",
       "        [0.885, 0.336, 0.727, 0.314, 0.367, 0.152, 0.96, 0.774, 0.756, 0.0637],\n",
       "        [0.296, 0.924, 0.495, 0.444, 0.762, 0.218, 0.322, 0.335, 0.716, 0.757]],\n",
       "\n",
       "       [[0.887, 0.45, 0.738, 0.831, 0.0355, 0.357, 0.677, 0.494, 0.472, 0.709],\n",
       "        [0.107, 0.158, 0.563, 0.962, 0.173, 0.764, 0.335, 0.0469, 0.408, 0.526],\n",
       "        [0.506, 0.688, 0.019, 0.769, 0.0799, 0.896, 0.344, 0.656, 0.828, 0.45],\n",
       "        [0.821, 0.984, 0.222, 0.414, 0.941, 0.595, 0.601, 0.245, 0.839, 0.187],\n",
       "        [0.268, 0.377, 0.964, 0.779, 0.347, 0.827, 0.186, 0.974, 0.732, 0.488],\n",
       "        [0.995, 0.528, 0.856, 0.847, 0.968, 0.728, 0.116, 0.19, 0.293, 0.991],\n",
       "        [0.437, 0.813, 0.947, 0.208, 0.478, 0.267, 0.421, 0.93, 0.61, 0.181]]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs# each input phrase is made by 13 words having lenght 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de56657a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 7, 10), (10, 10))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape,Q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed92a667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[0.742, 0.839, 1.04, 0.927, 0.653, 0.579, 0.895, 0.808, 0.76, 0.755],\n",
       "        [0.661, 0.85, 1.2, 1.07, 0.849, 0.989, 0.633, 0.935, 0.788, 0.848],\n",
       "        [0.767, 0.645, 0.993, 0.99, 0.681, 0.824, 0.681, 0.756, 0.646, 0.9],\n",
       "        [1.02, 0.98, 1.34, 1.23, 1.12, 0.881, 1.03, 1.11, 0.822, 0.981],\n",
       "        [0.44, 0.514, 0.836, 0.715, 0.603, 0.705, 0.245, 0.771, 0.615, 0.659],\n",
       "        [0.563, 0.576, 0.978, 0.86, 0.807, 0.795, 0.698, 0.909, 0.614, 0.734],\n",
       "        [0.683, 0.659, 0.841, 0.931, 0.723, 0.691, 0.644, 0.795, 0.641, 0.546]],\n",
       "\n",
       "       [[0.664, 0.694, 0.943, 0.828, 0.772, 0.796, 0.384, 0.777, 0.553, 0.596],\n",
       "        [0.724, 0.785, 1.18, 1.05, 0.86, 0.949, 0.803, 1.07, 0.771, 0.818],\n",
       "        [0.928, 0.805, 1.13, 0.974, 0.959, 0.852, 0.681, 0.969, 0.696, 0.776],\n",
       "        [0.943, 0.821, 1.2, 1.01, 0.958, 0.905, 0.731, 1.07, 0.738, 0.881],\n",
       "        [0.987, 0.949, 1.42, 1.24, 0.987, 1.03, 0.98, 1.14, 0.911, 1.09],\n",
       "        [0.464, 0.583, 0.907, 0.601, 0.468, 0.562, 0.527, 0.733, 0.618, 0.734],\n",
       "        [0.818, 0.956, 1.19, 1.09, 0.881, 0.755, 0.866, 1.03, 0.953, 0.807]],\n",
       "\n",
       "       [[0.626, 0.639, 0.911, 0.861, 0.752, 0.672, 0.588, 0.701, 0.585, 0.668],\n",
       "        [0.374, 0.508, 0.821, 0.538, 0.616, 0.595, 0.311, 0.655, 0.457, 0.599],\n",
       "        [0.775, 0.743, 1.15, 0.954, 0.919, 0.876, 0.806, 0.996, 0.78, 0.887],\n",
       "        [0.871, 0.815, 1.19, 1.14, 1.03, 0.987, 0.703, 1.06, 0.751, 0.792],\n",
       "        [0.456, 0.459, 0.579, 0.597, 0.48, 0.487, 0.346, 0.514, 0.458, 0.411],\n",
       "        [0.792, 0.782, 1.13, 1.05, 0.896, 0.882, 0.676, 1.01, 0.796, 0.874],\n",
       "        [0.753, 0.926, 1.14, 1.01, 0.898, 0.713, 0.905, 0.906, 0.736, 0.724]]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.matmul(inputs, Q) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fedbac",
   "metadata": {},
   "source": [
    "### Avendo fissato il numero di teste per il attezione ogni matrice Qval, Kval, Vval viene suddivisa in 4 parti uguali di 2 colonne. Otteniamo un array di 4 elementi che per ogni frase riportano le prime due colonne come di seguito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eef75667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jnp.array_split(jnp.matmul(inputs, Q),num_heads,axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d76673db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[0.742, 0.839],\n",
       "        [0.661, 0.85],\n",
       "        [0.767, 0.645],\n",
       "        [1.02, 0.98],\n",
       "        [0.44, 0.514],\n",
       "        [0.563, 0.576],\n",
       "        [0.683, 0.659]],\n",
       "\n",
       "       [[0.664, 0.694],\n",
       "        [0.724, 0.785],\n",
       "        [0.928, 0.805],\n",
       "        [0.943, 0.821],\n",
       "        [0.987, 0.949],\n",
       "        [0.464, 0.583],\n",
       "        [0.818, 0.956]],\n",
       "\n",
       "       [[0.626, 0.639],\n",
       "        [0.374, 0.508],\n",
       "        [0.775, 0.743],\n",
       "        [0.871, 0.815],\n",
       "        [0.456, 0.459],\n",
       "        [0.792, 0.782],\n",
       "        [0.753, 0.926]]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.array_split(jnp.matmul(inputs, Q),num_heads,axis=2)[0]# so i have basically num_heads chuncks of the Qval this is a list not array structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b35421",
   "metadata": {},
   "source": [
    "### Ridimensioniamo l'array in modo che ogni frase contenga la lista dei rispettivi attention heads, ottenendo 3 frasi contententi 4 attention heads che hanno dimensione 7 (come il numero di parole per ogni frase) per 2 (fetta di embedding assegnata ad ogni head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34aa11cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5, 7, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs, Q),num_heads,axis=2)), 0, 1).shape#  # here i actually transform it to a structure Qval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd7464e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[0.742, 0.839],\n",
       "        [0.661, 0.85],\n",
       "        [0.767, 0.645],\n",
       "        [1.02, 0.98],\n",
       "        [0.44, 0.514],\n",
       "        [0.563, 0.576],\n",
       "        [0.683, 0.659]],\n",
       "\n",
       "       [[1.04, 0.927],\n",
       "        [1.2, 1.07],\n",
       "        [0.993, 0.99],\n",
       "        [1.34, 1.23],\n",
       "        [0.836, 0.715],\n",
       "        [0.978, 0.86],\n",
       "        [0.841, 0.931]],\n",
       "\n",
       "       [[0.653, 0.579],\n",
       "        [0.849, 0.989],\n",
       "        [0.681, 0.824],\n",
       "        [1.12, 0.881],\n",
       "        [0.603, 0.705],\n",
       "        [0.807, 0.795],\n",
       "        [0.723, 0.691]],\n",
       "\n",
       "       [[0.895, 0.808],\n",
       "        [0.633, 0.935],\n",
       "        [0.681, 0.756],\n",
       "        [1.03, 1.11],\n",
       "        [0.245, 0.771],\n",
       "        [0.698, 0.909],\n",
       "        [0.644, 0.795]],\n",
       "\n",
       "       [[0.76, 0.755],\n",
       "        [0.788, 0.848],\n",
       "        [0.646, 0.9],\n",
       "        [0.822, 0.981],\n",
       "        [0.615, 0.659],\n",
       "        [0.614, 0.734],\n",
       "        [0.641, 0.546]]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs, Q),num_heads,axis=2)), 0, 1)[0]# refer to cell jnp.matmul(inputs, Q) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60adc1fc-9393-47c0-a181-ed799c4d3b0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qval.shape:  (3, 5, 7, 2)\n",
      "Kval.shape:  (3, 5, 7, 2)\n",
      "Vval.shape:  (3, 5, 7, 2)\n"
     ]
    }
   ],
   "source": [
    "Qval = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs, Q),num_heads,axis=2)), 0, 1)\n",
    "print(\"Qval.shape: \",Qval.shape)\n",
    "\n",
    "Kval = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs, K),num_heads,axis=2)), 0, 1)\n",
    "print(\"Kval.shape: \",Kval.shape)\n",
    "\n",
    "\n",
    "Vval = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs,V),num_heads,axis=2)), 0, 1)\n",
    "print(\"Vval.shape: \",Vval.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b98a1d",
   "metadata": {},
   "source": [
    "### Per calcolare ora i pesi dell'attenzione applichiamo la formua \n",
    "\n",
    "$$\n",
    "  \\frac{QK^T}{\\sqrt{d_k}}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6c38c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[0.742, 0.839],\n",
       "        [0.661, 0.85],\n",
       "        [0.767, 0.645],\n",
       "        [1.02, 0.98],\n",
       "        [0.44, 0.514],\n",
       "        [0.563, 0.576],\n",
       "        [0.683, 0.659]], dtype=float32),\n",
       " Array([[0.847, 0.728],\n",
       "        [1.05, 0.979],\n",
       "        [0.868, 0.849],\n",
       "        [1.23, 1.22],\n",
       "        [0.807, 0.543],\n",
       "        [1, 0.869],\n",
       "        [0.731, 0.734]], dtype=float32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qval[0][0],Kval[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e02d8827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[0.742, 0.839],\n",
       "        [0.661, 0.85],\n",
       "        [0.767, 0.645],\n",
       "        [1.02, 0.98],\n",
       "        [0.44, 0.514],\n",
       "        [0.563, 0.576],\n",
       "        [0.683, 0.659]], dtype=float32),\n",
       " Array([[0.847, 1.05, 0.868, 1.23, 0.807, 1, 0.731],\n",
       "        [0.728, 0.979, 0.849, 1.22, 0.543, 0.869, 0.734]], dtype=float32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qval[0][0],np.transpose(Kval, (0, 1, 3, 2))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44bbb725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.392, 0.506, 0.429, 0.611, 0.333, 0.466, 0.366],\n",
       "       [0.373, 0.483, 0.41, 0.584, 0.315, 0.443, 0.35],\n",
       "       [0.354, 0.455, 0.384, 0.546, 0.307, 0.421, 0.327],\n",
       "       [0.498, 0.642, 0.542, 0.772, 0.428, 0.592, 0.463],\n",
       "       [0.236, 0.306, 0.259, 0.369, 0.201, 0.281, 0.221],\n",
       "       [0.283, 0.365, 0.309, 0.44, 0.242, 0.337, 0.264],\n",
       "       [0.335, 0.431, 0.364, 0.519, 0.288, 0.398, 0.311]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qval[0][0]@jnp.transpose(Kval, (0, 1, 3, 2))[0][0]/ jnp.sqrt(dk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35bec325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5, 7, 7)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QKscaled = jnp.matmul(Qval, jnp.transpose(Kval, (0, 1, 3, 2))) / jnp.sqrt(dk)\n",
    " \n",
    "QKscaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14f39e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.392, 0.506, 0.429, 0.611, 0.333, 0.466, 0.366],\n",
       "       [0.373, 0.483, 0.41, 0.584, 0.315, 0.443, 0.35],\n",
       "       [0.354, 0.455, 0.384, 0.546, 0.307, 0.421, 0.327],\n",
       "       [0.498, 0.642, 0.542, 0.772, 0.428, 0.592, 0.463],\n",
       "       [0.236, 0.306, 0.259, 0.369, 0.201, 0.281, 0.221],\n",
       "       [0.283, 0.365, 0.309, 0.44, 0.242, 0.337, 0.264],\n",
       "       [0.335, 0.431, 0.364, 0.519, 0.288, 0.398, 0.311]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QKscaled[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d505c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention_weights shape: (3, 5, 7, 7)\n"
     ]
    }
   ],
   "source": [
    "Attention_weights = softmax(QKscaled)\n",
    "print(\"Attention_weights shape:\",Attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52a8b7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention shape: (3, 5, 7, 2)\n"
     ]
    }
   ],
   "source": [
    "Attention = jnp.matmul(Attention_weights, Vval)\n",
    "print(\"Attention shape:\",Attention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ec19e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[0.959, 0.954],\n",
       "        [0.958, 0.954],\n",
       "        [0.956, 0.953],\n",
       "        [0.963, 0.958],\n",
       "        [0.952, 0.949],\n",
       "        [0.954, 0.95],\n",
       "        [0.956, 0.952]],\n",
       "\n",
       "       [[0.736, 0.842],\n",
       "        [0.738, 0.845],\n",
       "        [0.736, 0.843],\n",
       "        [0.739, 0.847],\n",
       "        [0.734, 0.839],\n",
       "        [0.736, 0.842],\n",
       "        [0.735, 0.841]],\n",
       "\n",
       "       [[0.996, 0.671],\n",
       "        [0.998, 0.672],\n",
       "        [0.997, 0.672],\n",
       "        [0.999, 0.673],\n",
       "        [0.996, 0.671],\n",
       "        [0.997, 0.672],\n",
       "        [0.997, 0.671]],\n",
       "\n",
       "       [[0.586, 0.927],\n",
       "        [0.585, 0.927],\n",
       "        [0.585, 0.926],\n",
       "        [0.588, 0.93],\n",
       "        [0.583, 0.923],\n",
       "        [0.585, 0.927],\n",
       "        [0.585, 0.926]],\n",
       "\n",
       "       [[0.678, 0.897],\n",
       "        [0.679, 0.898],\n",
       "        [0.678, 0.896],\n",
       "        [0.679, 0.899],\n",
       "        [0.678, 0.895],\n",
       "        [0.678, 0.895],\n",
       "        [0.677, 0.894]]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10742435",
   "metadata": {},
   "source": [
    "### Now for tetrieving the attention correct size we need to horizontaly concatenate the attention output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c3a9217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[[0.959, 0.954, 0.736, 0.842, 0.996, 0.671, 0.586, 0.927, 0.678, 0.897],\n",
       "         [0.958, 0.954, 0.738, 0.845, 0.998, 0.672, 0.585, 0.927, 0.679, 0.898],\n",
       "         [0.956, 0.953, 0.736, 0.843, 0.997, 0.672, 0.585, 0.926, 0.678, 0.896],\n",
       "         [0.963, 0.958, 0.739, 0.847, 0.999, 0.673, 0.588, 0.93, 0.679, 0.899],\n",
       "         [0.952, 0.949, 0.734, 0.839, 0.996, 0.671, 0.583, 0.923, 0.678, 0.895],\n",
       "         [0.954, 0.95, 0.736, 0.842, 0.997, 0.672, 0.585, 0.927, 0.678, 0.895],\n",
       "         [0.956, 0.952, 0.735, 0.841, 0.997, 0.671, 0.585, 0.926, 0.677, 0.894]],\n",
       " \n",
       "        [[1.02, 1.06, 0.787, 0.83, 1.09, 0.707, 0.661, 0.974, 0.779, 0.996],\n",
       "         [1.02, 1.06, 0.789, 0.834, 1.09, 0.708, 0.664, 0.978, 0.781, 0.998],\n",
       "         [1.02, 1.06, 0.789, 0.833, 1.09, 0.708, 0.663, 0.977, 0.78, 0.998],\n",
       "         [1.02, 1.06, 0.789, 0.834, 1.09, 0.708, 0.663, 0.978, 0.781, 0.998],\n",
       "         [1.02, 1.07, 0.791, 0.837, 1.09, 0.708, 0.664, 0.979, 0.782, 1],\n",
       "         [1.02, 1.06, 0.786, 0.828, 1.08, 0.706, 0.662, 0.974, 0.78, 0.997],\n",
       "         [1.02, 1.06, 0.79, 0.834, 1.09, 0.707, 0.664, 0.978, 0.781, 0.999]],\n",
       " \n",
       "        [[0.907, 0.929, 0.743, 0.772, 0.941, 0.671, 0.562, 0.888, 0.691, 0.852],\n",
       "         [0.902, 0.925, 0.74, 0.768, 0.939, 0.67, 0.56, 0.885, 0.69, 0.85],\n",
       "         [0.91, 0.932, 0.746, 0.775, 0.944, 0.673, 0.565, 0.893, 0.694, 0.856],\n",
       "         [0.911, 0.934, 0.747, 0.777, 0.946, 0.675, 0.565, 0.893, 0.693, 0.855],\n",
       "         [0.903, 0.925, 0.738, 0.766, 0.936, 0.668, 0.56, 0.884, 0.689, 0.848],\n",
       "         [0.91, 0.933, 0.746, 0.776, 0.944, 0.673, 0.565, 0.892, 0.694, 0.856],\n",
       "         [0.911, 0.934, 0.746, 0.776, 0.942, 0.672, 0.565, 0.892, 0.692, 0.854]]], dtype=float32),\n",
       " (3, 7, 10),\n",
       " (3, 7, 10))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention=jnp.array([jnp.concatenate(Attention[i], axis=1) for i in range(num_phrases)])\n",
    "Attention,Attention.shape,inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb48e5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 10, 10), (3, 1, 10))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linearlayer= np.random.rand(num_phrases,dv, word2vec_len)   \n",
    "linear_bias = np.random.rand(num_phrases,1,word2vec_len)\n",
    "linearlayer.shape,linear_bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bd1665b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.149, 0.965, 0.688, 0.605, 0.158, 0.303, 0.201, 0.959, 0.635, 0.277]],\n",
       "\n",
       "       [[0.0505, 0.2, 0.491, 0.996, 0.123, 0.332, 0.941, 0.714, 0.573, 0.0514]],\n",
       "\n",
       "       [[0.49, 0.273, 0.311, 0.749, 0.808, 0.119, 0.587, 0.472, 0.206, 0.71]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1bebf84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[1.37, -1.24, -0.374, 0.015, -0.713, 0.942, -1.28, 0.597, -0.849, 1.53],\n",
       "        [1.66, 1.28, 1.02, -0.0108, -0.612, -1.48, -0.235, -1.21, -0.647, 0.237],\n",
       "        [0.201, -0.88, 1.31, 1.53, -0.0492, -0.848, 0.789, -0.75, -1.74, 0.437],\n",
       "        [0.406, -0.243, -1.89, 0.863, 0.165, -1.44, -0.0345, 1.41, -0.353, 1.12],\n",
       "        [-0.502, 0.561, 1.41, -0.722, 1.15, -1, -1.54, 0.0923, 1.29, -0.735],\n",
       "        [0.313, 1.08, -0.648, 1.51, 0.494, -1.31, -1.89, -0.363, 0.255, 0.561],\n",
       "        [0.537, 0.639, 0.987, -0.541, -0.1, -1.06, -1.14, 1.39, -1.66, 0.955]],\n",
       "\n",
       "       [[0.842, 1.43, 0.318, -1.15, 0.988, -1.57, 0.821, 0.0974, -1.14, -0.643],\n",
       "        [1.23, 1.21, 0.241, 0.842, 0.565, -1.21, -1.94, -0.278, -0.769, 0.116],\n",
       "        [-0.277, 1.09, -0.493, -0.871, 1.69, -0.454, 0.702, 0.704, -1.91, -0.187],\n",
       "        [-0.864, 0.762, -0.412, 0.307, 1.79, -0.417, 0.0304, 1.34, -1.24, -1.3],\n",
       "        [1.35, -0.467, 0.158, 0.316, 0.889, -0.273, -0.296, -1.86, -1.22, 1.4],\n",
       "        [0.283, -0.315, -0.144, 0.476, 1.11, 1.13, -2.14, -0.907, 1.13, -0.626],\n",
       "        [0.053, 0.173, 0.425, -1.48, -0.521, 0.551, -1.93, 1.33, 0.16, 1.23]],\n",
       "\n",
       "       [[1.23, 0.26, -0.118, -0.724, 0.128, -1.47, 0.0475, -1.09, -0.368, 2.1],\n",
       "        [0.219, 1.42, -1.06, 0.092, 1.05, -1.03, -0.771, -0.68, 1.66, -0.901],\n",
       "        [-1.37, 1.53, -0.402, 1.42, 0.136, 0.687, -1.64, -0.00297, -0.605, 0.249],\n",
       "        [1.05, 1.03, 0.423, -0.934, 1.21, -1.72, -0.274, -0.511, -1.13, 0.866],\n",
       "        [-0.615, 1.07, 1.45, -0.768, -0.315, -0.463, -0.723, 1.83, -1.25, -0.218],\n",
       "        [-1.25, 0.588, 1.12, 0.24, 0.331, -1.05, -1.34, 1.79, 0.258, -0.686],\n",
       "        [1.25, 0.775, -1.56, 0.25, -1.29, -0.693, -0.596, 1.44, -0.379, 0.79]]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def layer_norm(x, epsilon=1e-6):\n",
    "    # Calculate the mean and variance\n",
    "        mean = jnp.mean(x, axis=-1, keepdims=True)\n",
    "        var = jnp.var(x, axis=-1, keepdims=True) \n",
    "        # Normalize the output\n",
    "        x_norm = (x - mean) / jnp.sqrt(var + epsilon) \n",
    "        return x_norm\n",
    "\n",
    "\n",
    "#output_sublayer_one=layer_norm((Attention@linearlayer +linear_bias)+inputs)\n",
    "output_sublayer_one=layer_norm(Attention+inputs)\n",
    "output_sublayer_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac1a52a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 7, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sublayer_one.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c41521b",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c028923",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_word_embedding_size=10\n",
    "decoder_input_number_of_words_per_phrase=9\n",
    "num_heads_decoder=5# dv=10 \n",
    "dv_decoder=10\n",
    "inputs_decoder = np.random.rand(num_phrases,decoder_input_number_of_words_per_phrase, decoder_input_word_embedding_size)# for the target language suppose the \n",
    "target_decoder = inputs_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcdc4273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.519, 0.655, 0.631, 0.843, 0.515, 0.233, 0.721, 0.719, 0.754, 0.287],\n",
       "        [0.298, 0.412, 0.777, 0.535, 0.0521, 0.413, 0.804, 0.803, 0.689, 0.6],\n",
       "        [0.705, 0.191, 0.392, 0.681, 0.416, 0.42, 0.734, 0.14, 0.159, 0.259],\n",
       "        [0.763, 0.423, 0.091, 0.569, 0.328, 0.198, 0.768, 0.434, 0.678, 0.899],\n",
       "        [0.165, 0.115, 0.582, 0.133, 0.693, 0.453, 0.243, 0.962, 0.457, 0.185],\n",
       "        [0.267, 0.0362, 0.547, 0.0591, 0.605, 0.792, 0.415, 0.299, 0.986, 0.6],\n",
       "        [0.425, 0.335, 0.154, 0.462, 0.905, 0.116, 0.595, 0.893, 0.7, 0.437],\n",
       "        [0.476, 0.295, 0.0113, 0.51, 0.949, 0.671, 0.707, 0.229, 0.652, 0.348],\n",
       "        [0.31, 0.0342, 0.732, 0.503, 0.824, 0.907, 0.793, 0.277, 0.686, 0.418]],\n",
       "\n",
       "       [[0.706, 0.76, 0.217, 0.297, 0.938, 0.124, 0.461, 0.0958, 0.584, 0.281],\n",
       "        [0.842, 0.261, 0.798, 0.453, 0.259, 0.844, 0.217, 0.543, 0.392, 0.149],\n",
       "        [0.922, 0.227, 0.287, 0.778, 0.393, 0.94, 0.199, 0.305, 0.82, 0.842],\n",
       "        [0.818, 0.673, 0.232, 0.867, 0.196, 0.603, 0.518, 0.897, 0.93, 0.104],\n",
       "        [0.483, 0.841, 0.0414, 0.286, 0.361, 0.347, 0.354, 0.962, 0.711, 0.911],\n",
       "        [0.995, 0.151, 0.902, 0.658, 0.654, 0.274, 0.731, 0.561, 0.429, 0.541],\n",
       "        [0.25, 0.92, 0.709, 0.0121, 0.407, 0.523, 0.533, 0.0563, 0.936, 0.0357],\n",
       "        [0.607, 0.967, 0.872, 0.261, 0.601, 0.336, 0.502, 0.592, 0.326, 0.211],\n",
       "        [0.671, 0.0263, 0.277, 0.388, 0.578, 0.959, 0.537, 0.857, 0.582, 0.219]],\n",
       "\n",
       "       [[0.0336, 0.815, 0.493, 0.561, 0.69, 0.44, 0.13, 0.195, 0.33, 0.492],\n",
       "        [0.617, 0.65, 0.199, 0.0799, 0.394, 0.808, 0.226, 0.392, 0.85, 0.868],\n",
       "        [0.172, 0.0728, 0.0955, 0.574, 0.0192, 0.862, 0.476, 0.645, 0.329, 0.695],\n",
       "        [0.331, 0.962, 0.122, 0.856, 0.101, 0.616, 0.616, 0.43, 0.14, 0.932],\n",
       "        [0.463, 0.109, 0.843, 0.139, 0.231, 0.999, 0.567, 0.0678, 0.622, 0.522],\n",
       "        [0.693, 0.347, 0.378, 0.248, 0.208, 0.426, 0.627, 0.326, 0.0658, 0.534],\n",
       "        [0.275, 0.569, 0.393, 0.782, 0.196, 0.925, 0.939, 0.751, 0.333, 0.83],\n",
       "        [0.0969, 0.967, 0.42, 0.43, 0.828, 0.719, 0.428, 0.223, 0.103, 0.518],\n",
       "        [0.852, 0.138, 0.946, 0.315, 0.516, 0.0162, 0.351, 0.702, 0.815, 0.806]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49030921",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "input_translation=[]\n",
    "def pad_sequence(seq, max_len, pad_value=0):\n",
    "    \"\"\"Pad a sequence with a given value up to max_len.\"\"\"\n",
    "    current_len = seq.shape[0]\n",
    "    pad_width = max_len - current_len\n",
    "    if pad_width > 0:\n",
    "        # Pad sequence with zeros (or any pad_value you provide)\n",
    "        seq = jnp.pad(seq, ((0, pad_width), (0, 0)), mode='constant', constant_values=pad_value)\n",
    "    return seq\n",
    "\n",
    "# Example usage:\n",
    "max_len = decoder_input_number_of_words_per_phrase  # Max sequence length for decoder input\n",
    " \n",
    "for j in range(inputs_decoder.shape[0]):\n",
    "    # Create padded sequences\n",
    "    padded_sequences = [pad_sequence(inputs_decoder[j][0:i], max_len) for i in range(1, inputs_decoder.shape[1] + 1)]\n",
    "    input_translation.append(padded_sequences)\n",
    "\n",
    "\n",
    "# Convert to an array for batching\n",
    "input_translation = jnp.array(input_translation)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70947e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 9, 9, 10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_translation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04e7fbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[0.519, 0.655, 0.631, 0.843, 0.515, 0.233, 0.721, 0.719, 0.754, 0.287],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0.519, 0.655, 0.631, 0.843, 0.515, 0.233, 0.721, 0.719, 0.754, 0.287],\n",
       "        [0.298, 0.412, 0.777, 0.535, 0.0521, 0.413, 0.804, 0.803, 0.689, 0.6],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0.519, 0.655, 0.631, 0.843, 0.515, 0.233, 0.721, 0.719, 0.754, 0.287],\n",
       "        [0.298, 0.412, 0.777, 0.535, 0.0521, 0.413, 0.804, 0.803, 0.689, 0.6],\n",
       "        [0.705, 0.191, 0.392, 0.681, 0.416, 0.42, 0.734, 0.14, 0.159, 0.259],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0.519, 0.655, 0.631, 0.843, 0.515, 0.233, 0.721, 0.719, 0.754, 0.287],\n",
       "        [0.298, 0.412, 0.777, 0.535, 0.0521, 0.413, 0.804, 0.803, 0.689, 0.6],\n",
       "        [0.705, 0.191, 0.392, 0.681, 0.416, 0.42, 0.734, 0.14, 0.159, 0.259],\n",
       "        [0.763, 0.423, 0.091, 0.569, 0.328, 0.198, 0.768, 0.434, 0.678, 0.899],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0.519, 0.655, 0.631, 0.843, 0.515, 0.233, 0.721, 0.719, 0.754, 0.287],\n",
       "        [0.298, 0.412, 0.777, 0.535, 0.0521, 0.413, 0.804, 0.803, 0.689, 0.6],\n",
       "        [0.705, 0.191, 0.392, 0.681, 0.416, 0.42, 0.734, 0.14, 0.159, 0.259],\n",
       "        [0.763, 0.423, 0.091, 0.569, 0.328, 0.198, 0.768, 0.434, 0.678, 0.899],\n",
       "        [0.165, 0.115, 0.582, 0.133, 0.693, 0.453, 0.243, 0.962, 0.457, 0.185],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0.519, 0.655, 0.631, 0.843, 0.515, 0.233, 0.721, 0.719, 0.754, 0.287],\n",
       "        [0.298, 0.412, 0.777, 0.535, 0.0521, 0.413, 0.804, 0.803, 0.689, 0.6],\n",
       "        [0.705, 0.191, 0.392, 0.681, 0.416, 0.42, 0.734, 0.14, 0.159, 0.259],\n",
       "        [0.763, 0.423, 0.091, 0.569, 0.328, 0.198, 0.768, 0.434, 0.678, 0.899],\n",
       "        [0.165, 0.115, 0.582, 0.133, 0.693, 0.453, 0.243, 0.962, 0.457, 0.185],\n",
       "        [0.267, 0.0362, 0.547, 0.0591, 0.605, 0.792, 0.415, 0.299, 0.986, 0.6],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0.519, 0.655, 0.631, 0.843, 0.515, 0.233, 0.721, 0.719, 0.754, 0.287],\n",
       "        [0.298, 0.412, 0.777, 0.535, 0.0521, 0.413, 0.804, 0.803, 0.689, 0.6],\n",
       "        [0.705, 0.191, 0.392, 0.681, 0.416, 0.42, 0.734, 0.14, 0.159, 0.259],\n",
       "        [0.763, 0.423, 0.091, 0.569, 0.328, 0.198, 0.768, 0.434, 0.678, 0.899],\n",
       "        [0.165, 0.115, 0.582, 0.133, 0.693, 0.453, 0.243, 0.962, 0.457, 0.185],\n",
       "        [0.267, 0.0362, 0.547, 0.0591, 0.605, 0.792, 0.415, 0.299, 0.986, 0.6],\n",
       "        [0.425, 0.335, 0.154, 0.462, 0.905, 0.116, 0.595, 0.893, 0.7, 0.437],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0.519, 0.655, 0.631, 0.843, 0.515, 0.233, 0.721, 0.719, 0.754, 0.287],\n",
       "        [0.298, 0.412, 0.777, 0.535, 0.0521, 0.413, 0.804, 0.803, 0.689, 0.6],\n",
       "        [0.705, 0.191, 0.392, 0.681, 0.416, 0.42, 0.734, 0.14, 0.159, 0.259],\n",
       "        [0.763, 0.423, 0.091, 0.569, 0.328, 0.198, 0.768, 0.434, 0.678, 0.899],\n",
       "        [0.165, 0.115, 0.582, 0.133, 0.693, 0.453, 0.243, 0.962, 0.457, 0.185],\n",
       "        [0.267, 0.0362, 0.547, 0.0591, 0.605, 0.792, 0.415, 0.299, 0.986, 0.6],\n",
       "        [0.425, 0.335, 0.154, 0.462, 0.905, 0.116, 0.595, 0.893, 0.7, 0.437],\n",
       "        [0.476, 0.295, 0.0113, 0.51, 0.949, 0.671, 0.707, 0.229, 0.652, 0.348],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0.519, 0.655, 0.631, 0.843, 0.515, 0.233, 0.721, 0.719, 0.754, 0.287],\n",
       "        [0.298, 0.412, 0.777, 0.535, 0.0521, 0.413, 0.804, 0.803, 0.689, 0.6],\n",
       "        [0.705, 0.191, 0.392, 0.681, 0.416, 0.42, 0.734, 0.14, 0.159, 0.259],\n",
       "        [0.763, 0.423, 0.091, 0.569, 0.328, 0.198, 0.768, 0.434, 0.678, 0.899],\n",
       "        [0.165, 0.115, 0.582, 0.133, 0.693, 0.453, 0.243, 0.962, 0.457, 0.185],\n",
       "        [0.267, 0.0362, 0.547, 0.0591, 0.605, 0.792, 0.415, 0.299, 0.986, 0.6],\n",
       "        [0.425, 0.335, 0.154, 0.462, 0.905, 0.116, 0.595, 0.893, 0.7, 0.437],\n",
       "        [0.476, 0.295, 0.0113, 0.51, 0.949, 0.671, 0.707, 0.229, 0.652, 0.348],\n",
       "        [0.31, 0.0342, 0.732, 0.503, 0.824, 0.907, 0.793, 0.277, 0.686, 0.418]]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_translation[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ae7156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26a41656",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_decoder=input_translation[0]\n",
    "\n",
    "Q_decoder = np.random.rand(decoder_input_word_embedding_size, dv_decoder) / jnp.sqrt(decoder_input_word_embedding_size)\n",
    "K_decoder = np.random.rand(decoder_input_word_embedding_size, dv_decoder) / jnp.sqrt(decoder_input_word_embedding_size)\n",
    "V_decoder = np.random.rand(decoder_input_word_embedding_size, dv_decoder) / jnp.sqrt(decoder_input_word_embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ec1676b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_decoder.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60a0ed3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qval.shape:  (9, 9, 10)\n"
     ]
    }
   ],
   "source": [
    "Qval_decoder=input_decoder@Q_decoder\n",
    "print(\"Qval.shape: \",Qval_decoder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5fed93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qval.shape:  (9, 5, 9, 2)\n",
      "Kval.shape:  (9, 5, 9, 2)\n",
      "Vval.shape:  (9, 5, 9, 2)\n"
     ]
    }
   ],
   "source": [
    "Qval_decoder  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(input_decoder, Q_decoder),num_heads_decoder,axis=2)), 0, 1)\n",
    "print(\"Qval.shape: \",Qval_decoder.shape)\n",
    "\n",
    "Kval_decoder  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(input_decoder, K_decoder),num_heads_decoder,axis=2)), 0, 1)\n",
    "print(\"Kval.shape: \",Kval_decoder.shape)\n",
    "\n",
    "\n",
    "Vval_decoder  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(input_decoder,V_decoder),num_heads_decoder,axis=2)), 0, 1)\n",
    "print(\"Vval.shape: \",Vval_decoder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2df93660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[-1e+09, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.445, -1e+09, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.304, 0.282, -1e+09, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.455, 0.418, 0.299, -1e+09, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, -1e+09, -inf, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, -1e+09, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, -1e+09, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, 0, -1e+09, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, -1e+09]],\n",
       "\n",
       "       [[-1e+09, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.533, -1e+09, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.352, 0.336, -1e+09, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.399, 0.383, 0.323, -1e+09, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, -1e+09, -inf, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, -1e+09, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, -1e+09, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, 0, -1e+09, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, -1e+09]],\n",
       "\n",
       "       [[-1e+09, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.478, -1e+09, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.366, 0.327, -1e+09, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.43, 0.38, 0.312, -1e+09, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, -1e+09, -inf, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, -1e+09, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, -1e+09, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, 0, -1e+09, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, -1e+09]],\n",
       "\n",
       "       [[-1e+09, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.626, -1e+09, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.462, 0.404, -1e+09, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.625, 0.547, 0.427, -1e+09, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, -1e+09, -inf, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, -1e+09, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, -1e+09, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, 0, -1e+09, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, -1e+09]],\n",
       "\n",
       "       [[-1e+09, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.682, -1e+09, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.511, 0.461, -1e+09, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.672, 0.607, 0.406, -1e+09, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, -1e+09, -inf, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, -1e+09, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, -1e+09, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, 0, -1e+09, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, -1e+09]]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QKscaled_decoder  = jnp.matmul(Qval_decoder, jnp.transpose(Kval_decoder, (0, 1, 3, 2))) / jnp.sqrt(dk) + jnp.triu(jnp.ones((9, 9)))* -1e9 \n",
    "# Step 1: Create a causal mask of shape (1, 1, 9, 9) to broadcast across heads and batch\n",
    "mask = jnp.tril(jnp.ones((max_len, max_len)))  # (9, 9) lower triangular matrix\n",
    "mask = mask.at[mask == 0].set(-jnp.inf)  # Set future tokens to -inf\n",
    "mask = mask.at[mask == 1].set(0)  # Set allowed tokens to 0\n",
    "mask = mask.reshape(1, 1, max_len, max_len)  # Reshape to (1, 1, 9, 9)\n",
    "\n",
    "# Step 2: Apply mask to QKscaled_decoder (it will broadcast across batch and heads)\n",
    "QKscaled_decoder = QKscaled_decoder + mask \n",
    "QKscaled_decoder[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d5e0fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 5, 9, 9)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention_weights = softmax(QKscaled_decoder)\n",
    "Attention_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3feab27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[[0.939, 1.07],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.929, 1.18],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[1.08, 0.805],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.825, 0.794],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.681, 1.27],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0.939, 1.07],\n",
       "         [0.859, 1.08],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.929, 1.18],\n",
       "         [0.895, 1.01],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[1.08, 0.805],\n",
       "         [1.06, 0.918],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.825, 0.794],\n",
       "         [0.745, 0.611],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.681, 1.27],\n",
       "         [0.609, 1.23],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0.939, 1.07],\n",
       "         [0.859, 1.08],\n",
       "         [0.714, 0.877],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.929, 1.18],\n",
       "         [0.895, 1.01],\n",
       "         [0.66, 0.827],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[1.08, 0.805],\n",
       "         [1.06, 0.918],\n",
       "         [0.73, 0.546],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.825, 0.794],\n",
       "         [0.745, 0.611],\n",
       "         [0.598, 0.588],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.681, 1.27],\n",
       "         [0.609, 1.23],\n",
       "         [0.433, 0.999],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0.939, 1.07],\n",
       "         [0.859, 1.08],\n",
       "         [0.714, 0.877],\n",
       "         [0.859, 1.02],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.929, 1.18],\n",
       "         [0.895, 1.01],\n",
       "         [0.66, 0.827],\n",
       "         [0.864, 0.973],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[1.08, 0.805],\n",
       "         [1.06, 0.918],\n",
       "         [0.73, 0.546],\n",
       "         [0.864, 0.763],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.825, 0.794],\n",
       "         [0.745, 0.611],\n",
       "         [0.598, 0.588],\n",
       "         [0.606, 0.603],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.681, 1.27],\n",
       "         [0.609, 1.23],\n",
       "         [0.433, 0.999],\n",
       "         [0.478, 1.16],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0.939, 1.07],\n",
       "         [0.859, 1.08],\n",
       "         [0.714, 0.877],\n",
       "         [0.859, 1.02],\n",
       "         [0.588, 0.746],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.929, 1.18],\n",
       "         [0.895, 1.01],\n",
       "         [0.66, 0.827],\n",
       "         [0.864, 0.973],\n",
       "         [0.685, 0.869],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[1.08, 0.805],\n",
       "         [1.06, 0.918],\n",
       "         [0.73, 0.546],\n",
       "         [0.864, 0.763],\n",
       "         [0.836, 0.666],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.825, 0.794],\n",
       "         [0.745, 0.611],\n",
       "         [0.598, 0.588],\n",
       "         [0.606, 0.603],\n",
       "         [0.514, 0.425],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.681, 1.27],\n",
       "         [0.609, 1.23],\n",
       "         [0.433, 0.999],\n",
       "         [0.478, 1.16],\n",
       "         [0.585, 0.93],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0.939, 1.07],\n",
       "         [0.859, 1.08],\n",
       "         [0.714, 0.877],\n",
       "         [0.859, 1.02],\n",
       "         [0.588, 0.746],\n",
       "         [0.576, 0.808],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.929, 1.18],\n",
       "         [0.895, 1.01],\n",
       "         [0.66, 0.827],\n",
       "         [0.864, 0.973],\n",
       "         [0.685, 0.869],\n",
       "         [0.683, 0.886],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[1.08, 0.805],\n",
       "         [1.06, 0.918],\n",
       "         [0.73, 0.546],\n",
       "         [0.864, 0.763],\n",
       "         [0.836, 0.666],\n",
       "         [0.84, 0.879],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.825, 0.794],\n",
       "         [0.745, 0.611],\n",
       "         [0.598, 0.588],\n",
       "         [0.606, 0.603],\n",
       "         [0.514, 0.425],\n",
       "         [0.55, 0.413],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.681, 1.27],\n",
       "         [0.609, 1.23],\n",
       "         [0.433, 0.999],\n",
       "         [0.478, 1.16],\n",
       "         [0.585, 0.93],\n",
       "         [0.445, 1],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0.939, 1.07],\n",
       "         [0.859, 1.08],\n",
       "         [0.714, 0.877],\n",
       "         [0.859, 1.02],\n",
       "         [0.588, 0.746],\n",
       "         [0.576, 0.808],\n",
       "         [0.793, 0.905],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.929, 1.18],\n",
       "         [0.895, 1.01],\n",
       "         [0.66, 0.827],\n",
       "         [0.864, 0.973],\n",
       "         [0.685, 0.869],\n",
       "         [0.683, 0.886],\n",
       "         [0.882, 1.12],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[1.08, 0.805],\n",
       "         [1.06, 0.918],\n",
       "         [0.73, 0.546],\n",
       "         [0.864, 0.763],\n",
       "         [0.836, 0.666],\n",
       "         [0.84, 0.879],\n",
       "         [0.887, 0.725],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.825, 0.794],\n",
       "         [0.745, 0.611],\n",
       "         [0.598, 0.588],\n",
       "         [0.606, 0.603],\n",
       "         [0.514, 0.425],\n",
       "         [0.55, 0.413],\n",
       "         [0.636, 0.682],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.681, 1.27],\n",
       "         [0.609, 1.23],\n",
       "         [0.433, 0.999],\n",
       "         [0.478, 1.16],\n",
       "         [0.585, 0.93],\n",
       "         [0.445, 1],\n",
       "         [0.64, 1.12],\n",
       "         [0, 0],\n",
       "         [0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0.939, 1.07],\n",
       "         [0.859, 1.08],\n",
       "         [0.714, 0.877],\n",
       "         [0.859, 1.02],\n",
       "         [0.588, 0.746],\n",
       "         [0.576, 0.808],\n",
       "         [0.793, 0.905],\n",
       "         [0.713, 0.884],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.929, 1.18],\n",
       "         [0.895, 1.01],\n",
       "         [0.66, 0.827],\n",
       "         [0.864, 0.973],\n",
       "         [0.685, 0.869],\n",
       "         [0.683, 0.886],\n",
       "         [0.882, 1.12],\n",
       "         [0.756, 1.11],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[1.08, 0.805],\n",
       "         [1.06, 0.918],\n",
       "         [0.73, 0.546],\n",
       "         [0.864, 0.763],\n",
       "         [0.836, 0.666],\n",
       "         [0.84, 0.879],\n",
       "         [0.887, 0.725],\n",
       "         [0.809, 0.729],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.825, 0.794],\n",
       "         [0.745, 0.611],\n",
       "         [0.598, 0.588],\n",
       "         [0.606, 0.603],\n",
       "         [0.514, 0.425],\n",
       "         [0.55, 0.413],\n",
       "         [0.636, 0.682],\n",
       "         [0.615, 0.68],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.681, 1.27],\n",
       "         [0.609, 1.23],\n",
       "         [0.433, 0.999],\n",
       "         [0.478, 1.16],\n",
       "         [0.585, 0.93],\n",
       "         [0.445, 1],\n",
       "         [0.64, 1.12],\n",
       "         [0.475, 1.06],\n",
       "         [0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0.939, 1.07],\n",
       "         [0.859, 1.08],\n",
       "         [0.714, 0.877],\n",
       "         [0.859, 1.02],\n",
       "         [0.588, 0.746],\n",
       "         [0.576, 0.808],\n",
       "         [0.793, 0.905],\n",
       "         [0.713, 0.884],\n",
       "         [0.76, 1.06]],\n",
       "\n",
       "        [[0.929, 1.18],\n",
       "         [0.895, 1.01],\n",
       "         [0.66, 0.827],\n",
       "         [0.864, 0.973],\n",
       "         [0.685, 0.869],\n",
       "         [0.683, 0.886],\n",
       "         [0.882, 1.12],\n",
       "         [0.756, 1.11],\n",
       "         [0.861, 1.15]],\n",
       "\n",
       "        [[1.08, 0.805],\n",
       "         [1.06, 0.918],\n",
       "         [0.73, 0.546],\n",
       "         [0.864, 0.763],\n",
       "         [0.836, 0.666],\n",
       "         [0.84, 0.879],\n",
       "         [0.887, 0.725],\n",
       "         [0.809, 0.729],\n",
       "         [0.988, 0.939]],\n",
       "\n",
       "        [[0.825, 0.794],\n",
       "         [0.745, 0.611],\n",
       "         [0.598, 0.588],\n",
       "         [0.606, 0.603],\n",
       "         [0.514, 0.425],\n",
       "         [0.55, 0.413],\n",
       "         [0.636, 0.682],\n",
       "         [0.615, 0.68],\n",
       "         [0.785, 0.7]],\n",
       "\n",
       "        [[0.681, 1.27],\n",
       "         [0.609, 1.23],\n",
       "         [0.433, 0.999],\n",
       "         [0.478, 1.16],\n",
       "         [0.585, 0.93],\n",
       "         [0.445, 1],\n",
       "         [0.64, 1.12],\n",
       "         [0.475, 1.06],\n",
       "         [0.593, 1.28]]]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vval_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76e11962",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attention = jnp.matmul(Attention_weights, Vval_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56c951ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 5, 9, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a4b7f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[0.939, 1.07],\n",
       "        [0.939, 1.07],\n",
       "        [0.47, 0.534],\n",
       "        [0.313, 0.356],\n",
       "        [0.235, 0.267],\n",
       "        [0.188, 0.213],\n",
       "        [0.157, 0.178],\n",
       "        [0.134, 0.152],\n",
       "        [0.117, 0.133]],\n",
       "\n",
       "       [[0.929, 1.18],\n",
       "        [0.929, 1.18],\n",
       "        [0.464, 0.59],\n",
       "        [0.31, 0.393],\n",
       "        [0.232, 0.295],\n",
       "        [0.186, 0.236],\n",
       "        [0.155, 0.197],\n",
       "        [0.133, 0.169],\n",
       "        [0.116, 0.147]],\n",
       "\n",
       "       [[1.08, 0.805],\n",
       "        [1.08, 0.805],\n",
       "        [0.541, 0.402],\n",
       "        [0.361, 0.268],\n",
       "        [0.27, 0.201],\n",
       "        [0.216, 0.161],\n",
       "        [0.18, 0.134],\n",
       "        [0.155, 0.115],\n",
       "        [0.135, 0.101]],\n",
       "\n",
       "       [[0.825, 0.794],\n",
       "        [0.825, 0.794],\n",
       "        [0.412, 0.397],\n",
       "        [0.275, 0.265],\n",
       "        [0.206, 0.199],\n",
       "        [0.165, 0.159],\n",
       "        [0.137, 0.132],\n",
       "        [0.118, 0.113],\n",
       "        [0.103, 0.0993]],\n",
       "\n",
       "       [[0.681, 1.27],\n",
       "        [0.681, 1.27],\n",
       "        [0.34, 0.634],\n",
       "        [0.227, 0.423],\n",
       "        [0.17, 0.317],\n",
       "        [0.136, 0.254],\n",
       "        [0.113, 0.211],\n",
       "        [0.0973, 0.181],\n",
       "        [0.0851, 0.159]]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ed38f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9, 9, 10), (9, 9, 10))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention=jnp.array([jnp.concatenate(Attention[i], axis=1) for i in range(9)])\n",
    "Attention.shape,input_decoder.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b2a3377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.939, 1.07, 0.929, 1.18, 1.08, 0.805, 0.825, 0.794, 0.681, 1.27],\n",
       "       [0.939, 1.07, 0.929, 1.18, 1.08, 0.805, 0.825, 0.794, 0.681, 1.27],\n",
       "       [0.899, 1.07, 0.912, 1.09, 1.07, 0.861, 0.785, 0.703, 0.645, 1.25],\n",
       "       [0.599, 0.716, 0.608, 0.729, 0.715, 0.574, 0.523, 0.468, 0.43, 0.833],\n",
       "       [0.45, 0.537, 0.456, 0.547, 0.536, 0.431, 0.392, 0.351, 0.322, 0.625],\n",
       "       [0.36, 0.43, 0.365, 0.437, 0.429, 0.345, 0.314, 0.281, 0.258, 0.5],\n",
       "       [0.3, 0.358, 0.304, 0.365, 0.358, 0.287, 0.262, 0.234, 0.215, 0.416],\n",
       "       [0.257, 0.307, 0.261, 0.312, 0.307, 0.246, 0.224, 0.201, 0.184, 0.357],\n",
       "       [0.225, 0.269, 0.228, 0.273, 0.268, 0.215, 0.196, 0.176, 0.161, 0.312]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "727a0303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 9, 10)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def layer_norm(x, epsilon=1e-6):\n",
    "    # Calculate the mean and variance\n",
    "        mean = jnp.mean(x, axis=-1, keepdims=True)\n",
    "        var = jnp.var(x, axis=-1, keepdims=True) \n",
    "        # Normalize the output\n",
    "        x_norm = (x - mean) / jnp.sqrt(var + epsilon) \n",
    "        return x_norm\n",
    "residual_output = layer_norm(input_decoder + Attention)\n",
    "residual_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1238b3f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "5b234398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[198, 83.1, 140, 155, 193],\n",
       "       [122, 140, 196, 197, 120],\n",
       "       [156, 215, 188, 129, 183],\n",
       "       [133, 180, 203, 148, 146],\n",
       "       [143, 203, 119, 201, 205],\n",
       "       [204, 89.6, 88.3, 170, 116]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phlenght=6\n",
    "input_t=[34,55,67,27,45,78]\n",
    "embedding_l=5\n",
    "A=np.random.rand(len(input_t), len(input_t),embedding_l)\n",
    "ins=input_t@A\n",
    "ins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ec14a4",
   "metadata": {},
   "source": [
    "# Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4e497b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape:  (5, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(edgeitems=30, linewidth=100000, formatter=dict(float=lambda x: \"%.3g\" % x)) \n",
    "def get_positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Returns a non-learnable (sinusoidal) positional encoding.\n",
    "    \n",
    "    seq_len: Length of the input sequence.\n",
    "    d_model: Dimension of the embeddings.\n",
    "    \"\"\"\n",
    "    pos = np.arange(seq_len)[:, np.newaxis]  # Shape: [seq_len, 1]\n",
    "    i = np.arange(d_model)[np.newaxis, :]    # Shape: [1, d_model]\n",
    "\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "\n",
    "    # Apply sine to even indices, cosine to odd indices\n",
    "    pos_encoding = np.zeros((seq_len, d_model))\n",
    "    pos_encoding[:, 0::2] = np.sin(pos * angle_rates[:, 0::2])  # sine on even indices\n",
    "    pos_encoding[:, 1::2] = np.cos(pos * angle_rates[:, 1::2])  # cosine on odd indices\n",
    "\n",
    "    return pos_encoding\n",
    "\n",
    "\n",
    "num_phrases = 5\n",
    "words_per_phrase = 3 \n",
    "dk = dv = embedding_size = 4 # constrain of transformer all embedding size both input embedding and attention embedding are the same encoder\n",
    "num_heads=2\n",
    " \n",
    "pos_encoding=get_positional_encoding(words_per_phrase,embedding_size)\n",
    " \n",
    "inputs_e = np.random.rand(num_phrases,words_per_phrase, embedding_size)\n",
    "inputs_e=pos_encoding+inputs_e\n",
    "print(\"inputs.shape: \",inputs_e.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bac6f65",
   "metadata": {},
   "source": [
    "# Summary Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "58bc3550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qval.shape:  (5, 2, 3, 2)\n",
      "Kval.shape:  (5, 2, 3, 2)\n",
      "Vval.shape:  (5, 2, 3, 2)\n",
      "Attention_weights shape: (5, 2, 3, 3)\n",
      "Attention shape: (5, 2, 3, 2)\n",
      "Attention shape concat: (5, 3, 4)\n",
      "Ect1.shape (5, 3, 4) 4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "def softmax(x, axis=-1):\n",
    "    # Subtract the max value for numerical stability\n",
    "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "def layer_norm(x, epsilon=1e-6):\n",
    "    # Calculate the mean and variance\n",
    "        mean = jnp.mean(x, axis=-1, keepdims=True)\n",
    "        var = jnp.var(x, axis=-1, keepdims=True) \n",
    "        \n",
    "        # Normalize the output\n",
    "        x_norm = (x - mean) / jnp.sqrt(var + epsilon) \n",
    "        #print(x)\n",
    "        #print(mean)\n",
    "        #print(\"mean\",mean.shape)\n",
    "        #print(\"x_norm.shape\",x_norm.shape)\n",
    "        return x_norm,mean,var,x.shape[-1]\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "Qe = np.random.rand(embedding_size, dk) / jnp.sqrt(embedding_size)\n",
    "Ke = np.random.rand(embedding_size, dk) / jnp.sqrt(embedding_size)\n",
    "Ve = np.random.rand(embedding_size, dv) / jnp.sqrt(embedding_size)\n",
    "\n",
    "Q_E= jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs_e, Qe),num_heads,axis=2)), 0, 1)\n",
    "print(\"Qval.shape: \",Q_E.shape)\n",
    "\n",
    "K_E = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs_e, Ke),num_heads,axis=2)), 0, 1)\n",
    "print(\"Kval.shape: \",K_E.shape)\n",
    "\n",
    "\n",
    "V_E = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs_e,Ve),num_heads,axis=2)), 0, 1)\n",
    "print(\"Vval.shape: \",V_E.shape)\n",
    "\n",
    "\n",
    "QKscaled = jnp.matmul(Q_E, jnp.transpose(K_E, (0, 1, 3, 2))) / jnp.sqrt(dk)\n",
    "\n",
    "Attention_weights_e = softmax(QKscaled)\n",
    "print(\"Attention_weights shape:\",Attention_weights_e.shape)\n",
    "\n",
    "\n",
    "Ae = jnp.matmul(Attention_weights_e, V_E)\n",
    "print(\"Attention shape:\",Ae.shape)\n",
    "\n",
    "\n",
    "Ae=jnp.array([jnp.concatenate(Ae[i], axis=1) for i in range(num_phrases)])\n",
    "print(\"Attention shape concat:\",Ae.shape)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "Xe=Ae+inputs_e\n",
    "Ect1,mu_e,var_e,Ne=layer_norm(Xe)\n",
    "print(\"Ect1.shape\",Ect1.shape,Ne)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "574397f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wfl1e.shape (4, 100)\n",
      "bfl1e.shape (100,)\n",
      "Xe1.shape (5, 3, 100)\n",
      "FLe1.shape (5, 3, 100)\n",
      "FLe2.shape (5, 3, 4)\n",
      "Ecout.shape (5, 3, 4) 4\n"
     ]
    }
   ],
   "source": [
    "fl1_size=100\n",
    "Wfl1e=np.random.rand(dv, fl1_size)  \n",
    "print(\"Wfl1e.shape\",Wfl1e.shape) \n",
    "bfl1e=np.random.rand(fl1_size)\n",
    "print(\"bfl1e.shape\",bfl1e.shape)\n",
    "Xe1=jnp.matmul(Ect1,Wfl1e)+bfl1e\n",
    "print(\"Xe1.shape\",Xe1.shape)\n",
    "\n",
    "FLe1=relu(Xe1)\n",
    "print(\"FLe1.shape\",FLe1.shape)\n",
    "\n",
    "\n",
    " \n",
    "Wfl2e=np.random.rand(fl1_size, dv)   \n",
    "bfl2e=np.random.rand(dv)\n",
    "FLe2=jnp.matmul(FLe1,Wfl2e)+bfl2e\n",
    "print(\"FLe2.shape\",FLe2.shape)\n",
    "\n",
    "Xe2=FLe2+Ect1\n",
    "Ecout,mu_e2,var_e2,N_e2=layer_norm(Xe2)\n",
    "print(\"Ecout.shape\",Ecout.shape,N_e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "624c29ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ecout.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf3ce05",
   "metadata": {},
   "source": [
    "# Cross Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "924b8323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_C.shape:  (5, 2, 3, 2)\n",
      "V_C.shape:  (5, 2, 3, 2)\n"
     ]
    }
   ],
   "source": [
    " \n",
    " \n",
    "#Qc = np.random.rand(Xe2.shape[-1], dv_cross) / jnp.sqrt(Xe2.shape[-1])\n",
    "Kc = np.random.rand(Ecout.shape[-1], dk) / jnp.sqrt(Ecout.shape[-1])\n",
    "Vc = np.random.rand(Ecout.shape[-1], dv) / jnp.sqrt(Ecout.shape[-1])\n",
    "\n",
    "\n",
    "\n",
    "K_C  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(Ecout, Kc),num_heads,axis=2)), 0, 1)\n",
    "print(\"K_C.shape: \",K_C.shape)# shape is: num_phrases, numbheads, words_per_phrase, dv/num_heads\n",
    "\n",
    "\n",
    "V_C  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(Ecout,Vc),num_heads,axis=2)), 0, 1)\n",
    "print(\"V_C.shape: \",V_C.shape)\n",
    "\n",
    "#K_C = K_C[0]  # Use the first phrase from the encoder output\n",
    "#V_C = V_C[0]  # Use the first phrase from the encoder output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4523a2c6",
   "metadata": {},
   "source": [
    "# Summary Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "669eb644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_d complete shape (3, 5, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "pos_encoding=get_positional_encoding(words_per_phrase,dv)\n",
    " \n",
    "input_d = np.random.rand(num_phrases,words_per_phrase, embedding_size)\n",
    "inputs_d=input_d+pos_encoding\n",
    "\n",
    "\n",
    "def pad_sequence(seq, max_len, pad_value=0):\n",
    "    \"\"\"Pad a sequence with a given value up to max_len.\"\"\"\n",
    "    current_len = seq.shape[0]\n",
    "    pad_width = max_len - current_len\n",
    "    if pad_width > 0:\n",
    "        # Pad sequence with zeros (or any pad_value you provide)\n",
    "        seq = jnp.pad(seq, ((0, pad_width), (0, 0)), mode='constant', constant_values=pad_value)\n",
    "    return seq\n",
    "\n",
    "def create_timestaped_input(input_d,words_per_phrase):\n",
    "    input_translation=[]\n",
    "    for j in range(input_d.shape[0]):\n",
    "    # Create padded sequences\n",
    "        padded_sequences = [pad_sequence(input_d[j][0:i], words_per_phrase) for i in range(1, input_d.shape[1] + 1)]\n",
    "        input_translation.append(padded_sequences)\n",
    "    return jnp.array(input_translation)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "# Convert to an array for batching\n",
    "inputs_d = jnp.swapaxes(create_timestaped_input(input_d,words_per_phrase),0,1)\n",
    "target_d=jnp.swapaxes(create_timestaped_input(input_d,words_per_phrase),0,1)\n",
    "print(\"inputs_d complete shape\",inputs_d.shape)# shape is: words_per_phrase,num_phrases,words_per_phrase,embedding_size at each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d40956a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[0.7734617 , 0.5517923 , 0.19257937, 0.4584178 ],\n",
       "        [0.39098927, 0.3093471 , 0.6116354 , 0.70196253],\n",
       "        [0.        , 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.57904094, 0.18310618, 0.691789  , 0.4406786 ],\n",
       "        [0.7880311 , 0.16436549, 0.07612863, 0.77167237],\n",
       "        [0.        , 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.55278337, 0.5499917 , 0.994184  , 0.6375399 ],\n",
       "        [0.07377069, 0.4043719 , 0.87599754, 0.47893432],\n",
       "        [0.        , 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.14063965, 0.89727175, 0.37440255, 0.61404943],\n",
       "        [0.73230803, 0.46715078, 0.76712763, 0.70931613],\n",
       "        [0.        , 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.07864382, 0.02970559, 0.29639703, 0.8753549 ],\n",
       "        [0.86143214, 0.63131076, 0.08198933, 0.37497965],\n",
       "        [0.        , 0.        , 0.        , 0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_d[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4e373e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5, 3, 4)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa=jnp.matmul(inputs_d, Qd)\n",
    "aa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "66b96f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[0.5156234 , 0.5034434 , 0.75805223, 0.76384556],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.27928656, 0.2618942 , 0.32605287, 0.34919378],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.2606975 , 0.4479048 , 0.58078754, 0.572626  ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.28275195, 0.41469103, 0.4373928 , 0.40945017],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.13660015, 0.18570334, 0.08304717, 0.09367436],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d9fd80b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[[[0.5156234 , 0.5034434 ],\n",
       "          [0.        , 0.        ],\n",
       "          [0.        , 0.        ]],\n",
       " \n",
       "         [[0.75805223, 0.76384556],\n",
       "          [0.        , 0.        ],\n",
       "          [0.        , 0.        ]]],\n",
       " \n",
       " \n",
       "        [[[0.27928656, 0.2618942 ],\n",
       "          [0.        , 0.        ],\n",
       "          [0.        , 0.        ]],\n",
       " \n",
       "         [[0.32605287, 0.34919378],\n",
       "          [0.        , 0.        ],\n",
       "          [0.        , 0.        ]]],\n",
       " \n",
       " \n",
       "        [[[0.2606975 , 0.4479048 ],\n",
       "          [0.        , 0.        ],\n",
       "          [0.        , 0.        ]],\n",
       " \n",
       "         [[0.58078754, 0.572626  ],\n",
       "          [0.        , 0.        ],\n",
       "          [0.        , 0.        ]]],\n",
       " \n",
       " \n",
       "        [[[0.28275195, 0.41469103],\n",
       "          [0.        , 0.        ],\n",
       "          [0.        , 0.        ]],\n",
       " \n",
       "         [[0.4373928 , 0.40945017],\n",
       "          [0.        , 0.        ],\n",
       "          [0.        , 0.        ]]],\n",
       " \n",
       " \n",
       "        [[[0.13660015, 0.18570334],\n",
       "          [0.        , 0.        ],\n",
       "          [0.        , 0.        ]],\n",
       " \n",
       "         [[0.08304717, 0.09367436],\n",
       "          [0.        , 0.        ],\n",
       "          [0.        , 0.        ]]]], dtype=float32),\n",
       " (3, 5, 2, 3, 2))"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn=jnp.swapaxes(jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs_d, Qd),num_heads,axis=3)), 0, 1),1,2)\n",
    "nn[0],nn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "3116eb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2, 3, 2)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs_d[step], Qd),num_heads,axis=2)), 0, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ccd1638f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qval.shape:  (5, 2, 3, 2)\n",
      "Kval.shape:  (5, 2, 3, 2)\n",
      "Vval.shape:  (5, 2, 3, 2)\n",
      "A_mask.shape non concat:  (5, 2, 3, 2)\n",
      "A_mask.shape concat:  (5, 3, 4)\n",
      "inputs_d.shape:  (5, 3, 4)\n"
     ]
    }
   ],
   "source": [
    " \n",
    "step=0\n",
    "\n",
    "#inputs_d=input_translation[0]\n",
    "\n",
    "Qd = np.random.rand(embedding_size, dk) / jnp.sqrt(embedding_size)\n",
    "Kd = np.random.rand(embedding_size, dk) / jnp.sqrt(embedding_size)\n",
    "Vd = np.random.rand(embedding_size, dv) / jnp.sqrt(embedding_size)\n",
    "\n",
    "\n",
    "#Q_D  = jnp.swapaxes(jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs_d[step], Qd),num_heads,axis=3)), 0, 1),1,2)\n",
    "Q_D  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs_d[step], Qd),num_heads,axis=2)), 0, 1)\n",
    "print(\"Qval.shape: \",Q_D.shape)# numwords, num_phrases, numheads, num_words, dv/num_heads\n",
    "\n",
    "#K_D  = jnp.swapaxes(jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs_d[step], Kd),num_heads,axis=3)), 0, 1),1,2)\n",
    "K_D  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs_d[step], Kd),num_heads,axis=2)), 0, 1)\n",
    "print(\"Kval.shape: \",K_D.shape)\n",
    "\n",
    "\n",
    "#V_D  = jnp.swapaxes(jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs_d[step], Vd),num_heads,axis=3)), 0, 1),1,2)\n",
    "V_D  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs_d[step], Vd),num_heads,axis=2)), 0, 1)\n",
    "print(\"Vval.shape: \",V_D.shape)\n",
    "\n",
    "\n",
    "#QKscaled_decoder  = jnp.matmul(Q_D, jnp.transpose(K_D, (0, 1, 2, 4,3))) / jnp.sqrt(dv) #+ jnp.triu(jnp.ones((words_per_phrase, words_per_phrase)))* -1e9 \n",
    "QKscaled_decoder  = jnp.matmul(Q_D, jnp.transpose(K_D, (0, 1, 3, 2))) / jnp.sqrt(dv)\n",
    "# Step 1: Create a causal mask of shape (1, 1, 9, 9) to broadcast across heads and batch\n",
    "mask = jnp.tril(jnp.ones((words_per_phrase, words_per_phrase)))  # (9, 9) lower triangular matrix\n",
    "mask = mask.at[mask == 0].set(-jnp.inf)  # Set future tokens to -inf\n",
    "mask = mask.at[mask == 1].set(0)  # Set allowed tokens to 0\n",
    "mask = mask.reshape(1, 1, words_per_phrase, words_per_phrase)   \n",
    "\n",
    "# Step 2: Apply mask to QKscaled_decoder (it will broadcast across batch and heads)\n",
    "QKscaled_decoder = QKscaled_decoder + mask \n",
    "\n",
    "Attention_weights_masked = softmax(QKscaled_decoder)\n",
    "\n",
    "\n",
    "A_mask = jnp.matmul(Attention_weights_masked, V_D)\n",
    "print(\"A_mask.shape non concat: \",A_mask.shape)\n",
    " \n",
    "#A_mask=jnp.swapaxes(jnp.concatenate(jnp.swapaxes(A_mask,0,2),axis=-1),0,1)\n",
    "A_mask= jnp.concatenate(jnp.swapaxes(A_mask,0,1),axis=-1) \n",
    "print(\"A_mask.shape concat: \",A_mask.shape)\n",
    "print(\"inputs_d.shape: \",inputs_d[step].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71462da9",
   "metadata": {},
   "source": [
    " # Cross Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "210f7180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dt1.shape (5, 3, 4)\n",
      "Qc.shape (4, 4)\n",
      "Q_C.shape:  (5, 2, 3, 2)\n",
      "K_C.shape:  (5, 2, 3, 2)\n",
      "V_C.shape:  (5, 2, 3, 2)\n",
      "Acr.shape non concat (5, 2, 3, 2)\n",
      "Acr.shape concat (5, 3, 4)\n",
      "Dt2 shape: (5, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "#QKscaled_cross_attention  = jnp.matmul(Q_C, jnp.transpose(jnp.expand_dims(K_C, axis=0) , (0, 1, 3, 2))) / jnp.sqrt(dv_decoder)\n",
    "Xd = inputs_d[step] + A_mask\n",
    "Dt1,mu_d,var_d,N_d = layer_norm(Xd)\n",
    "print(\"Dt1.shape\",Dt1.shape)\n",
    "\n",
    "Qc = np.random.rand(Dt1.shape[-1], dv) / jnp.sqrt(Dt1.shape[-1])\n",
    "print(\"Qc.shape\",Qc.shape)\n",
    "\n",
    "#Q_C  = jnp.swapaxes(jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(Dt1, Qc),num_heads,axis=3)), 0, 1),1,2)\n",
    "Q_C  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(Dt1, Qc),num_heads,axis=2)), 0, 1)\n",
    "print(\"Q_C.shape: \",Q_C.shape)# shape words_per_phrase,num_heads,words_per_phrase,dv/num_heads\n",
    "print(\"K_C.shape: \",K_C.shape)# shape num_phrase,num_heads,words_per_phrase,dv/num_heads\n",
    "print(\"V_C.shape: \",V_C.shape)# shape num_phrase,num_heads,words_per_phrase,dv/num_heads\n",
    "\n",
    "QKscaled_cross_attention  = jnp.matmul(Q_C, jnp.transpose(K_C , (0, 1, 3, 2)))/ jnp.sqrt(dv)\n",
    "Attention_weights_cross = softmax(QKscaled_cross_attention)\n",
    "Acr = jnp.matmul(Attention_weights_cross, V_C)\n",
    "print(\"Acr.shape non concat\",Acr.shape)\n",
    "Acr=jnp.concatenate(jnp.swapaxes(Acr,0,1),axis=-1)\n",
    "print(\"Acr.shape concat\",Acr.shape)\n",
    "Res=Acr + Dt1\n",
    "Dt2, mu_res,var_res,N_res = layer_norm(Res)  # residual_output is (9, 9, 10)\n",
    "print(\"Dt2 shape:\", Dt2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2344681f",
   "metadata": {},
   "source": [
    "### Fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2da20ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wfl1d.shape (4, 100)\n",
      "bfl1d.shape (100,)\n",
      "Xd1.shape (5, 3, 100)\n",
      "FLe1.shape (5, 3, 100)\n",
      "Wfl2d.shape (100, 4)\n",
      "bfl2d.shape (4,)\n",
      "FLd2.shape (5, 3, 4)\n",
      "Dout.shape (5, 3, 4)\n",
      "Dout.shape concat (5, 12)\n"
     ]
    }
   ],
   "source": [
    "fl1d_size=100\n",
    "Wfl1d=np.random.rand(Dt2.shape[-1], fl1d_size)   \n",
    "print(\"Wfl1d.shape\",Wfl1d.shape)\n",
    "bfl1d=np.random.rand(fl1d_size)\n",
    "print(\"bfl1d.shape\",bfl1d.shape)\n",
    "Xd1=jnp.matmul(Dt2,Wfl1d)+bfl1d\n",
    "print(\"Xd1.shape\",Xd1.shape)\n",
    "\n",
    "FLd1=relu(Xd1)\n",
    "print(\"FLe1.shape\",FLd1.shape)\n",
    "\n",
    "\n",
    "Wfl2d=np.random.rand(FLd1.shape[-1], dv)   \n",
    "print(\"Wfl2d.shape\",Wfl2d.shape)\n",
    "bfl2d=np.random.rand(dv)\n",
    "print(\"bfl2d.shape\",bfl2d.shape)\n",
    "FLd2=jnp.matmul(FLd1,Wfl2d)+bfl2d\n",
    "print(\"FLd2.shape\",FLd2.shape)\n",
    "\n",
    "\n",
    "\n",
    "Xd2=FLd2+Dt2\n",
    "Dout,mu_d2,var_d2,N_d2=layer_norm(Xd2)\n",
    "Dout.shape\n",
    "print(\"Dout.shape\",Dout.shape)\n",
    "Dout=Dout.reshape(num_phrases,Dout.shape[1]*Dout.shape[2])\n",
    "print(\"Dout.shape concat\",Dout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "60add880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zout.shape (5, 7)\n",
      "SigmaZout.shape (5, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.0647, 0.0284, 0.034, 0.0435, 0.393, 0.222, 0.215],\n",
       "       [0.0714, 0.0299, 0.0379, 0.062, 0.447, 0.175, 0.177],\n",
       "       [0.0861, 0.0507, 0.0605, 0.055, 0.41, 0.188, 0.15],\n",
       "       [0.0663, 0.0286, 0.0348, 0.0272, 0.426, 0.202, 0.215],\n",
       "       [0.0747, 0.0302, 0.035, 0.0305, 0.382, 0.227, 0.221]], dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size=7 \n",
    "W0=np.random.rand(Dout.shape[-1],vocab_size)   \n",
    "b0=np.random.rand(vocab_size)\n",
    "Zout=jnp.matmul(Dout,W0)+b0\n",
    "print(\"Zout.shape\",Zout.shape)\n",
    "SigmaZout = softmax(Zout) \n",
    "print(\"SigmaZout.shape\",SigmaZout.shape)\n",
    "SigmaZout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c38108fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 8.304715\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def cross_entropy_loss(predictions, target):\n",
    "    # Cross-entropy loss for a batch of predictions and targets\n",
    "    batch_loss = -jnp.sum(target * jnp.log(predictions + 1e-9), axis=1)\n",
    "    return jnp.mean(batch_loss)\n",
    "target_d=np.random.rand(words_per_phrase,num_phrases,vocab_size)\n",
    "target_d.shape,target_d[step].shape\n",
    "print(\"Loss:\",cross_entropy_loss(SigmaZout, target_d[step]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "64251d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 12)"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f9fd3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dout.shape (5, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "# Dout=Dout.reshape(num_phrases,words_per_phrase,embedding_size)\n",
    "# print(\"Dout.shape\",Dout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6973c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e9313a8",
   "metadata": {},
   "source": [
    "## BackPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "eb8e176a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_dZout.shape (5, 7)\n"
     ]
    }
   ],
   "source": [
    "dLoss_dZout=SigmaZout-target_d[step]\n",
    "print(\"dLoss_dZout.shape\",dLoss_dZout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5418a28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 12)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f2ad66ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_W0.shape (7, 12) W0.shape (12, 7)\n"
     ]
    }
   ],
   "source": [
    "dLoss_W0=jnp.transpose(dLoss_dZout,(1,0))@Dout\n",
    "print(\"dLoss_W0.shape\",dLoss_W0.shape,\"W0.shape\",W0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e98ffe0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_b0.shape (7,) b0.shape (7,)\n"
     ]
    }
   ],
   "source": [
    "dLoss_b0=jnp.sum(dLoss_dZout, axis=0)\n",
    "print(\"dLoss_b0.shape\",dLoss_b0.shape,\"b0.shape\",b0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b80b093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_Dout.shape (5, 3, 4)\n",
      "Dout.shape (5, 12)\n"
     ]
    }
   ],
   "source": [
    "dLoss_Dout=dLoss_dZout@W0.T\n",
    "dLoss_Dout=dLoss_Dout.reshape(num_phrases,words_per_phrase,embedding_size)\n",
    "print(\"dLoss_Dout.shape\",dLoss_Dout.shape)\n",
    "print(\"Dout.shape\",Dout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fada037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_norm(X,var,mu,N):\n",
    "    epsilon=1e-6\n",
    "    AA=((1-(1/N))*(1/(jnp.sqrt(var+epsilon))))\n",
    "    BB=(1/N)*((X-mu)**2)\n",
    "    CC=((var+epsilon)**(3/2))\n",
    "    result=(AA-(BB/CC)) \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce5928ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_FLd2.shape (5, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dLoss_FLd2=dLoss_Dout*diff_norm(Xd2,var_d2,mu_d2,N_d2)\n",
    "print(\"dLoss_FLd2.shape\",dLoss_FLd2.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4097a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_Dt2_a.shape (5, 3, 4)\n",
      "Dt2.shape (5, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "dLoss_Dt2_a=dLoss_FLd2\n",
    "print(\"dLoss_Dt2_a.shape\",dLoss_Dt2_a.shape) \n",
    "print(\"Dt2.shape\",Dt2.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7185e5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wfl2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b94fda3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_FLd1.shape (5, 3, 100)\n",
      "FLd1.shape (5, 3, 100)\n"
     ]
    }
   ],
   "source": [
    "dLoss_FLd1=dLoss_FLd2@jnp.transpose(Wfl2d,(1,0))\n",
    "print(\"dLoss_FLd1.shape\",dLoss_FLd1.shape) \n",
    "print(\"FLd1.shape\",FLd1.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3577b63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_Wfl2d.shape (4, 100)\n",
      "Wfl2d.shape (100, 4)\n"
     ]
    }
   ],
   "source": [
    "dLoss_Wfl2d=jnp.sum(jnp.transpose(dLoss_FLd2,(0,2,1))@FLd1,axis=0)\n",
    "print(\"dLoss_Wfl2d.shape\",dLoss_Wfl2d.shape) # do the mean here over each phrase\n",
    "print(\"Wfl2d.shape\",Wfl2d.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e50a706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_bfl2d.shape (4,)\n",
      "bfl2d.shape (4,)\n"
     ]
    }
   ],
   "source": [
    "dLoss_bfl2d=jnp.sum(jnp.sum(dLoss_FLd2, axis=0),axis=0)\n",
    "print(\"dLoss_bfl2d.shape\",dLoss_bfl2d.shape) # do the mean here over each phrase\n",
    "print(\"bfl2d.shape\",bfl2d.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f2c6e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 3, 100), (4, 100))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLoss_FLd1.shape,Wfl1d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "420a7798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DLoss_Dt2.shape (5, 3, 4)\n",
      "Dt2.shape (5, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "if Xd1.all()>0:\n",
    "    DLoss_Dt2_b=dLoss_FLd1@jnp.transpose(Wfl1d,(1,0))\n",
    "else:\n",
    "    DLoss_Dt2_b=0\n",
    "DLoss_Dt2=dLoss_Dt2_a+DLoss_Dt2_b\n",
    "print(\"DLoss_Dt2.shape\",DLoss_Dt2.shape) # do the mean here over each phrase\n",
    "print(\"Dt2.shape\",Dt2.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9a10776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 3, 100), (5, 3, 4))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLoss_FLd1.shape,Dt2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22147ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_Wfl1d.shape (100, 4)\n",
      "Wfl1d.shape (4, 100)\n"
     ]
    }
   ],
   "source": [
    "if Xd1.all()>0:\n",
    "    dLoss_Wfl1d=jnp.sum(jnp.transpose(dLoss_FLd1,(0,2,1))@Dt2,axis=0)\n",
    "else:\n",
    "    dLoss_Wfl1d=0\n",
    "print(\"dLoss_Wfl1d.shape\",dLoss_Wfl1d.shape) # do the mean here over each phrase\n",
    "print(\"Wfl1d.shape\",Wfl1d.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52db258d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_bfl1d.shape (100,)\n",
      "bfl1d.shape (100,)\n"
     ]
    }
   ],
   "source": [
    "if Xd1.all()>0:\n",
    "    dLoss_bfl1d=jnp.sum(jnp.sum(dLoss_FLd1,axis=0),axis=0)\n",
    "else:\n",
    "    dLoss_bfl1d=0\n",
    "print(\"dLoss_bfl1d.shape\",dLoss_bfl1d.shape) # do the mean here over each phrase\n",
    "print(\"bfl1d.shape\",bfl1d.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37dfd2a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 3, 4), (5, 3, 4))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLoss_Dt2=dLoss_Dt2_a+DLoss_Dt2_b\n",
    "dLoss_Dt2.shape,diff_norm(Res,var_res,mu_res,N_res).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dae82935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_Acr.shape (5, 3, 4)\n",
      "Acr.shape (5, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "dLoss_Acr=dLoss_Dt2*diff_norm(Res,var_res,mu_res,N_res)\n",
    "print(\"dLoss_Acr.shape\",dLoss_Acr.shape) # do the mean here over each phrase\n",
    "print(\"Acr.shape\",Acr.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24166572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_Dt1.shape (5, 3, 4)\n",
      "Dt1.shape (5, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "dLoss_Dt1_a=dLoss_Dt2*diff_norm(Res,var_res,mu_res,N_res)\n",
    "print(\"dLoss_Dt1.shape\",dLoss_Dt1_a.shape) # do the mean here over each phrase\n",
    "print(\"Dt1.shape\",Dt1.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "5d072e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.016, 0.00565, 0.00262, -0.0106],\n",
       "       [-0.0052, -0.000982, -0.0046, 0.00351],\n",
       "       [-0.000971, 0.00154, -0.00372, -0.000475],\n",
       "       [0.00607, 0.0037, 0.00123, -0.00565]], dtype=float32)"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dAttention_weights_cross=Attention_weights_cross*(1-Attention_weights_cross)\n",
    "# dAttention_weights_cross.shape\n",
    "# V1=jnp.concatenate(jnp.swapaxes(dAttention_weights_cross@K_C/jnp.sqrt(dk),0,1),axis=-1)\n",
    " \n",
    "# V2=jnp.concatenate(jnp.swapaxes(V_C,0,1),axis=-1)\n",
    " \n",
    "# V3=V1*V2*Dt1\n",
    "# jnp.sum(jnp.transpose(Acr,(0,2,1))@V3,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b41368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12138469",
   "metadata": {},
   "source": [
    "## Cross Attention derivatives Qc, Vc, Kc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4606a4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def redimension(X):\n",
    "    return jnp.concatenate(jnp.swapaxes(X,0,1),axis=-1) \n",
    "\n",
    "def diffQKV(dAttention,Attention_weights,X1,X2,X3,dk):\n",
    "    dAttention_weights=Attention_weights*(1-Attention_weights)\n",
    "    V1=redimension(dAttention_weights@X1/jnp.sqrt(dk)) \n",
    "    \n",
    "    V2=redimension(X2)\n",
    "    \n",
    "    V3=V1*V2*X3\n",
    "    dLoss_dX=jnp.sum(jnp.transpose(dAttention,(0,2,1))@V3,axis=0)\n",
    "    return dLoss_dX\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6d31c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_dQc.shape (4, 4)\n",
      "Qc.shape (4, 4)\n"
     ]
    }
   ],
   "source": [
    "dLoss_Qc=diffQKV(dLoss_Acr,Attention_weights_cross,K_C,V_C,Dt1,dk)\n",
    "print(\"dLoss_dQc.shape\",dLoss_Qc.shape) # do the mean here over each phrase\n",
    "print(\"Qc.shape\",Qc.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a67983e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_dKc.shape (4, 4)\n",
      "Kc.shape (4, 4)\n"
     ]
    }
   ],
   "source": [
    "dLoss_Kc=diffQKV(dLoss_Acr,Attention_weights_cross,Q_C,V_C,Ecout,dk)\n",
    "print(\"dLoss_dKc.shape\",dLoss_Kc.shape) # do the mean here over each phrase\n",
    "print(\"Kc.shape\",Kc.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ccb20cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_dVc.shape (4, 4)\n",
      "Vc.shape (4, 4)\n"
     ]
    }
   ],
   "source": [
    "dLoss_Vc=f=np.sum(np.mean(np.transpose(np.expand_dims(dLoss_Acr, axis=1),(0,1,3,2))@(Attention_weights_cross@np.expand_dims(Ecout, axis=1)),axis=1),axis=0)\n",
    "print(\"dLoss_dVc.shape\",dLoss_Vc.shape) # do the mean here over each phrase\n",
    "print(\"Vc.shape\",Vc.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f626eabd",
   "metadata": {},
   "source": [
    "## Decoder route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "952cf3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 3, 4), (5, 3, 4), (5, 2, 3, 2), (1, 4, 4))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLoss_Dt1_a.shape,dLoss_Acr.shape,K_C.shape,jnp.expand_dims(Qc,axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2014aaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_Dt1_b.shape (5, 3, 4)\n",
      "dLoss_Dt1_a.shape (5, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "dAttention_weights_cross=Attention_weights_cross*(1-Attention_weights_cross)\n",
    "V1=redimension(dAttention_weights_cross@K_C/jnp.sqrt(dk)) \n",
    "\n",
    "V2=redimension(V_C)\n",
    "\n",
    "V3=V1*V2@Qc\n",
    "dLoss_Dt1_b=dLoss_Acr*V3\n",
    "print(\"dLoss_Dt1_b.shape\",dLoss_Dt1_b.shape) # do the mean here over each phrase\n",
    "print(\"dLoss_Dt1_a.shape\",dLoss_Dt1_a.shape) \n",
    "dLoss_Dt1=dLoss_Dt1_a+dLoss_Dt1_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e309a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 3, 4), (5, 3, 4))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_mask.shape,diff_norm(Xd,var_d,mu_d,N_d).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9b26a058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_DAmask.shape (5, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "dLoss_Amask=dLoss_Dt1*diff_norm(Xd,var_d,mu_d,N_d)\n",
    "print(\"dLoss_DAmask.shape\",dLoss_Amask.shape)  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8d744a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_Dinputd_a.shape (5, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "dLoss_inputd_a=dLoss_Amask\n",
    "print(\"dLoss_Dinputd_a.shape\",dLoss_inputd_a.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e1b21ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_Kd.shape (4, 4)\n"
     ]
    }
   ],
   "source": [
    "dLoss_Kd=diffQKV(dLoss_Amask,Attention_weights_masked,Q_D,V_D,input_d,dk) \n",
    "print(\"dLoss_Kd.shape\",dLoss_Kd.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c61f5f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_Qd.shape (4, 4)\n"
     ]
    }
   ],
   "source": [
    "dLoss_Qd=diffQKV(dLoss_Amask,Attention_weights_masked,K_D,V_D,input_d,dk) \n",
    "print(\"dLoss_Qd.shape\",dLoss_Qd.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0ce65e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_Vd.shape (4, 4)\n",
      "Vd.shape (4, 4)\n"
     ]
    }
   ],
   "source": [
    "dLoss_Vd=f=np.sum(np.mean(np.transpose(np.expand_dims(dLoss_Amask, axis=1),(0,1,3,2))@(Attention_weights_masked@np.expand_dims(input_d, axis=1)),axis=1),axis=0)\n",
    "print(\"dLoss_Vd.shape\",dLoss_Vd.shape) # do the mean here over each phrase\n",
    "print(\"Vd.shape\",Vd.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "93781189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_inputd_v.shape (5, 3, 4)\n",
      "input_d.shape (5, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "dLoss_V_D=np.transpose(np.mean(np.transpose(np.expand_dims(dLoss_Amask, axis=1),(0,1,3,2))@Attention_weights_masked,axis=1),(0,2,1))\n",
    "dLoss_V_D.shape\n",
    "dLoss_inputd_v=dLoss_V_D@Vd\n",
    "\n",
    "print(\"dLoss_inputd_v.shape\",dLoss_inputd_v.shape) # do the mean here over each phrase\n",
    "print(\"input_d.shape\",input_d.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df74566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a6324644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_inputd_q.shape (5, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "dAttention_weights_masked=Attention_weights_masked*(1-Attention_weights_masked)\n",
    "V1=redimension(dAttention_weights_masked@K_D/jnp.sqrt(dk)) \n",
    "V2=redimension(V_D)\n",
    "V3=V1*V2\n",
    "dLoss_Q_D=dLoss_Amask*V3\n",
    "dLoss_Q_D.shape\n",
    "dLoss_inputd_q=dLoss_Q_D@Qd\n",
    "print(\"dLoss_inputd_q.shape\",dLoss_inputd_q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c0dcfd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_inputd_k.shape (5, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "dAttention_weights_masked=Attention_weights_masked*(1-Attention_weights_masked)\n",
    "V1=redimension(dAttention_weights_masked@Q_D/jnp.sqrt(dk)) \n",
    "V2=redimension(V_D)\n",
    "V3=V1*V2\n",
    "dLoss_K_D=dLoss_Amask*V3\n",
    "dLoss_K_D.shape\n",
    "dLoss_inputd_k=dLoss_K_D@Kd\n",
    "print(\"dLoss_inputd_k.shape\",dLoss_inputd_k.shape)\n",
    "dLoss_inputd=dLoss_inputd_a+dLoss_inputd_k+dLoss_inputd_q+dLoss_inputd_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d4a10341",
   "metadata": {},
   "outputs": [],
   "source": [
    "dLoss_dWemb_decoder=dLoss_inputd*input_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd326ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "322b00a7",
   "metadata": {},
   "source": [
    "## Encoder route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3bb29b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_Ecout_k.shape (5, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "dAttention_weights_cross=Attention_weights_cross*(1-Attention_weights_cross)\n",
    "V1=redimension(dAttention_weights_cross@Q_C/jnp.sqrt(dk)) \n",
    "\n",
    "V2=redimension(V_C)\n",
    "\n",
    "V3=V1*V2\n",
    "\n",
    " \n",
    "dLoss_K_C=dLoss_Acr*V3\n",
    "dLoss_K_C.shape\n",
    "\n",
    "dLoss_Ecout_k=dLoss_K_C@Kc\n",
    "print(\"dLoss_Ecout_k.shape\",dLoss_Ecout_k.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b06578a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_Ecout_v.shape (5, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "dLoss_V_C=np.transpose(np.mean(np.transpose(np.expand_dims(dLoss_Acr, axis=1),(0,1,3,2))@Attention_weights_cross,axis=1),(0,2,1))\n",
    "dLoss_V_C.shape\n",
    "dLoss_Ecout_v=dLoss_V_C@Vc\n",
    "\n",
    "print(\"dLoss_Ecout_v.shape\",dLoss_Ecout_v.shape) # do the mean here over each phrase\n",
    "dLoss_Ecout=dLoss_Ecout_k+dLoss_Ecout_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "136960fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 3, 4), (5, 3, 4))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FLe2.shape,diff_norm(Xe2,var_e2,mu_e2,N_e2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "69aecc39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3, 4)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLoss_dFLe2=dLoss_Ecout*diff_norm(Xe2,var_e2,mu_e2,N_e2)\n",
    "dLoss_dFLe2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3b2871b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dLoss_Ect1_a=dLoss_dFLe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f20884c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wfl2e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "bb11c7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 3, 100), (5, 3, 100))"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLoss_dFLe1=dLoss_dFLe2@jnp.transpose(Wfl2e,(1,0))\n",
    "dLoss_dFLe1.shape,FLe1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d61cda10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 3, 4), (5, 3, 100))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLoss_dFLe2.shape,FLe1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "75aa8ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 4, 100), (100, 4))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLoss_dWfl2e=jnp.transpose(dLoss_dFLe2,(0,2,1))@FLe1\n",
    "dLoss_dWfl2e.shape,Wfl2e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8275f104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 4), (4,))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLoss_dbfl2e=jnp.sum(dLoss_dFLe2,axis=1)\n",
    "dLoss_dbfl2e.shape,bfl2e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a2fc00fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 3, 100), (5, 4, 100), (5, 3, 4))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLoss_dFLe1.shape,Wfl1e.shape,Ect1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "df3689e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Xe1.all()>0:\n",
    "    dLoss_Ect1_b=dLoss_dFLe1@jnp.transpose(Wfl1e,(1,0))\n",
    "else:\n",
    "    dLoss_Ect1_b=0\n",
    "\n",
    "dLoss_Ect1=dLoss_Ect1_b+dLoss_Ect1_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4b223d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 3, 100), (5, 3, 4), (5, 4, 100))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLoss_dFLe1.shape,Ect1.shape,Wfl1e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "24c3fa1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 100, 4), (5, 4, 100))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if Xe1.all()>0:\n",
    "    dLoss_Wfl1e=jnp.transpose(dLoss_dFLe1,(0,2,1))@Ect1\n",
    "else:\n",
    "    dLoss_Wfl1e=0\n",
    "\n",
    "dLoss_Wfl1e.shape,Wfl1e.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e961fffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 100, 3), (5, 1, 100))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if Xe1.all()>0:\n",
    "    dLoss_bfl1e=jnp.transpose(dLoss_dFLe1,(0,2,1)) \n",
    "else:\n",
    "    dLoss_bfl1e=0\n",
    "\n",
    "dLoss_bfl1e.shape,bfl1e.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3367ab4d",
   "metadata": {},
   "source": [
    "## Attention Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7a656bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 3, 4), (5, 3, 4))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLoss_Ect1.shape,diff_norm(Xe,var_e,mu_e,Ne).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0ae09e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3, 4)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLoss_Ae=dLoss_Ect1*diff_norm(Xe,var_e,mu_e,Ne)\n",
    "dLoss_Ae.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3f33ad4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3, 4)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLoss_inpute_a=dLoss_Ae\n",
    "dLoss_inpute_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fcf4d5ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 3, 4), (5, 2, 3, 3), (5, 2, 3, 2), (5, 2, 3, 2), (5, 3, 4))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLoss_Ae.shape,Attention_weights_e.shape,K_E.shape,V_E.shape,inputs_e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7b0d8605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_dQe.shape (4, 4)\n",
      "Qe.shape (4, 4)\n"
     ]
    }
   ],
   "source": [
    "dLoss_dQe=diffQKV(dLoss_Ae,Attention_weights_e,K_E,V_E,inputs_e,dk)\n",
    "print(\"dLoss_dQe.shape\",dLoss_dQe.shape) # do the mean here over each phrase\n",
    "print(\"Qe.shape\",Qe.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1cc8b27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_dKe.shape (4, 4)\n",
      "Ke.shape (4, 4)\n"
     ]
    }
   ],
   "source": [
    "dLoss_dKe=diffQKV(dLoss_Ae,Attention_weights_e,Q_E,V_E,inputs_e,dk)\n",
    "print(\"dLoss_dKe.shape\",dLoss_dKe.shape) # do the mean here over each phrase\n",
    "print(\"Ke.shape\",Ke.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "68f4e0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_dVe.shape (4, 4)\n",
      "Ve.shape (4, 4)\n"
     ]
    }
   ],
   "source": [
    "dLoss_dVe=f=np.sum(np.mean(np.transpose(np.expand_dims(dLoss_Ae, axis=1),(0,1,3,2))@(Attention_weights_e@np.expand_dims(inputs_e, axis=1)),axis=1),axis=0)\n",
    "print(\"dLoss_dVe.shape\",dLoss_dVe.shape) # do the mean here over each phrase\n",
    "print(\"Ve.shape\",Ke.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "id": "e79a9867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 4), (4, 4))"
      ]
     },
     "execution_count": 768,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLoss_dKe.shape,Ke.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eea519ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_inpute_v.shape (5, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "dLoss_V_E=np.transpose(np.mean(np.transpose(np.expand_dims(dLoss_Ae, axis=1),(0,1,3,2))@Attention_weights_e,axis=1),(0,2,1))\n",
    "dLoss_V_E.shape\n",
    "dLoss_inpute_v=dLoss_V_E@Ve\n",
    "\n",
    "print(\"dLoss_inpute_v.shape\",dLoss_inpute_v.shape)  \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9b1c5479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_inpute_q.shape (5, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "dAttention_weights_e=Attention_weights_e*(1-Attention_weights_e)\n",
    "V1=redimension(dAttention_weights_e@K_E/jnp.sqrt(dk)) \n",
    "\n",
    "V2=redimension(V_E)\n",
    "\n",
    "V3=V1*V2\n",
    "\n",
    " \n",
    "dLoss_Q_E=dLoss_Ae*V3\n",
    "dLoss_Q_E.shape\n",
    "\n",
    "dLoss_inpute_q=dLoss_Q_E@Qe\n",
    "print(\"dLoss_inpute_q.shape\",dLoss_inpute_q.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dc29b8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_inpute_k.shape (5, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "dAttention_weights_e=Attention_weights_e*(1-Attention_weights_e)\n",
    "V1=redimension(dAttention_weights_e@Q_E/jnp.sqrt(dk)) \n",
    "\n",
    "V2=redimension(V_E)\n",
    "\n",
    "V3=V1*V2\n",
    "\n",
    " \n",
    "dLoss_K_E=dLoss_Ae*V3\n",
    "dLoss_K_E.shape\n",
    "\n",
    "dLoss_inpute_k=dLoss_K_E@Ke\n",
    "print(\"dLoss_inpute_k.shape\",dLoss_inpute_k.shape)\n",
    "dLoss_inpute=dLoss_inpute_a+dLoss_inpute_k+dLoss_inpute_q+dLoss_inpute_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bdb18654",
   "metadata": {},
   "outputs": [],
   "source": [
    "dLoss_dWemb_encoder=dLoss_inpute*inputs_e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
