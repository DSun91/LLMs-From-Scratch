{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9b3f3e9",
   "metadata": {},
   "source": [
    "# Multi-Head Attention from Scratch Using CuPy\n",
    "\n",
    "In this notebook, we will implement the **Multi-Head Attention** mechanism from scratch using **CuPy**, a GPU-accelerated library similar to NumPy. The multi-head attention mechanism is a key component of the Transformer model, enabling it to attend to different parts of the input simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Overview of Multi-Head Attention\n",
    "\n",
    "Multi-head attention allows the model to have multiple attention \"heads,\" each of which focuses on different parts of the input sequence. Each head computes its own attention values, and the results are concatenated and transformed into the final output.\n",
    "\n",
    "The steps involved in multi-head attention:\n",
    "- **Linear transformations**: Apply learned weight matrices to the queries (Q), keys (K), and values (V).\n",
    "- **Scaled Dot-Product Attention**: For each head, compute the attention scores and apply them to the values.\n",
    "- **Concatenation**: Concatenate the outputs from all heads.\n",
    "- **Final Linear Transformation**: Apply a final linear transformation to the concatenated output.\n",
    "\n",
    "Mathematically, the output of the multi-head attention mechanism can be written as:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h)W^O\n",
    "$$\n",
    "where each attention head is computed as:\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$$\n",
    "---\n",
    "\n",
    "## 2. CuPy Setup\n",
    "\n",
    "Before we begin, make sure you have **CuPy** installed. You can install it via:\n",
    "\n",
    "```bash\n",
    "!pip install cupy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "db372a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 7, 10), (10, 10), (10, 10), (10, 10))"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "import numpy as np\n",
    "import re\n",
    "import cupy as cp\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np \n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jax\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "np.set_printoptions(edgeitems=30, linewidth=100000, formatter=dict(float=lambda x: \"%.3g\" % x)) \n",
    "def softmax(x, axis=-1):\n",
    "    # Subtract the max value for numerical stability\n",
    "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "num_classes=2\n",
    "word2vec_len = 10\n",
    "num_phrases = 3\n",
    "words_per_phrase = 7 \n",
    "dk = dv = 10\n",
    " \n",
    "num_heads=5\n",
    " \n",
    " \n",
    "inputs = np.random.rand(num_phrases,words_per_phrase, word2vec_len)\n",
    "target = softmax(np.random.rand(num_phrases,num_classes))\n",
    "\n",
    "Q = np.random.rand(word2vec_len, dk) / jnp.sqrt(word2vec_len)\n",
    "K = np.random.rand(word2vec_len, dk) / jnp.sqrt(word2vec_len)\n",
    "V = np.random.rand(word2vec_len, dv) / jnp.sqrt(word2vec_len)\n",
    "inputs.shape,Q.shape,K.shape,V.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26957032",
   "metadata": {},
   "source": [
    "### Consideriamo input formato da 3 frasi composte da 7 parole ciasuna ed ogni parola avente rappresentazione vettoriale di dimensione 10. Mentre vogliamo dopo l'attentione che ogni parola abbia rappresentazione vettoriale di 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "25dd2618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.218, 0.563, 0.196, 0.275, 0.568, 0.155, 0.426, 0.0049, 0.0325, 0.463],\n",
       "        [0.762, 0.183, 0.711, 0.974, 0.514, 0.945, 0.548, 0.534, 0.942, 0.313],\n",
       "        [0.999, 0.963, 0.784, 0.742, 0.302, 0.408, 0.382, 0.0891, 0.324, 0.414],\n",
       "        [0.35, 0.131, 0.818, 0.348, 0.508, 0.908, 0.41, 0.463, 0.907, 0.862],\n",
       "        [0.224, 0.312, 0.908, 0.26, 0.129, 0.0262, 0.0881, 0.778, 0.285, 0.946],\n",
       "        [0.611, 0.243, 0.21, 0.748, 0.684, 0.506, 0.42, 0.107, 0.7, 0.41],\n",
       "        [0.552, 0.159, 0.151, 0.51, 0.923, 0.934, 0.609, 0.809, 0.26, 0.0536]],\n",
       "\n",
       "       [[0.993, 0.179, 0.283, 0.982, 0.213, 0.898, 0.028, 0.295, 0.166, 0.795],\n",
       "        [0.297, 0.493, 0.0927, 0.986, 0.307, 0.0541, 0.404, 0.766, 0.0191, 0.981],\n",
       "        [0.24, 0.603, 0.928, 0.829, 0.719, 0.502, 0.742, 0.161, 0.611, 0.717],\n",
       "        [0.26, 0.547, 0.817, 0.995, 0.393, 0.628, 0.731, 0.446, 0.959, 0.918],\n",
       "        [0.641, 0.0695, 0.112, 0.784, 0.725, 0.718, 0.537, 0.0113, 0.916, 0.151],\n",
       "        [0.885, 0.336, 0.727, 0.314, 0.367, 0.152, 0.96, 0.774, 0.756, 0.0637],\n",
       "        [0.296, 0.924, 0.495, 0.444, 0.762, 0.218, 0.322, 0.335, 0.716, 0.757]],\n",
       "\n",
       "       [[0.887, 0.45, 0.738, 0.831, 0.0355, 0.357, 0.677, 0.494, 0.472, 0.709],\n",
       "        [0.107, 0.158, 0.563, 0.962, 0.173, 0.764, 0.335, 0.0469, 0.408, 0.526],\n",
       "        [0.506, 0.688, 0.019, 0.769, 0.0799, 0.896, 0.344, 0.656, 0.828, 0.45],\n",
       "        [0.821, 0.984, 0.222, 0.414, 0.941, 0.595, 0.601, 0.245, 0.839, 0.187],\n",
       "        [0.268, 0.377, 0.964, 0.779, 0.347, 0.827, 0.186, 0.974, 0.732, 0.488],\n",
       "        [0.995, 0.528, 0.856, 0.847, 0.968, 0.728, 0.116, 0.19, 0.293, 0.991],\n",
       "        [0.437, 0.813, 0.947, 0.208, 0.478, 0.267, 0.421, 0.93, 0.61, 0.181]]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs# each input phrase is made by 13 words having lenght 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de56657a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 7, 10), (10, 10))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape,Q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed92a667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[0.742, 0.839, 1.04, 0.927, 0.653, 0.579, 0.895, 0.808, 0.76, 0.755],\n",
       "        [0.661, 0.85, 1.2, 1.07, 0.849, 0.989, 0.633, 0.935, 0.788, 0.848],\n",
       "        [0.767, 0.645, 0.993, 0.99, 0.681, 0.824, 0.681, 0.756, 0.646, 0.9],\n",
       "        [1.02, 0.98, 1.34, 1.23, 1.12, 0.881, 1.03, 1.11, 0.822, 0.981],\n",
       "        [0.44, 0.514, 0.836, 0.715, 0.603, 0.705, 0.245, 0.771, 0.615, 0.659],\n",
       "        [0.563, 0.576, 0.978, 0.86, 0.807, 0.795, 0.698, 0.909, 0.614, 0.734],\n",
       "        [0.683, 0.659, 0.841, 0.931, 0.723, 0.691, 0.644, 0.795, 0.641, 0.546]],\n",
       "\n",
       "       [[0.664, 0.694, 0.943, 0.828, 0.772, 0.796, 0.384, 0.777, 0.553, 0.596],\n",
       "        [0.724, 0.785, 1.18, 1.05, 0.86, 0.949, 0.803, 1.07, 0.771, 0.818],\n",
       "        [0.928, 0.805, 1.13, 0.974, 0.959, 0.852, 0.681, 0.969, 0.696, 0.776],\n",
       "        [0.943, 0.821, 1.2, 1.01, 0.958, 0.905, 0.731, 1.07, 0.738, 0.881],\n",
       "        [0.987, 0.949, 1.42, 1.24, 0.987, 1.03, 0.98, 1.14, 0.911, 1.09],\n",
       "        [0.464, 0.583, 0.907, 0.601, 0.468, 0.562, 0.527, 0.733, 0.618, 0.734],\n",
       "        [0.818, 0.956, 1.19, 1.09, 0.881, 0.755, 0.866, 1.03, 0.953, 0.807]],\n",
       "\n",
       "       [[0.626, 0.639, 0.911, 0.861, 0.752, 0.672, 0.588, 0.701, 0.585, 0.668],\n",
       "        [0.374, 0.508, 0.821, 0.538, 0.616, 0.595, 0.311, 0.655, 0.457, 0.599],\n",
       "        [0.775, 0.743, 1.15, 0.954, 0.919, 0.876, 0.806, 0.996, 0.78, 0.887],\n",
       "        [0.871, 0.815, 1.19, 1.14, 1.03, 0.987, 0.703, 1.06, 0.751, 0.792],\n",
       "        [0.456, 0.459, 0.579, 0.597, 0.48, 0.487, 0.346, 0.514, 0.458, 0.411],\n",
       "        [0.792, 0.782, 1.13, 1.05, 0.896, 0.882, 0.676, 1.01, 0.796, 0.874],\n",
       "        [0.753, 0.926, 1.14, 1.01, 0.898, 0.713, 0.905, 0.906, 0.736, 0.724]]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.matmul(inputs, Q) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fedbac",
   "metadata": {},
   "source": [
    "### Avendo fissato il numero di teste per il attezione ogni matrice Qval, Kval, Vval viene suddivisa in 4 parti uguali di 2 colonne. Otteniamo un array di 4 elementi che per ogni frase riportano le prime due colonne come di seguito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eef75667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jnp.array_split(jnp.matmul(inputs, Q),num_heads,axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d76673db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[0.742, 0.839],\n",
       "        [0.661, 0.85],\n",
       "        [0.767, 0.645],\n",
       "        [1.02, 0.98],\n",
       "        [0.44, 0.514],\n",
       "        [0.563, 0.576],\n",
       "        [0.683, 0.659]],\n",
       "\n",
       "       [[0.664, 0.694],\n",
       "        [0.724, 0.785],\n",
       "        [0.928, 0.805],\n",
       "        [0.943, 0.821],\n",
       "        [0.987, 0.949],\n",
       "        [0.464, 0.583],\n",
       "        [0.818, 0.956]],\n",
       "\n",
       "       [[0.626, 0.639],\n",
       "        [0.374, 0.508],\n",
       "        [0.775, 0.743],\n",
       "        [0.871, 0.815],\n",
       "        [0.456, 0.459],\n",
       "        [0.792, 0.782],\n",
       "        [0.753, 0.926]]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.array_split(jnp.matmul(inputs, Q),num_heads,axis=2)[0]# so i have basically num_heads chuncks of the Qval this is a list not array structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b35421",
   "metadata": {},
   "source": [
    "### Ridimensioniamo l'array in modo che ogni frase contenga la lista dei rispettivi attention heads, ottenendo 3 frasi contententi 4 attention heads che hanno dimensione 7 (come il numero di parole per ogni frase) per 2 (fetta di embedding assegnata ad ogni head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34aa11cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5, 7, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs, Q),num_heads,axis=2)), 0, 1).shape#  # here i actually transform it to a structure Qval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd7464e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[0.742, 0.839],\n",
       "        [0.661, 0.85],\n",
       "        [0.767, 0.645],\n",
       "        [1.02, 0.98],\n",
       "        [0.44, 0.514],\n",
       "        [0.563, 0.576],\n",
       "        [0.683, 0.659]],\n",
       "\n",
       "       [[1.04, 0.927],\n",
       "        [1.2, 1.07],\n",
       "        [0.993, 0.99],\n",
       "        [1.34, 1.23],\n",
       "        [0.836, 0.715],\n",
       "        [0.978, 0.86],\n",
       "        [0.841, 0.931]],\n",
       "\n",
       "       [[0.653, 0.579],\n",
       "        [0.849, 0.989],\n",
       "        [0.681, 0.824],\n",
       "        [1.12, 0.881],\n",
       "        [0.603, 0.705],\n",
       "        [0.807, 0.795],\n",
       "        [0.723, 0.691]],\n",
       "\n",
       "       [[0.895, 0.808],\n",
       "        [0.633, 0.935],\n",
       "        [0.681, 0.756],\n",
       "        [1.03, 1.11],\n",
       "        [0.245, 0.771],\n",
       "        [0.698, 0.909],\n",
       "        [0.644, 0.795]],\n",
       "\n",
       "       [[0.76, 0.755],\n",
       "        [0.788, 0.848],\n",
       "        [0.646, 0.9],\n",
       "        [0.822, 0.981],\n",
       "        [0.615, 0.659],\n",
       "        [0.614, 0.734],\n",
       "        [0.641, 0.546]]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs, Q),num_heads,axis=2)), 0, 1)[0]# refer to cell jnp.matmul(inputs, Q) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60adc1fc-9393-47c0-a181-ed799c4d3b0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qval.shape:  (3, 5, 7, 2)\n",
      "Kval.shape:  (3, 5, 7, 2)\n",
      "Vval.shape:  (3, 5, 7, 2)\n"
     ]
    }
   ],
   "source": [
    "Qval = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs, Q),num_heads,axis=2)), 0, 1)\n",
    "print(\"Qval.shape: \",Qval.shape)\n",
    "\n",
    "Kval = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs, K),num_heads,axis=2)), 0, 1)\n",
    "print(\"Kval.shape: \",Kval.shape)\n",
    "\n",
    "\n",
    "Vval = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs,V),num_heads,axis=2)), 0, 1)\n",
    "print(\"Vval.shape: \",Vval.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b98a1d",
   "metadata": {},
   "source": [
    "### Per calcolare ora i pesi dell'attenzione applichiamo la formua \n",
    "\n",
    "$$\n",
    "  \\frac{QK^T}{\\sqrt{d_k}}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6c38c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[0.742, 0.839],\n",
       "        [0.661, 0.85],\n",
       "        [0.767, 0.645],\n",
       "        [1.02, 0.98],\n",
       "        [0.44, 0.514],\n",
       "        [0.563, 0.576],\n",
       "        [0.683, 0.659]], dtype=float32),\n",
       " Array([[0.847, 0.728],\n",
       "        [1.05, 0.979],\n",
       "        [0.868, 0.849],\n",
       "        [1.23, 1.22],\n",
       "        [0.807, 0.543],\n",
       "        [1, 0.869],\n",
       "        [0.731, 0.734]], dtype=float32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qval[0][0],Kval[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e02d8827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[0.742, 0.839],\n",
       "        [0.661, 0.85],\n",
       "        [0.767, 0.645],\n",
       "        [1.02, 0.98],\n",
       "        [0.44, 0.514],\n",
       "        [0.563, 0.576],\n",
       "        [0.683, 0.659]], dtype=float32),\n",
       " Array([[0.847, 1.05, 0.868, 1.23, 0.807, 1, 0.731],\n",
       "        [0.728, 0.979, 0.849, 1.22, 0.543, 0.869, 0.734]], dtype=float32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qval[0][0],np.transpose(Kval, (0, 1, 3, 2))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44bbb725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.392, 0.506, 0.429, 0.611, 0.333, 0.466, 0.366],\n",
       "       [0.373, 0.483, 0.41, 0.584, 0.315, 0.443, 0.35],\n",
       "       [0.354, 0.455, 0.384, 0.546, 0.307, 0.421, 0.327],\n",
       "       [0.498, 0.642, 0.542, 0.772, 0.428, 0.592, 0.463],\n",
       "       [0.236, 0.306, 0.259, 0.369, 0.201, 0.281, 0.221],\n",
       "       [0.283, 0.365, 0.309, 0.44, 0.242, 0.337, 0.264],\n",
       "       [0.335, 0.431, 0.364, 0.519, 0.288, 0.398, 0.311]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qval[0][0]@jnp.transpose(Kval, (0, 1, 3, 2))[0][0]/ jnp.sqrt(dk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35bec325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5, 7, 7)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QKscaled = jnp.matmul(Qval, jnp.transpose(Kval, (0, 1, 3, 2))) / jnp.sqrt(dk)\n",
    " \n",
    "QKscaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14f39e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.392, 0.506, 0.429, 0.611, 0.333, 0.466, 0.366],\n",
       "       [0.373, 0.483, 0.41, 0.584, 0.315, 0.443, 0.35],\n",
       "       [0.354, 0.455, 0.384, 0.546, 0.307, 0.421, 0.327],\n",
       "       [0.498, 0.642, 0.542, 0.772, 0.428, 0.592, 0.463],\n",
       "       [0.236, 0.306, 0.259, 0.369, 0.201, 0.281, 0.221],\n",
       "       [0.283, 0.365, 0.309, 0.44, 0.242, 0.337, 0.264],\n",
       "       [0.335, 0.431, 0.364, 0.519, 0.288, 0.398, 0.311]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QKscaled[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d505c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention_weights shape: (3, 5, 7, 7)\n"
     ]
    }
   ],
   "source": [
    "Attention_weights = softmax(QKscaled)\n",
    "print(\"Attention_weights shape:\",Attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52a8b7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention shape: (3, 5, 7, 2)\n"
     ]
    }
   ],
   "source": [
    "Attention = jnp.matmul(Attention_weights, Vval)\n",
    "print(\"Attention shape:\",Attention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ec19e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[0.959, 0.954],\n",
       "        [0.958, 0.954],\n",
       "        [0.956, 0.953],\n",
       "        [0.963, 0.958],\n",
       "        [0.952, 0.949],\n",
       "        [0.954, 0.95],\n",
       "        [0.956, 0.952]],\n",
       "\n",
       "       [[0.736, 0.842],\n",
       "        [0.738, 0.845],\n",
       "        [0.736, 0.843],\n",
       "        [0.739, 0.847],\n",
       "        [0.734, 0.839],\n",
       "        [0.736, 0.842],\n",
       "        [0.735, 0.841]],\n",
       "\n",
       "       [[0.996, 0.671],\n",
       "        [0.998, 0.672],\n",
       "        [0.997, 0.672],\n",
       "        [0.999, 0.673],\n",
       "        [0.996, 0.671],\n",
       "        [0.997, 0.672],\n",
       "        [0.997, 0.671]],\n",
       "\n",
       "       [[0.586, 0.927],\n",
       "        [0.585, 0.927],\n",
       "        [0.585, 0.926],\n",
       "        [0.588, 0.93],\n",
       "        [0.583, 0.923],\n",
       "        [0.585, 0.927],\n",
       "        [0.585, 0.926]],\n",
       "\n",
       "       [[0.678, 0.897],\n",
       "        [0.679, 0.898],\n",
       "        [0.678, 0.896],\n",
       "        [0.679, 0.899],\n",
       "        [0.678, 0.895],\n",
       "        [0.678, 0.895],\n",
       "        [0.677, 0.894]]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10742435",
   "metadata": {},
   "source": [
    "### Now for tetrieving the attention correct size we need to horizontaly concatenate the attention output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c3a9217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[[0.959, 0.954, 0.736, 0.842, 0.996, 0.671, 0.586, 0.927, 0.678, 0.897],\n",
       "         [0.958, 0.954, 0.738, 0.845, 0.998, 0.672, 0.585, 0.927, 0.679, 0.898],\n",
       "         [0.956, 0.953, 0.736, 0.843, 0.997, 0.672, 0.585, 0.926, 0.678, 0.896],\n",
       "         [0.963, 0.958, 0.739, 0.847, 0.999, 0.673, 0.588, 0.93, 0.679, 0.899],\n",
       "         [0.952, 0.949, 0.734, 0.839, 0.996, 0.671, 0.583, 0.923, 0.678, 0.895],\n",
       "         [0.954, 0.95, 0.736, 0.842, 0.997, 0.672, 0.585, 0.927, 0.678, 0.895],\n",
       "         [0.956, 0.952, 0.735, 0.841, 0.997, 0.671, 0.585, 0.926, 0.677, 0.894]],\n",
       " \n",
       "        [[1.02, 1.06, 0.787, 0.83, 1.09, 0.707, 0.661, 0.974, 0.779, 0.996],\n",
       "         [1.02, 1.06, 0.789, 0.834, 1.09, 0.708, 0.664, 0.978, 0.781, 0.998],\n",
       "         [1.02, 1.06, 0.789, 0.833, 1.09, 0.708, 0.663, 0.977, 0.78, 0.998],\n",
       "         [1.02, 1.06, 0.789, 0.834, 1.09, 0.708, 0.663, 0.978, 0.781, 0.998],\n",
       "         [1.02, 1.07, 0.791, 0.837, 1.09, 0.708, 0.664, 0.979, 0.782, 1],\n",
       "         [1.02, 1.06, 0.786, 0.828, 1.08, 0.706, 0.662, 0.974, 0.78, 0.997],\n",
       "         [1.02, 1.06, 0.79, 0.834, 1.09, 0.707, 0.664, 0.978, 0.781, 0.999]],\n",
       " \n",
       "        [[0.907, 0.929, 0.743, 0.772, 0.941, 0.671, 0.562, 0.888, 0.691, 0.852],\n",
       "         [0.902, 0.925, 0.74, 0.768, 0.939, 0.67, 0.56, 0.885, 0.69, 0.85],\n",
       "         [0.91, 0.932, 0.746, 0.775, 0.944, 0.673, 0.565, 0.893, 0.694, 0.856],\n",
       "         [0.911, 0.934, 0.747, 0.777, 0.946, 0.675, 0.565, 0.893, 0.693, 0.855],\n",
       "         [0.903, 0.925, 0.738, 0.766, 0.936, 0.668, 0.56, 0.884, 0.689, 0.848],\n",
       "         [0.91, 0.933, 0.746, 0.776, 0.944, 0.673, 0.565, 0.892, 0.694, 0.856],\n",
       "         [0.911, 0.934, 0.746, 0.776, 0.942, 0.672, 0.565, 0.892, 0.692, 0.854]]], dtype=float32),\n",
       " (3, 7, 10),\n",
       " (3, 7, 10))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention=jnp.array([jnp.concatenate(Attention[i], axis=1) for i in range(num_phrases)])\n",
    "Attention,Attention.shape,inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb48e5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 10, 10), (3, 1, 10))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linearlayer= np.random.rand(num_phrases,dv, word2vec_len)   \n",
    "linear_bias = np.random.rand(num_phrases,1,word2vec_len)\n",
    "linearlayer.shape,linear_bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bd1665b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.149, 0.965, 0.688, 0.605, 0.158, 0.303, 0.201, 0.959, 0.635, 0.277]],\n",
       "\n",
       "       [[0.0505, 0.2, 0.491, 0.996, 0.123, 0.332, 0.941, 0.714, 0.573, 0.0514]],\n",
       "\n",
       "       [[0.49, 0.273, 0.311, 0.749, 0.808, 0.119, 0.587, 0.472, 0.206, 0.71]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1bebf84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[1.37, -1.24, -0.374, 0.015, -0.713, 0.942, -1.28, 0.597, -0.849, 1.53],\n",
       "        [1.66, 1.28, 1.02, -0.0108, -0.612, -1.48, -0.235, -1.21, -0.647, 0.237],\n",
       "        [0.201, -0.88, 1.31, 1.53, -0.0492, -0.848, 0.789, -0.75, -1.74, 0.437],\n",
       "        [0.406, -0.243, -1.89, 0.863, 0.165, -1.44, -0.0345, 1.41, -0.353, 1.12],\n",
       "        [-0.502, 0.561, 1.41, -0.722, 1.15, -1, -1.54, 0.0923, 1.29, -0.735],\n",
       "        [0.313, 1.08, -0.648, 1.51, 0.494, -1.31, -1.89, -0.363, 0.255, 0.561],\n",
       "        [0.537, 0.639, 0.987, -0.541, -0.1, -1.06, -1.14, 1.39, -1.66, 0.955]],\n",
       "\n",
       "       [[0.842, 1.43, 0.318, -1.15, 0.988, -1.57, 0.821, 0.0974, -1.14, -0.643],\n",
       "        [1.23, 1.21, 0.241, 0.842, 0.565, -1.21, -1.94, -0.278, -0.769, 0.116],\n",
       "        [-0.277, 1.09, -0.493, -0.871, 1.69, -0.454, 0.702, 0.704, -1.91, -0.187],\n",
       "        [-0.864, 0.762, -0.412, 0.307, 1.79, -0.417, 0.0304, 1.34, -1.24, -1.3],\n",
       "        [1.35, -0.467, 0.158, 0.316, 0.889, -0.273, -0.296, -1.86, -1.22, 1.4],\n",
       "        [0.283, -0.315, -0.144, 0.476, 1.11, 1.13, -2.14, -0.907, 1.13, -0.626],\n",
       "        [0.053, 0.173, 0.425, -1.48, -0.521, 0.551, -1.93, 1.33, 0.16, 1.23]],\n",
       "\n",
       "       [[1.23, 0.26, -0.118, -0.724, 0.128, -1.47, 0.0475, -1.09, -0.368, 2.1],\n",
       "        [0.219, 1.42, -1.06, 0.092, 1.05, -1.03, -0.771, -0.68, 1.66, -0.901],\n",
       "        [-1.37, 1.53, -0.402, 1.42, 0.136, 0.687, -1.64, -0.00297, -0.605, 0.249],\n",
       "        [1.05, 1.03, 0.423, -0.934, 1.21, -1.72, -0.274, -0.511, -1.13, 0.866],\n",
       "        [-0.615, 1.07, 1.45, -0.768, -0.315, -0.463, -0.723, 1.83, -1.25, -0.218],\n",
       "        [-1.25, 0.588, 1.12, 0.24, 0.331, -1.05, -1.34, 1.79, 0.258, -0.686],\n",
       "        [1.25, 0.775, -1.56, 0.25, -1.29, -0.693, -0.596, 1.44, -0.379, 0.79]]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def layer_norm(x, epsilon=1e-6):\n",
    "    # Calculate the mean and variance\n",
    "        mean = jnp.mean(x, axis=-1, keepdims=True)\n",
    "        var = jnp.var(x, axis=-1, keepdims=True) \n",
    "        # Normalize the output\n",
    "        x_norm = (x - mean) / jnp.sqrt(var + epsilon) \n",
    "        return x_norm\n",
    "\n",
    "\n",
    "#output_sublayer_one=layer_norm((Attention@linearlayer +linear_bias)+inputs)\n",
    "output_sublayer_one=layer_norm(Attention+inputs)\n",
    "output_sublayer_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac1a52a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 7, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sublayer_one.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c41521b",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c028923",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_word_embedding_size=10\n",
    "decoder_input_number_of_words_per_phrase=9\n",
    "num_heads_decoder=5# dv=10 \n",
    "dv_decoder=10\n",
    "inputs_decoder = np.random.rand(num_phrases,decoder_input_number_of_words_per_phrase, decoder_input_word_embedding_size)# for the target language suppose the \n",
    "target_decoder = inputs_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcdc4273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.519, 0.655, 0.631, 0.843, 0.515, 0.233, 0.721, 0.719, 0.754, 0.287],\n",
       "        [0.298, 0.412, 0.777, 0.535, 0.0521, 0.413, 0.804, 0.803, 0.689, 0.6],\n",
       "        [0.705, 0.191, 0.392, 0.681, 0.416, 0.42, 0.734, 0.14, 0.159, 0.259],\n",
       "        [0.763, 0.423, 0.091, 0.569, 0.328, 0.198, 0.768, 0.434, 0.678, 0.899],\n",
       "        [0.165, 0.115, 0.582, 0.133, 0.693, 0.453, 0.243, 0.962, 0.457, 0.185],\n",
       "        [0.267, 0.0362, 0.547, 0.0591, 0.605, 0.792, 0.415, 0.299, 0.986, 0.6],\n",
       "        [0.425, 0.335, 0.154, 0.462, 0.905, 0.116, 0.595, 0.893, 0.7, 0.437],\n",
       "        [0.476, 0.295, 0.0113, 0.51, 0.949, 0.671, 0.707, 0.229, 0.652, 0.348],\n",
       "        [0.31, 0.0342, 0.732, 0.503, 0.824, 0.907, 0.793, 0.277, 0.686, 0.418]],\n",
       "\n",
       "       [[0.706, 0.76, 0.217, 0.297, 0.938, 0.124, 0.461, 0.0958, 0.584, 0.281],\n",
       "        [0.842, 0.261, 0.798, 0.453, 0.259, 0.844, 0.217, 0.543, 0.392, 0.149],\n",
       "        [0.922, 0.227, 0.287, 0.778, 0.393, 0.94, 0.199, 0.305, 0.82, 0.842],\n",
       "        [0.818, 0.673, 0.232, 0.867, 0.196, 0.603, 0.518, 0.897, 0.93, 0.104],\n",
       "        [0.483, 0.841, 0.0414, 0.286, 0.361, 0.347, 0.354, 0.962, 0.711, 0.911],\n",
       "        [0.995, 0.151, 0.902, 0.658, 0.654, 0.274, 0.731, 0.561, 0.429, 0.541],\n",
       "        [0.25, 0.92, 0.709, 0.0121, 0.407, 0.523, 0.533, 0.0563, 0.936, 0.0357],\n",
       "        [0.607, 0.967, 0.872, 0.261, 0.601, 0.336, 0.502, 0.592, 0.326, 0.211],\n",
       "        [0.671, 0.0263, 0.277, 0.388, 0.578, 0.959, 0.537, 0.857, 0.582, 0.219]],\n",
       "\n",
       "       [[0.0336, 0.815, 0.493, 0.561, 0.69, 0.44, 0.13, 0.195, 0.33, 0.492],\n",
       "        [0.617, 0.65, 0.199, 0.0799, 0.394, 0.808, 0.226, 0.392, 0.85, 0.868],\n",
       "        [0.172, 0.0728, 0.0955, 0.574, 0.0192, 0.862, 0.476, 0.645, 0.329, 0.695],\n",
       "        [0.331, 0.962, 0.122, 0.856, 0.101, 0.616, 0.616, 0.43, 0.14, 0.932],\n",
       "        [0.463, 0.109, 0.843, 0.139, 0.231, 0.999, 0.567, 0.0678, 0.622, 0.522],\n",
       "        [0.693, 0.347, 0.378, 0.248, 0.208, 0.426, 0.627, 0.326, 0.0658, 0.534],\n",
       "        [0.275, 0.569, 0.393, 0.782, 0.196, 0.925, 0.939, 0.751, 0.333, 0.83],\n",
       "        [0.0969, 0.967, 0.42, 0.43, 0.828, 0.719, 0.428, 0.223, 0.103, 0.518],\n",
       "        [0.852, 0.138, 0.946, 0.315, 0.516, 0.0162, 0.351, 0.702, 0.815, 0.806]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49030921",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "input_translation=[]\n",
    "def pad_sequence(seq, max_len, pad_value=0):\n",
    "    \"\"\"Pad a sequence with a given value up to max_len.\"\"\"\n",
    "    current_len = seq.shape[0]\n",
    "    pad_width = max_len - current_len\n",
    "    if pad_width > 0:\n",
    "        # Pad sequence with zeros (or any pad_value you provide)\n",
    "        seq = jnp.pad(seq, ((0, pad_width), (0, 0)), mode='constant', constant_values=pad_value)\n",
    "    return seq\n",
    "\n",
    "# Example usage:\n",
    "max_len = decoder_input_number_of_words_per_phrase  # Max sequence length for decoder input\n",
    " \n",
    "for j in range(inputs_decoder.shape[0]):\n",
    "    # Create padded sequences\n",
    "    padded_sequences = [pad_sequence(inputs_decoder[j][0:i], max_len) for i in range(1, inputs_decoder.shape[1] + 1)]\n",
    "    input_translation.append(padded_sequences)\n",
    "\n",
    "\n",
    "# Convert to an array for batching\n",
    "input_translation = jnp.array(input_translation)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70947e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 9, 9, 10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_translation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04e7fbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[0.519, 0.655, 0.631, 0.843, 0.515, 0.233, 0.721, 0.719, 0.754, 0.287],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0.519, 0.655, 0.631, 0.843, 0.515, 0.233, 0.721, 0.719, 0.754, 0.287],\n",
       "        [0.298, 0.412, 0.777, 0.535, 0.0521, 0.413, 0.804, 0.803, 0.689, 0.6],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0.519, 0.655, 0.631, 0.843, 0.515, 0.233, 0.721, 0.719, 0.754, 0.287],\n",
       "        [0.298, 0.412, 0.777, 0.535, 0.0521, 0.413, 0.804, 0.803, 0.689, 0.6],\n",
       "        [0.705, 0.191, 0.392, 0.681, 0.416, 0.42, 0.734, 0.14, 0.159, 0.259],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0.519, 0.655, 0.631, 0.843, 0.515, 0.233, 0.721, 0.719, 0.754, 0.287],\n",
       "        [0.298, 0.412, 0.777, 0.535, 0.0521, 0.413, 0.804, 0.803, 0.689, 0.6],\n",
       "        [0.705, 0.191, 0.392, 0.681, 0.416, 0.42, 0.734, 0.14, 0.159, 0.259],\n",
       "        [0.763, 0.423, 0.091, 0.569, 0.328, 0.198, 0.768, 0.434, 0.678, 0.899],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0.519, 0.655, 0.631, 0.843, 0.515, 0.233, 0.721, 0.719, 0.754, 0.287],\n",
       "        [0.298, 0.412, 0.777, 0.535, 0.0521, 0.413, 0.804, 0.803, 0.689, 0.6],\n",
       "        [0.705, 0.191, 0.392, 0.681, 0.416, 0.42, 0.734, 0.14, 0.159, 0.259],\n",
       "        [0.763, 0.423, 0.091, 0.569, 0.328, 0.198, 0.768, 0.434, 0.678, 0.899],\n",
       "        [0.165, 0.115, 0.582, 0.133, 0.693, 0.453, 0.243, 0.962, 0.457, 0.185],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0.519, 0.655, 0.631, 0.843, 0.515, 0.233, 0.721, 0.719, 0.754, 0.287],\n",
       "        [0.298, 0.412, 0.777, 0.535, 0.0521, 0.413, 0.804, 0.803, 0.689, 0.6],\n",
       "        [0.705, 0.191, 0.392, 0.681, 0.416, 0.42, 0.734, 0.14, 0.159, 0.259],\n",
       "        [0.763, 0.423, 0.091, 0.569, 0.328, 0.198, 0.768, 0.434, 0.678, 0.899],\n",
       "        [0.165, 0.115, 0.582, 0.133, 0.693, 0.453, 0.243, 0.962, 0.457, 0.185],\n",
       "        [0.267, 0.0362, 0.547, 0.0591, 0.605, 0.792, 0.415, 0.299, 0.986, 0.6],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0.519, 0.655, 0.631, 0.843, 0.515, 0.233, 0.721, 0.719, 0.754, 0.287],\n",
       "        [0.298, 0.412, 0.777, 0.535, 0.0521, 0.413, 0.804, 0.803, 0.689, 0.6],\n",
       "        [0.705, 0.191, 0.392, 0.681, 0.416, 0.42, 0.734, 0.14, 0.159, 0.259],\n",
       "        [0.763, 0.423, 0.091, 0.569, 0.328, 0.198, 0.768, 0.434, 0.678, 0.899],\n",
       "        [0.165, 0.115, 0.582, 0.133, 0.693, 0.453, 0.243, 0.962, 0.457, 0.185],\n",
       "        [0.267, 0.0362, 0.547, 0.0591, 0.605, 0.792, 0.415, 0.299, 0.986, 0.6],\n",
       "        [0.425, 0.335, 0.154, 0.462, 0.905, 0.116, 0.595, 0.893, 0.7, 0.437],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0.519, 0.655, 0.631, 0.843, 0.515, 0.233, 0.721, 0.719, 0.754, 0.287],\n",
       "        [0.298, 0.412, 0.777, 0.535, 0.0521, 0.413, 0.804, 0.803, 0.689, 0.6],\n",
       "        [0.705, 0.191, 0.392, 0.681, 0.416, 0.42, 0.734, 0.14, 0.159, 0.259],\n",
       "        [0.763, 0.423, 0.091, 0.569, 0.328, 0.198, 0.768, 0.434, 0.678, 0.899],\n",
       "        [0.165, 0.115, 0.582, 0.133, 0.693, 0.453, 0.243, 0.962, 0.457, 0.185],\n",
       "        [0.267, 0.0362, 0.547, 0.0591, 0.605, 0.792, 0.415, 0.299, 0.986, 0.6],\n",
       "        [0.425, 0.335, 0.154, 0.462, 0.905, 0.116, 0.595, 0.893, 0.7, 0.437],\n",
       "        [0.476, 0.295, 0.0113, 0.51, 0.949, 0.671, 0.707, 0.229, 0.652, 0.348],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0.519, 0.655, 0.631, 0.843, 0.515, 0.233, 0.721, 0.719, 0.754, 0.287],\n",
       "        [0.298, 0.412, 0.777, 0.535, 0.0521, 0.413, 0.804, 0.803, 0.689, 0.6],\n",
       "        [0.705, 0.191, 0.392, 0.681, 0.416, 0.42, 0.734, 0.14, 0.159, 0.259],\n",
       "        [0.763, 0.423, 0.091, 0.569, 0.328, 0.198, 0.768, 0.434, 0.678, 0.899],\n",
       "        [0.165, 0.115, 0.582, 0.133, 0.693, 0.453, 0.243, 0.962, 0.457, 0.185],\n",
       "        [0.267, 0.0362, 0.547, 0.0591, 0.605, 0.792, 0.415, 0.299, 0.986, 0.6],\n",
       "        [0.425, 0.335, 0.154, 0.462, 0.905, 0.116, 0.595, 0.893, 0.7, 0.437],\n",
       "        [0.476, 0.295, 0.0113, 0.51, 0.949, 0.671, 0.707, 0.229, 0.652, 0.348],\n",
       "        [0.31, 0.0342, 0.732, 0.503, 0.824, 0.907, 0.793, 0.277, 0.686, 0.418]]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_translation[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ae7156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26a41656",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_decoder=input_translation[0]\n",
    "\n",
    "Q_decoder = np.random.rand(decoder_input_word_embedding_size, dv_decoder) / jnp.sqrt(decoder_input_word_embedding_size)\n",
    "K_decoder = np.random.rand(decoder_input_word_embedding_size, dv_decoder) / jnp.sqrt(decoder_input_word_embedding_size)\n",
    "V_decoder = np.random.rand(decoder_input_word_embedding_size, dv_decoder) / jnp.sqrt(decoder_input_word_embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ec1676b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_decoder.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60a0ed3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qval.shape:  (9, 9, 10)\n"
     ]
    }
   ],
   "source": [
    "Qval_decoder=input_decoder@Q_decoder\n",
    "print(\"Qval.shape: \",Qval_decoder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5fed93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qval.shape:  (9, 5, 9, 2)\n",
      "Kval.shape:  (9, 5, 9, 2)\n",
      "Vval.shape:  (9, 5, 9, 2)\n"
     ]
    }
   ],
   "source": [
    "Qval_decoder  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(input_decoder, Q_decoder),num_heads_decoder,axis=2)), 0, 1)\n",
    "print(\"Qval.shape: \",Qval_decoder.shape)\n",
    "\n",
    "Kval_decoder  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(input_decoder, K_decoder),num_heads_decoder,axis=2)), 0, 1)\n",
    "print(\"Kval.shape: \",Kval_decoder.shape)\n",
    "\n",
    "\n",
    "Vval_decoder  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(input_decoder,V_decoder),num_heads_decoder,axis=2)), 0, 1)\n",
    "print(\"Vval.shape: \",Vval_decoder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2df93660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[-1e+09, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.445, -1e+09, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.304, 0.282, -1e+09, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.455, 0.418, 0.299, -1e+09, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, -1e+09, -inf, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, -1e+09, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, -1e+09, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, 0, -1e+09, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, -1e+09]],\n",
       "\n",
       "       [[-1e+09, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.533, -1e+09, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.352, 0.336, -1e+09, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.399, 0.383, 0.323, -1e+09, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, -1e+09, -inf, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, -1e+09, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, -1e+09, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, 0, -1e+09, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, -1e+09]],\n",
       "\n",
       "       [[-1e+09, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.478, -1e+09, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.366, 0.327, -1e+09, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.43, 0.38, 0.312, -1e+09, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, -1e+09, -inf, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, -1e+09, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, -1e+09, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, 0, -1e+09, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, -1e+09]],\n",
       "\n",
       "       [[-1e+09, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.626, -1e+09, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.462, 0.404, -1e+09, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.625, 0.547, 0.427, -1e+09, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, -1e+09, -inf, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, -1e+09, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, -1e+09, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, 0, -1e+09, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, -1e+09]],\n",
       "\n",
       "       [[-1e+09, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.682, -1e+09, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.511, 0.461, -1e+09, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.672, 0.607, 0.406, -1e+09, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, -1e+09, -inf, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, -1e+09, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, -1e+09, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, 0, -1e+09, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, -1e+09]]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QKscaled_decoder  = jnp.matmul(Qval_decoder, jnp.transpose(Kval_decoder, (0, 1, 3, 2))) / jnp.sqrt(dk) + jnp.triu(jnp.ones((9, 9)))* -1e9 \n",
    "# Step 1: Create a causal mask of shape (1, 1, 9, 9) to broadcast across heads and batch\n",
    "mask = jnp.tril(jnp.ones((max_len, max_len)))  # (9, 9) lower triangular matrix\n",
    "mask = mask.at[mask == 0].set(-jnp.inf)  # Set future tokens to -inf\n",
    "mask = mask.at[mask == 1].set(0)  # Set allowed tokens to 0\n",
    "mask = mask.reshape(1, 1, max_len, max_len)  # Reshape to (1, 1, 9, 9)\n",
    "\n",
    "# Step 2: Apply mask to QKscaled_decoder (it will broadcast across batch and heads)\n",
    "QKscaled_decoder = QKscaled_decoder + mask \n",
    "QKscaled_decoder[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d5e0fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 5, 9, 9)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention_weights = softmax(QKscaled_decoder)\n",
    "Attention_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3feab27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[[0.939, 1.07],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.929, 1.18],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[1.08, 0.805],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.825, 0.794],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.681, 1.27],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0.939, 1.07],\n",
       "         [0.859, 1.08],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.929, 1.18],\n",
       "         [0.895, 1.01],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[1.08, 0.805],\n",
       "         [1.06, 0.918],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.825, 0.794],\n",
       "         [0.745, 0.611],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.681, 1.27],\n",
       "         [0.609, 1.23],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0.939, 1.07],\n",
       "         [0.859, 1.08],\n",
       "         [0.714, 0.877],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.929, 1.18],\n",
       "         [0.895, 1.01],\n",
       "         [0.66, 0.827],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[1.08, 0.805],\n",
       "         [1.06, 0.918],\n",
       "         [0.73, 0.546],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.825, 0.794],\n",
       "         [0.745, 0.611],\n",
       "         [0.598, 0.588],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.681, 1.27],\n",
       "         [0.609, 1.23],\n",
       "         [0.433, 0.999],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0.939, 1.07],\n",
       "         [0.859, 1.08],\n",
       "         [0.714, 0.877],\n",
       "         [0.859, 1.02],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.929, 1.18],\n",
       "         [0.895, 1.01],\n",
       "         [0.66, 0.827],\n",
       "         [0.864, 0.973],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[1.08, 0.805],\n",
       "         [1.06, 0.918],\n",
       "         [0.73, 0.546],\n",
       "         [0.864, 0.763],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.825, 0.794],\n",
       "         [0.745, 0.611],\n",
       "         [0.598, 0.588],\n",
       "         [0.606, 0.603],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.681, 1.27],\n",
       "         [0.609, 1.23],\n",
       "         [0.433, 0.999],\n",
       "         [0.478, 1.16],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0.939, 1.07],\n",
       "         [0.859, 1.08],\n",
       "         [0.714, 0.877],\n",
       "         [0.859, 1.02],\n",
       "         [0.588, 0.746],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.929, 1.18],\n",
       "         [0.895, 1.01],\n",
       "         [0.66, 0.827],\n",
       "         [0.864, 0.973],\n",
       "         [0.685, 0.869],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[1.08, 0.805],\n",
       "         [1.06, 0.918],\n",
       "         [0.73, 0.546],\n",
       "         [0.864, 0.763],\n",
       "         [0.836, 0.666],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.825, 0.794],\n",
       "         [0.745, 0.611],\n",
       "         [0.598, 0.588],\n",
       "         [0.606, 0.603],\n",
       "         [0.514, 0.425],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.681, 1.27],\n",
       "         [0.609, 1.23],\n",
       "         [0.433, 0.999],\n",
       "         [0.478, 1.16],\n",
       "         [0.585, 0.93],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0.939, 1.07],\n",
       "         [0.859, 1.08],\n",
       "         [0.714, 0.877],\n",
       "         [0.859, 1.02],\n",
       "         [0.588, 0.746],\n",
       "         [0.576, 0.808],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.929, 1.18],\n",
       "         [0.895, 1.01],\n",
       "         [0.66, 0.827],\n",
       "         [0.864, 0.973],\n",
       "         [0.685, 0.869],\n",
       "         [0.683, 0.886],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[1.08, 0.805],\n",
       "         [1.06, 0.918],\n",
       "         [0.73, 0.546],\n",
       "         [0.864, 0.763],\n",
       "         [0.836, 0.666],\n",
       "         [0.84, 0.879],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.825, 0.794],\n",
       "         [0.745, 0.611],\n",
       "         [0.598, 0.588],\n",
       "         [0.606, 0.603],\n",
       "         [0.514, 0.425],\n",
       "         [0.55, 0.413],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.681, 1.27],\n",
       "         [0.609, 1.23],\n",
       "         [0.433, 0.999],\n",
       "         [0.478, 1.16],\n",
       "         [0.585, 0.93],\n",
       "         [0.445, 1],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0.939, 1.07],\n",
       "         [0.859, 1.08],\n",
       "         [0.714, 0.877],\n",
       "         [0.859, 1.02],\n",
       "         [0.588, 0.746],\n",
       "         [0.576, 0.808],\n",
       "         [0.793, 0.905],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.929, 1.18],\n",
       "         [0.895, 1.01],\n",
       "         [0.66, 0.827],\n",
       "         [0.864, 0.973],\n",
       "         [0.685, 0.869],\n",
       "         [0.683, 0.886],\n",
       "         [0.882, 1.12],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[1.08, 0.805],\n",
       "         [1.06, 0.918],\n",
       "         [0.73, 0.546],\n",
       "         [0.864, 0.763],\n",
       "         [0.836, 0.666],\n",
       "         [0.84, 0.879],\n",
       "         [0.887, 0.725],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.825, 0.794],\n",
       "         [0.745, 0.611],\n",
       "         [0.598, 0.588],\n",
       "         [0.606, 0.603],\n",
       "         [0.514, 0.425],\n",
       "         [0.55, 0.413],\n",
       "         [0.636, 0.682],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.681, 1.27],\n",
       "         [0.609, 1.23],\n",
       "         [0.433, 0.999],\n",
       "         [0.478, 1.16],\n",
       "         [0.585, 0.93],\n",
       "         [0.445, 1],\n",
       "         [0.64, 1.12],\n",
       "         [0, 0],\n",
       "         [0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0.939, 1.07],\n",
       "         [0.859, 1.08],\n",
       "         [0.714, 0.877],\n",
       "         [0.859, 1.02],\n",
       "         [0.588, 0.746],\n",
       "         [0.576, 0.808],\n",
       "         [0.793, 0.905],\n",
       "         [0.713, 0.884],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.929, 1.18],\n",
       "         [0.895, 1.01],\n",
       "         [0.66, 0.827],\n",
       "         [0.864, 0.973],\n",
       "         [0.685, 0.869],\n",
       "         [0.683, 0.886],\n",
       "         [0.882, 1.12],\n",
       "         [0.756, 1.11],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[1.08, 0.805],\n",
       "         [1.06, 0.918],\n",
       "         [0.73, 0.546],\n",
       "         [0.864, 0.763],\n",
       "         [0.836, 0.666],\n",
       "         [0.84, 0.879],\n",
       "         [0.887, 0.725],\n",
       "         [0.809, 0.729],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.825, 0.794],\n",
       "         [0.745, 0.611],\n",
       "         [0.598, 0.588],\n",
       "         [0.606, 0.603],\n",
       "         [0.514, 0.425],\n",
       "         [0.55, 0.413],\n",
       "         [0.636, 0.682],\n",
       "         [0.615, 0.68],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.681, 1.27],\n",
       "         [0.609, 1.23],\n",
       "         [0.433, 0.999],\n",
       "         [0.478, 1.16],\n",
       "         [0.585, 0.93],\n",
       "         [0.445, 1],\n",
       "         [0.64, 1.12],\n",
       "         [0.475, 1.06],\n",
       "         [0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0.939, 1.07],\n",
       "         [0.859, 1.08],\n",
       "         [0.714, 0.877],\n",
       "         [0.859, 1.02],\n",
       "         [0.588, 0.746],\n",
       "         [0.576, 0.808],\n",
       "         [0.793, 0.905],\n",
       "         [0.713, 0.884],\n",
       "         [0.76, 1.06]],\n",
       "\n",
       "        [[0.929, 1.18],\n",
       "         [0.895, 1.01],\n",
       "         [0.66, 0.827],\n",
       "         [0.864, 0.973],\n",
       "         [0.685, 0.869],\n",
       "         [0.683, 0.886],\n",
       "         [0.882, 1.12],\n",
       "         [0.756, 1.11],\n",
       "         [0.861, 1.15]],\n",
       "\n",
       "        [[1.08, 0.805],\n",
       "         [1.06, 0.918],\n",
       "         [0.73, 0.546],\n",
       "         [0.864, 0.763],\n",
       "         [0.836, 0.666],\n",
       "         [0.84, 0.879],\n",
       "         [0.887, 0.725],\n",
       "         [0.809, 0.729],\n",
       "         [0.988, 0.939]],\n",
       "\n",
       "        [[0.825, 0.794],\n",
       "         [0.745, 0.611],\n",
       "         [0.598, 0.588],\n",
       "         [0.606, 0.603],\n",
       "         [0.514, 0.425],\n",
       "         [0.55, 0.413],\n",
       "         [0.636, 0.682],\n",
       "         [0.615, 0.68],\n",
       "         [0.785, 0.7]],\n",
       "\n",
       "        [[0.681, 1.27],\n",
       "         [0.609, 1.23],\n",
       "         [0.433, 0.999],\n",
       "         [0.478, 1.16],\n",
       "         [0.585, 0.93],\n",
       "         [0.445, 1],\n",
       "         [0.64, 1.12],\n",
       "         [0.475, 1.06],\n",
       "         [0.593, 1.28]]]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vval_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76e11962",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attention = jnp.matmul(Attention_weights, Vval_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56c951ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 5, 9, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a4b7f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[0.939, 1.07],\n",
       "        [0.939, 1.07],\n",
       "        [0.47, 0.534],\n",
       "        [0.313, 0.356],\n",
       "        [0.235, 0.267],\n",
       "        [0.188, 0.213],\n",
       "        [0.157, 0.178],\n",
       "        [0.134, 0.152],\n",
       "        [0.117, 0.133]],\n",
       "\n",
       "       [[0.929, 1.18],\n",
       "        [0.929, 1.18],\n",
       "        [0.464, 0.59],\n",
       "        [0.31, 0.393],\n",
       "        [0.232, 0.295],\n",
       "        [0.186, 0.236],\n",
       "        [0.155, 0.197],\n",
       "        [0.133, 0.169],\n",
       "        [0.116, 0.147]],\n",
       "\n",
       "       [[1.08, 0.805],\n",
       "        [1.08, 0.805],\n",
       "        [0.541, 0.402],\n",
       "        [0.361, 0.268],\n",
       "        [0.27, 0.201],\n",
       "        [0.216, 0.161],\n",
       "        [0.18, 0.134],\n",
       "        [0.155, 0.115],\n",
       "        [0.135, 0.101]],\n",
       "\n",
       "       [[0.825, 0.794],\n",
       "        [0.825, 0.794],\n",
       "        [0.412, 0.397],\n",
       "        [0.275, 0.265],\n",
       "        [0.206, 0.199],\n",
       "        [0.165, 0.159],\n",
       "        [0.137, 0.132],\n",
       "        [0.118, 0.113],\n",
       "        [0.103, 0.0993]],\n",
       "\n",
       "       [[0.681, 1.27],\n",
       "        [0.681, 1.27],\n",
       "        [0.34, 0.634],\n",
       "        [0.227, 0.423],\n",
       "        [0.17, 0.317],\n",
       "        [0.136, 0.254],\n",
       "        [0.113, 0.211],\n",
       "        [0.0973, 0.181],\n",
       "        [0.0851, 0.159]]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ed38f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9, 9, 10), (9, 9, 10))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention=jnp.array([jnp.concatenate(Attention[i], axis=1) for i in range(9)])\n",
    "Attention.shape,input_decoder.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b2a3377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.939, 1.07, 0.929, 1.18, 1.08, 0.805, 0.825, 0.794, 0.681, 1.27],\n",
       "       [0.939, 1.07, 0.929, 1.18, 1.08, 0.805, 0.825, 0.794, 0.681, 1.27],\n",
       "       [0.899, 1.07, 0.912, 1.09, 1.07, 0.861, 0.785, 0.703, 0.645, 1.25],\n",
       "       [0.599, 0.716, 0.608, 0.729, 0.715, 0.574, 0.523, 0.468, 0.43, 0.833],\n",
       "       [0.45, 0.537, 0.456, 0.547, 0.536, 0.431, 0.392, 0.351, 0.322, 0.625],\n",
       "       [0.36, 0.43, 0.365, 0.437, 0.429, 0.345, 0.314, 0.281, 0.258, 0.5],\n",
       "       [0.3, 0.358, 0.304, 0.365, 0.358, 0.287, 0.262, 0.234, 0.215, 0.416],\n",
       "       [0.257, 0.307, 0.261, 0.312, 0.307, 0.246, 0.224, 0.201, 0.184, 0.357],\n",
       "       [0.225, 0.269, 0.228, 0.273, 0.268, 0.215, 0.196, 0.176, 0.161, 0.312]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "727a0303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 9, 10)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def layer_norm(x, epsilon=1e-6):\n",
    "    # Calculate the mean and variance\n",
    "        mean = jnp.mean(x, axis=-1, keepdims=True)\n",
    "        var = jnp.var(x, axis=-1, keepdims=True) \n",
    "        # Normalize the output\n",
    "        x_norm = (x - mean) / jnp.sqrt(var + epsilon) \n",
    "        return x_norm\n",
    "residual_output = layer_norm(input_decoder + Attention)\n",
    "residual_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1238b3f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b234398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e497b81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bac6f65",
   "metadata": {},
   "source": [
    "# Summary Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "58bc3550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape:  (1, 5, 4)\n",
      "Qval.shape:  (1, 2, 5, 2)\n",
      "Kval.shape:  (1, 2, 5, 2)\n",
      "Vval.shape:  (1, 2, 5, 2)\n",
      "Attention_weights shape: (1, 2, 5, 5)\n",
      "Attention shape: (1, 2, 5, 2)\n",
      "Attention shape concat: (1, 5, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 5, 4)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x, axis=-1):\n",
    "    # Subtract the max value for numerical stability\n",
    "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    " \n",
    "\n",
    "num_phrases = 1\n",
    "words_per_phrase = 5 \n",
    "dk = dv = word2vec_len = 4 # constrain of transformer\n",
    " \n",
    "num_heads=2\n",
    " \n",
    " \n",
    "inputs = np.random.rand(num_phrases,words_per_phrase, word2vec_len)\n",
    "print(\"inputs.shape: \",inputs.shape)\n",
    "\n",
    "Q = np.random.rand(word2vec_len, dk) / jnp.sqrt(word2vec_len)\n",
    "K = np.random.rand(word2vec_len, dk) / jnp.sqrt(word2vec_len)\n",
    "V = np.random.rand(word2vec_len, dv) / jnp.sqrt(word2vec_len)\n",
    "\n",
    "Qval = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs, Q),num_heads,axis=2)), 0, 1)\n",
    "print(\"Qval.shape: \",Qval.shape)\n",
    "\n",
    "Kval = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs, K),num_heads,axis=2)), 0, 1)\n",
    "print(\"Kval.shape: \",Kval.shape)\n",
    "\n",
    "\n",
    "Vval = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs,V),num_heads,axis=2)), 0, 1)\n",
    "print(\"Vval.shape: \",Vval.shape)\n",
    "\n",
    "\n",
    "QKscaled = jnp.matmul(Qval, jnp.transpose(Kval, (0, 1, 3, 2))) / jnp.sqrt(dk)\n",
    "\n",
    "Attention_weights = softmax(QKscaled)\n",
    "print(\"Attention_weights shape:\",Attention_weights.shape)\n",
    "\n",
    "\n",
    "Attention = jnp.matmul(Attention_weights, Vval)\n",
    "print(\"Attention shape:\",Attention.shape)\n",
    "\n",
    "\n",
    "Attention=jnp.array([jnp.concatenate(Attention[i], axis=1) for i in range(num_phrases)])\n",
    "print(\"Attention shape concat:\",Attention.shape)\n",
    "\n",
    "\n",
    "def layer_norm(x, epsilon=1e-6):\n",
    "    # Calculate the mean and variance\n",
    "        mean = jnp.mean(x, axis=-1, keepdims=True)\n",
    "        var = jnp.var(x, axis=-1, keepdims=True) \n",
    "        # Normalize the output\n",
    "        x_norm = (x - mean) / jnp.sqrt(var + epsilon) \n",
    "        return x_norm\n",
    "\n",
    "\n",
    "encoder_output=layer_norm(Attention+inputs)\n",
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4523a2c6",
   "metadata": {},
   "source": [
    "# Summary Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4be68b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qval.shape:  (7, 2, 7, 2)\n",
      "Kval.shape:  (7, 2, 7, 2)\n",
      "Vval.shape:  (7, 2, 7, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7, 7, 4)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_word_embedding_size=4\n",
    "decoder_input_number_of_words_per_phrase=7\n",
    "num_heads_decoder=2# dv=10 \n",
    "dv_decoder=4     \n",
    "inputs_decoder = np.random.rand(num_phrases,decoder_input_number_of_words_per_phrase, decoder_input_word_embedding_size)# for the target language suppose the \n",
    "target_decoder = inputs_decoder\n",
    "\n",
    "input_translation=[]\n",
    "def pad_sequence(seq, max_len, pad_value=0):\n",
    "    \"\"\"Pad a sequence with a given value up to max_len.\"\"\"\n",
    "    current_len = seq.shape[0]\n",
    "    pad_width = max_len - current_len\n",
    "    if pad_width > 0:\n",
    "        # Pad sequence with zeros (or any pad_value you provide)\n",
    "        seq = jnp.pad(seq, ((0, pad_width), (0, 0)), mode='constant', constant_values=pad_value)\n",
    "    return seq\n",
    "\n",
    "# Example usage:\n",
    "max_len = decoder_input_number_of_words_per_phrase  # Max sequence length for decoder input\n",
    " \n",
    "for j in range(inputs_decoder.shape[0]):\n",
    "    # Create padded sequences\n",
    "    padded_sequences = [pad_sequence(inputs_decoder[j][0:i], max_len) for i in range(1, inputs_decoder.shape[1] + 1)]\n",
    "    input_translation.append(padded_sequences)\n",
    "\n",
    "\n",
    "# Convert to an array for batching\n",
    "input_translation = jnp.array(input_translation)\n",
    "\n",
    "input_decoder=input_translation[0]\n",
    "\n",
    "Q_decoder = np.random.rand(decoder_input_word_embedding_size, dv_decoder) / jnp.sqrt(decoder_input_word_embedding_size)\n",
    "K_decoder = np.random.rand(decoder_input_word_embedding_size, dv_decoder) / jnp.sqrt(decoder_input_word_embedding_size)\n",
    "V_decoder = np.random.rand(decoder_input_word_embedding_size, dv_decoder) / jnp.sqrt(decoder_input_word_embedding_size)\n",
    "\n",
    "\n",
    "Qval_decoder  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(input_decoder, Q_decoder),num_heads_decoder,axis=2)), 0, 1)\n",
    "print(\"Qval.shape: \",Qval_decoder.shape)\n",
    "\n",
    "Kval_decoder  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(input_decoder, K_decoder),num_heads_decoder,axis=2)), 0, 1)\n",
    "print(\"Kval.shape: \",Kval_decoder.shape)\n",
    "\n",
    "\n",
    "Vval_decoder  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(input_decoder,V_decoder),num_heads_decoder,axis=2)), 0, 1)\n",
    "print(\"Vval.shape: \",Vval_decoder.shape)\n",
    "\n",
    "\n",
    "QKscaled_decoder  = jnp.matmul(Qval_decoder, jnp.transpose(Kval_decoder, (0, 1, 3, 2))) / jnp.sqrt(dk) + jnp.triu(jnp.ones((decoder_input_number_of_words_per_phrase, decoder_input_number_of_words_per_phrase)))* -1e9 \n",
    "# Step 1: Create a causal mask of shape (1, 1, 9, 9) to broadcast across heads and batch\n",
    "mask = jnp.tril(jnp.ones((max_len, max_len)))  # (9, 9) lower triangular matrix\n",
    "mask = mask.at[mask == 0].set(-jnp.inf)  # Set future tokens to -inf\n",
    "mask = mask.at[mask == 1].set(0)  # Set allowed tokens to 0\n",
    "mask = mask.reshape(1, 1, max_len, max_len)  # Reshape to (1, 1, 9, 9)\n",
    "\n",
    "# Step 2: Apply mask to QKscaled_decoder (it will broadcast across batch and heads)\n",
    "QKscaled_decoder = QKscaled_decoder + mask \n",
    "\n",
    "Attention_weights = softmax(QKscaled_decoder)\n",
    "\n",
    "\n",
    "Attention = jnp.matmul(Attention_weights, Vval_decoder)\n",
    "\n",
    "\n",
    "Attention=jnp.array([jnp.concatenate(Attention[i], axis=1) for i in range(num_phrases)])\n",
    "\n",
    "\n",
    "def layer_norm(x, epsilon=1e-6):\n",
    "    # Calculate the mean and variance\n",
    "        mean = jnp.mean(x, axis=-1, keepdims=True)\n",
    "        var = jnp.var(x, axis=-1, keepdims=True) \n",
    "        # Normalize the output\n",
    "        x_norm = (x - mean) / jnp.sqrt(var + epsilon) \n",
    "        return x_norm\n",
    "residual_output = layer_norm(input_decoder + Attention)\n",
    "residual_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0bf2cd",
   "metadata": {},
   "source": [
    " # Cross Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "598dba57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 5, 4), (7, 7, 4))"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output.shape,residual_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "73306f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qval.shape:  (7, 2, 7, 2)\n",
      "Kval.shape:  (1, 2, 5, 2)\n",
      "Vval.shape:  (1, 2, 5, 2)\n",
      "output_sublayer_two_decoder shape: (7, 7, 4)\n"
     ]
    }
   ],
   "source": [
    "Q_cross_attention = np.random.rand(decoder_input_word_embedding_size, dv_decoder) / jnp.sqrt(decoder_input_word_embedding_size)\n",
    "K_cross_attention = np.random.rand(encoder_output.shape[-1], dv_decoder) / jnp.sqrt(encoder_output.shape[-1])\n",
    "V_cross_attention = np.random.rand(encoder_output.shape[-1], dv_decoder) / jnp.sqrt(encoder_output.shape[-1])\n",
    "\n",
    "Qval_cross_attention  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(residual_output, Q_cross_attention),num_heads_decoder,axis=2)), 0, 1)\n",
    "print(\"Qval.shape: \",Qval_cross_attention.shape)\n",
    "\n",
    "Kval_cross_attention  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(encoder_output, K_cross_attention),num_heads_decoder,axis=2)), 0, 1)\n",
    "print(\"Kval.shape: \",Kval_cross_attention.shape)\n",
    "\n",
    "\n",
    "Vval_cross_attention  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(encoder_output,V_cross_attention),num_heads_decoder,axis=2)), 0, 1)\n",
    "print(\"Vval.shape: \",Vval_cross_attention.shape)\n",
    "\n",
    "Kval_cross_attention = Kval_cross_attention[0]  # Use the first phrase from the encoder output\n",
    "Vval_cross_attention = Vval_cross_attention[0]  # Use the first phrase from the encoder output\n",
    "\n",
    "QKscaled_cross_attention  = jnp.matmul(Qval_cross_attention, jnp.transpose(jnp.expand_dims(Kval_cross_attention, axis=0) , (0, 1, 3, 2))) / jnp.sqrt(dv_decoder)\n",
    "Attention_weights_cross = softmax(QKscaled_cross_attention)\n",
    "Attention_cross = jnp.matmul(Attention_weights_cross, jnp.expand_dims(Vval_cross_attention, axis=0))\n",
    "Attention_cross=jnp.array([jnp.concatenate(Attention_cross[i], axis=1) for i in range(decoder_input_number_of_words_per_phrase)]) \n",
    "output_decoder = layer_norm(Attention_cross + residual_output)  # residual_output is (9, 9, 10)\n",
    "print(\"output_sublayer_two_decoder shape:\", output_decoder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "351c8e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[0.768, 1.14, -1.34, -0.564],\n",
       "        [1.09, 0.499, -1.6, 0.00659],\n",
       "        [1.09, 0.499, -1.6, 0.00659],\n",
       "        [1.09, 0.499, -1.6, 0.00659],\n",
       "        [1.09, 0.499, -1.6, 0.00658],\n",
       "        [1.09, 0.499, -1.6, 0.00657],\n",
       "        [1.09, 0.499, -1.6, 0.00656]],\n",
       "\n",
       "       [[0.768, 1.14, -1.34, -0.564],\n",
       "        [-0.158, 1.23, 0.431, -1.51],\n",
       "        [1.09, 0.499, -1.6, 0.00659],\n",
       "        [1.09, 0.499, -1.6, 0.00659],\n",
       "        [1.09, 0.499, -1.6, 0.00658],\n",
       "        [1.09, 0.499, -1.6, 0.00657],\n",
       "        [1.09, 0.499, -1.6, 0.00656]],\n",
       "\n",
       "       [[0.768, 1.14, -1.34, -0.564],\n",
       "        [-0.158, 1.23, 0.431, -1.51],\n",
       "        [-0.685, 1.72, -0.345, -0.685],\n",
       "        [1.09, 0.499, -1.6, 0.00659],\n",
       "        [1.09, 0.499, -1.6, 0.00658],\n",
       "        [1.09, 0.499, -1.6, 0.00657],\n",
       "        [1.09, 0.499, -1.6, 0.00656]],\n",
       "\n",
       "       [[0.768, 1.14, -1.34, -0.564],\n",
       "        [-0.158, 1.23, 0.431, -1.51],\n",
       "        [-0.685, 1.72, -0.345, -0.685],\n",
       "        [-1.31, 1.21, -0.596, 0.692],\n",
       "        [1.09, 0.499, -1.6, 0.00658],\n",
       "        [1.09, 0.499, -1.6, 0.00657],\n",
       "        [1.09, 0.499, -1.6, 0.00656]],\n",
       "\n",
       "       [[0.768, 1.14, -1.34, -0.564],\n",
       "        [-0.158, 1.23, 0.431, -1.51],\n",
       "        [-0.685, 1.72, -0.345, -0.685],\n",
       "        [-1.31, 1.21, -0.596, 0.692],\n",
       "        [1.51, -1.14, -0.593, 0.222],\n",
       "        [1.09, 0.499, -1.6, 0.00657],\n",
       "        [1.09, 0.499, -1.6, 0.00656]],\n",
       "\n",
       "       [[0.768, 1.14, -1.34, -0.564],\n",
       "        [-0.158, 1.23, 0.431, -1.51],\n",
       "        [-0.685, 1.72, -0.345, -0.685],\n",
       "        [-1.31, 1.21, -0.596, 0.692],\n",
       "        [1.51, -1.14, -0.593, 0.222],\n",
       "        [-0.571, -1.3, 0.601, 1.27],\n",
       "        [1.09, 0.499, -1.6, 0.00656]],\n",
       "\n",
       "       [[0.768, 1.14, -1.34, -0.564],\n",
       "        [-0.158, 1.23, 0.431, -1.51],\n",
       "        [-0.685, 1.72, -0.345, -0.685],\n",
       "        [-1.31, 1.21, -0.596, 0.692],\n",
       "        [1.51, -1.14, -0.593, 0.222],\n",
       "        [-0.571, -1.3, 0.601, 1.27],\n",
       "        [-1.23, 1.05, 0.912, -0.731]]], dtype=float32)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "204a3f57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "93245019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 5, 9, 7)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention_weights_cross = softmax(QKscaled_cross_attention)\n",
    "Attention_weights_cross.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b4f13f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5, 7, 2)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.expand_dims(Vval_cross_attention, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "983da380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9, 5, 9, 2), (9, 9, 10))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention_cross = jnp.matmul(Attention_weights_cross, jnp.expand_dims(Vval_cross_attention, axis=0) )\n",
    "Attention_cross.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0ab6b444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9, 9, 10), (9, 9, 10))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention_cross=jnp.array([jnp.concatenate(Attention_cross[i], axis=1) for i in range(decoder_input_number_of_words_per_phrase)]) \n",
    "Attention_cross.shape,residual_output.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "28951de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_sublayer_two_decoder shape: (9, 9, 10)\n"
     ]
    }
   ],
   "source": [
    "output_decoder = layer_norm(Attention_cross + residual_output)  # residual_output is (9, 9, 10)\n",
    "print(\"output_sublayer_two_decoder shape:\", output_decoder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a55a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ca80b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "linearlayer= np.random.rand(output_decoder.shape[1], num_classes)   \n",
    "linear_bias = np.random.rand(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cead9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dc9890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3486f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe206b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a37a52d-62e9-4fb1-8126-7b73b592ad53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
