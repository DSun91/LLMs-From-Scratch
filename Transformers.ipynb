{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "430cf7fc-f02d-414f-b79a-28fe47b7fbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  29582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape  (2, 10, 15) <class 'jaxlib.xla_extension.ArrayImpl'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 2/2 [00:00<00:00,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_sublayer_one shape (2, 10, 15)\n",
      "input shape  (2, 10, 15) <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "output_sublayer_one shape (2, 10, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    " \n",
    "import re\n",
    "import cupy as cp\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np \n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jax\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "def log_time(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()  # Record start time\n",
    "        result = func(*args, **kwargs)  # Execute the wrapped function\n",
    "        end_time = time.time()  # Record end time\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Function '{func.__name__}' executed in {elapsed_time:.4f} seconds\")\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "class Transformer:\n",
    "\n",
    "    def __init__(self,embedding_size,\n",
    "                 complete_text_origin,\n",
    "                 complete_text_target,\n",
    "                 max_lenght_phrase,\n",
    "                 epochs,\n",
    "                 batch_size,\n",
    "                 dv,\n",
    "                 num_heads,\n",
    "                 flush_vocab=True):\n",
    "        \n",
    "        self.embedding_size=embedding_size\n",
    "        self.max_lenght_phrase=max_lenght_phrase\n",
    "        self.defaultkey=jax.random.key(55)\n",
    "        self.epochs=epochs\n",
    "        self.batch_size= batch_size\n",
    "        self.flush_vocab=flush_vocab\n",
    "        self.dv=dv\n",
    "        self.dk=dv\n",
    "        self.num_heads=num_heads\n",
    "        if complete_text_target!=\"\":\n",
    "            complete_text=complete_text_origin+\" \"+complete_text_target+\" [START] [PAD] [END] \"\n",
    "            self.vocabulary=self.create_vocabulary(complete_text,\"vocabulary\") \n",
    "        else:\n",
    "            complete_text=complete_text_origin+\" [START] [PAD] [END] \"\n",
    "            self.vocabulary=self.create_vocabulary(complete_text,\"vocabulary\")\n",
    "\n",
    "        # Initialize weights with Xavier/Glorot initialization\n",
    "        self.Q_Encoder = np.random.randn(self.embedding_size, self.dv) / np.sqrt(self.embedding_size)  # * 0.01\n",
    "        self.K_Encoder = np.random.randn(self.embedding_size, self.dv) / np.sqrt(self.embedding_size)  # * 0.01\n",
    "        self.V_Encoder = np.random.randn(self.embedding_size, self.dv) / np.sqrt(self.embedding_size)  # * 0.01\n",
    "        self.linearlayerAttentionEncoder= np.random.rand(self.batch_size,dv, embedding_size)   \n",
    "        self.linear_biasAttentionEncoder = np.random.rand(self.batch_size,1,embedding_size)\n",
    "\n",
    "    def cross_entropy_loss(self, predictions, target):\n",
    "        # Cross-entropy loss for a batch of predictions and targets\n",
    "        batch_loss = -jnp.sum(target * jnp.log(predictions + 1e-9), axis=1)\n",
    "        return jnp.mean(batch_loss)\n",
    "\n",
    "    def softmax(self, x, axis=-1):\n",
    "        x = jnp.clip(x, -1e4, 1e4)  # Clip for numerical stability\n",
    "        e_x = jnp.exp(x - jnp.max(x, axis=axis, keepdims=True))\n",
    "        return e_x / jnp.sum(e_x, axis=axis, keepdims=True)\n",
    "    \n",
    "    #@log_time\n",
    "    def create_vocabulary(self,complete_text,name):\n",
    "\n",
    "        existing_vocab = Path(f\"data/{name}.pkl\")\n",
    "        if existing_vocab.is_file() and self.flush_vocab==False:\n",
    "            with open('data/vocabulary.pkl', 'rb') as f:\n",
    "                vocabulary=pickle.load(f)\n",
    "\n",
    "        else: \n",
    "            # Use re.findall to split considering punctuation\n",
    "            text = re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]', complete_text)\n",
    "            words_list = list(set(text))\n",
    "            vocabulary=dict()\n",
    "            for i in words_list:\n",
    "                vocabulary[i]=jax.random.uniform(jax.random.key(np.random.randint(10000)),self.embedding_size)\n",
    "            \n",
    "            print(\"Vocabulary size: \", len(vocabulary))\n",
    "            with open(f\"data/{name}.pkl\", 'wb') as handle:\n",
    "                pickle.dump(vocabulary, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "        return vocabulary\n",
    "    \n",
    "  \n",
    " \n",
    "    #@log_time\n",
    "    def generate_input(self, x_batch,y_batch):\n",
    "\n",
    "        #print(\"batch prases original:\\n\",x_batch)\n",
    "         \n",
    "        xi=[]\n",
    "        y_batch=[\" \".join(y) for y in y_batch]\n",
    "        #print(y_batch)\n",
    "        phrase_vectors_x = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]', x) for x in x_batch]\n",
    "        phrase_vectors_y = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]', y) for y in y_batch]\n",
    "        #print(\"phrase_vectors:\\n\",phrase_vectors)\n",
    "\n",
    "        xi=jnp.array([[self.vocabulary[word] for word in phrase_vector] for phrase_vector in phrase_vectors_x])\n",
    "        yi=jnp.array([[self.vocabulary[word] for word in phrase_vector] for phrase_vector in phrase_vectors_y])\n",
    "        \n",
    "        yield xi,yi\n",
    " \n",
    "    #@log_time\n",
    "    def pad_sequences(self,sentences, pad_token='[PAD]'):\n",
    "        \"\"\"\n",
    "        Pads the input sentences to have the same length by adding [PAD] tokens at the end.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Split each sentence into words\n",
    "        tokenized_sentences = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]', sentence) for sentence in sentences]\n",
    "        \n",
    "        max_lenght=max(len(sentence) for sentence in tokenized_sentences)\n",
    "\n",
    "        if self.max_lenght_phrase==0: \n",
    "            # Find the maximum sentence length\n",
    "            self.max_lenght_phrase = max_lenght\n",
    "         \n",
    "        if self.max_lenght_phrase>max_lenght:\n",
    "            #print(\"self.max_lenght_phrase>max_lenght\")\n",
    "            # Pad each sentence with the [PAD] token to make them of equal length\n",
    "            padded_sentences = [\" \".join(map(str, sentence + [pad_token] * (self.max_lenght_phrase - len(sentence)))) for sentence in tokenized_sentences]\n",
    "        else: \n",
    "            padded_sentences=[\" \".join(map(str, (sentence + [pad_token] * (self.max_lenght_phrase - len(sentence)))[0:self.max_lenght_phrase])) for sentence in tokenized_sentences]\n",
    "         \n",
    "\n",
    "\n",
    "        # print(\"-----------------\")\n",
    "        # self.print_matrix(padded_sentences)\n",
    "        # print(\"-----------------\\n\\n\")\n",
    "        return padded_sentences\n",
    "    \n",
    "    \n",
    "    # add <start> <end> tokens\n",
    "    #@log_time\n",
    "    def padding_start_end_tokens_target(self,yi):\n",
    "        return [f'[START] {sentence} [END]' for sentence in yi]\n",
    "    \n",
    "\n",
    "    #@log_time     \n",
    "    def preprocess_target(self,yi):\n",
    "        y=[]\n",
    "        yi=sorted(yi, key=lambda x: len(x.split()), reverse=True)\n",
    "        for j in yi:\n",
    "            phrase=j.split()\n",
    "            for i in range(1,len(phrase)):\n",
    "                y.append(phrase[0:i]+[\"[PAD]\" for x in range(len(phrase)-i-1)]+[\"[END]\"])\n",
    "        return y\n",
    "     \n",
    "    def print_matrix(self,x):\n",
    "        for i in x:\n",
    "            print(i)\n",
    "\n",
    "    def layer_norm(self,x, epsilon=1e-6):\n",
    "    # Calculate the mean and variance\n",
    "        mean = jnp.mean(x, axis=-1, keepdims=True)\n",
    "        var = jnp.var(x, axis=-1, keepdims=True) \n",
    "        # Normalize the output\n",
    "        x_norm = (x - mean) / jnp.sqrt(var + epsilon) \n",
    "        return x_norm\n",
    "\n",
    "    #@log_time \n",
    "    def MultiHeadsAttentionEncoder(self, Inputs):\n",
    "        #print(\"cupy.matmul(Inputs, self.Q)\", cupy.matmul(Inputs, self.Q).shape)\n",
    "        #print(\"cupy.array_split:\",cupy.array(cupy.array_split(cupy.matmul(Inputs, self.Q), self.num_heads, axis=2)).shape)\n",
    "        # print(\"Kval shape:\", self.Kval.shape)\n",
    "        # print(\"Vval shape:\", self.Vval.shape)\n",
    "        # print(\"Q  shape:\", self.Q_Encoder.shape)\n",
    "        # print(\"K  shape:\", self.K_Encoder.shape)\n",
    "        # print(\"V  shape:\", self.V_Encoder.shape)\n",
    "        self.Qval_Encoder = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(Inputs, self.Q_Encoder), self.num_heads, axis=2)), 0, 1)\n",
    "        self.Kval_Encoder = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(Inputs, self.K_Encoder), self.num_heads, axis=2)), 0, 1)\n",
    "        self.Vval_Encoder = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(Inputs, self.V_Encoder), self.num_heads, axis=2)), 0, 1)\n",
    "        # print(\"Qval shape:\", self.Qval_Encoder.shape)\n",
    "        # print(\"Kval shape:\", self.Kval_Encoder.shape)\n",
    "        # print(\"Vval shape:\", self.Vval_Encoder.shape)\n",
    "\n",
    "        QKscaled_Encoder = jnp.matmul(self.Qval_Encoder, jnp.transpose(self.Kval_Encoder, (0, 1, 3, 2))) / jnp.sqrt(self.dk)\n",
    "        # print(\"QKscaled shape:\", QKscaled.shape)\n",
    "\n",
    "        self.Attention_weights_Encoder = self.softmax(QKscaled_Encoder)\n",
    "        # print(\"Attention_weights shape:\", self.Attention_weights.shape)\n",
    "\n",
    "        Attention_output_Encoder = jnp.matmul(self.Attention_weights_Encoder, self.Vval_Encoder)\n",
    "        # print(\"Attention output shape:\", Attention_output.shape)\n",
    "        Attention_output_Encoder=jnp.array([jnp.concatenate(Attention_output_Encoder[i], axis=1) for i in range(self.batch_size)])\n",
    "\n",
    "        return Attention_output_Encoder\n",
    "    \n",
    "    def linear_layer_attention_encoder(self,Encoder_attention_output): \n",
    "        return jnp.matmul(Encoder_attention_output,self.linearlayerAttentionEncoder)+self.linear_biasAttentionEncoder\n",
    "\n",
    "    def inputs_add_and_norm_encoder(self,Encoder_attention_output, Inputs): \n",
    "        input_dimension_remappedEncoder=self.linear_layer_attention_encoder(Encoder_attention_output)+Inputs\n",
    "        return self.layer_norm(input_dimension_remappedEncoder)\n",
    "    \n",
    "    def forward_step_encoder(self,Inputs):\n",
    "        Encoder_attention_output=self.MultiHeadsAttentionEncoder(Inputs)\n",
    " \n",
    "        output_sublayer_one=self.inputs_add_and_norm_encoder(Encoder_attention_output, Inputs)\n",
    "\n",
    "        \n",
    "        \n",
    "        print(\"output_sublayer_one shape\",output_sublayer_one.shape)\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def train(self,X_train,y_train):\n",
    "         y_train=self.padding_start_end_tokens_target(self.pad_sequences(y_train))\n",
    "         \n",
    "         X_train=self.pad_sequences(X_train)\n",
    "         \n",
    "         for epoch in range(self.epochs):\n",
    "            self.iterations=0\n",
    "            total_loss = 0 \n",
    "            num_batches_per_epoch = len(X_train) // self.batch_size\n",
    "         \n",
    "            for i in tqdm(range(num_batches_per_epoch), desc=f\"Epoch {epoch + 1}/{self.epochs}\"):\n",
    "\n",
    "                start = i * self.batch_size\n",
    "                end = start + self.batch_size\n",
    "                X_batch_phrases = X_train[start:end]\n",
    "                y_batch_phrases= y_train[start:end]\n",
    "                y_batch_phrases=self.preprocess_target(y_batch_phrases)\n",
    "                #self.print_matrix(y_batch_phrases)\n",
    "\n",
    "                for xi,yi in self.generate_input(X_batch_phrases,y_batch_phrases): \n",
    "                    #print(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n",
    "                    #self.print_matrix(xi)\n",
    "                    print(\"input shape \",xi.shape,type(xi)) \n",
    "                    #print(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n",
    "                    #self.print_matrix(yi)\n",
    "                    #print(\"target shape\",yi.shape,type(yi)) \n",
    "                    self.forward_step_encoder(xi)\n",
    "                     \n",
    "\n",
    "\n",
    "X_train=[\"i love soy sauce!\", \n",
    "         \"my dog... is cute\", \n",
    "         \"you are crazy strong!\",\n",
    "         \"the friend is good, you know\"]\n",
    "y_train=[\"amo la salsa di soia!\",\n",
    "        \"il cane... è tenero\",\n",
    "        \"sei pazzo potente!\",\n",
    "        \"l'amico è buono, vero?\"]        \n",
    " \n",
    "df = pd.read_csv(\"data/bbc-text.csv\")\n",
    "# complete_text = ' '.join(df['text'].str.split()) \n",
    "complete_text_origin = ' '.join(df[\"text\"].tolist())\n",
    "complete_text_target = ' '.join(y_train)\n",
    "embedding_size = 15 #this is the initial size for the word embedding\n",
    "max_words_per_phrase=10# consider +2 then adding start and end token this fix max lengh\n",
    "batch_size=2 # cosidering 2 phrase per time\n",
    "\n",
    "model = Transformer(embedding_size=embedding_size,\n",
    "                    complete_text_origin=complete_text_origin,\n",
    "                    complete_text_target=complete_text_target,\n",
    "                    max_lenght_phrase=max_words_per_phrase,\n",
    "                    epochs=1,\n",
    "                    batch_size=2,\n",
    "                    dv=8,\n",
    "                    num_heads=4,\n",
    "                    flush_vocab=True)\n",
    " \n",
    "\n",
    "model.train(X_train,y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f068c65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=[\"amo la salsa di soia!\",\n",
    "        \"il cane... è tenero\",\n",
    "        \"sei pazzo potente!\",\n",
    "        \"l'amico è buono, gia lo sai o no puo essere\"]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8521bb58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"l'amico è buono, gia lo sai o no puo essere\",\n",
       " 'amo la salsa di soia!',\n",
       " 'il cane... è tenero',\n",
       " 'sei pazzo potente!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(y_train, key=lambda x: len(x.split()), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dac634d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"l'amico è buono, gia lo sai o no puo essere\",\n",
       " 'amo la salsa di soia!',\n",
       " 'il cane... è tenero',\n",
       " 'sei pazzo potente!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(y_train, key=lambda x: len(x.split()), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e9a43e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[START]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[END]']\n",
      "['[START]', 'amo', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[END]']\n",
      "['[START]', 'amo', 'la', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[END]']\n",
      "['[START]', 'amo', 'la', 'salsa', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[END]']\n",
      "['[START]', 'amo', 'la', 'salsa', 'di', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[END]']\n",
      "['[START]', 'amo', 'la', 'salsa', 'di', 'soia', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[END]']\n",
      "['[START]', 'amo', 'la', 'salsa', 'di', 'soia', '!', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[END]']\n",
      "['[START]', 'amo', 'la', 'salsa', 'di', 'soia', '!', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[END]']\n",
      "['[START]', 'amo', 'la', 'salsa', 'di', 'soia', '!', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[END]']\n",
      "['[START]', 'amo', 'la', 'salsa', 'di', 'soia', '!', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[END]']\n",
      "['[START]', 'amo', 'la', 'salsa', 'di', 'soia', '!', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[END]']\n"
     ]
    }
   ],
   "source": [
    "phrase='[START] amo la salsa di soia ! [PAD] [PAD] [PAD] [PAD] [END]'\n",
    "phrase=phrase.split()\n",
    "for i in range(1,len(phrase)):\n",
    "    print(phrase[0:i]+[\"[PAD]\" for x in range(len(phrase)-i-1)]+[\"[END]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0bfad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe9a64f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This', 'is', 'a', 'test', '[PAD]', '!'],\n",
       " ['[START]', 'Hello', ',', 'how', 'are', 'you', '?', '[CLOSE]']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_phrases = [\"This is a test [PAD]!\", \"[START] Hello, how are you? [CLOSE]\"]\n",
    "\n",
    "# Modified regex to capture words like [PAD], [START], and [CLOSE]\n",
    "phrase_vectors = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]', x) for x in batch_phrases]\n",
    "phrase_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a2c903-f003-42ef-9d3a-a216b1a3185f",
   "metadata": {},
   "source": [
    "# Prepare input lookup table\n",
    "\n",
    "Step-by-Step Process:\n",
    "Create a Vocabulary:\n",
    "Map each word to an index (token) and initialize a random vector for each word.\n",
    "Initialize Embedding Vectors:\n",
    "For each word in the vocabulary, initialize a random embedding vector (say of dimension 3 or 512).\n",
    "For Each Input Sequence:\n",
    "Convert the words to their corresponding vectors using the vocabulary.\n",
    "Stack the vectors to form an input matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9e19b31-64ea-4dd2-bc04-9bf120a00f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.3952378 , 0.45534575, 0.91380334, 0.29122305, 0.82850766],\n",
       "       [0.58672273, 0.6880075 , 0.23995149, 0.81122804, 0.08536363],\n",
       "       [0.34470356, 0.04894471, 0.00256085, 0.6435065 , 0.50082767],\n",
       "       [0.22316742, 0.05539286, 0.23274505, 0.45073962, 0.51079834],\n",
       "       [0.01448357, 0.23985529, 0.0051235 , 0.70521474, 0.73882663]],      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.random   \n",
    "seed=55\n",
    "\n",
    "@log_time\n",
    "def create_vocabulary(complete_text):\n",
    "\n",
    "    existing_vocab = Path(\"data/vocabulary.pkl\")\n",
    "    if existing_vocab.is_file() and flush_vocab==False:\n",
    "        with open('data/vocabulary.pkl', 'rb') as f:\n",
    "            vocabulary=pickle.load(f)\n",
    "\n",
    "    else: \n",
    "       # Use re.findall to split considering punctuation\n",
    "        text = re.findall(r'\\w+|[^\\w\\s]', complete_text)\n",
    "        words_list = list(set(text))\n",
    "        vocabulary=dict()\n",
    "        for i in words_list:\n",
    "            vocabulary[i]=jax.random.uniform(jax.random.key(np.random.randint(10000)),self.embedding_size)\n",
    "        \n",
    "        print(\"Vocabulary size: \", len(vocabulary))\n",
    "        with open('data/vocabulary.pkl', 'wb') as handle:\n",
    "            pickle.dump(vocabulary, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return vocabulary\n",
    "\n",
    "@log_time\n",
    "def generate_input(batch_phrases):\n",
    "\n",
    "    phrase_vectors=[]   \n",
    "    xi=[]\n",
    "    \n",
    "    phrase_vectors=[x.split() for x in batch_phrases]\n",
    "    \n",
    "    xi=jnp.array([[vocabulary[word] for word in phrase_vector] for phrase_vector in phrase_vectors])\n",
    "    \n",
    "    return xi\n",
    "\n",
    "\n",
    "embedding_size=512\n",
    "key=jax.random.key(seed)\n",
    "x = jax.random.uniform(key,(5,5))\n",
    "y = jnp.arange(5)\n",
    "x\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1360d2",
   "metadata": {},
   "source": [
    "# Prepare input for decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba893391-ff82-47a9-8a60-baf6c6b3fe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sentences,lenght=0, pad_token='[PAD]'):\n",
    "        \"\"\"\n",
    "        Pads the input sentences to have the same length by adding [PAD] tokens at the end.\n",
    "        \"\"\"\n",
    "        # Split each sentence into words\n",
    "        tokenized_sentences = [sentence.split() for sentence in sentences]\n",
    "        \n",
    "        if lenght==0: \n",
    "            # Find the maximum sentence length\n",
    "            max_len = max(len(sentence) for sentence in tokenized_sentences)\n",
    "        else:\n",
    "            max_len=lenght\n",
    "        \n",
    "        # Pad each sentence with the [PAD] token to make them of equal length\n",
    "        padded_sentences = [\" \".join(sentence + [pad_token] * (max_len - len(sentence))) for sentence in tokenized_sentences]\n",
    "        \n",
    "        return padded_sentences\n",
    "    \n",
    "    \n",
    "    # add <start> <end> tokens\n",
    "def prepropcess_target(yi):\n",
    "    return [f'[START] {sentence} [END]' for sentence in yi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3c96b25-a83f-4dcf-b4e3-6944a2fb33f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'soy', 'sauce', '!']\n"
     ]
    }
   ],
   "source": [
    "text = \"i love soy sauce!\"\n",
    "\n",
    "# Use re.findall to split considering punctuation\n",
    "words = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c912210c-6de6-438b-bc56-3aff61dfac77",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m prepropcess_target(pad_sequences(\u001b[43myi\u001b[49m,\u001b[38;5;241m8\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'yi' is not defined"
     ]
    }
   ],
   "source": [
    "prepropcess_target(pad_sequences(yi,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b321eaaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i love soy sauce [PAD] [PAD] [PAD] [PAD]',\n",
       " 'my dog is cute [PAD] [PAD] [PAD] [PAD]',\n",
       " 'you are crazy strong [PAD] [PAD] [PAD] [PAD]',\n",
       " 'the friend is good [PAD] [PAD] [PAD] [PAD]']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences(X_train,8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
