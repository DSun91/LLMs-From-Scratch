{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00344db5-aab6-4409-84d9-ef8cdf2e2db9",
   "metadata": {},
   "source": [
    " # NLP Model Implementation Using the QVC Attention Mechanism\n",
    "\n",
    "This project focuses on developing a Natural Language Processing (NLP) model using the QVC (Query, Value, Context) attention mechanism from scratch using Python and Numpy. The attention mechanism is a critical component in modern NLP models, enhancing their ability to focus on different parts of the input sequence to make more accurate predictions.\n",
    "\n",
    "## Key Components:\n",
    "\n",
    "- **QVC Attention Mechanism**: Understanding and implementing the Query, Value, and Context (QVC) attention mechanism from scratch.\n",
    "- **Model Architecture**: Building the architecture of the NLP model utilizing QVC attention.\n",
    "- **Training and Evaluation**: Training the model with appropriate datasets and evaluating its performance.\n",
    "\n",
    "This project aims to provide a comprehensive guide to implementing and experimenting with attention mechanisms in NLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7146946e-0ba5-4973-84d5-b9e5b5086ab8",
   "metadata": {},
   "source": [
    "# Preparing Input Data for NLP Model\n",
    "\n",
    "In this section, we are preparing the input data for our NLP model by defining arrays representing word embeddings and combining them into a structured format.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa334299-b0e7-483b-ae85-95b418fd030e",
   "metadata": {},
   "source": [
    "Let's consider as starting point for example 3 phrases made of 4 words each where each word have embedding size 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9826cbc-8702-4db0-8063-c3425427ed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('c:\\\\python312\\\\lib\\\\site-packages')\n",
    "import numpy as np\n",
    " \n",
    "\n",
    "# Define four arrays of size 5\n",
    "word1 = np.array([0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "word2 = np.array([0.5, 0.4, 0.7,0.3, 0.2])\n",
    "word3 = np.array([0.2,0.7, 0.3, 0.5, 0.4])\n",
    "word4 = np.array([0.4, 0.1,0.7, 0.2, 0.5])\n",
    "\n",
    "word5 = np.array([0.1, 0.9, 0.3, 0.4, 0.5])\n",
    "word6 = np.array([0.4, 0.4, 0.7,0.3, 0.8])\n",
    "word7 = np.array([0.2,0.7, 0.4, 0.5, 0.4])\n",
    "word8 = np.array([0.4, 0.5,0.7, 0.7, 0.8])\n",
    "\n",
    "word9 = np.array([0.1, 0.2, 0.3, 0.8, 0.5])\n",
    "word10 = np.array([0.4, 0.5, 0.7,0.3, 0.8])\n",
    "word11 = np.array([0.9,0.7, 0.3, 0.5, 0.4])\n",
    "word12 = np.array([0.4, 0.5,0.1, 0.7, 0.4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa490b91-9b93-48fe-a5f4-c68e4323be09",
   "metadata": {},
   "source": [
    "Finally, we combine all these word embeddings into a single matrix. This matrix, `inputs`, has the shape `(3, 4, 5)`, where:\n",
    "- `3` represents the number of phrases (batch size),\n",
    "- `4` is the number of words in each phrase (sequence length),\n",
    "- `5` is the dimensionality of each word embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "010108fc-3efd-4649-84d4-1e405ea6af37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.1, 0.2, 0.3, 0.4, 0.5],\n",
       "        [0.5, 0.4, 0.7, 0.3, 0.2],\n",
       "        [0.2, 0.7, 0.3, 0.5, 0.4],\n",
       "        [0.4, 0.1, 0.7, 0.2, 0.5]],\n",
       "\n",
       "       [[0.1, 0.9, 0.3, 0.4, 0.5],\n",
       "        [0.4, 0.4, 0.7, 0.3, 0.8],\n",
       "        [0.2, 0.7, 0.4, 0.5, 0.4],\n",
       "        [0.4, 0.5, 0.7, 0.7, 0.8]],\n",
       "\n",
       "       [[0.1, 0.2, 0.3, 0.8, 0.5],\n",
       "        [0.4, 0.5, 0.7, 0.3, 0.8],\n",
       "        [0.9, 0.7, 0.3, 0.5, 0.4],\n",
       "        [0.4, 0.5, 0.1, 0.7, 0.4]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = np.stack([[word1, word2, word3, word4],[word5, word6, word7, word8],[word9, word10, word11, word12]])\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c96488-770c-47f3-b174-b0ae50cb4de3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32f97bf-07f9-4931-834f-98b0a2c86862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3984e9-6eeb-4f22-844e-2d715328e259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e2cd7714-cc64-42d0-9501-42697d23a826",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.random.rand(5, 3)/ np.sqrt(5)\n",
    "K = np.random.rand(5, 3)/ np.sqrt(5)\n",
    "V = np.random.rand(5, 3)/ np.sqrt(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6fbbecb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0.425201  , 0.40500248, 0.49409738],\n",
       "         [0.48972278, 0.60121395, 0.58693049],\n",
       "         [0.55304515, 0.61197165, 0.71089506],\n",
       "         [0.48882271, 0.54546272, 0.51623665]],\n",
       " \n",
       "        [[0.61746062, 0.71337737, 0.74854206],\n",
       "         [0.7022129 , 0.77874207, 0.77742381],\n",
       "         [0.58464292, 0.65247304, 0.72424078],\n",
       "         [0.82157489, 0.84860538, 0.98189856]],\n",
       " \n",
       "        [[0.51709733, 0.43081224, 0.66222289],\n",
       "         [0.72967856, 0.82279562, 0.81377305],\n",
       "         [0.57788218, 0.69460224, 0.91895465],\n",
       "         [0.48796275, 0.47937538, 0.7550135 ]]]),\n",
       " (3, 4, 3))"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qval=np.dot(inputs, Q)\n",
    "Qval,Qval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "176b671d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.2439634 , 0.42241932, 0.36283375],\n",
       "        [0.26863076, 0.60383225, 0.53556176],\n",
       "        [0.2943189 , 0.5631973 , 0.5252599 ],\n",
       "        [0.23434532, 0.5958629 , 0.43157899]],\n",
       "\n",
       "       [[0.24525167, 0.62492683, 0.50190133],\n",
       "        [0.30037985, 0.80506284, 0.56375154],\n",
       "        [0.30140748, 0.60107104, 0.54871196],\n",
       "        [0.46828168, 0.87394052, 0.76164825]],\n",
       "\n",
       "       [[0.41168119, 0.46236734, 0.54086365],\n",
       "        [0.30056389, 0.83399249, 0.58361834],\n",
       "        [0.40181679, 0.72851921, 0.72083694],\n",
       "        [0.39434624, 0.49679936, 0.58351628]]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Kval=np.dot(inputs, K)\n",
    "Kval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c099fc73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.21777027, 0.4314999 , 0.42197821],\n",
       "        [0.27344964, 0.4831857 , 0.51122975],\n",
       "        [0.24391528, 0.56372772, 0.5131798 ],\n",
       "        [0.30615055, 0.44759465, 0.49360769]],\n",
       "\n",
       "       [[0.24340562, 0.58048731, 0.48836437],\n",
       "        [0.40250886, 0.63577634, 0.64760763],\n",
       "        [0.25184159, 0.58747352, 0.53467971],\n",
       "        [0.43852881, 0.82996639, 0.83161395]],\n",
       "\n",
       "       [[0.25012802, 0.60440604, 0.59650079],\n",
       "        [0.40617106, 0.65706025, 0.65709137],\n",
       "        [0.42245289, 0.63104091, 0.70538781],\n",
       "        [0.2879279 , 0.57935369, 0.59339037]]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vval=np.dot(inputs, V)\n",
    "Vval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "050c9c23-1e02-4dfb-8baa-02fd67dfccb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 4 is different from 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[166], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m QKscaled\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mKval\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m/\u001b[39mnp\u001b[38;5;241m.\u001b[39msqrt(K\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      2\u001b[0m QKscaled\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 4 is different from 3)"
     ]
    }
   ],
   "source": [
    "QKscaled=np.matmul(Qval, np.transpose(Kval))/np.sqrt(K.shape[1])\n",
    "QKscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "02bc65e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.26216873, 0.35991744, 0.35378323, 0.31997437],\n",
       "        [0.33855647, 0.46703278, 0.45670011, 0.41933654],\n",
       "        [0.37606757, 0.51893476, 0.50855153, 0.46249343],\n",
       "        [0.31002377, 0.42559803, 0.41698043, 0.38242019]],\n",
       "\n",
       "       [[0.56172495, 0.68229989, 0.59214874, 0.85604915],\n",
       "        [0.60567839, 0.73678022, 0.63873026, 0.92464445],\n",
       "        [0.52806186, 0.64039035, 0.55760373, 0.8057598 ],\n",
       "        [0.70703762, 0.85650607, 0.74852332, 1.08208166]],\n",
       "\n",
       "       [[0.44470118, 0.52030828, 0.57676605, 0.46439773],\n",
       "        [0.64719177, 0.79700507, 0.85402644, 0.6762851 ],\n",
       "        [0.60973603, 0.74437906, 0.80866583, 0.64039034],\n",
       "        [0.4797157 , 0.56990197, 0.6290505 , 0.50295425]]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QKscaled=np.matmul(Qval, np.transpose(Kval, (0, 2, 1)))/np.sqrt(K.shape[1])\n",
    "QKscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "39f61cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.23484449, 0.25895965, 0.257376  , 0.24881986],\n",
       "        [0.23006315, 0.26160354, 0.25891439, 0.24941892],\n",
       "        [0.22802376, 0.26304287, 0.26032577, 0.24860759],\n",
       "        [0.23199244, 0.26041566, 0.25818114, 0.24941077]],\n",
       "\n",
       "       [[0.22216024, 0.25062902, 0.22902306, 0.29818768],\n",
       "        [0.21980956, 0.25060134, 0.22719607, 0.30239303],\n",
       "        [0.2237662 , 0.25036759, 0.23047528, 0.29539093],\n",
       "        [0.21465535, 0.24926141, 0.22374778, 0.31233545]],\n",
       "\n",
       "       [[0.23587059, 0.25439557, 0.26917135, 0.24056248],\n",
       "        [0.2261974 , 0.26275483, 0.27817287, 0.2328749 ],\n",
       "        [0.22751298, 0.26030404, 0.27758775, 0.23459523],\n",
       "        [0.23370217, 0.25575854, 0.27134263, 0.23919666]]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    # Subtract the max value for numerical stability\n",
    "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "Attention_weights=softmax(QKscaled)\n",
    "Attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a0f484e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.26090885, 0.4829214 , 0.48638669],\n",
       "        [0.26114922, 0.48327111, 0.48680588],\n",
       "        [0.26119456, 0.48351907, 0.48700495],\n",
       "        [0.26106319, 0.48311262, 0.4866324 ]],\n",
       "\n",
       "       [[0.34339687, 0.67033594, 0.64123543],\n",
       "        [0.34419762, 0.67137079, 0.64258989],\n",
       "        [0.34282182, 0.6696339 , 0.64030108],\n",
       "        [0.34589534, 0.67375304, 0.64562954]],\n",
       "\n",
       "       [[0.34530283, 0.61894373, 0.64047578],\n",
       "        [0.34786783, 0.61981622, 0.64198633],\n",
       "        [0.3474496 , 0.6196285 , 0.64176878],\n",
       "        [0.34583805, 0.61910754, 0.64079904]]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention=np.matmul(Attention_weights, Vval)\n",
    "Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a2ba23ff-b819-4218-b98c-ca9c2d5628eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase Representation:\n",
      "[[0.32162485 0.58945342 0.59956795]\n",
      " [0.32284648 0.59020857 0.60069388]\n",
      " [0.32225555 0.5896471  0.59991074]\n",
      " [0.32270605 0.59071878 0.60125639]]\n"
     ]
    }
   ],
   "source": [
    "phrase_representation = np.mean(Attention, axis=0)\n",
    "print(\"Phrase Representation:\")\n",
    "print(phrase_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a81cdf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase Representation:\n",
      "[[0.26107895 0.48320605 0.48670748]\n",
      " [0.34407791 0.67127342 0.64243898]\n",
      " [0.34661458 0.619374   0.64125748]]\n"
     ]
    }
   ],
   "source": [
    "phrase_representation = np.mean(Attention, axis=1)\n",
    "print(\"Phrase Representation:\")\n",
    "print(phrase_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "968095b9-45ce-45ad-894f-2f0bb9f2d0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_representation.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "1fc018e0-f761-4fba-976c-22b640e28873",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2  # Example number of classes\n",
    "linearlayer= np.random.rand(phrase_representation.shape[1], num_classes)   \n",
    "linear_bias = np.random.rand(num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a38f7175",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2  # Example number of classes\n",
    "linearlayer= np.random.rand(phrase_representation.shape[0], num_classes)   \n",
    "linear_bias = np.random.rand(num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915892b4-4e77-4fe4-8d21-ec3cdafbc11b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f164ac74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48288356, 0.51711644],\n",
       "       [0.46798522, 0.53201478],\n",
       "       [0.46533757, 0.53466243]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities=softmax(np.matmul(phrase_representation, linearlayer)+linear_bias)\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "aee98d18-7903-4ad1-a2bb-3b98b9159727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy Loss:\n",
      "1.9166908691853788\n"
     ]
    }
   ],
   "source": [
    "# Cross-Entropy Loss\n",
    "target =np.array([0, 1])\n",
    "def cross_entropy_loss(predictions, target):\n",
    "    return -np.sum(target * np.log(predictions + 1e-8))  # Adding a small constant to avoid log(0)\n",
    "loss = cross_entropy_loss(probabilities, target)\n",
    "print(\"Cross-Entropy Loss:\")\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "2c487b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy Loss:\n",
      "0.7279326240583354\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# True target (one-hot encoded)\n",
    "target = [np.array([0, 1]),np.array([1, 0]),np.array([1, 0])]\n",
    "\n",
    "\n",
    "def cross_entropy_loss(predictions, target):\n",
    "    # Cross-entropy loss for a batch of predictions and targets\n",
    "    batch_loss = -np.sum(target * np.log(predictions + 1e-8), axis=1)\n",
    "    return np.mean(batch_loss) \n",
    "# Calculate loss\n",
    "loss = cross_entropy_loss(probabilities, target)\n",
    "print(\"Cross-Entropy Loss:\")\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a46ee5c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.39263926, -0.39263926],\n",
       "       [-0.63592837,  0.63592837],\n",
       "       [-0.63681096,  0.63681096]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient of loss with respect to output probabilities\n",
    "d_probabilities = probabilities - target \n",
    "d_probabilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1ee69e40-a234-40f8-88dd-02bfb0f3d55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.14645888, -0.14645888],\n",
       "        [ 0.13668903, -0.13668903],\n",
       "        [ 0.17016618, -0.17016618]]),\n",
       " array([ 0.29630688, -0.29630688]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient for linear layer and bias\n",
    "d_linear = np.outer(phrase_representation, d_probabilities)\n",
    "d_bias = d_probabilities\n",
    "d_linear,d_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2f3098d2-304b-4007-84fd-a07e3652b289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.34254126,  0.34254126],\n",
       "        [-0.63022567,  0.63022567],\n",
       "        [-0.63682398,  0.63682398]]),\n",
       " array([-0.88010008,  0.88010008]))"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient for linear layer and bias\n",
    "d_linear = np.dot(phrase_representation.T, d_probabilities) \n",
    "d_bias =  np.sum(d_probabilities, axis=0)\n",
    "d_linear,d_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "57f28c04-c59d-40ef-bd6c-03591fb0fedb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.25311242, -0.26720907,  0.01037652])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient for phrase representation\n",
    "d_phrase_rep = np.dot(d_probabilities, linearlayer.T)\n",
    "d_phrase_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a862abe4-f015-41d0-a806-84f7499c413b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.13413652,  0.03503775, -0.26959773],\n",
       "       [ 0.21725086, -0.05674802,  0.43664723],\n",
       "       [ 0.21755238, -0.05682678,  0.43725324]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_phrase_rep = np.dot(d_probabilities, linearlayer.T)\n",
    "d_phrase_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0c86ee4b-5ff5-4cdf-8c1b-7f3c9259b449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(inputs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2734b391-4b7c-49a9-9151-3828b844b17d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.25311242, -0.26720907,  0.01037652])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_phrase_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1c179624-513e-4b6e-81f2-0e468523eb1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.25311242, -0.26720907,  0.01037652],\n",
       "       [-0.25311242, -0.26720907,  0.01037652],\n",
       "       [-0.25311242, -0.26720907,  0.01037652],\n",
       "       [-0.25311242, -0.26720907,  0.01037652]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient for attention\n",
    "d_attention = np.outer(np.ones(inputs.shape[0]), d_phrase_rep)# / inputs.shape[0]\n",
    "d_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "38068080-9bb5-4ddd-a7b4-316adb15dd01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(inputs.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "18ac92ef-5152-409b-ac74-57b2c5e331e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0070818 , -0.18506487, -0.20596266],\n",
       "       [-0.01861819,  0.48653924,  0.54147994]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_phrase_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "0ac95741-e7f5-48d0-a22a-0b37008692cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.13413652,  0.03503775, -0.26959773],\n",
       "        [-0.13413652,  0.03503775, -0.26959773],\n",
       "        [-0.13413652,  0.03503775, -0.26959773],\n",
       "        [-0.13413652,  0.03503775, -0.26959773]],\n",
       "\n",
       "       [[ 0.21725086, -0.05674802,  0.43664723],\n",
       "        [ 0.21725086, -0.05674802,  0.43664723],\n",
       "        [ 0.21725086, -0.05674802,  0.43664723],\n",
       "        [ 0.21725086, -0.05674802,  0.43664723]],\n",
       "\n",
       "       [[ 0.21755238, -0.05682678,  0.43725324],\n",
       "        [ 0.21755238, -0.05682678,  0.43725324],\n",
       "        [ 0.21755238, -0.05682678,  0.43725324],\n",
       "        [ 0.21755238, -0.05682678,  0.43725324]]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_attention = np.array([np.outer(np.ones(inputs.shape[1]), d_phrase_rep[i, :]) for i in range(d_phrase_rep.shape[0])])\n",
    "d_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ca028aef-e7a1-4632-b568-8b624f0ac226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.1, 0.2, 0.3, 0.4, 0.5],\n",
       "        [0.5, 0.4, 0.7, 0.3, 0.2],\n",
       "        [0.2, 0.7, 0.3, 0.5, 0.4],\n",
       "        [0.4, 0.1, 0.7, 0.2, 0.5]],\n",
       "\n",
       "       [[0.1, 0.9, 0.3, 0.4, 0.5],\n",
       "        [0.4, 0.4, 0.7, 0.3, 0.8],\n",
       "        [0.2, 0.7, 0.4, 0.5, 0.4],\n",
       "        [0.4, 0.5, 0.7, 0.7, 0.8]]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "652792e4-644e-4f74-9a90-61affd107e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.1, 0.5, 0.2, 0.4],\n",
       "        [0.2, 0.4, 0.7, 0.1],\n",
       "        [0.3, 0.7, 0.3, 0.7],\n",
       "        [0.4, 0.3, 0.5, 0.2],\n",
       "        [0.5, 0.2, 0.4, 0.5]],\n",
       "\n",
       "       [[0.1, 0.4, 0.2, 0.4],\n",
       "        [0.9, 0.4, 0.7, 0.5],\n",
       "        [0.3, 0.7, 0.4, 0.7],\n",
       "        [0.4, 0.3, 0.5, 0.7],\n",
       "        [0.5, 0.8, 0.4, 0.8]]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(inputs,(0,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "71a69f1a-e346-44ae-ac8d-6de1661e1041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.24019827, 0.25219573, 0.25849304, 0.24911296],\n",
       "        [0.23715059, 0.25280595, 0.26098206, 0.24906141],\n",
       "        [0.23799408, 0.25216139, 0.25932752, 0.25051701],\n",
       "        [0.23686983, 0.25351364, 0.26210051, 0.24751602]],\n",
       "\n",
       "       [[0.238775  , 0.25485377, 0.23422579, 0.27214544],\n",
       "        [0.23865045, 0.25306951, 0.23228578, 0.27599426],\n",
       "        [0.23895129, 0.25474077, 0.23408144, 0.27222649],\n",
       "        [0.23549621, 0.25485798, 0.22796979, 0.28167601]]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b0c5e55a-c095-4927-a710-9d4188d377cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.0070818 , -0.18506487, -0.20596266],\n",
       "        [ 0.0070818 , -0.18506487, -0.20596266],\n",
       "        [ 0.0070818 , -0.18506487, -0.20596266],\n",
       "        [ 0.0070818 , -0.18506487, -0.20596266]],\n",
       "\n",
       "       [[-0.01861819,  0.48653924,  0.54147994],\n",
       "        [-0.01861819,  0.48653924,  0.54147994],\n",
       "        [-0.01861819,  0.48653924,  0.54147994],\n",
       "        [-0.01861819,  0.48653924,  0.54147994]]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5b500618-6883-49dc-b886-ed4fecf1dbdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.0070818 , -0.18506487, -0.20596266],\n",
       "        [ 0.0070818 , -0.18506487, -0.20596266],\n",
       "        [ 0.0070818 , -0.18506487, -0.20596266],\n",
       "        [ 0.0070818 , -0.18506487, -0.20596266]],\n",
       "\n",
       "       [[-0.01861819,  0.48653924,  0.54147994],\n",
       "        [-0.01861819,  0.48653924,  0.54147994],\n",
       "        [-0.01861819,  0.48653924,  0.54147994],\n",
       "        [-0.01861819,  0.48653924,  0.54147994]]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(Attention_weights, d_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "aed335d9-a341-4f9b-92fc-707225826556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.00849816, -0.22207784, -0.24715519],\n",
       "        [ 0.00991452, -0.25909082, -0.28834772],\n",
       "        [ 0.0141636 , -0.37012974, -0.41192531],\n",
       "        [ 0.00991452, -0.25909082, -0.28834772],\n",
       "        [ 0.01133088, -0.29610379, -0.32954025]],\n",
       "\n",
       "       [[-0.02048001,  0.53519316,  0.59562793],\n",
       "        [-0.04654548,  1.2163481 ,  1.35369985],\n",
       "        [-0.0390982 ,  1.0217324 ,  1.13710787],\n",
       "        [-0.03537456,  0.92442456,  1.02881188],\n",
       "        [-0.04654548,  1.2163481 ,  1.35369985]]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(np.transpose(inputs,(0,2,1)),np.matmul(Attention_weights, d_attention))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ac16574a-7033-424a-9817-8cced80f1598",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[[-0.00115364,  0.03014744,  0.03355173],\n",
       "          [-0.00115364,  0.03014744,  0.03355173],\n",
       "          [-0.00115364,  0.03014744,  0.03355173],\n",
       "          [-0.00115364,  0.03014744,  0.03355173]],\n",
       "\n",
       "         [[-0.00115364,  0.03014744,  0.03355173],\n",
       "          [-0.00115364,  0.03014744,  0.03355173],\n",
       "          [-0.00115364,  0.03014744,  0.03355173],\n",
       "          [-0.00115364,  0.03014744,  0.03355173]]],\n",
       "\n",
       "\n",
       "        [[[-0.00390638,  0.10208326,  0.11361065],\n",
       "          [-0.00390638,  0.10208326,  0.11361065],\n",
       "          [-0.00390638,  0.10208326,  0.11361065],\n",
       "          [-0.00390638,  0.10208326,  0.11361065]],\n",
       "\n",
       "         [[-0.00390638,  0.10208326,  0.11361065],\n",
       "          [-0.00390638,  0.10208326,  0.11361065],\n",
       "          [-0.00390638,  0.10208326,  0.11361065],\n",
       "          [-0.00390638,  0.10208326,  0.11361065]]],\n",
       "\n",
       "\n",
       "        [[[-0.00230728,  0.06029487,  0.06710346],\n",
       "          [-0.00230728,  0.06029487,  0.06710346],\n",
       "          [-0.00230728,  0.06029487,  0.06710346],\n",
       "          [-0.00230728,  0.06029487,  0.06710346]],\n",
       "\n",
       "         [[-0.00230728,  0.06029487,  0.06710346],\n",
       "          [-0.00230728,  0.06029487,  0.06710346],\n",
       "          [-0.00230728,  0.06029487,  0.06710346],\n",
       "          [-0.00230728,  0.06029487,  0.06710346]]],\n",
       "\n",
       "\n",
       "        [[[-0.00461456,  0.12058975,  0.13420691],\n",
       "          [-0.00461456,  0.12058975,  0.13420691],\n",
       "          [-0.00461456,  0.12058975,  0.13420691],\n",
       "          [-0.00461456,  0.12058975,  0.13420691]],\n",
       "\n",
       "         [[-0.00461456,  0.12058975,  0.13420691],\n",
       "          [-0.00461456,  0.12058975,  0.13420691],\n",
       "          [-0.00461456,  0.12058975,  0.13420691],\n",
       "          [-0.00461456,  0.12058975,  0.13420691]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[-0.01534001,  0.40087234,  0.44613941],\n",
       "          [-0.01534001,  0.40087234,  0.44613941],\n",
       "          [-0.01534001,  0.40087234,  0.44613941],\n",
       "          [-0.01534001,  0.40087234,  0.44613941]],\n",
       "\n",
       "         [[-0.01534001,  0.40087234,  0.44613941],\n",
       "          [-0.01534001,  0.40087234,  0.44613941],\n",
       "          [-0.01534001,  0.40087234,  0.44613941],\n",
       "          [-0.01534001,  0.40087234,  0.44613941]]],\n",
       "\n",
       "\n",
       "        [[[-0.00461456,  0.12058975,  0.13420691],\n",
       "          [-0.00461456,  0.12058975,  0.13420691],\n",
       "          [-0.00461456,  0.12058975,  0.13420691],\n",
       "          [-0.00461456,  0.12058975,  0.13420691]],\n",
       "\n",
       "         [[-0.00461456,  0.12058975,  0.13420691],\n",
       "          [-0.00461456,  0.12058975,  0.13420691],\n",
       "          [-0.00461456,  0.12058975,  0.13420691],\n",
       "          [-0.00461456,  0.12058975,  0.13420691]]],\n",
       "\n",
       "\n",
       "        [[[-0.00807547,  0.21103206,  0.2348621 ],\n",
       "          [-0.00807547,  0.21103206,  0.2348621 ],\n",
       "          [-0.00807547,  0.21103206,  0.2348621 ],\n",
       "          [-0.00807547,  0.21103206,  0.2348621 ]],\n",
       "\n",
       "         [[-0.00807547,  0.21103206,  0.2348621 ],\n",
       "          [-0.00807547,  0.21103206,  0.2348621 ],\n",
       "          [-0.00807547,  0.21103206,  0.2348621 ],\n",
       "          [-0.00807547,  0.21103206,  0.2348621 ]]],\n",
       "\n",
       "\n",
       "        [[[-0.00860092,  0.22476313,  0.2501437 ],\n",
       "          [-0.00860092,  0.22476313,  0.2501437 ],\n",
       "          [-0.00860092,  0.22476313,  0.2501437 ],\n",
       "          [-0.00860092,  0.22476313,  0.2501437 ]],\n",
       "\n",
       "         [[-0.00860092,  0.22476313,  0.2501437 ],\n",
       "          [-0.00860092,  0.22476313,  0.2501437 ],\n",
       "          [-0.00860092,  0.22476313,  0.2501437 ],\n",
       "          [-0.00860092,  0.22476313,  0.2501437 ]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[-0.00346092,  0.09044231,  0.10065518],\n",
       "          [-0.00346092,  0.09044231,  0.10065518],\n",
       "          [-0.00346092,  0.09044231,  0.10065518],\n",
       "          [-0.00346092,  0.09044231,  0.10065518]],\n",
       "\n",
       "         [[-0.00346092,  0.09044231,  0.10065518],\n",
       "          [-0.00346092,  0.09044231,  0.10065518],\n",
       "          [-0.00346092,  0.09044231,  0.10065518],\n",
       "          [-0.00346092,  0.09044231,  0.10065518]]],\n",
       "\n",
       "\n",
       "        [[[-0.00807547,  0.21103206,  0.2348621 ],\n",
       "          [-0.00807547,  0.21103206,  0.2348621 ],\n",
       "          [-0.00807547,  0.21103206,  0.2348621 ],\n",
       "          [-0.00807547,  0.21103206,  0.2348621 ]],\n",
       "\n",
       "         [[-0.00807547,  0.21103206,  0.2348621 ],\n",
       "          [-0.00807547,  0.21103206,  0.2348621 ],\n",
       "          [-0.00807547,  0.21103206,  0.2348621 ],\n",
       "          [-0.00807547,  0.21103206,  0.2348621 ]]],\n",
       "\n",
       "\n",
       "        [[[-0.00532274,  0.13909624,  0.15480318],\n",
       "          [-0.00532274,  0.13909624,  0.15480318],\n",
       "          [-0.00532274,  0.13909624,  0.15480318],\n",
       "          [-0.00532274,  0.13909624,  0.15480318]],\n",
       "\n",
       "         [[-0.00532274,  0.13909624,  0.15480318],\n",
       "          [-0.00532274,  0.13909624,  0.15480318],\n",
       "          [-0.00532274,  0.13909624,  0.15480318],\n",
       "          [-0.00532274,  0.13909624,  0.15480318]]],\n",
       "\n",
       "\n",
       "        [[[-0.00807547,  0.21103206,  0.2348621 ],\n",
       "          [-0.00807547,  0.21103206,  0.2348621 ],\n",
       "          [-0.00807547,  0.21103206,  0.2348621 ],\n",
       "          [-0.00807547,  0.21103206,  0.2348621 ]],\n",
       "\n",
       "         [[-0.00807547,  0.21103206,  0.2348621 ],\n",
       "          [-0.00807547,  0.21103206,  0.2348621 ],\n",
       "          [-0.00807547,  0.21103206,  0.2348621 ],\n",
       "          [-0.00807547,  0.21103206,  0.2348621 ]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[-0.00461456,  0.12058975,  0.13420691],\n",
       "          [-0.00461456,  0.12058975,  0.13420691],\n",
       "          [-0.00461456,  0.12058975,  0.13420691],\n",
       "          [-0.00461456,  0.12058975,  0.13420691]],\n",
       "\n",
       "         [[-0.00461456,  0.12058975,  0.13420691],\n",
       "          [-0.00461456,  0.12058975,  0.13420691],\n",
       "          [-0.00461456,  0.12058975,  0.13420691],\n",
       "          [-0.00461456,  0.12058975,  0.13420691]]],\n",
       "\n",
       "\n",
       "        [[[-0.00346092,  0.09044231,  0.10065518],\n",
       "          [-0.00346092,  0.09044231,  0.10065518],\n",
       "          [-0.00346092,  0.09044231,  0.10065518],\n",
       "          [-0.00346092,  0.09044231,  0.10065518]],\n",
       "\n",
       "         [[-0.00346092,  0.09044231,  0.10065518],\n",
       "          [-0.00346092,  0.09044231,  0.10065518],\n",
       "          [-0.00346092,  0.09044231,  0.10065518],\n",
       "          [-0.00346092,  0.09044231,  0.10065518]]],\n",
       "\n",
       "\n",
       "        [[[-0.0057682 ,  0.15073719,  0.16775864],\n",
       "          [-0.0057682 ,  0.15073719,  0.16775864],\n",
       "          [-0.0057682 ,  0.15073719,  0.16775864],\n",
       "          [-0.0057682 ,  0.15073719,  0.16775864]],\n",
       "\n",
       "         [[-0.0057682 ,  0.15073719,  0.16775864],\n",
       "          [-0.0057682 ,  0.15073719,  0.16775864],\n",
       "          [-0.0057682 ,  0.15073719,  0.16775864],\n",
       "          [-0.0057682 ,  0.15073719,  0.16775864]]],\n",
       "\n",
       "\n",
       "        [[[-0.01161637,  0.30356449,  0.33784343],\n",
       "          [-0.01161637,  0.30356449,  0.33784343],\n",
       "          [-0.01161637,  0.30356449,  0.33784343],\n",
       "          [-0.01161637,  0.30356449,  0.33784343]],\n",
       "\n",
       "         [[-0.01161637,  0.30356449,  0.33784343],\n",
       "          [-0.01161637,  0.30356449,  0.33784343],\n",
       "          [-0.01161637,  0.30356449,  0.33784343],\n",
       "          [-0.01161637,  0.30356449,  0.33784343]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[-0.0057682 ,  0.15073719,  0.16775864],\n",
       "          [-0.0057682 ,  0.15073719,  0.16775864],\n",
       "          [-0.0057682 ,  0.15073719,  0.16775864],\n",
       "          [-0.0057682 ,  0.15073719,  0.16775864]],\n",
       "\n",
       "         [[-0.0057682 ,  0.15073719,  0.16775864],\n",
       "          [-0.0057682 ,  0.15073719,  0.16775864],\n",
       "          [-0.0057682 ,  0.15073719,  0.16775864],\n",
       "          [-0.0057682 ,  0.15073719,  0.16775864]]],\n",
       "\n",
       "\n",
       "        [[[-0.01347819,  0.35221842,  0.39199142],\n",
       "          [-0.01347819,  0.35221842,  0.39199142],\n",
       "          [-0.01347819,  0.35221842,  0.39199142],\n",
       "          [-0.01347819,  0.35221842,  0.39199142]],\n",
       "\n",
       "         [[-0.01347819,  0.35221842,  0.39199142],\n",
       "          [-0.01347819,  0.35221842,  0.39199142],\n",
       "          [-0.01347819,  0.35221842,  0.39199142],\n",
       "          [-0.01347819,  0.35221842,  0.39199142]]],\n",
       "\n",
       "\n",
       "        [[[-0.00461456,  0.12058975,  0.13420691],\n",
       "          [-0.00461456,  0.12058975,  0.13420691],\n",
       "          [-0.00461456,  0.12058975,  0.13420691],\n",
       "          [-0.00461456,  0.12058975,  0.13420691]],\n",
       "\n",
       "         [[-0.00461456,  0.12058975,  0.13420691],\n",
       "          [-0.00461456,  0.12058975,  0.13420691],\n",
       "          [-0.00461456,  0.12058975,  0.13420691],\n",
       "          [-0.00461456,  0.12058975,  0.13420691]]],\n",
       "\n",
       "\n",
       "        [[[-0.01135365,  0.29669896,  0.33020262],\n",
       "          [-0.01135365,  0.29669896,  0.33020262],\n",
       "          [-0.01135365,  0.29669896,  0.33020262],\n",
       "          [-0.01135365,  0.29669896,  0.33020262]],\n",
       "\n",
       "         [[-0.01135365,  0.29669896,  0.33020262],\n",
       "          [-0.01135365,  0.29669896,  0.33020262],\n",
       "          [-0.01135365,  0.29669896,  0.33020262],\n",
       "          [-0.01135365,  0.29669896,  0.33020262]]]]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient for V\n",
    "d_Vval = np.dot(Attention_weights, d_attention)\n",
    "d_V = np.dot(inputs.T, d_Vval)\n",
    "d_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "376ea445-1081-474f-a1a4-58cde3dd3dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.15653547, -0.04088858,  0.31461684],\n",
       "       [ 0.25622851, -0.06692936,  0.51498747],\n",
       "       [ 0.1641757 , -0.04288428,  0.32997275],\n",
       "       [ 0.24178533, -0.06315666,  0.48595846],\n",
       "       [ 0.26178957, -0.06838196,  0.5261645 ]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient for V\n",
    "d_Vval = np.matmul(Attention_weights, d_attention)\n",
    "d_V = np.mean(np.matmul(np.transpose(inputs,(0,2,1)),d_Vval),axis=0)\n",
    "d_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c7fd7102-5e2b-4f90-bd41-34b43b59cd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.13021179, -0.16045778, -0.15464488, -0.16107885],\n",
       "        [-0.13021179, -0.16045778, -0.15464488, -0.16107885],\n",
       "        [-0.13021179, -0.16045778, -0.15464488, -0.16107885],\n",
       "        [-0.13021179, -0.16045778, -0.15464488, -0.16107885]],\n",
       "\n",
       "       [[ 0.23898649,  0.34037878,  0.26041125,  0.41888764],\n",
       "        [ 0.23898649,  0.34037878,  0.26041125,  0.41888764],\n",
       "        [ 0.23898649,  0.34037878,  0.26041125,  0.41888764],\n",
       "        [ 0.23898649,  0.34037878,  0.26041125,  0.41888764]],\n",
       "\n",
       "       [[ 0.28578605,  0.34486993,  0.37109162,  0.29452467],\n",
       "        [ 0.28578605,  0.34486993,  0.37109162,  0.29452467],\n",
       "        [ 0.28578605,  0.34486993,  0.37109162,  0.29452467],\n",
       "        [ 0.28578605,  0.34486993,  0.37109162,  0.29452467]]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Gradient for attention weights\n",
    "Vval_T = np.transpose(Vval, (0, 2, 1))  # (batch_size, hidden_dim, seq_len)\n",
    "\n",
    "# Compute gradient w.r.t. attention weights\n",
    "# Shape: (batch_size, seq_len, seq_len)\n",
    "d_attention_weights = np.matmul(d_attention, Vval_T)\n",
    "d_attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b28c4e49-9dde-4c3d-b544-d202f3c1ce28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.02338979, -0.03079541, -0.02956496, -0.03010524],\n",
       "        [-0.0230545 , -0.03099962, -0.0296818 , -0.0301531 ],\n",
       "        [-0.02290924, -0.03110991, -0.02978778, -0.03008716],\n",
       "        [-0.02319052, -0.03090817, -0.02962635, -0.03015267]],\n",
       "\n",
       "       [[ 0.04127577,  0.06392592,  0.04595515,  0.08772291],\n",
       "        [ 0.04096013,  0.06392066,  0.04569394,  0.08843089],\n",
       "        [ 0.04148957,  0.06388187,  0.04616095,  0.08724443],\n",
       "        [ 0.04025815,  0.06369105,  0.04519472,  0.09004549]],\n",
       "\n",
       "       [[ 0.05148291,  0.06543583,  0.07302731,  0.05379028],\n",
       "        [ 0.0499843 ,  0.06683555,  0.07454869,  0.05259008],\n",
       "        [ 0.0501912 ,  0.06643143,  0.07445131,  0.05286093],\n",
       "        [ 0.051152  ,  0.06566757,  0.07339946,  0.05357948]]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient for QK scaled\n",
    "d_QKscaled = d_attention_weights * Attention_weights * (1 - Attention_weights)\n",
    "d_QKscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "fbf6f022-32a3-4759-8130-e29c61768dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient for Q and K\n",
    "d_Qval = np.matmul(d_QKscaled, Kval) / np.sqrt(K.shape[1])\n",
    "d_Kval = np.matmul(d_QKscaled, Qval) / np.sqrt(K.shape[1])\n",
    "d_Q = np.mean(np.matmul(np.transpose(inputs,(0,2,1)), d_Qval),axis=0)\n",
    "d_K =  np.mean(np.matmul(np.transpose(inputs,(0,2,1)), d_Kval),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "40bec269-0f97-4581-b54d-f6e49c4318f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.01729639, -0.03654878, -0.03105323],\n",
       "        [-0.01730729, -0.03659279, -0.03109365],\n",
       "        [-0.01731308, -0.03660766, -0.03111312],\n",
       "        [-0.01730273, -0.03657585, -0.03107688]],\n",
       "\n",
       "       [[ 0.04902884,  0.10522794,  0.08639036],\n",
       "        [ 0.04912971,  0.10537874,  0.08652642],\n",
       "        [ 0.04895755,  0.10511419,  0.08629228],\n",
       "        [ 0.04934113,  0.10566123,  0.08680138]],\n",
       "\n",
       "       [[ 0.05313395,  0.09177562,  0.08709111],\n",
       "        [ 0.05310171,  0.09234668,  0.08732535],\n",
       "        [ 0.05311962,  0.09224382,  0.08730424],\n",
       "        [ 0.05313416,  0.09189526,  0.08715012]]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_Qval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "4fc06de8-498e-47f3-8d4d-fe6aa0f1dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "94a76a92-f88d-4202-ad7d-0fe79bfc9165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update weights\n",
    "Q -= learning_rate * d_Q\n",
    "K -= learning_rate * d_K\n",
    "V -= learning_rate * d_V\n",
    "linearlayer -= learning_rate * d_linear\n",
    "linear_bias -= learning_rate * d_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c8ed9d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.41492789, 0.03156887, 0.15722164],\n",
       "       [0.09636694, 0.38189275, 0.08675516],\n",
       "       [0.22720729, 0.1759972 , 0.39875149],\n",
       "       [0.35855727, 0.29835989, 0.07211897],\n",
       "       [0.35133922, 0.0524147 , 0.40287874]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7807ae1-7d06-4bc2-a676-45b1731c3b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67d73886-7275-4092-bca5-0206deb5b699",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m \n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m \n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m df\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/bbc-text.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_lg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "df=pd.read_csv(\"data/bbc-text.csv\")\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    " \n",
    "def preprocess_text(text, max_words=70):\n",
    "    # Process the text using SpaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Filter out stopwords, punctuation, and spaces\n",
    "    tokens = [token.text for token in doc if not token.is_stop and not token.is_punct and not token.is_space]\n",
    "    \n",
    "    # Limit to the top 'max_words' words and pad if necessary\n",
    "    if len(tokens) > max_words:\n",
    "        tokens = tokens[:max_words]  # Keep the first max_words words\n",
    "    else:\n",
    "        tokens += ['<PAD>'] * (max_words - len(tokens))  # Pad the list with '<PAD>' token\n",
    "    \n",
    "    # Join the tokens back into a string or return a list\n",
    "    return tokens\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "df['processed_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "de34b964-7998-4380-904f-799f4343b40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Processed_dataframe.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9f2986-c5b1-4810-920d-09fede622e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy.tokens import Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "73d48560-3e6e-4f76-b6fb-ea168a2988ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs = [] \n",
    "# Process each phrase in the 'category' column\n",
    "for phrase in list(df['processed_text']):\n",
    "    doc = nlp(\" \".join(phrase))  # Process the phrase with SpaCy\n",
    "    # Extract word vectors\n",
    "    matrix = np.array([token.vector for token in doc])\n",
    "    inputs.append(matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "545e6eec-6621-49f6-a7cc-249374e06e6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('InputProcessed.pkl', 'wb') as f:\n",
    "#     pickle.dump(np.array(inputs, dtype=object), f)\n",
    "\n",
    "# Load from pickle file\n",
    "with open('InputProcessed.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "\n",
    "y = np.array(pd.get_dummies(df[\"category\"], dtype=int))\n",
    "tts=0.85\n",
    "X_train,X_test=X[0:round(tts*len(X))],X[round(tts*len(X)):]\n",
    "y_train,y_test=y[0:round(tts*len(X))],y[round(tts*len(X)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "999a54cd-90c6-47fb-963c-b00b0e408177",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1891"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "53df7b57-7dd4-48e9-933d-d12b102d6fa2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([[-1.0279  ,  5.8798  , -9.6866  , ..., -5.2057  , -6.7824  ,\n",
       "               -0.97569 ],\n",
       "              [ 1.7561  ,  3.5755  , -2.56    , ..., -3.9903  , -4.6985  ,\n",
       "                6.7232  ],\n",
       "              [-3.7766  ,  0.69426 , -3.3805  , ..., -7.698   ,  0.60605 ,\n",
       "               -5.3688  ],\n",
       "              ...,\n",
       "              [-3.2771  ,  5.7918  , -6.2498  , ...,  0.3389  , -4.6256  ,\n",
       "                3.8239  ],\n",
       "              [ 4.3092  ,  6.3754  , -6.2211  , ...,  2.7577  , -5.5191  ,\n",
       "               -0.42839 ],\n",
       "              [-0.076454, -4.6896  , -4.0431  , ...,  1.304   , -0.52699 ,\n",
       "               -1.3622  ]], dtype=float32)                                ,\n",
       "       array([[  0.      ,   0.      ,   0.      , ...,   0.      ,   0.      ,\n",
       "                 0.      ],\n",
       "              [  1.2616  ,  -0.74531 ,  -0.17045 , ...,   4.4011  ,   1.5951  ,\n",
       "                 1.6841  ],\n",
       "              [  0.      ,   0.      ,   0.      , ...,   0.      ,   0.      ,\n",
       "                 0.      ],\n",
       "              ...,\n",
       "              [ -3.2153  ,   5.2955  ,  -3.8263  , ...,  -1.7964  , -14.864   ,\n",
       "                -6.2588  ],\n",
       "              [ -0.15953 ,   1.9901  ,  -1.9055  , ...,   0.050346,  -1.5561  ,\n",
       "                 3.5901  ],\n",
       "              [ -0.076454,  -4.6896  ,  -4.0431  , ...,   1.304   ,  -0.52699 ,\n",
       "                -1.3622  ]], dtype=float32)                                    ,\n",
       "       array([[ -1.1864  ,   3.4226  ,  -4.3479  , ...,  -0.24687 ,  -6.8606  ,\n",
       "                 0.12618 ],\n",
       "              [ -0.87788 ,   3.0073  ,   1.4323  , ...,   1.6826  ,  -4.961   ,\n",
       "                 4.2043  ],\n",
       "              [-12.667   ,  -6.568   ,  -0.61537 , ...,  -8.0021  ,  -0.31712 ,\n",
       "                -7.7062  ],\n",
       "              ...,\n",
       "              [ -9.1307  ,  -3.3238  ,   5.5397  , ...,   0.90841 ,   4.2281  ,\n",
       "                -2.086   ],\n",
       "              [ -3.4155  ,  -0.063132,  -0.42182 , ...,  -5.0289  ,   1.1463  ,\n",
       "                 0.73824 ],\n",
       "              [ -0.076454,  -4.6896  ,  -4.0431  , ...,   1.304   ,  -0.52699 ,\n",
       "                -1.3622  ]], dtype=float32)                                    ,\n",
       "       ...,\n",
       "       array([[-0.68095 ,  0.52379 ,  0.21572 , ..., -1.0876  , -0.88285 ,\n",
       "               -0.37898 ],\n",
       "              [ 0.40761 , -0.55844 ,  3.8539  , ...,  0.31717 ,  2.9583  ,\n",
       "                0.82619 ],\n",
       "              [ 2.593   ,  4.3454  , -1.466   , ..., -1.261   , -5.4478  ,\n",
       "                4.2885  ],\n",
       "              ...,\n",
       "              [-5.1043  ,  2.3496  ,  3.2472  , ..., -7.6875  , -2.5128  ,\n",
       "                0.69342 ],\n",
       "              [ 0.43663 , -0.65813 , -3.0258  , ..., -6.6467  , -1.3645  ,\n",
       "                1.8548  ],\n",
       "              [-0.076454, -4.6896  , -4.0431  , ...,  1.304   , -0.52699 ,\n",
       "               -1.3622  ]], dtype=float32)                                ,\n",
       "       array([[  3.4452  ,   4.198   , -11.406   , ...,   0.052334,   1.2355  ,\n",
       "                 3.8899  ],\n",
       "              [ -1.202   ,  -0.52737 ,  -3.1273  , ...,  -5.0814  ,  -2.9944  ,\n",
       "                 3.7352  ],\n",
       "              [ -0.078211,   0.11997 ,  -0.71818 , ...,  -0.36895 ,  -2.1665  ,\n",
       "                -1.1886  ],\n",
       "              ...,\n",
       "              [  0.01689 ,  -0.058385,  -2.288   , ...,   3.4964  ,  -1.892   ,\n",
       "                 1.3722  ],\n",
       "              [  1.9695  ,   3.2152  ,  -3.7083  , ...,  -1.8374  ,  -0.94861 ,\n",
       "                 1.936   ],\n",
       "              [ -0.076454,  -4.6896  ,  -4.0431  , ...,   1.304   ,  -0.52699 ,\n",
       "                -1.3622  ]], dtype=float32)                                    ,\n",
       "       array([[  0.      ,   0.      ,   0.      , ...,   0.      ,   0.      ,\n",
       "                 0.      ],\n",
       "              [  2.0102  ,  -1.7591  ,  -2.0875  , ...,   0.99108 ,  -5.6952  ,\n",
       "                 1.2593  ],\n",
       "              [  3.8102  ,  -7.6937  ,  -9.6256  , ...,  -3.0209  ,   2.2616  ,\n",
       "                -1.7278  ],\n",
       "              ...,\n",
       "              [ -1.2129  ,   8.4406  ,   1.2737  , ...,   8.2656  , -14.539   ,\n",
       "                 4.9051  ],\n",
       "              [ -0.67821 ,   0.63517 ,   3.2564  , ...,   6.45    ,   2.9919  ,\n",
       "                 1.1298  ],\n",
       "              [ -0.076454,  -4.6896  ,  -4.0431  , ...,   1.304   ,  -0.52699 ,\n",
       "                -1.3622  ]], dtype=float32)                                    ],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2898e162-809a-4934-8945-a7bebc9cf396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue:\n",
      "Lucas: Hey! How was your day?\n",
      "Demi: Hey there! \n",
      "Demi: It was pretty fine, actually, thank you!\n",
      "Demi: I just got promoted! :D\n",
      "Lucas: Whoa! Great news!\n",
      "Lucas: Congratulations!\n",
      "Lucas: Such a success has to be celebrated.\n",
      "Demi: I agree! :D\n",
      "Demi: Tonight at Death & Co.?\n",
      "Lucas: Sure!\n",
      "Lucas: See you there at 10pm?\n",
      "Demi: Yeah! See you there! :D\n",
      "\n",
      "Summary:\n",
      "Demi got promoted. She will celebrate that with Lucas at Death & Co at 10 pm.\n"
     ]
    }
   ],
   "source": [
    "def get_train_test_data(data_dir):\n",
    "    # Get the train data\n",
    "    train_data = pd.read_json(f\"{data_dir}/train.json\")\n",
    "    train_data.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "    # Get the test data\n",
    "    test_data = pd.read_json(f\"{data_dir}/test.json\")\n",
    "    test_data.drop(['id'], axis=1, inplace=True)\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "data_dir = \"corpus\"\n",
    "\n",
    "train_data, test_data = get_train_test_data(data_dir)\n",
    "\n",
    "# Take one example from the dataset and print it\n",
    "example_summary, example_dialogue = train_data.iloc[10]\n",
    "print(f\"Dialogue:\\n{example_dialogue}\")\n",
    "print(f\"\\nSummary:\\n{example_summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc8c722-8eaa-4b51-90e4-7a93733b5168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ac30db-4f3f-493b-8d6c-6b2088fdfb76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2bedae5-8028-4d89-870e-421e1697a36e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing matrices:  57%|███████████████████████████████▋                        | 1071/1891 [00:01<00:01, 681.16it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 147\u001b[0m\n\u001b[0;32m    140\u001b[0m model \u001b[38;5;241m=\u001b[39m AttentionClassifier(input_dim, hidden_dim, num_classes)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Sample data (replace with your actual dataset)\u001b[39;00m\n\u001b[0;32m    143\u001b[0m   \u001b[38;5;66;03m# One-hot encoded targets\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# Add more targets...\u001b[39;00m\n\u001b[0;32m    145\u001b[0m  \n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 147\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 121\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, X, y, epochs, learning_rate)\u001b[0m\n\u001b[0;32m    118\u001b[0m target \u001b[38;5;241m=\u001b[39m y[i]\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m    124\u001b[0m loss \u001b[38;5;241m=\u001b[39m cross_entropy_loss(probabilities, target)\n",
      "Cell \u001b[1;32mIn[2], line 55\u001b[0m, in \u001b[0;36mAttentionClassifier.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQval \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKval \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK)\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mVval \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQKscaled \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQval, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKval\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_weights \u001b[38;5;241m=\u001b[39m softmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQKscaled)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('c:\\\\python312\\\\lib\\\\site-packages')\n",
    "import pickle\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    # Subtract the max value for numerical stability\n",
    "    x = np.clip(x, -1500, 1500)\n",
    "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "def cross_entropy_loss(predictions, target):\n",
    "    return -np.sum(target * np.log(predictions + 1e-9))  # Adding a small constant to avoid log(0)\n",
    "\n",
    "\n",
    "df=pd.read_csv(\"data/bbc-text.csv\")\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "with open('InputProcessed.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "\n",
    "y = np.array(pd.get_dummies(df[\"category\"], dtype=int))\n",
    "tts=0.85\n",
    "X_train,X_test=X[0:round(tts*len(X))],X[round(tts*len(X)):]\n",
    "y_train,y_test=y[0:round(tts*len(X))],y[round(tts*len(X)):]\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "class AttentionClassifier:\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.Q = np.random.randn(input_dim, hidden_dim) / np.sqrt(input_dim)\n",
    "        self.K = np.random.randn(input_dim, hidden_dim) / np.sqrt(input_dim)\n",
    "        self.V = np.random.randn(input_dim, hidden_dim) / np.sqrt(input_dim)\n",
    "        self.linear = np.random.randn(hidden_dim, num_classes) / np.sqrt(hidden_dim)\n",
    "        self.bias = np.zeros(num_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs  # Store for backprop\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.Qval = np.dot(inputs, self.Q)\n",
    "        self.Kval = np.dot(inputs, self.K)\n",
    "        self.Vval = np.dot(inputs, self.V)\n",
    "        \n",
    "        self.QKscaled = np.dot(self.Qval, self.Kval.T) / np.sqrt(self.K.shape[1])\n",
    "        self.attention_weights = softmax(self.QKscaled)\n",
    "        self.attention = np.dot(self.attention_weights, self.Vval)\n",
    "        \n",
    "        # Phrase representation\n",
    "        self.phrase_rep = np.mean(self.attention, axis=0)\n",
    "        \n",
    "        # Classification\n",
    "        self.logits = np.dot(self.phrase_rep, self.linear) + self.bias\n",
    "        probabilities = softmax(self.logits)\n",
    "        \n",
    "        return probabilities\n",
    " \n",
    "    def backward(self, target, learning_rate):\n",
    "        m = self.inputs.shape[0]  # Number of words in the input\n",
    "        \n",
    "        # Gradient of loss with respect to output probabilities\n",
    "        d_probabilities = self.forward(self.inputs) - target\n",
    "        \n",
    "        # Gradient for linear layer and bias\n",
    "        d_linear = np.outer(self.phrase_rep, d_probabilities)\n",
    "        d_bias = d_probabilities\n",
    "        \n",
    "        # Gradient for phrase representation\n",
    "        d_phrase_rep = np.dot(d_probabilities, self.linear.T)\n",
    "\n",
    "      \n",
    "        # Gradient for attention\n",
    "        d_attention = np.outer(np.ones(m), d_phrase_rep) / m\n",
    "         \n",
    "        # Gradient for V\n",
    "        d_Vval = np.dot(self.attention_weights, d_attention)\n",
    "        d_V = np.dot(self.inputs.T, d_Vval)\n",
    "        \n",
    "        \n",
    "        # Gradient for attention weights\n",
    "        d_attention_weights = np.dot(d_attention, self.Vval.T)\n",
    "        \n",
    "        # Gradient for QK scaled\n",
    "        d_QKscaled = d_attention_weights * self.attention_weights * (1 - self.attention_weights)\n",
    "        \n",
    "        # Gradient for Q and K\n",
    "        d_Qval = np.dot(d_QKscaled, self.Kval) / np.sqrt(self.K.shape[1])\n",
    "        d_Kval = np.dot(d_QKscaled.T, self.Qval) / np.sqrt(self.K.shape[1])\n",
    "        \n",
    "        d_Q = np.dot(self.inputs.T, d_Qval)\n",
    "        d_K = np.dot(self.inputs.T, d_Kval)\n",
    "        \n",
    "        # Update weights\n",
    "        self.Q -= learning_rate * d_Q\n",
    "        self.K -= learning_rate * d_K\n",
    "        self.V -= learning_rate * d_V\n",
    "        self.linear -= learning_rate * d_linear\n",
    "        self.bias -= learning_rate * d_bias\n",
    " \n",
    "\n",
    "def train(model, X, y, epochs, learning_rate):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i in tqdm(range(len(X)), desc=\"Processing matrices\"):\n",
    "            inputs = X[i]\n",
    "            target = y[i]\n",
    "            \n",
    "            # Forward pass\n",
    "            probabilities = model.forward(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = cross_entropy_loss(probabilities, target)\n",
    "            total_loss += loss\n",
    "            \n",
    "            # Backward pass and update weights\n",
    "            model.backward(target, learning_rate)\n",
    "        \n",
    "        # Print average loss for the epoch\n",
    "        avg_loss = total_loss / len(X)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_dim = 300 # Word embedding size\n",
    "hidden_dim = 32  # Adjust based on your needs\n",
    "num_classes = 5  # Adjust based on your classification task\n",
    "model = AttentionClassifier(input_dim, hidden_dim, num_classes)\n",
    "\n",
    "# Sample data (replace with your actual dataset)\n",
    "  # One-hot encoded targets\n",
    "    # Add more targets...\n",
    " \n",
    "# Train the model\n",
    "train(model, X_train, y_train, epochs=20, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e5af27a6-d5c7-47ff-a2c2-413d4d053296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business         0\n",
       "entertainment    0\n",
       "politics         0\n",
       "sport            1\n",
       "tech             0\n",
       "Name: 1897, dtype: int32"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=6\n",
    "pd.get_dummies(df[\"category\"], dtype=int).iloc[n+1891]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7b1c89f5-b976-466a-9a05-4226caa9a869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'henry tipped for fifa award fifa president sepp blatter hopes arsenal s thierry henry will be named world player of the year on monday.  henry is on the fifa shortlist with barcelona s ronaldinho and newly-crowned european footballer of the year  ac milan s andriy shevchenko. blatter said:  henry  for me  is the personality on the field. he is the man who can run and organise the game.  the winner of the accolade will be named at a glittering ceremony at zurich s opera house. the three shortlisted candidates for the women s award are mia hamm of the united states  germany s birgit prinz and brazilian youngster marta.  hamm  who recently retired - is looking to regain the women s award  which she lost last year to striker prinz. fifa has changed the panel of voters for this year s awards. male and female captains of every national team will be able to vote  as well as their coaches and fipro - the global organisation for professional players.'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"].iloc[n+1891]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9211e297-1c95-40f3-9b7b-5648c0c46a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0] [0.001 0.117 0.018 0.864 0.000]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "print(y_test[n],model.forward(X_test[n])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c60fbba-fc94-4589-9672-208d618dac32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.99738797e-01, 4.12926975e-05, 3.19123457e-06, 4.18725669e-07,\n",
       "       2.16300504e-04])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49782f4a-a0b7-46ea-973e-e428899554f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba720f59-28b9-4c2f-b129-677a567505fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2b1ae2f0-0f75-4eac-8298-c32e119d58f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1891,), (334,), (1891, 5), (334, 5))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('c:\\\\python312\\\\lib\\\\site-packages')\n",
    "import pickle\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    # Subtract the max value for numerical stability\n",
    "    x = np.clip(x, -1500, 1500)\n",
    "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "def cross_entropy_loss(predictions, target):\n",
    "    return -np.sum(target * np.log(predictions + 1e-3))  # Adding a small constant to avoid log(0)\n",
    "\n",
    "\n",
    "df=pd.read_csv(\"data/bbc-text.csv\")\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "with open('InputProcessed.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "\n",
    "y = np.array(pd.get_dummies(df[\"category\"], dtype=int))\n",
    "tts=0.85\n",
    "X_train,X_test=X[0:round(tts*len(X))],X[round(tts*len(X)):]\n",
    "y_train,y_test=y[0:round(tts*len(X))],y[round(tts*len(X)):]\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8a186784-bf8c-42a3-b86a-48eb0e970b9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing matrices:   0%|                                                                                                        | 0/1891 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'K' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 84\u001b[0m\n\u001b[0;32m     80\u001b[0m     Loss \u001b[38;5;241m=\u001b[39m cross_entropy_loss(yi, yt)\n\u001b[0;32m     82\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m Loss\n\u001b[1;32m---> 84\u001b[0m     \u001b[43mbackpropagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43myi\u001b[49m\u001b[43m,\u001b[49m\u001b[43myt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mphrase_representation\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlinearlayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_train)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Average Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[123], line 59\u001b[0m, in \u001b[0;36mbackpropagation\u001b[1;34m(yi, yt, phrase_representation, linearlayer)\u001b[0m\n\u001b[0;32m     56\u001b[0m d_QKscaled \u001b[38;5;241m=\u001b[39m d_sigma_attention \u001b[38;5;241m*\u001b[39m Attention_weights \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m Attention_weights)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Gradient for Q and K\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m d_Qval \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(d_QKscaled, Kval) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mK\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     60\u001b[0m d_Kval \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(d_QKscaled\u001b[38;5;241m.\u001b[39mT, Qval) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(K\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     61\u001b[0m d_Q \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(Xi\u001b[38;5;241m.\u001b[39mT, d_Qval)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'K' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "\n",
    "Attention_Embedding_Dimension=64\n",
    "word_embedding_dim=X_train[0].shape[1]\n",
    "\n",
    "Q = np.random.rand(word_embedding_dim, Attention_Embedding_Dimension)/ np.sqrt(word_embedding_dim)\n",
    "K = np.random.rand(word_embedding_dim, Attention_Embedding_Dimension)/ np.sqrt(word_embedding_dim)\n",
    "V = np.random.rand(word_embedding_dim, Attention_Embedding_Dimension)/ np.sqrt(word_embedding_dim)\n",
    "\n",
    "num_classes = 5  # Example number of classes\n",
    "\n",
    "        \n",
    "linearlayer = np.random.rand(Attention_Embedding_Dimension, num_classes)\n",
    "linear_bias = np.random.rand(num_classes)\n",
    "learning_rate=0.002\n",
    "epochs=10\n",
    "words_in_phrase=70\n",
    "\n",
    "def forward(Xi):\n",
    "    Qval = np.dot(Xi, Q)\n",
    "    Kval = np.dot(Xi, K)\n",
    "    Vval = np.dot(Xi, V)\n",
    "\n",
    "    QKscaled = np.dot(Qval, Kval.T) / np.sqrt(K.shape[1])\n",
    "\n",
    "    Attention_weights = softmax(QKscaled)\n",
    "\n",
    "    Attention = np.dot(Attention_weights, Vval)\n",
    "\n",
    "    phrase_representation = np.mean(Attention, axis=0)\n",
    "\n",
    "    yi = softmax(np.dot(phrase_representation, linearlayer) + linear_bias)\n",
    "\n",
    "    return yi,phrase_representation,linearlayer\n",
    "    \n",
    "def backpropagation(yi,yt,phrase_representation,linearlayer):\n",
    "    # Gradient of loss with respect to output probabilities\n",
    "    dL_yi = yi - yt\n",
    "\n",
    "    # Gradient for linear layer and bias\n",
    "    dL_dw = np.outer(phrase_representation, dL_yi)\n",
    "    dL_bias = dL_yi\n",
    "\n",
    "    # Gradient for phrase representation\n",
    "    d_phrase_rep = np.dot(dL_yi, linearlayer.T)\n",
    "\n",
    "    # Gradient for attention\n",
    "    d_attention = np.outer(np.ones(Xi.shape[0]), d_phrase_rep) / Xi.shape[0]\n",
    "\n",
    "    # Gradient for V\n",
    "    d_Vval = np.dot(Attention_weights, d_attention)\n",
    "    d_V = np.dot(Xi.T, d_Vval)\n",
    "\n",
    "    # Gradient for attention weights\n",
    "    d_sigma_attention = np.dot(d_attention, Vval.T)\n",
    "\n",
    "    # Gradient for QK scaled\n",
    "    d_QKscaled = d_sigma_attention * Attention_weights * (1 - Attention_weights)\n",
    "\n",
    "    # Gradient for Q and K\n",
    "    d_Qval = np.dot(d_QKscaled, Kval) / np.sqrt(K.shape[1])\n",
    "    d_Kval = np.dot(d_QKscaled.T, Qval) / np.sqrt(K.shape[1])\n",
    "    d_Q = np.dot(Xi.T, d_Qval)\n",
    "    d_K = np.dot(Xi.T, d_Kval)\n",
    "\n",
    "    # Update weights\n",
    "    Q -= learning_rate * d_Q\n",
    "    K -= learning_rate * d_K\n",
    "    V -= learning_rate * d_V\n",
    "    linearlayer -= learning_rate * dL_dw\n",
    "    linear_bias -= learning_rate * dL_bias\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in tqdm(range(len(y_train)), desc=\"Processing matrices\"):\n",
    "        Xi = X_train[i]\n",
    "        yt = y_train[i]\n",
    "       \n",
    "       \n",
    "        yi,phrase_representation,linearlayer=forward(Xi)\n",
    "        \n",
    "        Loss = cross_entropy_loss(yi, yt)\n",
    "        \n",
    "        total_loss += Loss\n",
    "         \n",
    "        backpropagation(yi,yt,phrase_representation,linearlayer)\n",
    "        \n",
    "    avg_loss = total_loss / len(y_train)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Average Loss: {avg_loss:.5f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1446c00-9370-4bf4-ad93-ae0f79e846f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b036f928-2902-4ece-92f6-abf185e4fe51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c3a014-7c52-47d6-b0f0-4aab54cf6c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.outer(np.ones(5), np.array([[0.1,0.1],[0.1,0.1],[0.1,0.1]])) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "9f2289d7-cacb-4288-b9a1-86b0d81db14a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca43a430-cf21-4903-9d77-f4dfceccf107",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
