{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3b060598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import re\n",
    "import cupy as cp\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jax\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "np.set_printoptions(edgeitems=30, linewidth=100000, formatter=dict(float=lambda x: \"%.3g\" % x))\n",
    "\n",
    "\n",
    "def log_time(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()  # Record start time\n",
    "        result = func(*args, **kwargs)  # Execute the wrapped function\n",
    "        end_time = time.time()  # Record end time\n",
    "        elapsed_time = end_time - start_time\n",
    "        # print(f\"Function '{func.__name__}' executed in {elapsed_time:.4f} seconds\")\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def get_positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Returns a non-learnable (sinusoidal) positional encoding.\n",
    "\n",
    "\n",
    "    seq_len: Length of the input sequence.\n",
    "    d_model: Dimension of the embeddings.\n",
    "    \"\"\"\n",
    "    pos = cp.arange(seq_len)[:, cp.newaxis]  # Shape: [seq_len, 1]\n",
    "    i = cp.arange(d_model)[cp.newaxis, :]  # Shape: [1, d_model]\n",
    "\n",
    "    angle_rates = 1 / cp.power(10000, (2 * (i // 2)) / cp.float32(d_model))\n",
    "\n",
    "    # Apply sine to even indices, cosine to odd indices\n",
    "    pos_encoding = cp.zeros((seq_len, d_model))\n",
    "    pos_encoding[:, 0::2] = cp.sin(pos * angle_rates[:, 0::2])  # sine on even indices\n",
    "    pos_encoding[:, 1::2] = cp.cos(pos * angle_rates[:, 1::2])  # cosine on odd indices\n",
    "\n",
    "    return pos_encoding\n",
    "\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    # Subtract the max value for numerical stability\n",
    "    e_x = cp.exp(x - cp.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / cp.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# @log_time\n",
    "def pad_sequence(seq, max_len, pad_value=0):\n",
    "    \"\"\"Pad a sequence with a given value up to max_len.\"\"\"\n",
    "    current_len = seq.shape[0]\n",
    "    pad_width = max_len - current_len\n",
    "    if pad_width > 0:\n",
    "        # Pad sequence with zeros (or any pad_value you provide)\n",
    "        seq = cp.pad(seq, ((0, pad_width), (0, 0)), mode='constant', constant_values=pad_value)\n",
    "    return seq\n",
    "\n",
    "\n",
    "@log_time\n",
    "def create_timestaped_input(input_d, words_per_phrase):\n",
    "    input_translation = []\n",
    "    for j in range(input_d.shape[0]):\n",
    "        # Create padded sequences\n",
    "        padded_sequences = [pad_sequence(input_d[j][0:i], words_per_phrase) for i in range(1, input_d.shape[1] + 1)]\n",
    "        input_translation.append(padded_sequences)\n",
    "    return cp.array(input_translation)\n",
    "\n",
    "  \n",
    "def redimension(X):\n",
    "    return cp.concatenate(cp.swapaxes(X, 0, 1), axis=-1)\n",
    "\n",
    " \n",
    "@log_time\n",
    "def create_vocabulary(complete_text, name, nlp):\n",
    "    # Use re.findall to split considering punctuation\n",
    "    text = re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', complete_text)\n",
    "\n",
    "    words_list = list(set(text))\n",
    "\n",
    "    vocabulary = dict()\n",
    "\n",
    "    for i, j in enumerate(words_list):\n",
    "        # vocabulary[j]=(jax.random.uniform(jax.random.key(cp.random.randint(10000)),embedding_size),i)\n",
    "        vocabulary[j] = (cp.array(nlp(j).vector), i)#(cp.random.rand(50), i)\n",
    "        # print(j,len(cp.array(nlp(j).vector)))\n",
    "\n",
    "    # print(vocabulary)\n",
    "    # print(\"Vocabulary size: \", len(vocabulary))\n",
    "    with open(f\"data/{name}.pkl\", 'wb') as handle:\n",
    "        pickle.dump(vocabulary, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "@log_time\n",
    "def pad_sequences(sentences, lenght, pad_token='[PAD]', target_type=None):\n",
    "    \"\"\"\n",
    "    Pads the input sentences to have the same length by adding [PAD] tokens at the end.\n",
    "    \"\"\"\n",
    "\n",
    "    if target_type == \"encoder\":\n",
    "        # Split each sentence into words\n",
    "        tokenized_sentences = [[\"[START]\"] + re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]\\n', sentence) + [\"[END]\"] for sentence in\n",
    "                               sentences]\n",
    "    elif target_type == \"decoder\":\n",
    "        tokenized_sentences = [[\"[START]\"] + re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', sentence) for sentence in sentences]\n",
    "    elif target_type == \"target\":\n",
    "        tokenized_sentences = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', sentence) + [\"[END]\"] for sentence in sentences]\n",
    "    # print(tokenized_sentences)\n",
    "    if lenght == 0:\n",
    "        # Find the maximum sentence length\n",
    "        max_len = max(len(sentence) for sentence in tokenized_sentences)\n",
    "    else:\n",
    "        max_len = lenght\n",
    "\n",
    "    # Pad each sentence with the [PAD] token to make them of equal length\n",
    "    padded_sentences = [\" \".join(sentence + [pad_token] * (max_len - len(sentence))) for sentence in\n",
    "                        tokenized_sentences]\n",
    "\n",
    "    return padded_sentences\n",
    "\n",
    "def print_matrix(X):\n",
    "    for i in X:\n",
    "        print(i)\n",
    "@log_time\n",
    "def create_decoder_target(y_train, embedding_size, max_words_per_phrase, vocabulary_decoder):\n",
    "\n",
    "    decoder_input = pad_sequences(y_train, lenght=max_words_per_phrase, target_type=\"target\")\n",
    "    #print_matrix(decoder_input)\n",
    "    decoder_input = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', i) for i in decoder_input]\n",
    "    \n",
    "    if max_words_per_phrase == None:\n",
    "        max_words_per_phrase = len(decoder_input[0])\n",
    "\n",
    "    phrase_vectors_y = [i[0:max_words_per_phrase] for i in decoder_input]\n",
    "    # for sentence in phrase_vectors_y:\n",
    "    #     print(sentence)\n",
    "    #print_matrix(phrase_vectors_y)\n",
    "    \n",
    "\n",
    "    yi = cp.array([[get_one_hot(word,vocabulary_decoder) for word in phrase_vector] for phrase_vector in phrase_vectors_y]) \n",
    "     \n",
    "    return yi\n",
    "@log_time\n",
    "def generate_input_encoder(x_batch, vocabulary_encoder, max_words_per_phrase):\n",
    "\n",
    "    x_train = pad_sequences(x_batch, max_words_per_phrase, target_type=\"encoder\")# here are string\n",
    "    \n",
    "    xi = [] \n",
    "    phrase_vectors_x = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', x) for x in x_train] \n",
    "    phrase_vectors_x = [i[0:max_words_per_phrase] for i in phrase_vectors_x] \n",
    "    #print(\"x batch\",phrase_vectors_x)\n",
    "    xi = cp.array([[vocabulary_encoder[word][0] for word in phrase_vector] for phrase_vector in phrase_vectors_x])\n",
    "\n",
    "    return xi\n",
    "\n",
    "\n",
    "# @log_time\n",
    "def update_wembedding_encoder(x_batch, inputs_e, vocabulary, max_words_per_phrase):\n",
    "    x_train = pad_sequences(x_batch, max_words_per_phrase, target_type=\"encoder\")\n",
    "    # print(x_train)\n",
    "\n",
    "    phrase_vectors_x = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', x) for x in x_train]\n",
    "\n",
    "    phrase_vectors_x = [i[0:max_words_per_phrase] for i in phrase_vectors_x]\n",
    "    # print(\"inputs_e\",inputs_e.shape)\n",
    "    # print(\"(phrase_vectors_x).shape:\\n\",cp.array(phrase_vectors_x).shape)\n",
    "\n",
    "    for phrase in range(inputs_e.shape[0]):\n",
    "        # print(phrase)\n",
    "        for position, word in enumerate(phrase_vectors_x[phrase]): \n",
    "            vocabulary[word] = (inputs_e[phrase][position], vocabulary[word][1])\n",
    "\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "# @log_time\n",
    "def update_wembedding_decoder(y_batch, inputs_decoder, max_words_per_phrase, vocabulary):\n",
    "    # print(\"inputs_decoder\",inputs_decoder.shape)\n",
    "    decoder_input = pad_sequences(y_batch, lenght=max_words_per_phrase, target_type=\"decoder\")\n",
    "    decoder_input = [i.split() for i in decoder_input]\n",
    "    # print(max_words_per_phrase)\n",
    "    if max_words_per_phrase == None:\n",
    "        max_words_per_phrase = len(decoder_input[0])\n",
    "\n",
    "    phrase_vectors_y = [i[0:max_words_per_phrase] for i in decoder_input]\n",
    "    # for sentence in phrase_vectors_y:\n",
    "    #     print(sentence)\n",
    "    for phrase in range(inputs_decoder.shape[0]):\n",
    "        # print(phrase)\n",
    "        for position, word in enumerate(phrase_vectors_y[phrase]): \n",
    "            vocabulary[word] = (inputs_decoder[phrase][position], vocabulary[word][1])\n",
    "\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "@log_time\n",
    "def create_input_encoder(X, vocabulary_encoder, max_words_per_phrase, embedding_size):\n",
    "\n",
    "    pos_encoding = get_positional_encoding(max_words_per_phrase, embedding_size)\n",
    "    #print(pos_encoding)\n",
    "    inputs_e = generate_input_encoder(X, vocabulary_encoder, max_words_per_phrase)\n",
    "    \n",
    "    #print(inputs_e)\n",
    "\n",
    "    inputs_e += pos_encoding\n",
    "    return inputs_e\n",
    "\n",
    "\n",
    "@log_time\n",
    "def create_decoder_input(y_train, embedding_size, max_words_per_phrase, vocabulary_decoder):\n",
    "    decoder_input = pad_sequences(y_train, lenght=max_words_per_phrase, target_type=\"decoder\")\n",
    "    #print_matrix(decoder_input)\n",
    "    decoder_input = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', i) for i in decoder_input]\n",
    "    \n",
    "    if max_words_per_phrase == None:\n",
    "        max_words_per_phrase = len(decoder_input[0])\n",
    "\n",
    "    phrase_vectors_y = [i[0:max_words_per_phrase] for i in decoder_input]\n",
    "    # for sentence in phrase_vectors_y:\n",
    "    #     print(sentence)\n",
    "    #print_matrix(phrase_vectors_y)\n",
    "    \n",
    "\n",
    "    yi = cp.array([[vocabulary_decoder[word][0] for word in phrase_vector] for phrase_vector in phrase_vectors_y])\n",
    "    \n",
    "    pos_encoding = get_positional_encoding(max_words_per_phrase, embedding_size)\n",
    "    # print(pos_encoding.shape,yi.shape)\n",
    "    yi = yi + pos_encoding\n",
    "    #print_matrix(yi)\n",
    "    # decoder_inputs = cp.array(cp.swapaxes(create_timestaped_input(yi, max_words_per_phrase), 0, 1))\n",
    "    \n",
    "    # # decoder_inputs[zero_rows] = vocabulary_decoder[\"[PAD]\"][0]\n",
    "    # for i in range(decoder_inputs.shape[0]):\n",
    "    #     for j in range(decoder_inputs[i].shape[0]):\n",
    "    #         zero_rows = cp.all(decoder_inputs[i][j] == 0, axis=1)\n",
    "\n",
    "    #         decoder_inputs[i][j][zero_rows] =cp.zeros(embedding_size)# vocabulary_decoder[\"[PAD]\"][0]\n",
    "\n",
    "    # decoder_inputs = cp.array(decoder_inputs)\n",
    "    #print(decoder_inputs[2])\n",
    "    #print(decoder_inputs)\n",
    "    return  yi\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "def bucket_by_length(x_train, y_train, batch_size):\n",
    "    # Pair x_train and y_train together\n",
    "    paired_data = list(zip(x_train, y_train))\n",
    "\n",
    "    # Sort the pairs by the length of the x_train phrases\n",
    "    paired_data_sorted = sorted(paired_data, key=lambda pair: len(pair[1]))\n",
    "\n",
    "    # Initialize buckets for x_train and y_train\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "\n",
    "    # Group phrases into batches of batch_size\n",
    "    for i in range(0, len(paired_data_sorted), batch_size):\n",
    "        batch = paired_data_sorted[i:i + batch_size]\n",
    "\n",
    "        # Split the batch back into x_train and y_train\n",
    "        x_batch, y_batch = zip(*batch)\n",
    "\n",
    "        x_batches.append(list(x_batch))\n",
    "        y_batches.append(list(y_batch))\n",
    "\n",
    "    return x_batches, y_batches\n",
    "\n",
    "\n",
    "def pad_inputs(sentences, target_type):\n",
    "    if target_type == \"encoder\":\n",
    "        tokenized_sentences = [['[START]'] + sentence + ['[END]'] for sentence in sentences]\n",
    "    elif target_type == \"decoder\":\n",
    "        tokenized_sentences = [[\"[START]\"] + sentence for sentence in sentences]\n",
    "    elif target_type == \"target\":\n",
    "        tokenized_sentences = [sentence + [\"[END]\"] for sentence in sentences]\n",
    "    return tokenized_sentences\n",
    "\n",
    "\n",
    "def generate_inputs(sentences, vocabulary, pad_token='[PAD]', target_type=None):\n",
    "    \"\"\"\n",
    "    Pads the input sentences to have the same length by adding [PAD] tokens at the end.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_sentences = pad_inputs(sentences, target_type)\n",
    "    max_len = max(len(sentence) for sentence in tokenized_sentences)\n",
    "    # Pad each sentence with the [PAD] token to make them of equal length\n",
    "    padded_sentences = [sentence + [pad_token] * (max_len - len(sentence)) for sentence in tokenized_sentences]\n",
    "\n",
    "    # print(padded_sentences)\n",
    "    xi = cp.array([[vocabulary[word][0] for word in phrase_vector] for phrase_vector in padded_sentences])\n",
    "    # print(xi.shape)\n",
    "    pos_encoding = get_positional_encoding(max_len, 300)\n",
    "    xi += pos_encoding\n",
    "\n",
    "    if target_type == \"decoder\":\n",
    "        xi = cp.array(cp.swapaxes(create_timestaped_input(xi, max_len), 0, 1))\n",
    "\n",
    "    return xi\n",
    "\n",
    "\n",
    "def forward_attention_encoder(inputs_e,Qe, Ke, Ve, num_heads, batch_size, dk):\n",
    "    \n",
    "\n",
    "    Q_E = cp.swapaxes(cp.array(cp.array_split(cp.matmul(inputs_e, Qe), num_heads, axis=2)), 0, 1)\n",
    "    # print(\"Qval.shape: \",Q_E.shape)\n",
    "\n",
    "    K_E = cp.swapaxes(cp.array(cp.array_split(cp.matmul(inputs_e, Ke), num_heads, axis=2)), 0, 1)\n",
    "    # print(\"Kval.shape: \",K_E.shape)\n",
    "\n",
    "    V_E = cp.swapaxes(cp.array(cp.array_split(cp.matmul(inputs_e, Ve), num_heads, axis=2)), 0, 1)\n",
    "    # print(\"Vval.shape: \",V_E.shape)\n",
    "\n",
    "    QKscaled = cp.matmul(Q_E, cp.transpose(K_E, (0, 1, 3, 2))) / cp.sqrt(dk)\n",
    "\n",
    "    Attention_weights_e = softmax(QKscaled)\n",
    "    # print(\"Attention_weights shape:\",Attention_weights_e.shape)\n",
    "\n",
    "    Ae = cp.matmul(Attention_weights_e, V_E)\n",
    "    # print(\"Attention shape:\",Ae.shape)\n",
    "\n",
    "    Ae = cp.array([cp.concatenate(Ae[i], axis=1) for i in range(batch_size)])\n",
    "    # print(\"Attention encoder shape concat:\",Ae.shape)\n",
    "\n",
    "    return Ae, Attention_weights_e, K_E, V_E, Q_E\n",
    "\n",
    "import cupy as cp\n",
    "\n",
    "def compute_derivative_A(X, mu,sqrVar,sqrVar_3_2):\n",
    "     \n",
    "    A=1 /sqrVar\n",
    "     \n",
    "    B=X*(X-mu)\n",
    "    \n",
    "    result=A-(B/sqrVar_3_2)\n",
    "    return result\n",
    "\n",
    "\n",
    "def compute_derivative_B(X, mu,sqrVar,N,sqrVar_3_2):\n",
    "   \n",
    "    A=-1/(N*sqrVar)\n",
    "\n",
    "    B=mu*(X-mu)\n",
    "    \n",
    "    result=A+(B/sqrVar_3_2)\n",
    "    #print(A)\n",
    "    return result\n",
    "\n",
    "def diff_norm(X, var, mu, N,epsilon=1e-6): \n",
    "     \n",
    "    sqrVar=cp.sqrt(var+epsilon)\n",
    "    sqrVar_3_2=N*((var + epsilon) ** (3 / 2))\n",
    " \n",
    "    A=compute_derivative_A(X, mu, sqrVar,sqrVar_3_2)\n",
    "    B=compute_derivative_B(X, mu,sqrVar,N,sqrVar_3_2)\n",
    "    result=A+B\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class layer_normalization: \n",
    "    def __init__(self,epsilon):\n",
    "        self.epsilon=epsilon\n",
    "        self.mu=0\n",
    "        self.var=0\n",
    "        self.N=0\n",
    "        pass\n",
    "\n",
    "    def forward(self,x):\n",
    "        self.X=x\n",
    "        self.mu = cp.mean(x, axis=-1, keepdims=True)\n",
    "        self.var = cp.var(x, axis=-1, keepdims=True) \n",
    "        self.N=x.shape[-1]\n",
    "        x_norm = (x - self.mu) / cp.sqrt(self.var + self.epsilon)  \n",
    "        return x_norm\n",
    "    \n",
    "    def compute_derivative_A(self,X, mu,sqrVar,sqrVar_3_2):\n",
    "        A=1 /sqrVar\n",
    "        B=X*(X-mu)\n",
    "        result=A-(B/sqrVar_3_2)\n",
    "        return result\n",
    "    \n",
    "    def compute_derivative_B(self,X, mu,sqrVar,N,sqrVar_3_2):\n",
    "        A=-1/(N*sqrVar)\n",
    "        B=mu*(X-mu)\n",
    "        result=A+(B/sqrVar_3_2)\n",
    "        return result\n",
    "    \n",
    "    def backpropagation(self):\n",
    "        sqrVar=cp.sqrt(self.var+self.epsilon)\n",
    "        sqrVar_3_2=self.N*((self.var + self.epsilon) ** (3 / 2)) \n",
    "        A=compute_derivative_A(self.X, self.mu, sqrVar,sqrVar_3_2)\n",
    "        B=compute_derivative_B(self.X, self.mu,sqrVar,self.N,sqrVar_3_2)\n",
    "        result=A+B\n",
    "        return result\n",
    "\n",
    "class linear_layer: \n",
    "    def __init__(self,input_size,output_size,out=False):\n",
    "        if out==True:\n",
    "            self.W=cp.random.rand(input_size, output_size)/cp.sqrt(input_size)\n",
    "        else:\n",
    "            self.W=cp.random.rand(input_size, output_size) \n",
    "        self.b=cp.random.rand(output_size)\n",
    "      \n",
    "    def forward(self,X): \n",
    "        self.X=X\n",
    "        Xout = cp.matmul(X, self.W) + self.b \n",
    "        return Xout\n",
    "    \n",
    "    def forward_weights_only(self,X): \n",
    "        self.X=X\n",
    "        Xout = cp.matmul(X, self.W) \n",
    "        return Xout\n",
    "    \n",
    "    def dW(self): \n",
    "        return self.X\n",
    "    \n",
    "    def db(self): \n",
    "        return 1\n",
    "    \n",
    "    def update_weights(self,dLoss_dW,dLoss_db,learning_rate):\n",
    "        self.W=self.W-learning_rate*dLoss_dW\n",
    "        self.b=self.b-learning_rate*dLoss_db\n",
    "\n",
    "    def update_weights_only(self,dLoss_dW,learning_rate):\n",
    "        self.W=self.W-learning_rate*dLoss_dW\n",
    "       \n",
    "\n",
    "class ReLu_layer:\n",
    "    def __init__(self,alpha):\n",
    "        self.alpha=alpha\n",
    "         \n",
    "    \n",
    "    def forward_leaky(self,X):\n",
    "        self.X=X\n",
    "        return cp.where(X > 0, X, self.alpha * X)\n",
    "\n",
    "    def forward(self,X): \n",
    "        self.X=X\n",
    "        return cp.maximum(0,self.X)\n",
    "    \n",
    "    def backward(self, dLoss): \n",
    "        # Gradient of ReLU is 1 for x > 0, else 0\n",
    "        dx = dLoss * (self.X > 0)  # Only propagate gradients for inputs > 0\n",
    "        return dx\n",
    "    \n",
    "    def backward_leaky(self, dLoss): \n",
    "        dx = dLoss * cp.where(self.X > 0, 1, self.alpha)  # Gradient: 1 for x > 0, else alpha\n",
    "        return dx\n",
    "\n",
    "\n",
    "def layer_norm(x, epsilon=1e-6): \n",
    "    # Calculate the mean and variance\n",
    "    mean = cp.mean(x, axis=-1, keepdims=True)\n",
    "    var = cp.var(x, axis=-1, keepdims=True) \n",
    "    x_norm = (x - mean) / cp.sqrt(var + epsilon) \n",
    "    return x_norm, mean, var, x.shape[-1]\n",
    "\n",
    "\n",
    "def residual_and_norm(Ae, inputs_e):\n",
    "    Xe = Ae + inputs_e\n",
    "    Ect1, mu_e, var_e, Ne = layer_norm(Xe)\n",
    "    # print(\"Ect1.shape\",Ect1.shape,Ne)\n",
    "    return Ect1, Xe, mu_e, var_e, Ne\n",
    "\n",
    "\n",
    "def leaky_relu(X, alpha=0.01):\n",
    "    return cp.where(X > 0, X, alpha * X)\n",
    "\n",
    "def relu(X): \n",
    "    return cp.max(0,X)\n",
    "def relu_backward(dA, Xd1,alpha=0.01):\n",
    "    # Create mask where Xd1 > 0\n",
    "    relu_mask = (Xd1 > 0).astype(float)  # This will give 1 where Xd1 > 0, else 0\n",
    "    # Multiply the incoming gradient dA by the mask\n",
    "    dXd1 = dA * np.where(Xd1 > 0, 1,alpha)\n",
    "    return dXd1\n",
    "\n",
    "def fully_connected_layers_encoder(Ect1,Wfl1e, bfl1e, Wfl2e, bfl2e): \n",
    "    Xe1 = cp.matmul(Ect1, Wfl1e) + bfl1e\n",
    "    FLe1 = leaky_relu(Xe1)\n",
    "    FLe1=dropout(FLe1, dropout_rate=0.1, training=True)\n",
    "    FLe2 = cp.matmul(FLe1, Wfl2e) + bfl2e\n",
    "    Xe2 = FLe2 + Ect1\n",
    "    Ecout, mu_e2, var_e2, N_e2 = layer_norm(Xe2)\n",
    "    return Ecout,mu_e2,var_e2,N_e2,FLe1,Xe1,Xe2\n",
    "    \n",
    "     \n",
    "\n",
    " \n",
    "\n",
    "def cross_attention_encoder(Ecout):\n",
    "    global Kc, Vc\n",
    "    K_C = cp.swapaxes(cp.array(cp.array_split(cp.matmul(Ecout, Kc), num_heads, axis=2)), 0, 1)\n",
    "    # print(\"K_C.shape: \",K_C.shape)# shape is: num_phrases, numbheads, words_per_phrase, dv/num_heads\n",
    "    V_C = cp.swapaxes(cp.array(cp.array_split(cp.matmul(Ecout, Vc), num_heads, axis=2)), 0, 1)\n",
    "    # print(\"V_C.shape: \",V_C.shape)\n",
    "    return K_C, V_C\n",
    "\n",
    "class multihead_attention: \n",
    "    def __init__(self,num_heads,dk,batch_size):\n",
    "        self.num_heads=num_heads\n",
    "        self.dk=dk\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "    def reshape_heads(self,Q,K,V):\n",
    "        self.Q = cp.swapaxes(cp.array(cp.array_split(Q, self.num_heads, axis=2)), 0, 1)\n",
    "        # print(\"Qval.shape: \",Q_E.shape)\n",
    "        self.K = cp.swapaxes(cp.array(cp.array_split(K, self.num_heads, axis=2)), 0, 1)\n",
    "        # print(\"Kval.shape: \",K_E.shape)\n",
    "        self.V = cp.swapaxes(cp.array(cp.array_split(V, self.num_heads, axis=2)), 0, 1)\n",
    "        return self.Q,self.K,self.V\n",
    "\n",
    "    def attention_weights(self):\n",
    "        QKscaled = cp.matmul(self.Q, cp.transpose(self.K, (0, 1, 3, 2))) / cp.sqrt(self.dk) \n",
    "        self.Attention_weights = softmax(QKscaled)\n",
    "         \n",
    "\n",
    "    def attention(self):\n",
    "        self.attention_weights()\n",
    "        Ae = cp.matmul(self.Attention_weights, self.V)\n",
    "        # print(\"Attention shape:\",Ae.shape)\n",
    "        Ae = cp.array([cp.concatenate(Ae[i], axis=1) for i in range(self.batch_size)])\n",
    "        # print(\"Attention encoder shape concat:\",Ae.shape)\n",
    "        return Ae\n",
    "    \n",
    "    def masked_attention(self,mask_size):\n",
    "        self.attention_weights_masked(mask_size)\n",
    "        Ae = cp.matmul(self.Attention_weights, self.V)\n",
    "         \n",
    "        Ae = cp.array([cp.concatenate(Ae[i], axis=1) for i in range(self.batch_size)])\n",
    "        # print(\"Attention encoder shape concat:\",Ae.shape)\n",
    "        return Ae\n",
    "    \n",
    "    def attention_weights_masked(self,mask_size):\n",
    "        #mask_size =  words_per_phrase \n",
    "\n",
    "        QKscaled = cp.matmul(self.Q, cp.transpose(self.K, (0, 1, 3, 2))) / cp.sqrt(self.dk)\n",
    "         \n",
    "        mask = cp.tril(cp.ones((mask_size, mask_size)))  # (9, 9) lower triangular matrix\n",
    "        mask[mask == 0]=-cp.inf  # Set future tokens to -inf\n",
    "        mask[mask == 1]=0  # Set allowed tokens to 0\n",
    "         \n",
    "        mask = mask.reshape(1, 1, mask_size, mask_size)\n",
    "        QKscaled = QKscaled + mask\n",
    "\n",
    "        self.Attention_weights = softmax(QKscaled)\n",
    "         \n",
    "    \n",
    "    def diffQi(self,dAttention,X):\n",
    "        dAttention_weights = self.Attention_weights * (1 - self.Attention_weights) \n",
    "        dLoss_dX=cp.transpose(dAttention, (0, 2, 1)) @ (redimension(dAttention_weights @ (self.K * self.V) / cp.sqrt(self.dk))*X)\n",
    "        return cp.sum(dLoss_dX,axis=0)\n",
    "    \n",
    "    def diffKi(self,dAttention,X):\n",
    "        dAttention_weights = self.Attention_weights * (1 - self.Attention_weights) \n",
    "        X = cp.swapaxes(cp.array(cp.array_split(X, self.num_heads, axis=2)), 0, 1) \n",
    "         \n",
    "        dLoss_dX = cp.transpose(dAttention, (0, 2, 1)) @ redimension(\n",
    "            (dAttention_weights * (self.Q @ cp.transpose(self.V, (0, 1, 3, 2))) @ X) / cp.sqrt(self.dk)) \n",
    "        return cp.sum(dLoss_dX,axis=0)\n",
    "    \n",
    "    def diffVi(self,dAttention,X):\n",
    "        dLoss_Vc = cp.sum(cp.sum(cp.transpose(cp.expand_dims(dAttention, axis=1), (0, 1, 3, 2)) @ (\n",
    "                self.Attention_weights @ cp.expand_dims(X, axis=1)), axis=1), axis=0)\n",
    "        return dLoss_Vc\n",
    "    \n",
    "    def diffKInput(self,dAttention,Ki):\n",
    "        dAttention_weights = self.Attention_weights * (1 - self.Attention_weights) \n",
    "        dLoss_KI=dAttention*(redimension(dAttention_weights @ self.Q / cp.sqrt(self.dk))*redimension(self.V)@Ki)\n",
    "        return dLoss_KI\n",
    "    \n",
    "    def diffVInput(self,dAttention,Vi):\n",
    "        dLoss_V_E = cp.transpose(\n",
    "        cp.mean(cp.transpose(cp.expand_dims(dAttention, axis=1), (0, 1, 3, 2)) @ self.Attention_weights, axis=1), (0, 2, 1))\n",
    "        dLoss_inpute_v = dLoss_V_E @ Vi\n",
    "        return dLoss_inpute_v\n",
    "    \n",
    "    def diffQInput(self,dAttention,Qi):\n",
    "        dAttention_weights = self.Attention_weights * (1 - self.Attention_weights) \n",
    "        dLoss_KI=dAttention*(redimension(dAttention_weights @ self.K / cp.sqrt(self.dk))*redimension(self.V)@Qi)\n",
    "        return dLoss_KI\n",
    "\n",
    "\n",
    "def derivative_input_encoder(dLoss_Ae, Attention_weights_e, K_E, V_E, Q_E, dLoss_inpute_a, inputs_e,Ve, Qe, Ke, dk):\n",
    "    \n",
    "\n",
    "    dLoss_V_E = cp.transpose(\n",
    "        cp.mean(cp.transpose(cp.expand_dims(dLoss_Ae, axis=1), (0, 1, 3, 2)) @ Attention_weights_e, axis=1), (0, 2, 1))\n",
    "    dLoss_inpute_v = dLoss_V_E @ Ve\n",
    "\n",
    "    dAttention_weights_e = Attention_weights_e * (1 - Attention_weights_e)\n",
    "    V1 = redimension(dAttention_weights_e @ K_E / cp.sqrt(dk))\n",
    "    V2 = redimension(V_E)\n",
    "    V3 = V1 * V2\n",
    "    dLoss_Q_E = dLoss_Ae * V3\n",
    "    dLoss_inpute_q = dLoss_Q_E @ Qe\n",
    "    # print(\"dLoss_inpute_q.shape\",dLoss_inpute_q.shape)\n",
    "\n",
    "    V1 = redimension(dAttention_weights_e @ Q_E / cp.sqrt(dk))\n",
    "    V2 = redimension(V_E)\n",
    "    V3 = V1 * V2\n",
    "    dLoss_K_E = dLoss_Ae * V3\n",
    "    dLoss_inpute_k = dLoss_K_E @ Ke\n",
    "    # print(\"dLoss_inpute_k.shape\",dLoss_inpute_k.shape)\n",
    "    dLoss_inpute = dLoss_inpute_a + dLoss_inpute_k + dLoss_inpute_q + dLoss_inpute_v\n",
    "    dLoss_dWemb_encoder = dLoss_inpute * inputs_e\n",
    "    return dLoss_inpute, dLoss_dWemb_encoder\n",
    "\n",
    "def diffQKV(dAttention, Attention_weights, X1, X2, X3, dk, matrix=\"\"):\n",
    "    global num_heads \n",
    "    dAttention_weights = Attention_weights * (1 - Attention_weights) \n",
    "    if matrix != \"k\":\n",
    "        dLoss_dX = cp.transpose(dAttention, (0, 2, 1)) @ (\n",
    "                    redimension(dAttention_weights @ (X1 * X2) / cp.sqrt(dk)) * X3)\n",
    "    else: \n",
    "        X3 = cp.swapaxes(cp.array(cp.array_split(X3, num_heads, axis=2)), 0, 1) \n",
    "        dLoss_dX = cp.transpose(dAttention, (0, 2, 1)) @ redimension(\n",
    "            (dAttention_weights * (X1 @ cp.transpose(X2, (0, 1, 3, 2))) @ X3) / cp.sqrt(dk)) \n",
    "    return dLoss_dX\n",
    "\n",
    "########################################################DECODER\n",
    "def forward_attention_decoder(input_decoder):\n",
    "    global Qd, Kd, Vd  # ,words_per_phrase\n",
    "\n",
    "    Q_D = cp.swapaxes(cp.array(cp.array_split(cp.matmul(input_decoder, Qd), num_heads, axis=2)), 0, 1)\n",
    "    # print(\"Qval.shape: \",Q_D.shape)# numwords, num_phrases, numheads, num_words, dv/num_heads\n",
    "\n",
    "    # K_D  = cp.swapaxes(cp.swapaxes(cp.array(cp.array_split(cp.matmul(inputs_d[step], Kd),num_heads,axis=3)), 0, 1),1,2)\n",
    "    K_D = cp.swapaxes(cp.array(cp.array_split(cp.matmul(input_decoder, Kd), num_heads, axis=2)), 0, 1)\n",
    "    # print(\"Kval.shape: \",K_D.shape)\n",
    "\n",
    "    # V_D  = cp.swapaxes(cp.swapaxes(cp.array(cp.array_split(cp.matmul(inputs_d[step], Vd),num_heads,axis=3)), 0, 1),1,2)\n",
    "    V_D = cp.swapaxes(cp.array(cp.array_split(cp.matmul(input_decoder, Vd), num_heads, axis=2)), 0, 1)\n",
    "\n",
    "    QKscaled_decoder = cp.matmul(Q_D, cp.transpose(K_D, (0, 1, 3, 2))) / cp.sqrt(dv)\n",
    "    # Step 1: Create a causal mask of shape (1, 1, 9, 9) to broadcast across heads and batch\n",
    "    mask_size = input_decoder.shape[1]  # words_per_phrase\n",
    "    # print(mask_size)\n",
    "    mask = cp.tril(cp.ones((mask_size, mask_size)))  # (9, 9) lower triangular matrix\n",
    "    mask[mask == 0]=-cp.inf  # Set future tokens to -inf\n",
    "    mask[mask == 1]=0  # Set allowed tokens to 0\n",
    "    mask = mask.reshape(1, 1, mask_size, mask_size)\n",
    "\n",
    "    # Step 2: Apply mask to QKscaled_decoder (it will broadcast across batch and heads)\n",
    "    QKscaled_decoder = QKscaled_decoder + mask\n",
    "\n",
    "    Attention_weights_masked = softmax(QKscaled_decoder)\n",
    "\n",
    "    A_mask = cp.matmul(Attention_weights_masked, V_D)\n",
    "    # print(\"A_mask.shape non concat: \",A_mask.shape)\n",
    "\n",
    "    # A_mask=cp.swapaxes(cp.concatenate(cp.swapaxes(A_mask,0,2),axis=-1),0,1)\n",
    "    A_mask = cp.concatenate(cp.swapaxes(A_mask, 0, 1), axis=-1)\n",
    "\n",
    "    # print(\"A_mask.shape concat: \",A_mask.shape)\n",
    "    # print(\"inputs_d.shape: \",input_decoder.shape)\n",
    "    # print(\"Dt1.shape: \",Dt1.shape)\n",
    "    return A_mask, Attention_weights_masked, Q_D, K_D, V_D\n",
    "\n",
    "\n",
    "def decoder_first_residual_and_norm(A_mask, input_decoder):\n",
    "    Xd = input_decoder + A_mask\n",
    "    Dt1, mu_d, var_d, N_d = layer_norm(Xd)\n",
    "    return Xd, Dt1, mu_d, var_d, N_d\n",
    "\n",
    "\n",
    "def cross_attention_decoder(Dt1):\n",
    "    global Qc\n",
    "    Q_C = cp.swapaxes(cp.array(cp.array_split(cp.matmul(Dt1, Qc), num_heads, axis=2)), 0, 1)\n",
    "    # print(\"Q_C.shape: \",Q_C.shape)\n",
    "    return Q_C\n",
    "\n",
    "\n",
    "def cross_attention(Q_C, K_C, V_C, Dt1):\n",
    "    global dv\n",
    "    # print(\"Q_C.shape\",Q_C.shape)\n",
    "    # print(\"K_C.shape\",K_C.shape)\n",
    "    # print(\"V_C.shape\",V_C.shape)\n",
    "    QKscaled_cross_attention = cp.matmul(Q_C, cp.transpose(K_C, (0, 1, 3, 2))) / cp.sqrt(dv)\n",
    "    Attention_weights_cross = softmax(QKscaled_cross_attention)\n",
    "    Acr = cp.matmul(Attention_weights_cross, V_C)\n",
    "    # print(\"Acr.shape non concat\",Acr.shape)\n",
    "    Acr = cp.concatenate(cp.swapaxes(Acr, 0, 1), axis=-1)\n",
    "    # print(\"Acr.shape concat\",Acr.shape)\n",
    "\n",
    "    return Acr, Attention_weights_cross\n",
    "\n",
    "\n",
    "def cross_attention_residual_and_norm(Acr, Dt1):\n",
    "    Res = Acr + Dt1\n",
    "    Dt2, mu_res, var_res, N_res = layer_norm(Res)\n",
    "    return Dt2, mu_res, var_res, N_res, Res\n",
    "\n",
    "\n",
    "def fully_connected_layers_decoder(Dt2):\n",
    "    global Wfl1d, bfl1d, Wfl2d, bfl2d, num_phrases\n",
    "\n",
    "    Xd1 = cp.matmul(Dt2, Wfl1d) + bfl1d\n",
    "    num_zeros = cp.sum(Xd1 == 0)\n",
    "    #print(f\"Number of zeros in the input: {num_zeros}\")\n",
    "    FLd1 = leaky_relu(Xd1)\n",
    "    num_zeros = cp.sum(FLd1 == 0)\n",
    "       \n",
    "    #print(f\"Number of zeros in the output: {num_zeros}\")\n",
    "\n",
    "    FLd1=dropout(FLd1, dropout_rate=0.1, training=True)\n",
    "\n",
    "    FLd2 = cp.matmul(FLd1, Wfl2d) + bfl2d\n",
    "    # print(\"FLd2.shape\",FLd2.shape)\n",
    "\n",
    "    Xd2 = FLd2 + Dt2\n",
    "\n",
    "\n",
    "    Dout, mu_d2, var_d2, N_d2 = layer_norm(Xd2)\n",
    "\n",
    "    print(\"Dout.shape\",Dout.shape)\n",
    "    #Dout = Dout.reshape(Dout.shape[0], Dout.shape[1] * Dout.shape[2])\n",
    "    #Dout = Dout[.reshape(Dout.shape[0], Dout.shape[1] * Dout.shape[2])]\n",
    "    # print(\"Dout.shape\",Dout.shape)\n",
    "    return Dout, mu_d2, var_d2, N_d2, Xd2, Xd1, FLd1\n",
    "\n",
    "\n",
    "def output_layer(Dout):\n",
    "    global Wo, bo\n",
    "\n",
    "    Zout = cp.matmul(Dout, Wo) + bo\n",
    "\n",
    "    SigmaZout = softmax(Zout)\n",
    "    # print(\"SigmaZout.shape\",SigmaZout.shape)\n",
    "\n",
    "    return SigmaZout\n",
    "\n",
    "\n",
    "def loss_calculation(SigmaZout, target):\n",
    "    # print(\"target.shape\",cp.array(target).shape)\n",
    "    Loss = cross_entropy_loss(SigmaZout, target)\n",
    "    # print(\"Loss:\",Loss)\n",
    "    return Loss\n",
    "\n",
    "def dropout(X, dropout_rate=0.1, training=True): \n",
    "\n",
    "    #print(\"X.shape\",X.shape)\n",
    "\n",
    "    if training:\n",
    "        # Create a mask\n",
    "        mask = cp.random.rand(*X.shape) > dropout_rate\n",
    "\n",
    "        #print(X.shape,X.size)\n",
    "        #print(\"mask.shape\",mask.shape)\n",
    "        result = X * mask / (1 - dropout_rate)  # Scale output\n",
    "        \n",
    "        # Count the number of zeros\n",
    "        num_zeros = cp.sum(mask == 0)\n",
    "        num_zeros1 = cp.sum(X == 0)\n",
    "        \n",
    "        # Calculate expected number of zeros\n",
    "        expected_zeros = int(dropout_rate * mask.size)  # Total elements\n",
    "        #print(f\"Number of zeros in the output: {num_zeros}, Expected: {expected_zeros},{num_zeros1}\")\n",
    "\n",
    "\n",
    "        # # Debugging information\n",
    "        # print(\"result.shape\",result.shape)\n",
    "         \n",
    "\n",
    "        return result\n",
    "    else:\n",
    "        return X  # Return as-is during evaluation\n",
    "\n",
    "##################################################################BACKPROPAGATION\n",
    "def derivate_dout(SigmaZout, target, Dout):\n",
    "\n",
    "    global bo,Wo, embedding_size, batch_size, words_per_phrase\n",
    "    dLoss_dZout = SigmaZout - target\n",
    "    print(\"dLoss_dZout.shape\",dLoss_dZout.shape)\n",
    "    dLoss_W0 = cp.transpose(dLoss_dZout, (1, 0)) @ Dout\n",
    "    print(\"dLoss_W0.shape\",dLoss_W0.shape,\"W0.shape\",Wo.shape)\n",
    "    dLoss_b0 = cp.sum(dLoss_dZout, axis=0)\n",
    "    print(\"dLoss_b0.shape\",dLoss_b0.shape,\"b0.shape\",bo.shape)\n",
    "    dLoss_Dout = dLoss_dZout @ Wo.T\n",
    "    #dLoss_Dout = dLoss_Dout.reshape(batch_size, words_per_phrase, embedding_size)\n",
    "    print(\"dLoss_Dout.shape\",dLoss_Dout.shape)\n",
    "    return dLoss_dZout, dLoss_Dout, dLoss_W0, dLoss_b0\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "def derivate_fully_connected_layers_decoder(dLoss_Dout, Dt2, Xd2, var_d2, mu_d2, N_d2, FLd1, Xd1):\n",
    "    global Wfl1d, Wfl2d\n",
    "\n",
    "    dLoss_FLd2 = dLoss_Dout * diff_norm(Xd2, var_d2, mu_d2, N_d2)\n",
    "    # print(\"dLoss_FLd2.shape\",dLoss_FLd2.shape)\n",
    "    dLoss_Dt2_a = dLoss_FLd2\n",
    "    # print(\"dLoss_Dt2_a.shape\",dLoss_Dt2_a.shape)\n",
    "    # print(\"Dt2.shape\",Dt2.shape)\n",
    "    dLoss_FLd1 = dLoss_FLd2 @ cp.transpose(Wfl2d, (1, 0))\n",
    "    # print(\"dLoss_FLd1.shape\",dLoss_FLd1.shape)\n",
    "    # print(\"FLd1.shape\",FLd1.shape)\n",
    "    dLoss_Wfl2d = cp.sum(cp.transpose(dLoss_FLd2, (0, 2, 1)) @ FLd1, axis=0).T\n",
    "    # print(\"dLoss_Wfl2d.shape\",dLoss_Wfl2d.shape) # do the mean here over each phrase\n",
    "    # print(\"Wfl2d.shape\",Wfl2d.shape)\n",
    "    dLoss_bfl2d = cp.sum(cp.sum(dLoss_FLd2, axis=0), axis=0)\n",
    "    # print(\"dLoss_bfl2d.shape\",dLoss_bfl2d.shape) # do the mean here over each phrase\n",
    "    # print(\"bfl2d.shape\",bfl2d.shape)\n",
    "\n",
    "    DLoss_Dt2_b = relu_backward(dLoss_FLd1, Xd1) @ cp.transpose(Wfl1d, (1, 0))\n",
    "    DLoss_Dt2 = dLoss_Dt2_a + DLoss_Dt2_b\n",
    "    # print(\"DLoss_Dt2.shape\",DLoss_Dt2.shape) # do the mean here over each phrase\n",
    "    # print(\"Dt2.shape\",Dt2.shape)\n",
    "\n",
    "    dLoss_Wfl1d = cp.sum(cp.transpose(relu_backward(dLoss_FLd1, Xd1), (0, 2, 1)) @ Dt2, axis=0).T\n",
    "    # print(\"dLoss_Wfl1d.shape\",dLoss_Wfl1d.shape) # do the mean here over each phrase\n",
    "    # print(\"Wfl1d.shape\",Wfl1d.shape)\n",
    "\n",
    "    dLoss_bfl1d = cp.sum(cp.sum(relu_backward(dLoss_FLd1, Xd1), axis=0), axis=0)\n",
    "\n",
    "    return dLoss_Wfl2d, dLoss_bfl2d, dLoss_Wfl1d, dLoss_bfl1d, DLoss_Dt2\n",
    "\n",
    "\n",
    "def derivative_cross_attention(dLoss_Dt2, Res, var_res, mu_res, N_res, Attention_weights_cross, K_C, V_C, Q_C, Ecout,\n",
    "                               Dt1):\n",
    "    # print(\"dLoss_bfl1d.shape\",dLoss_bfl1d.shape) # do the mean here over each phrase\n",
    "    # print(\"bfl1d.shape\",bfl1d.shape)\n",
    "    global dk\n",
    "\n",
    "    dLoss_Acr = dLoss_Dt2 * diff_norm(Res, var_res, mu_res, N_res)\n",
    "    # print(\"dLoss_Acr.shape\",dLoss_Acr.shape) # do the mean here over each phrase\n",
    "    # print(\"Acr.shape\",Acr.shape)\n",
    "    dLoss_Dt1_a = dLoss_Dt2 * diff_norm(Res, var_res, mu_res, N_res)\n",
    "    # print(\"dLoss_Dt1.shape-------\",dLoss_Dt1_a.shape) # do the mean here over each phrase\n",
    "    # print(\"Dt1.shape\",Dt1.shape)\n",
    "\n",
    "    dLoss_Qc = diffQKV(dLoss_Acr, Attention_weights_cross, K_C, V_C, Dt1, dk)\n",
    "    # print(\"dLoss_dQc.shape\",dLoss_Qc.shape) # do the mean here over each phrase\n",
    "    # print(\"Qc.shape\",Qc.shape)\n",
    "    dLoss_Kc = diffQKV(dLoss_Acr, Attention_weights_cross, Q_C, V_C, Ecout, dk, matrix=\"k\")\n",
    "    # print(\"dLoss_dKc.shape\",dLoss_Kc.shape) # do the mean here over each phrase\n",
    "    # print(\"Kc.shape\",Kc.shape)\n",
    "    dLoss_Vc = cp.sum(cp.sum(cp.transpose(cp.expand_dims(dLoss_Acr, axis=1), (0, 1, 3, 2)) @ (\n",
    "                Attention_weights_cross @ cp.expand_dims(Ecout, axis=1)), axis=1), axis=0)\n",
    "    # print(\"dLoss_dVc.shape\",dLoss_Vc.shape) # do the mean here over each phrase\n",
    "    # print(\"Vc.shape\",Vc.shape)\n",
    "    return dLoss_Qc, dLoss_Kc, dLoss_Vc, Attention_weights_cross, dLoss_Dt1_a, dLoss_Acr\n",
    "\n",
    "\n",
    "def derivative_attention_decoder(dLoss_Acr, Attention_weights_cross, dLoss_Dt1_a, Attention_weights_masked, Q_D, V_D,\n",
    "                                 K_D, K_C, V_C, Xd, var_d, mu_d, N_d, input_d):\n",
    "    global Qc, dk\n",
    "\n",
    "    dAttention_weights_cross = Attention_weights_cross * (1 - Attention_weights_cross)\n",
    "    # print(\"dAttention_weights_cross.shape\",dAttention_weights_cross.shape)\n",
    "    # V1=redimension(dAttention_weights_cross@K_C/cp.sqrt(dk))\n",
    "    # print(\"K_C.shape\",K_C.shape)\n",
    "    # V2=redimension(V_C)\n",
    "    # print(\"V_C.shape\",V_C.shape)\n",
    "    # print(\"Qc.shape\",Qc.shape)\n",
    "\n",
    "    # print(\"dLoss_Dt1_b.shape\",dLoss_Dt1_a.shape)\n",
    "    # V3=V1*V2@Qc\n",
    "    # dLoss_Dt1_b=dLoss_Acr*V3\n",
    "    # V3=V1@cp.transpose(V2@Qc,(0,2,1))\n",
    "    # dLoss_Dt1_b=cp.transpose(dLoss_Acr,(0,2,1))@V3\n",
    "\n",
    "    V1 = dAttention_weights_cross\n",
    "    V2 = K_C * V_C / cp.sqrt(dk)\n",
    "    V3 = redimension(V1 @ V2) @ Qc\n",
    "\n",
    "    # print(\"V3.shape\",dLoss_Dt1_a.shape)\n",
    "    # print(\"dLoss_Acr.shape\",dLoss_Acr.shape)\n",
    "    dLoss_Dt1_b = dLoss_Acr * V3\n",
    "\n",
    "    dLoss_Dt1 = dLoss_Dt1_a + dLoss_Dt1_b\n",
    "\n",
    "    dLoss_Amask = dLoss_Dt1 * diff_norm(Xd, var_d, mu_d, N_d)\n",
    "    # print(\"dLoss_DAmask.shape\",dLoss_Amask.shape)\n",
    "    dLoss_inputd_a = dLoss_Amask\n",
    "    # print(\"dLoss_Dinputd_a.shape\",dLoss_inputd_a.shape)\n",
    "    dLoss_Kd = diffQKV(dLoss_Amask, Attention_weights_masked, Q_D, V_D, input_d, dk)\n",
    "    # print(\"dLoss_Kd.shape\",dLoss_Kd.shape)\n",
    "    dLoss_Qd = diffQKV(dLoss_Amask, Attention_weights_masked, K_D, V_D, input_d, dk)\n",
    "    # print(\"dLoss_Qd.shape\",dLoss_Qd.shape)\n",
    "    dLoss_Vd = cp.sum(cp.sum(cp.transpose(cp.expand_dims(dLoss_Amask, axis=1), (0, 1, 3, 2)) @ (\n",
    "                Attention_weights_masked @ cp.expand_dims(input_d, axis=1)), axis=1), axis=0)\n",
    "    return dLoss_Kd, dLoss_Qd, dLoss_Vd, dLoss_inputd_a, dLoss_Amask\n",
    "\n",
    "\n",
    "def derivative_input_decoder(dLoss_Amask, Attention_weights_masked, K_D, V_D, Q_D,Qd, Kd, Vd, dk, dLoss_inputd_a, input_d):\n",
    "    \n",
    "\n",
    "    dLoss_V_D = cp.transpose(\n",
    "        cp.sum(cp.transpose(cp.expand_dims(dLoss_Amask, axis=1), (0, 1, 3, 2)) @ Attention_weights_masked, axis=1),\n",
    "        (0, 2, 1))\n",
    "    \n",
    "    dLoss_inputd_v = dLoss_V_D @ Vd\n",
    "\n",
    "    # print(\"dLoss_inputd_v.shape\",dLoss_inputd_v.shape) # do the mean here over each phrase\n",
    "    # print(\"input_d.shape\",input_d.shape)\n",
    "\n",
    "    dAttention_weights_masked = Attention_weights_masked * (1 - Attention_weights_masked)\n",
    "    V1 = redimension(dAttention_weights_masked @ K_D / cp.sqrt(dk))\n",
    "    V2 = redimension(V_D)\n",
    "    V3 = V1 * V2\n",
    "    dLoss_Q_D = dLoss_Amask * V3\n",
    "\n",
    "    dLoss_inputd_q = dLoss_Q_D @ Qd\n",
    "    # print(\"dLoss_inputd_q.shape\",dLoss_inputd_q.shape)\n",
    "\n",
    "    V1 = redimension(dAttention_weights_masked @ Q_D / cp.sqrt(dk))\n",
    "    V2 = redimension(V_D)\n",
    "    V3 = V1 * V2\n",
    "    dLoss_K_D = dLoss_Amask * V3\n",
    "    dLoss_inputd_k = dLoss_K_D @ Kd\n",
    "    # print(\"dLoss_inputd_k.shape\",dLoss_inputd_k.shape)\n",
    "    dLoss_inputd = dLoss_inputd_a + dLoss_inputd_k + dLoss_inputd_q + dLoss_inputd_v\n",
    "\n",
    "    dLoss_dWemb_decoder = dLoss_inputd * input_d\n",
    "\n",
    "    return dLoss_inputd, dLoss_dWemb_decoder\n",
    "\n",
    "\n",
    "def derivative_Ecout(Attention_weights_cross, dLoss_Acr, Q_C, V_C, Kc, Vc, num_heads):\n",
    "     \n",
    "\n",
    "    dAttention_weights_cross = Attention_weights_cross * (1 - Attention_weights_cross)\n",
    "    # dLoss_Acr=cp.expand_dims(dLoss_Acr,axis=1)\n",
    "    # print(\"--------------\")\n",
    "    # print(\"dLoss_Acr.shape\",dLoss_Acr.shape)\n",
    "    # print(\"dAttention_weights_cross.shape\",dAttention_weights_cross.shape)\n",
    "    # print(\"Q_C.shape\",Q_C.shape)\n",
    "    # print(\"V_C.shape\",V_C.shape)\n",
    "    # print(\"Kc.shape\",Kc.shape)\n",
    "    # V1=redimension(dAttention_weights_cross@Q_C/cp.sqrt(dk))\n",
    "\n",
    "    # V2=redimension(V_C)\n",
    "\n",
    "    # V3=V1*V2\n",
    "\n",
    "    # dLoss_K_C=dLoss_Acr*V3\n",
    "    dLoss_Acr = cp.swapaxes(cp.array(cp.array_split(dLoss_Acr, num_heads, axis=2)), 0, 1)\n",
    "    V1 = Q_C @ cp.transpose(V_C, (0, 1, 3, 2)) / cp.sqrt(dk)\n",
    "    # print(\"V1.shape\",V1.shape)\n",
    "    V2 = dAttention_weights_cross * V1\n",
    "    # print(\"V2.shape\",V2.shape,\"dLoss_Acr.shape\",dLoss_Acr.shape)\n",
    "    dLoss_K_C = cp.transpose(cp.transpose(dLoss_Acr, (0, 1, 3, 2)) @ V2, (0, 1, 3, 2))\n",
    "    # print(\"dLoss_K_C.shape\",dLoss_K_C.shape)\n",
    "\n",
    "    dLoss_Ecout_k = redimension(dLoss_K_C) @ Kc\n",
    "    # print(\"dLoss_Ecout_k.shape\",dLoss_Ecout_k.shape)\n",
    "    # dLoss_K_C=dLoss_Acr@cp.transpose(redimension(V2),(0,2,1))\n",
    "    # print(\"dLoss_K_C.shape\",dLoss_K_C.shape)\n",
    "\n",
    "    # dLoss_Ecout_k=dLoss_K_C@Kc\n",
    "    # print(\"dLoss_Ecout_k.shape\",dLoss_Ecout_k.shape)\n",
    "    dLoss_V_C = cp.transpose(cp.transpose(dLoss_Acr, (0, 1, 3, 2)) @ Attention_weights_cross, (0, 1, 3, 2))\n",
    "    # print(\"dLoss_V_C.shape\",dLoss_V_C.shape)\n",
    "    # dLoss_V_C=cp.transpose(cp.sum(cp.transpose(cp.expand_dims(dLoss_Acr, axis=1),(0,1,3,2))@Attention_weights_cross,axis=1),(0,2,1))\n",
    "    # dLoss_V_C.shape\n",
    "    dLoss_Ecout_v = redimension(dLoss_V_C) @ Vc\n",
    "    # print(\"dLoss_Ecout_v.shape\",dLoss_Ecout_v.shape)\n",
    "    # print(\"dLoss_Ecout_v.shape\",dLoss_Ecout_v.shape) # do the mean here over each phrase\n",
    "    dLoss_Ecout = dLoss_Ecout_k + dLoss_Ecout_v\n",
    "    return dLoss_Ecout\n",
    "\n",
    "\n",
    "def derivate_fully_connected_layers_encoder(dLoss_Ecout, Ect1, Xe2, var_e2, mu_e2, N_e2, FLe1, Xe1,Wfl2e, Wfl1e):\n",
    "    \n",
    "    dLoss_dFLe2 = dLoss_Ecout * diff_norm(Xe2, var_e2, mu_e2, N_e2)\n",
    "    dLoss_Ect1_a = dLoss_dFLe2\n",
    "    # print(Wfl2e.shape)\n",
    "    dLoss_dFLe1 = dLoss_dFLe2 @ cp.transpose(Wfl2e, (1, 0))\n",
    "    dLoss_dWfl2e = cp.transpose(dLoss_dFLe2, (0, 2, 1)) @ FLe1\n",
    "    # print(dLoss_dWfl2e)\n",
    "    dLoss_dbfl2e = cp.sum(dLoss_dFLe2, axis=1)\n",
    "\n",
    "    dLoss_Ect1_b = relu_backward(dLoss_dFLe1, Xe1) @ cp.transpose(Wfl1e, (1, 0))\n",
    "\n",
    "    dLoss_Ect1 = dLoss_Ect1_b + dLoss_Ect1_a\n",
    "\n",
    "    dLoss_Wfl1e = cp.transpose(relu_backward(dLoss_dFLe1, Xe1), (0, 2, 1)) @ Ect1\n",
    "\n",
    "    dLoss_bfl1e = cp.transpose(relu_backward(dLoss_dFLe1, Xe1), (0, 2, 1))\n",
    "\n",
    "    return dLoss_dWfl2e, dLoss_dbfl2e, dLoss_Wfl1e, dLoss_bfl1e, dLoss_Ect1\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "def derivative_attention_encoder(dLoss_Ect1, Xe, var_e, mu_e, Ne, Attention_weights_e, K_E, V_E, Q_E, inputs_e,dk):\n",
    "     \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"error\", RuntimeWarning)\n",
    "        try:\n",
    "            dLoss_Ae = dLoss_Ect1 * diff_norm(Xe, var_e, mu_e, Ne)\n",
    "\n",
    "            dLoss_inpute_a = dLoss_Ae\n",
    "\n",
    "            dLoss_dQe = diffQKV(dLoss_Ae, Attention_weights_e, K_E, V_E, inputs_e, dk)\n",
    "            # print(\"dLoss_dQe.shape\",dLoss_dQe.shape) # do the mean here over each phrase\n",
    "            # print(\"Qe.shape\",Qe.shape)\n",
    "            dLoss_dKe = diffQKV(dLoss_Ae, Attention_weights_e, Q_E, V_E, inputs_e, dk)\n",
    "            # print(\"dLoss_dKe.shape\",dLoss_dKe.shape) # do the mean here over each phrase\n",
    "            # print(\"Ke.shape\",Ke.shape)\n",
    "\n",
    "            dLoss_dVe = cp.sum(cp.sum(cp.transpose(cp.expand_dims(dLoss_Ae, axis=1), (0, 1, 3, 2)) @ (\n",
    "                        Attention_weights_e @ cp.expand_dims(inputs_e, axis=1)), axis=1), axis=0)\n",
    "            # print(\"dLoss_dVe.shape\",dLoss_dVe.shape) # do the mean here over each phrase\n",
    "            return dLoss_dQe, dLoss_dKe, dLoss_dVe, dLoss_inpute_a, dLoss_Ae\n",
    "\n",
    "        except RuntimeWarning as rw:\n",
    "            # Check for NaN or inf values in inputs and matrices\n",
    "            # print(\"dLoss_Ae check \", dLoss_Ae)\n",
    "            # print(\"Attention_weights_e  \", Attention_weights_e)\n",
    "            # print(\"inputs_e  \", inputs_e)\n",
    "            print(f\"Caught a RuntimeWarning: {rw}\")\n",
    "            return None  # Return None if a warning occurs\n",
    "\n",
    "        except Exception as e:\n",
    "            # Additional checks in case of other exceptions\n",
    "            # print(\"inputs_e check \", cp.isnan(inputs_e).any(), cp.isinf(inputs_e).any())\n",
    "            # print(\"Attention_weights_e check \", cp.isnan(Attention_weights_e).any(), cp.isinf(Attention_weights_e).any())\n",
    "            # print(\"dLoss_Ae check \", cp.isnan(dLoss_Ae).any(), cp.isinf(dLoss_Ae).any())\n",
    "            # print(f\"Caught an error: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "def derivative_input_encoder(dLoss_Ae, Attention_weights_e, K_E, V_E, Q_E, dLoss_inpute_a, inputs_e,Ve, Qe, Ke, dk):\n",
    "    \n",
    "\n",
    "    dLoss_V_E = cp.transpose(\n",
    "        cp.mean(cp.transpose(cp.expand_dims(dLoss_Ae, axis=1), (0, 1, 3, 2)) @ Attention_weights_e, axis=1), (0, 2, 1))\n",
    "    dLoss_inpute_v = dLoss_V_E @ Ve\n",
    "\n",
    "    dAttention_weights_e = Attention_weights_e * (1 - Attention_weights_e)\n",
    "    V1 = redimension(dAttention_weights_e @ K_E / cp.sqrt(dk))\n",
    "    V2 = redimension(V_E)\n",
    "    V3 = V1 * V2\n",
    "    dLoss_Q_E = dLoss_Ae * V3\n",
    "    dLoss_inpute_q = dLoss_Q_E @ Qe\n",
    "    # print(\"dLoss_inpute_q.shape\",dLoss_inpute_q.shape)\n",
    "\n",
    "    V1 = redimension(dAttention_weights_e @ Q_E / cp.sqrt(dk))\n",
    "    V2 = redimension(V_E)\n",
    "    V3 = V1 * V2\n",
    "    dLoss_K_E = dLoss_Ae * V3\n",
    "    dLoss_inpute_k = dLoss_K_E @ Ke\n",
    "    # print(\"dLoss_inpute_k.shape\",dLoss_inpute_k.shape)\n",
    "    dLoss_inpute = dLoss_inpute_a + dLoss_inpute_k + dLoss_inpute_q + dLoss_inpute_v\n",
    "    dLoss_dWemb_encoder = dLoss_inpute * inputs_e\n",
    "    return dLoss_inpute, dLoss_dWemb_encoder\n",
    "# @log_time\n",
    "def get_one_hot(word, vocabulary_decoder):\n",
    "    # print(word)\n",
    "    vocab_size = len(vocabulary_decoder)\n",
    "    one_hot_vector = cp.zeros(vocab_size)\n",
    "    one_hot_vector[vocabulary_decoder[word][1]] = 1\n",
    "    # print(vocabulary_decoder[word][1])\n",
    "    # print(np.where(one_hot_vector== 1))\n",
    "    # print(cp.sum(one_hot_vector))\n",
    "    return one_hot_vector\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "def print_vocabs(ans, vocabulary,yy,step,target,counter_correct):\n",
    "     \n",
    "    print(\"----DECODER-----\")\n",
    "    #print_matrix(yy)\n",
    "    yy=[re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', xx) for xx in yy] \n",
    "     \n",
    "    for idx, values in enumerate(ans):\n",
    "        max_index = cp.argmax(values)\n",
    "        \n",
    "        # Step 2: Find the word in the vocabulary with the corresponding position\n",
    "        matched_word = None\n",
    "        for word, (_, position) in vocabulary.items():\n",
    "            if position == max_index:\n",
    "                matched_word = word\n",
    "                # if np.where(target[idx]== 1)[0][0]==position:\n",
    "                #     print(\"Matched\")\n",
    "                #     ct+=1\n",
    "                break\n",
    "        print(f\"{idx + 1} base: {' '.join(yy[idx][0:step+1])} -> {matched_word}\")\n",
    "    #print()\n",
    " \n",
    "\n",
    "\n",
    "def get_word_from_index(index,vocabulary):\n",
    "    for word, (_, position) in vocabulary.items():\n",
    "        if position == index:\n",
    "            return word\n",
    "        \n",
    "def get_word_from_vector(word_vector, vocabulary):\n",
    "    word_vector_np = word_vector.get()  # Convert word_vector to NumPy\n",
    "    for word, (vector, _) in vocabulary.items(): \n",
    "        if (vector.get() == word_vector_np).all():  # Convert vector to NumPy and compare\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "def print_accuracy(ans, vocabulary,y_batch,step,target,counter_correct):\n",
    "     \n",
    "    print(\"----DECODER-----\")\n",
    "    print(step)\n",
    "    counter=0\n",
    "    accuracy_phrase=0\n",
    "    accuracy_phrases=0\n",
    "    for i,phrase in enumerate(ans):\n",
    "        counter=0\n",
    "        for j,word in enumerate(phrase):\n",
    "            max_index = cp.argmax(word)\n",
    "            predicted_word=get_word_from_index(max_index,vocabulary)\n",
    "            target_index=cp.argmax(target[i][j])\n",
    "            target_word=get_word_from_index(target_index,vocabulary)\n",
    "            #print(\"y_batch[i][j]\",y_batch[i][j])\n",
    "            training_word=get_word_from_vector(y_batch[i][j],vocabulary)\n",
    "            print(f\"{i} training word: {training_word} -> predicted_word: {predicted_word} -> target_word: {target_word} \")     \n",
    "            if target_word==predicted_word and predicted_word!=\"[PAD]\":\n",
    "                counter+=1\n",
    "        accuracy_phrase=counter/len(phrase)\n",
    "        accuracy_phrases+=accuracy_phrase\n",
    "    accuracy_batch=accuracy_phrases/len(ans)\n",
    "    print(accuracy_batch)\n",
    "\n",
    "\n",
    "# def cross_entropy_loss(predictions, target):\n",
    "#     # Cross-entropy loss for a batch of predictions and targets\n",
    "#     batch_loss = -cp.sum(target * cp.log(predictions + 1e-9), axis=1)\n",
    "#     print(\"batch_loss\",batch_loss.shape)\n",
    "#     print(\"predictions\",predictions.shape)\n",
    "#     print(\"target\",target.shape)\n",
    "#     return cp.mean(batch_loss)\n",
    "\n",
    "def cross_entropy_loss(predictions, target, vocabulary):\n",
    "    # Identify the index of the padding token\n",
    "    pad_token_index = vocabulary[\"[PAD]\"][1]\n",
    "    \n",
    "    # Create a mask where `1` indicates a non-padding token, and `0` indicates padding\n",
    "    mask = (target.argmax(axis=2) != pad_token_index).astype(cp.float32)  # Shape: (4, 8)\n",
    "    \n",
    "    # Compute the cross-entropy loss for all positions\n",
    "    batch_loss = -cp.sum(target * cp.log(predictions + 1e-9), axis=2)  # Shape: (4, 8)\n",
    "    \n",
    "    # Apply mask to exclude padding positions from the loss calculation\n",
    "    masked_loss = batch_loss * mask  # Shape: (4, 8)\n",
    "\n",
    "    # Compute mean only over non-padding tokens\n",
    "    masked_loss_mean = cp.sum(masked_loss) / cp.sum(mask)\n",
    "    \n",
    "    return masked_loss_mean\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f54ac5bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"data/bbc-text.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73351bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345 Chandler will pay for his coffee tomorrow .\n",
      "vocabulary size 1060\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "min_v = 8\n",
    "max_v = 8\n",
    "\n",
    "def load_x_y_train_plain():\n",
    "    with open('corpus/train.json', 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            dataset = json.load(f)  # Load the JSON data\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    # #Loop through the list and process each dialogue and summary\n",
    "    for data in dataset:\n",
    "        dialogue = data['dialogue']  # Split dialogue into a list of lines\n",
    "        summary = data['summary']\n",
    "\n",
    "        X_train.append(dialogue)\n",
    "        y_train.append(summary)\n",
    "    return X_train, y_train\n",
    "\n",
    "\n",
    "def split_x_y_train(X_train, y_train):\n",
    "    X_train = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', x) for x in X_train]\n",
    "    y_train = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', y) for y in y_train]\n",
    "    return X_train, y_train\n",
    "\n",
    "\n",
    "# with open('data/vocabolary_full.pkl', 'rb') as f:\n",
    "#     vocabulary=pickle.load(f)\n",
    "def filter_train_data(X_train, y_train, to_eliminate):\n",
    "    filtered_X_train = []\n",
    "    filtered_y_train = []\n",
    "\n",
    "    for x, y in zip(X_train, y_train):\n",
    "        if not any(to_eliminate_str in x for to_eliminate_str in to_eliminate):\n",
    "            filtered_X_train.append(x)\n",
    "            filtered_y_train.append(y)\n",
    "\n",
    "    return filtered_X_train, filtered_y_train\n",
    "\n",
    "\n",
    "def create_complete_vocabulary(X_train, y_train):\n",
    "    nlp_model = spacy.load('en_core_web_lg')\n",
    "    nlp_model.disable_pipes([\"parser\", \"ner\"])\n",
    "    complete_text_target = ' '.join(y_train)\n",
    "    complete_text_origin = ' '.join(X_train)\n",
    "    complete_text = complete_text_target + \" [START] [PAD] [END] \" + complete_text_origin\n",
    "\n",
    "    vocabulary = create_vocabulary(complete_text, \"vocabolary_full\", nlp_model)\n",
    "    print(\"vocabulary size\", len(vocabulary))\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "X_train, y_train = load_x_y_train_plain()\n",
    "to_eliminate = [\n",
    "    \"[I hope I'm not coming off as rude - If I am, I'm sorry. I just thought it would be beneficial for the both of us...]\",\n",
    "    \"[pulls back the curtain and checks out the window]\",\n",
    "    \"[hopefully, masses of]\"]\n",
    "X_train, y_train = filter_train_data(X_train, y_train, to_eliminate)\n",
    "\n",
    "\n",
    "sample = [i for i in range(0,len(y_train))]\n",
    "\n",
    "\n",
    "X_train = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', x) for x in X_train]\n",
    "y_train = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', y) for y in y_train]\n",
    "\n",
    "\n",
    "X_train = [X_train[i] for i in sample if len(y_train[i]) <= max_v and len(y_train[i]) >= min_v]\n",
    "y_train = [y_train[i] for i in sample if len(y_train[i]) <= max_v and len(y_train[i]) >= min_v]\n",
    "\n",
    "\n",
    "# Calculate lengths of the tokenized phrases\n",
    "\n",
    "\n",
    "def plot_lenghts(X_train,y_train):\n",
    "    X_lengths = [len(x) for x in X_train]\n",
    "    y_lengths = [len(y) for y in y_train]\n",
    "    # Plot histograms\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Histogram for X_train lengths\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(X_lengths, bins=20, kde=False)\n",
    "    plt.title('Histogram of X_train Phrase Lengths')\n",
    "    plt.xlabel('Length of Phrases')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Histogram for y_train lengths\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(y_lengths, bins=20, kde=False)\n",
    "    plt.title('Histogram of y_train Phrase Lengths')\n",
    "    plt.xlabel('Length of Phrases')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Display the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "X_train=[i[::-1] for i in y_train]\n",
    "plot_lenghts(X_train,y_train)\n",
    " \n",
    "\n",
    "X_train=[\" \".join(x) for x in X_train]\n",
    "y_train=[\" \".join(y) for y in y_train]\n",
    "\n",
    "print(len(y_train),y_train[0])\n",
    "\n",
    "vocabulary=create_complete_vocabulary(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ff63b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('. tomorrow coffee his for pay will Chandler',\n",
       " 'Chandler will pay for his coffee tomorrow .')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0],y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "140f64f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate=0.001\n",
    "\n",
    "# vocab_size=len(vocabulary)\n",
    "# batch_size=1\n",
    "\n",
    "\n",
    "# #batch_size = len(X_train)\n",
    "# words_per_phrase = num_phrases= max_v\n",
    "# dk = dv = embedding_size = 300 # constrain of transformer all embedding size both input embedding and attention embedding are the same encoder\n",
    "# num_heads=10\n",
    "# Qe = cp.random.rand(embedding_size, embedding_size) / cp.sqrt(embedding_size)\n",
    "# Ke = cp.random.rand(embedding_size, embedding_size) / cp.sqrt(embedding_size)\n",
    "# Ve = cp.random.rand(embedding_size, embedding_size) / cp.sqrt(embedding_size)\n",
    "# Qc = cp.random.rand(embedding_size, embedding_size) / cp.sqrt(embedding_size)\n",
    "# Kc = cp.random.rand(embedding_size, embedding_size) / cp.sqrt(embedding_size)\n",
    "# Vc = cp.random.rand(embedding_size, embedding_size) / cp.sqrt(embedding_size)\n",
    "# Qd = cp.random.rand(embedding_size, embedding_size) / cp.sqrt(embedding_size)\n",
    "# Kd = cp.random.rand(embedding_size, embedding_size) / cp.sqrt(embedding_size)\n",
    "# Vd = cp.random.rand(embedding_size, embedding_size) / cp.sqrt(embedding_size)\n",
    "\n",
    "# fl1_size=1024*2\n",
    "# Wfl1e=cp.random.rand(embedding_size, fl1_size)\n",
    "# bfl1e=cp.random.rand(fl1_size)\n",
    "\n",
    "# Wfl2e=cp.random.rand(fl1_size, dv)\n",
    "# bfl2e=cp.random.rand(dv)\n",
    "\n",
    "\n",
    "# Wfl1d=cp.random.rand(embedding_size, fl1_size)\n",
    "# bfl1d=cp.random.rand(fl1_size)\n",
    "\n",
    "# Wfl2d=cp.random.rand(fl1_size, dv)\n",
    "# bfl2d=cp.random.rand(dv)\n",
    "\n",
    "# Wo=cp.random.rand(words_per_phrase*embedding_size,vocab_size)\n",
    "# bo=cp.random.rand(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7cae2b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fl1_size=1024*1 \n",
    "# batch_size=32\n",
    "# vocab_size=len(vocabulary) \n",
    " \n",
    " \n",
    "# words_per_phrase = num_phrases= max_v\n",
    "# dk = dv = embedding_size = 300 # constrain of transformer all embedding size both input embedding and attention embedding are the same encoder\n",
    "# num_heads=10\n",
    "\n",
    "# num_batches_per_epoch = len(X_train) // batch_size\n",
    "# num_epochs=550\n",
    "# learning_rate=0.001\n",
    "\n",
    "# Qe = linear_layer(embedding_size,embedding_size)\n",
    "# Ke = linear_layer(embedding_size,embedding_size)\n",
    "# Ve = linear_layer(embedding_size,embedding_size)\n",
    "# linear_layer_econder_1=linear_layer(embedding_size,fl1_size)\n",
    "# linear_layer_econder_2=linear_layer(fl1_size,embedding_size)\n",
    "\n",
    "# normalization_layer_encoder_1=layer_normalization(epsilon=1e-6)\n",
    "# relu_layer_encoder=ReLu_layer(alpha=0.001)\n",
    "# normalization_layer_encoder_2=layer_normalization(epsilon=1e-6)\n",
    "# multihead_attention_encoder=multihead_attention(num_heads=num_heads,dk=embedding_size,batch_size=batch_size)\n",
    "# multihead_cross_attention=multihead_attention(num_heads=num_heads,dk=embedding_size,batch_size=batch_size)\n",
    "# multihead_attention_decoder=multihead_attention(num_heads=num_heads,dk=embedding_size,batch_size=batch_size)\n",
    "# Qc = linear_layer(embedding_size,embedding_size)\n",
    "# Kc = linear_layer(embedding_size,embedding_size)\n",
    "# Vc = linear_layer(embedding_size,embedding_size)\n",
    "# Qd = linear_layer(embedding_size,embedding_size)\n",
    "# Kd = linear_layer(embedding_size,embedding_size)\n",
    "# Vd = linear_layer(embedding_size,embedding_size)\n",
    "\n",
    "# normalization_layer_decoder_1=layer_normalization(epsilon=1e-6)\n",
    "# normalization_layer_decoder_2=layer_normalization(epsilon=1e-6)\n",
    "# normalization_layer_decoder_3=layer_normalization(epsilon=1e-6)\n",
    "\n",
    "# linear_layer_decoder_1=linear_layer(embedding_size,fl1_size)\n",
    "# linear_layer_decoder_2=linear_layer(fl1_size,embedding_size)\n",
    "# relu_layer_decoder=ReLu_layer(alpha=0.001)\n",
    "\n",
    "# output_linear_layer=linear_layer(embedding_size,vocab_size)\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "22fea315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_categorical_crossentropy(probabilities, labels):\n",
    "    \"\"\"\n",
    "    Compute sparse categorical cross-entropy loss for next token prediction.\n",
    "    \n",
    "    Args:\n",
    "        logits: Numpy array of shape (batch_size, num_classes), raw model outputs.\n",
    "        labels: Numpy array of shape (batch_size,), integer-encoded labels of the next tokens.\n",
    "\n",
    "    Returns:\n",
    "        Scalar: The average cross-entropy loss across the batch.\n",
    "    \"\"\"\n",
    "    # Step 1: Apply softmax to logits to get probabilities\n",
    "     \n",
    "\n",
    "    # Step 2: Extract probabilities for the correct class (target labels)\n",
    "    batch_size = probabilities.shape[0]\n",
    "    correct_class_probs = probabilities[np.arange(batch_size), labels]\n",
    "\n",
    "    # Step 3: Compute the log of probabilities and then the negative log-likelihood\n",
    "    loss = -np.log(correct_class_probs + 1e-8)  # Add small epsilon for numerical stability\n",
    "\n",
    "    # Step 4: Return the average loss across the batch\n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1de163b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate=0.0005\n",
    "# Loss=0\n",
    "# tot_loss_epoch=0\n",
    "# for epoch in range(num_epochs):\n",
    "#     print(\"Loss\",tot_loss_epoch/num_batches_per_epoch)\n",
    "#     tot_loss_epoch=0\n",
    "#     for i in tqdm(range(0,num_batches_per_epoch),desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "#         # try: \n",
    "#         start = i * batch_size\n",
    "#         end = start + batch_size \n",
    "#         X_batch = X_train[start:end]\n",
    "#         y_batch = y_train[start:end] \n",
    "#         inputs_e=create_input_encoder(X_batch,vocabulary,words_per_phrase,embedding_size) \n",
    "#         inputs_d=create_decoder_input(y_batch,embedding_size,words_per_phrase,vocabulary)\n",
    "#         Q_E=Qe.forward_weights_only(inputs_e)\n",
    "#         K_E=Ke.forward_weights_only(inputs_e)\n",
    "#         V_E=Ve.forward_weights_only(inputs_e) \n",
    "#         Q_E,K_E,V_E=multihead_attention_encoder.reshape_heads(Q_E,K_E,V_E)\n",
    "#         Attention_weights_e=multihead_attention_encoder.attention_weights() \n",
    "#         Ae=multihead_attention_encoder.attention() \n",
    "#         Xe=inputs_e+Ae \n",
    "#         Ect1=normalization_layer_encoder_1.forward(Xe) \n",
    "#         Xe1=linear_layer_econder_1.forward(Ect1) \n",
    "#         FLe1=relu_layer_encoder.forward_leaky(Xe1) \n",
    "#         FLe1=dropout(FLe1, dropout_rate=0.1, training=True)\n",
    "#         FLe2=linear_layer_econder_2.forward(FLe1)\n",
    "#         Xe2=FLe2+Ect1\n",
    "#         Ecout=normalization_layer_encoder_2.forward(Xe2)\n",
    "        \n",
    "#         #print(\"K_C.shape\",K_C.shape)\n",
    "#         #print(\"Ecout.shape\",Ecout.shape,\"inputs_e.shape\",inputs_e.shape) \n",
    "\n",
    "#         target_d=pad_sequences(y_batch,lenght=words_per_phrase,target_type=\"target\") \n",
    "#         target_d=[re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', xx) for xx in target_d] \n",
    "#         tot_loss=0\n",
    "#         counter_correct=0\n",
    "#         for step in range(0,inputs_d.shape[0]):\n",
    "#             inputs_decoder=inputs_d[step] \n",
    "#             #target_i=cp.array([get_one_hot(x[step], vocabulary) for x in target_d])\n",
    "\n",
    "#             target_i=cp.array([vocabulary[x[step]][1] for x in target_d])\n",
    "#             #print(target_i)\n",
    "#             Q_D=Qd.forward_weights_only(inputs_decoder)\n",
    "#             K_D=Kd.forward_weights_only(inputs_decoder)\n",
    "#             V_D=Vd.forward_weights_only(inputs_decoder) \n",
    "#             Q_D,K_D,V_D=multihead_attention_decoder.reshape_heads(Q_D,K_D,V_D)\n",
    "#             Attention_weights_masked=multihead_attention_decoder.attention_weights_masked(inputs_decoder.shape[1])\n",
    "#             A_mask=multihead_attention_decoder.attention()\n",
    "#             Xd=inputs_decoder+A_mask\n",
    "#             Dt1=normalization_layer_decoder_1.forward(Xd)\n",
    "#             Q_C=Qc.forward_weights_only(Dt1)\n",
    "#             V_C=Vc.forward_weights_only(Ecout)\n",
    "#             K_C=Kc.forward_weights_only(Ecout)\n",
    "#             Q_C,K_C,V_C=multihead_cross_attention.reshape_heads(Q_C,K_C,V_C) \n",
    "#             Attention_weights_cross=multihead_cross_attention.attention_weights() \n",
    "#             Acr=multihead_cross_attention.attention()\n",
    "#             Res=Acr+Dt1\n",
    "#             Dt2=normalization_layer_decoder_2.forward(Res)\n",
    "#             Xd1=linear_layer_decoder_1.forward(Dt2)\n",
    "#             FLd1=relu_layer_decoder.forward_leaky(Xd1)\n",
    "#             FLd1=dropout(FLd1, dropout_rate=0.1, training=True)\n",
    "#             FLd2=linear_layer_decoder_2.forward(FLd1)\n",
    "#             Xd2=FLd2+Dt2 \n",
    "#             Dout=normalization_layer_decoder_3.forward(Xd2)\n",
    "#             Dout = Dout[:, step, :]\n",
    "#             #print(\"Dout\",Dout.shape,\"output_linear_layer\",output_linear_layer.W.shape)\n",
    "#             Zout=output_linear_layer.forward(Dout)\n",
    "#             SigmaZout=softmax(Zout) \n",
    "#             #print(Zout) \n",
    "#             #print_vocabs(SigmaZout,vocabulary,y_batch,step,target_i,counter_correct)\n",
    "#             Loss = sparse_categorical_crossentropy(SigmaZout, target_i)\n",
    "\n",
    "#             #print(Loss) \n",
    "#             tot_loss+=Loss\n",
    "#             #print(\"Loss\",Loss)\n",
    "#             #dLoss_dZout=SigmaZout-target_i\n",
    "#             dLoss_dZout = SigmaZout.copy()  # Create a copy of the softmax output\n",
    "\n",
    "#             # Subtract 1 for the true class indices\n",
    "#             dLoss_dZout[np.arange(target_i.shape[0]), target_i] -= 1 \n",
    "#             #print(dLoss_dZout)\n",
    "             \n",
    "#             dLoss_dWo=(dLoss_dZout.T@Dout).T\n",
    "#             dLoss_dbo= cp.sum(dLoss_dZout,axis=0) \n",
    "#             #\n",
    " \n",
    "#             dLoss_dDout=dLoss_dZout@output_linear_layer.W.T \n",
    "#             dLoss_dFLd2=cp.expand_dims(dLoss_dDout,axis=1)*normalization_layer_decoder_3.backpropagation()\n",
    "#             dLoss_dDt2_a=dLoss_dFLd2\n",
    "            \n",
    "             \n",
    "#             dLoss_dFLd1 = dLoss_dFLd2 @ linear_layer_decoder_2.W.T\n",
    "\n",
    "            \n",
    "#             dLoss_dWfl2d = cp.sum(cp.transpose(dLoss_dFLd2, (0, 2, 1)) @ FLd1, axis=0).T\n",
    "#             dLoss_dbfl2d = cp.sum(cp.sum(dLoss_dFLd2, axis=0), axis=0)\n",
    "#             #\n",
    "\n",
    "\n",
    "            \n",
    "#             DLoss_dDt2_b =relu_layer_decoder.backward_leaky(dLoss_dFLd1) @ cp.transpose(linear_layer_decoder_1.W, (1, 0))\n",
    "#             DLoss_dDt2 = dLoss_dDt2_a + DLoss_dDt2_b\n",
    "            \n",
    "            \n",
    "            \n",
    "#             dLoss_dWfl1d = cp.sum(cp.transpose(relu_layer_decoder.backward_leaky(dLoss_dFLd1), (0, 2, 1)) @ Dt2, axis=0).T\n",
    "#             dLoss_dbfl1d = cp.sum(cp.sum(relu_layer_decoder.backward_leaky(dLoss_dFLd1), axis=0), axis=0)\n",
    "#             #\n",
    "            \n",
    "#             dLoss_dAcr = DLoss_dDt2 * normalization_layer_decoder_2.backpropagation()\n",
    "#             dLoss_dDt1_a = dLoss_dAcr\n",
    "\n",
    "            \n",
    "#             dLoss_dQc = multihead_cross_attention.diffQi(dLoss_dAcr,Dt1)\n",
    "#             #\n",
    "             \n",
    "#             dLoss_dKc=multihead_cross_attention.diffKi(dLoss_dAcr,Ecout)\n",
    "#             #\n",
    "             \n",
    "#             dLoss_dVc=multihead_cross_attention.diffVi(dLoss_dAcr,Ecout)\n",
    "#             #\n",
    "            \n",
    "#             dAttention_weights_cross = Attention_weights_cross * (1 - Attention_weights_cross)\n",
    "#             dLoss_dDt1_b= dLoss_dAcr * redimension(dAttention_weights_cross @ (K_C * V_C / cp.sqrt(embedding_size))) @ Qc.W\n",
    "\n",
    "#             dLoss_dDt1 = dLoss_dDt1_a + dLoss_dDt1_b\n",
    "             \n",
    "            \n",
    "#             dLoss_Amask = dLoss_dDt1 * normalization_layer_decoder_1.backpropagation() \n",
    "\n",
    "#             dLoss_inputd_a = dLoss_Amask\n",
    "\n",
    "#             dLoss_Kd=multihead_attention_decoder.diffKi(dLoss_Amask,inputs_decoder)\n",
    "#             # \n",
    "#             dLoss_Qd =multihead_attention_decoder.diffQi(dLoss_Amask,inputs_decoder)\n",
    "#             #\n",
    "#             dLoss_Vd =multihead_attention_decoder.diffVi(dLoss_Amask,inputs_decoder)\n",
    "#             #\n",
    "            \n",
    "           \n",
    "            \n",
    "#             dLoss_inputd_k=multihead_attention_decoder.diffKInput(dLoss_Amask,Kd.W)\n",
    "#             dLoss_inputd_q=multihead_attention_decoder.diffQInput(dLoss_Amask,Qd.W)\n",
    "#             dLoss_inputd_v=multihead_attention_decoder.diffVInput(dLoss_Amask,Vd.W)\n",
    "#             dLoss_input_d = dLoss_inputd_a + dLoss_inputd_k + dLoss_inputd_q + dLoss_inputd_v\n",
    "#             dLoss_dWemb_decoder= dLoss_input_d * inputs_decoder\n",
    "\n",
    "\n",
    "#             output_linear_layer.update_weights(dLoss_dWo,dLoss_dbo,learning_rate)\n",
    "#             linear_layer_decoder_2.update_weights(dLoss_dWfl2d,dLoss_dbfl2d,learning_rate)\n",
    "#             linear_layer_decoder_1.update_weights(dLoss_dWfl1d,dLoss_dbfl1d,learning_rate)\n",
    "#             Qc.update_weights_only(dLoss_dQc,learning_rate)\n",
    "#             Kc.update_weights_only(dLoss_dKc,learning_rate)\n",
    "#             Vc.update_weights_only(dLoss_dVc,learning_rate)\n",
    "#             Kd.update_weights_only(dLoss_Kd,learning_rate)\n",
    "#             Qd.update_weights_only(dLoss_Qd,learning_rate)\n",
    "#             Vd.update_weights_only(dLoss_Vd,learning_rate)\n",
    "#             inputs_decoder=inputs_decoder-learning_rate*dLoss_dWemb_decoder\n",
    "#             vocabulary=update_wembedding_decoder(y_batch,inputs_decoder,words_per_phrase,vocabulary)\n",
    "\n",
    "#         tot_loss_epoch+=tot_loss/inputs_d.shape[0]\n",
    "#         #print(tot_loss/inputs_d.shape[0])\n",
    "#         #dLoss_dFLe2 = dLoss_Ecout * diff_norm(Xe2, var_e2, mu_e2, N_e2)\n",
    "        \n",
    "         \n",
    "\n",
    "#         dLoss_Ecout_k=multihead_cross_attention.diffKInput(dLoss_dAcr,Kc.W)\n",
    "#         dLoss_Ecout_v=multihead_cross_attention.diffVInput(dLoss_dAcr,Vc.W)\n",
    "#         dLoss_Ecout = dLoss_Ecout_k + dLoss_Ecout_v\n",
    "#         #dLoss_dAcr = cp.swapaxes(cp.array(cp.array_split(dLoss_dAcr, num_heads, axis=2)), 0, 1)\n",
    "        \n",
    "        \n",
    "#         #print(\"dLoss_Ecout\",dLoss_Ecout_v.shape)\n",
    "\n",
    "#         #dLoss_Ecout=derivative_Ecout(Attention_weights_cross,dLoss_dAcr,Q_C,V_C,Kc.W,Vc.W,num_heads)\n",
    "         \n",
    "#         dLoss_dFLe2 = dLoss_Ecout * normalization_layer_encoder_2.backpropagation()\n",
    "#         dLoss_Ect1_a = dLoss_dFLe2\n",
    "\n",
    "#         dLoss_dFLe1 = dLoss_dFLe2 @ linear_layer_econder_2.W.T\n",
    "\n",
    "#         dLoss_dWfl2e = cp.sum(cp.transpose(dLoss_dFLe2, (0, 2, 1)) @ FLe1,axis=0).T\n",
    " \n",
    "#         dLoss_dbfl2e = cp.sum(cp.sum(dLoss_dFLe2, axis=1),axis=0)\n",
    "        \n",
    "\n",
    "#         dLoss_Ect1_b = relu_layer_encoder.backward_leaky(dLoss_dFLe1) @ cp.transpose(linear_layer_econder_1.W, (1, 0))\n",
    "\n",
    "#         dLoss_Ect1 = dLoss_Ect1_b + dLoss_Ect1_a\n",
    "        \n",
    "#         dLoss_dWfl1e = cp.sum(cp.transpose(relu_layer_encoder.backward_leaky(dLoss_dFLe1), (0, 2, 1)) @ Ect1,axis=0).T\n",
    "        \n",
    "#         dLoss_dbfl1e = cp.sum(cp.sum(relu_layer_encoder.backward_leaky(dLoss_dFLe1),axis=0),axis=0)\n",
    "\n",
    "#         #print(dLoss_dbfl1e.shape,linear_layer_econder_1.b.shape)\n",
    "#         dLoss_Ae = dLoss_Ect1 * normalization_layer_encoder_1.backpropagation()\n",
    "        \n",
    "#         dLoss_inpute_a = dLoss_Ae\n",
    "\n",
    "#         dLoss_dQe=multihead_attention_encoder.diffQi(dLoss_Ae,inputs_e)\n",
    "\n",
    "#         dLoss_dKe=multihead_attention_encoder.diffKi(dLoss_Ae, inputs_e)\n",
    "\n",
    "#         dLoss_dVe=multihead_attention_encoder.diffVi(dLoss_Ae,inputs_e)\n",
    "        \n",
    "#         dLoss_inpute_k=multihead_attention_encoder.diffKInput(dLoss_Ae,Ke.W)\n",
    "#         dLoss_inpute_q=multihead_attention_encoder.diffQInput(dLoss_Ae,Qe.W)\n",
    "#         dLoss_inpute_v=multihead_attention_encoder.diffVInput(dLoss_Ae,Ve.W)\n",
    "#         dLoss_input_e = dLoss_inpute_a + dLoss_inpute_k + dLoss_inpute_q + dLoss_inpute_v\n",
    "#         dLoss_dWemb_encoder = dLoss_input_e * inputs_e\n",
    "\n",
    "        \n",
    "        \n",
    "#         linear_layer_econder_2.update_weights(dLoss_dWfl2e,dLoss_dbfl2e,learning_rate)\n",
    "#         linear_layer_econder_1.update_weights(dLoss_dWfl1e,dLoss_dbfl1e,learning_rate)\n",
    "\n",
    "#         Qe.update_weights_only(dLoss_dQe,learning_rate)\n",
    "#         Ke.update_weights_only(dLoss_dKe,learning_rate)\n",
    "#         Ve.update_weights_only(dLoss_dVe,learning_rate)\n",
    "      \n",
    "#         inputs_e=inputs_e-learning_rate*dLoss_dWemb_encoder\n",
    "#         vocabulary=update_wembedding_encoder(X_batch,inputs_e,vocabulary,words_per_phrase)\n",
    "\n",
    "#         # except Exception as e:\n",
    "#         #     print(e)\n",
    "#         #     traceback.print_exc()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d4c7d792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self,embedding_size,num_heads,linear_layer_size,dropout_rate,batch_size,words_per_phrase):\n",
    "        self.batch_size=batch_size\n",
    "        self.words_per_phrase=words_per_phrase\n",
    "        self.num_heads=num_heads\n",
    "        self.dropout_rate=dropout_rate\n",
    "        self.linear_layer_size=linear_layer_size\n",
    "        self.embedding_size=embedding_size\n",
    "        self.Qe = linear_layer(self.embedding_size,self.embedding_size)\n",
    "        self.Ke = linear_layer(self.embedding_size,self.embedding_size)\n",
    "        self.Ve = linear_layer(self.embedding_size,self.embedding_size)\n",
    "        self.linear_layer_econder_1=linear_layer(self.embedding_size,self.linear_layer_size)\n",
    "        self.linear_layer_econder_2=linear_layer(self.linear_layer_size,self.embedding_size)\n",
    "        self.multihead_attention_encoder=multihead_attention(num_heads=2,dk=embedding_size,batch_size=batch_size)\n",
    "        self.normalization_layer_encoder_1=layer_normalization(epsilon=1e-6)\n",
    "        self.relu_layer_encoder=ReLu_layer(alpha=0.01)\n",
    "        self.normalization_layer_encoder_2=layer_normalization(epsilon=1e-6)\n",
    "        self.projection_layer=linear_layer(self.embedding_size,self.embedding_size)\n",
    "      \n",
    "\n",
    "    def forward(self,inputs_e):\n",
    "        self.inputs_e=inputs_e\n",
    "        Q_E=self.Qe.forward_weights_only(self.inputs_e)\n",
    "        K_E=self.Ke.forward_weights_only(self.inputs_e)\n",
    "        V_E=self.Ve.forward_weights_only(self.inputs_e) \n",
    "        Q_E,K_E,V_E=self.multihead_attention_encoder.reshape_heads(Q_E,K_E,V_E)\n",
    "        Ae=self.multihead_attention_encoder.attention() \n",
    "        Xe=self.inputs_e+Ae \n",
    "        Ect1=self.normalization_layer_encoder_1.forward(Xe) \n",
    "        Xe1=self.linear_layer_econder_1.forward(Ect1) \n",
    "        FLe1=self.relu_layer_encoder.forward_leaky(Xe1) \n",
    "        FLe1=dropout(FLe1, dropout_rate=self.dropout_rate, training=True)\n",
    "        FLe2=self.linear_layer_econder_2.forward(FLe1)\n",
    "        Xe2=FLe2+Ect1\n",
    "        Ecout=self.normalization_layer_encoder_2.forward(Xe2)\n",
    "        return Ecout\n",
    "\n",
    "\n",
    "    def backpropagation(self,dLoss_Ecout,vocabulary,X_batch,learning_rate):\n",
    "        \n",
    "        dLoss_dFLe2 = dLoss_Ecout * self.normalization_layer_encoder_2.backpropagation()\n",
    "        dLoss_Ect1_a = dLoss_dFLe2\n",
    "\n",
    "        dLoss_dFLe1 = dLoss_dFLe2 @ self.linear_layer_econder_2.W.T\n",
    "\n",
    "        dLoss_dWfl2e = cp.sum(cp.transpose(dLoss_dFLe2, (0, 2, 1)) @ self.linear_layer_econder_2.X,axis=0).T\n",
    " \n",
    "        dLoss_dbfl2e = cp.sum(cp.sum(dLoss_dFLe2, axis=1),axis=0)\n",
    "        \n",
    "\n",
    "        dLoss_Ect1_b = self.relu_layer_encoder.backward_leaky(dLoss_dFLe1) @ cp.transpose(self.linear_layer_econder_1.W, (1, 0))\n",
    "\n",
    "        dLoss_Ect1 = dLoss_Ect1_b + dLoss_Ect1_a\n",
    "        \n",
    "        dLoss_dWfl1e = cp.sum(cp.transpose(self.relu_layer_encoder.backward_leaky(dLoss_dFLe1), (0, 2, 1)) @ self.linear_layer_econder_1.X,axis=0).T\n",
    "        \n",
    "        dLoss_dbfl1e = cp.sum(cp.sum(self.relu_layer_encoder.backward_leaky(dLoss_dFLe1),axis=0),axis=0)\n",
    "\n",
    "        #print(dLoss_dbfl1e.shape,linear_layer_econder_1.b.shape)\n",
    "        dLoss_Ae = dLoss_Ect1 * self.normalization_layer_encoder_1.backpropagation()\n",
    "        \n",
    "        dLoss_inpute_a = dLoss_Ae\n",
    "\n",
    "        dLoss_dQe=self.multihead_attention_encoder.diffQi(dLoss_Ae,self.inputs_e)\n",
    "\n",
    "        dLoss_dKe=self.multihead_attention_encoder.diffKi(dLoss_Ae, self.inputs_e)\n",
    "\n",
    "        dLoss_dVe=self.multihead_attention_encoder.diffVi(dLoss_Ae,self.inputs_e)\n",
    "        \n",
    "        dLoss_inpute_k=self.multihead_attention_encoder.diffKInput(dLoss_Ae,self.Ke.W)\n",
    "        dLoss_inpute_q=self.multihead_attention_encoder.diffQInput(dLoss_Ae,self.Qe.W)\n",
    "        dLoss_inpute_v=self.multihead_attention_encoder.diffVInput(dLoss_Ae,self.Ve.W)\n",
    "        dLoss_input_e = dLoss_inpute_a + dLoss_inpute_k + dLoss_inpute_q + dLoss_inpute_v\n",
    "        dLoss_dWemb_encoder = dLoss_input_e * self.inputs_e\n",
    "\n",
    "        vocabulary=self.update_weights(dLoss_dWfl2e,\n",
    "                                       dLoss_dbfl2e,\n",
    "                                       dLoss_dWfl1e\n",
    "                                       ,dLoss_dbfl1e,\n",
    "                                       dLoss_dQe,\n",
    "                                       dLoss_dKe,\n",
    "                                       dLoss_dVe,dLoss_dWemb_encoder,vocabulary,X_batch,learning_rate)\n",
    "        return vocabulary\n",
    "    \n",
    "    def update_weights(self,dLoss_dWfl2e,dLoss_dbfl2e,dLoss_dWfl1e,dLoss_dbfl1e,dLoss_dQe,dLoss_dKe,dLoss_dVe,dLoss_dWemb_encoder,vocabulary,X_batch,learning_rate):\n",
    "        self.linear_layer_econder_2.update_weights(dLoss_dWfl2e,dLoss_dbfl2e,learning_rate)\n",
    "        self.linear_layer_econder_1.update_weights(dLoss_dWfl1e,dLoss_dbfl1e,learning_rate)\n",
    "\n",
    "        self.Qe.update_weights_only(dLoss_dQe,learning_rate)\n",
    "        self.Ke.update_weights_only(dLoss_dKe,learning_rate)\n",
    "        self.Ve.update_weights_only(dLoss_dVe,learning_rate)\n",
    "        self.inputs_e=self.inputs_e-learning_rate*dLoss_dWemb_encoder\n",
    "        vocabulary=update_wembedding_encoder(X_batch,self.inputs_e,vocabulary,self.words_per_phrase)\n",
    "        return vocabulary\n",
    "    \n",
    "\n",
    "class Decoder:\n",
    "    def __init__(self,embedding_size,num_heads,linear_layer_size,dropout_rate,batch_size,words_per_phrase):\n",
    "        self.words_per_phrase=words_per_phrase\n",
    "        self.batch_size=batch_size\n",
    "        self.num_heads=num_heads\n",
    "        self.dropout_rate=dropout_rate\n",
    "        self.linear_layer_size=linear_layer_size\n",
    "        self.embedding_size=embedding_size\n",
    "        self.Qd = linear_layer(self.embedding_size,self.embedding_size)\n",
    "        self.Kd = linear_layer(self.embedding_size,self.embedding_size)\n",
    "        self.Vd = linear_layer(self.embedding_size,self.embedding_size)\n",
    "        self.Qc = linear_layer(self.embedding_size,self.embedding_size)\n",
    "        self.Kc = linear_layer(self.embedding_size,self.embedding_size)\n",
    "        self.Vc = linear_layer(self.embedding_size,self.embedding_size)\n",
    "        self.linear_layer_decoder_1=linear_layer(self.embedding_size,self.linear_layer_size)\n",
    "        self.linear_layer_decoder_2=linear_layer(self.linear_layer_size,self.embedding_size)\n",
    "        self.multihead_cross_attention=multihead_attention(num_heads=num_heads,dk=embedding_size,batch_size=batch_size)\n",
    "        self.multihead_attention_decoder=multihead_attention(num_heads=num_heads,dk=embedding_size,batch_size=batch_size)\n",
    "        self.normalization_layer_decoder_1=layer_normalization(epsilon=1e-6)\n",
    "        self.relu_layer_decoder=ReLu_layer(alpha=0.01)\n",
    "        self.normalization_layer_decoder_2=layer_normalization(epsilon=1e-6)\n",
    "        self.normalization_layer_decoder_3=layer_normalization(epsilon=1e-6)\n",
    "        \n",
    "\n",
    "    def forward(self,inputs_decoder,Ecout):\n",
    "        self.inputs_decoder=inputs_decoder\n",
    "        Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
    "        K_D=self.Kd.forward_weights_only(inputs_decoder)\n",
    "        V_D=self.Vd.forward_weights_only(inputs_decoder) \n",
    "        Q_D,K_D,V_D=self.multihead_attention_decoder.reshape_heads(Q_D,K_D,V_D)\n",
    "        A_mask=self.multihead_attention_decoder.masked_attention(inputs_decoder.shape[1])\n",
    "        Xd=self.inputs_decoder+A_mask\n",
    "        Dt1=self.normalization_layer_decoder_1.forward(Xd)\n",
    "        Q_C=self.Qc.forward_weights_only(Dt1)\n",
    "        V_C=self.Vc.forward_weights_only(Ecout)\n",
    "        K_C=self.Kc.forward_weights_only(Ecout)\n",
    "        Q_C,K_C,V_C=self.multihead_cross_attention.reshape_heads(Q_C,K_C,V_C)  \n",
    "        Acr=self.multihead_cross_attention.attention()\n",
    "        Res=Acr+Dt1\n",
    "        Dt2=self.normalization_layer_decoder_2.forward(Res)\n",
    "        Xd1=self.linear_layer_decoder_1.forward(Dt2)\n",
    "        FLd1=self.relu_layer_decoder.forward_leaky(Xd1)\n",
    "        FLd1=dropout(FLd1, dropout_rate=self.dropout_rate, training=True)\n",
    "        FLd2=self.linear_layer_decoder_2.forward(FLd1)\n",
    "        \n",
    "        Xd2=FLd2+Dt2 \n",
    "        \n",
    "        Dout=self.normalization_layer_decoder_3.forward(Xd2)\n",
    "        #print(Dout)\n",
    "       \n",
    "        #print(Dout)\n",
    "        return Dout\n",
    "\n",
    "\n",
    "    def backpropagation(self,dLoss_dDout,vocabulary,y_batch,learning_rate):\n",
    "        \n",
    "        #print(\"normalization_layer_decoder_3.backpropagation()\",self.normalization_layer_decoder_3.backpropagation().shape) \n",
    "        #dLoss_dFLd2=cp.expand_dims(dLoss_dDout,axis=1)*self.normalization_layer_decoder_3.backpropagation()\n",
    "        dLoss_dFLd2=dLoss_dDout*self.normalization_layer_decoder_3.backpropagation()\n",
    "        dLoss_dDt2_a=dLoss_dFLd2\n",
    "        \n",
    "        #print(\"dLoss_dFLd2\",dLoss_dFLd2.shape)    \n",
    "        dLoss_dFLd1 = dLoss_dFLd2 @ self.linear_layer_decoder_2.W.T\n",
    "\n",
    "        \n",
    "        dLoss_dWfl2d = cp.sum(cp.transpose(dLoss_dFLd2, (0, 2, 1)) @ self.linear_layer_decoder_2.X, axis=0).T\n",
    "        dLoss_dbfl2d = cp.sum(cp.sum(dLoss_dFLd2, axis=0), axis=0)\n",
    "        \n",
    "        DLoss_dDt2_b =self.relu_layer_decoder.backward_leaky(dLoss_dFLd1) @ cp.transpose(self.linear_layer_decoder_1.W, (1, 0))\n",
    "        DLoss_dDt2 = dLoss_dDt2_a + DLoss_dDt2_b\n",
    "         \n",
    "        dLoss_dWfl1d = cp.sum(cp.transpose(self.relu_layer_decoder.backward_leaky(dLoss_dFLd1), (0, 2, 1)) @ self.linear_layer_decoder_1.X, axis=0).T\n",
    "        dLoss_dbfl1d = cp.sum(cp.sum(self.relu_layer_decoder.backward_leaky(dLoss_dFLd1), axis=0), axis=0)\n",
    "        # \n",
    "        dLoss_dAcr = DLoss_dDt2 * self.normalization_layer_decoder_2.backpropagation()\n",
    "        dLoss_dDt1_a = dLoss_dAcr\n",
    " \n",
    "        dLoss_dQc = self.multihead_cross_attention.diffQi(dLoss_dAcr,self.Qc.X)\n",
    "         \n",
    "        dLoss_dKc=self.multihead_cross_attention.diffKi(dLoss_dAcr,self.Kc.X)\n",
    "         \n",
    "        dLoss_dVc=self.multihead_cross_attention.diffVi(dLoss_dAcr,self.Vc.X)\n",
    "         \n",
    "        dAttention_weights_cross = self.multihead_cross_attention.Attention_weights * (1 - self.multihead_cross_attention.Attention_weights)\n",
    "\n",
    "        dLoss_dDt1_b= dLoss_dAcr * redimension(dAttention_weights_cross @ (self.multihead_cross_attention.K * self.multihead_cross_attention.V / cp.sqrt(embedding_size))) @ self.Qc.W\n",
    "\n",
    "        dLoss_dDt1 = dLoss_dDt1_a + dLoss_dDt1_b\n",
    "           \n",
    "        dLoss_Amask = dLoss_dDt1 * self.normalization_layer_decoder_1.backpropagation() \n",
    "\n",
    "        dLoss_inputd_a = dLoss_Amask\n",
    "\n",
    "        dLoss_Kd=self.multihead_attention_decoder.diffKi(dLoss_Amask,self.inputs_decoder)\n",
    "        # \n",
    "        dLoss_Qd =self.multihead_attention_decoder.diffQi(dLoss_Amask,self.inputs_decoder)\n",
    "        #\n",
    "        dLoss_Vd =self.multihead_attention_decoder.diffVi(dLoss_Amask,self.inputs_decoder)\n",
    "        #\n",
    "        dLoss_inputd_k=self.multihead_attention_decoder.diffKInput(dLoss_Amask,self.Kd.W)\n",
    "        dLoss_inputd_q=self.multihead_attention_decoder.diffQInput(dLoss_Amask,self.Qd.W)\n",
    "        dLoss_inputd_v=self.multihead_attention_decoder.diffVInput(dLoss_Amask,self.Vd.W)\n",
    "        dLoss_input_d = dLoss_inputd_a + dLoss_inputd_k + dLoss_inputd_q + dLoss_inputd_v\n",
    "        dLoss_dWemb_decoder= dLoss_input_d * self.inputs_decoder\n",
    "\n",
    "        vocabulary=self.update_weights(dLoss_dWfl2d,\n",
    "                                        dLoss_dbfl2d,\n",
    "                                        dLoss_dWfl1d,\n",
    "                                        dLoss_dbfl1d,\n",
    "                                        dLoss_dQc,\n",
    "                                        dLoss_dKc,\n",
    "                                        dLoss_dVc,\n",
    "                                        dLoss_Kd,\n",
    "                                        dLoss_Qd,\n",
    "                                        dLoss_Vd,\n",
    "                                        dLoss_dWemb_decoder,\n",
    "                                        vocabulary,\n",
    "                                        y_batch,\n",
    "                                        learning_rate)\n",
    "        return vocabulary,dLoss_dAcr\n",
    "    \n",
    "    def update_weights(self,dLoss_dWfl2d,\n",
    "                       dLoss_dbfl2d,\n",
    "                       dLoss_dWfl1d,\n",
    "                       dLoss_dbfl1d,\n",
    "                       dLoss_dQc,\n",
    "                       dLoss_dKc,\n",
    "                       dLoss_dVc,\n",
    "                       dLoss_Kd,\n",
    "                       dLoss_Qd,\n",
    "                       dLoss_Vd,\n",
    "                       dLoss_dWemb_decoder,\n",
    "                       vocabulary,\n",
    "                       y_batch,\n",
    "                       learning_rate):\n",
    "        self.linear_layer_decoder_2.update_weights(dLoss_dWfl2d,dLoss_dbfl2d,learning_rate)\n",
    "        self.linear_layer_decoder_1.update_weights(dLoss_dWfl1d,dLoss_dbfl1d,learning_rate)\n",
    "        self.Qc.update_weights_only(dLoss_dQc,learning_rate)\n",
    "        self.Kc.update_weights_only(dLoss_dKc,learning_rate)\n",
    "        self.Vc.update_weights_only(dLoss_dVc,learning_rate)\n",
    "        self.Kd.update_weights_only(dLoss_Kd,learning_rate)\n",
    "        self.Qd.update_weights_only(dLoss_Qd,learning_rate)\n",
    "        self.Vd.update_weights_only(dLoss_Vd,learning_rate)\n",
    "        self.inputs_decoder=self.inputs_decoder-learning_rate*dLoss_dWemb_decoder\n",
    "        vocabulary=update_wembedding_decoder(y_batch,self.inputs_decoder,self.words_per_phrase,vocabulary) \n",
    "        return vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0c0e5bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fl1_size=1024*1 \n",
    "# batch_size=32\n",
    "# vocab_size=len(vocabulary) \n",
    " \n",
    " \n",
    "# words_per_phrase = num_phrases= max_v\n",
    "# dk = dv = embedding_size = 300 # constrain of transformer all embedding size both input embedding and attention embedding are the same encoder\n",
    "# num_heads=10\n",
    "\n",
    "# num_batches_per_epoch = len(X_train) // batch_size\n",
    "# num_epochs=550\n",
    "# learning_rate=0.001\n",
    "\n",
    "# Qe = linear_layer(embedding_size,embedding_size)\n",
    "# Ke = linear_layer(embedding_size,embedding_size)\n",
    "# Ve = linear_layer(embedding_size,embedding_size)\n",
    "# linear_layer_econder_1=linear_layer(embedding_size,fl1_size)\n",
    "# linear_layer_econder_2=linear_layer(fl1_size,embedding_size)\n",
    "\n",
    "# normalization_layer_encoder_1=layer_normalization(epsilon=1e-6)\n",
    "# relu_layer_encoder=ReLu_layer(alpha=0.01)\n",
    "# normalization_layer_encoder_2=layer_normalization(epsilon=1e-6)\n",
    "# multihead_attention_encoder=multihead_attention(num_heads=num_heads,dk=embedding_size,batch_size=batch_size)\n",
    "# multihead_cross_attention=multihead_attention(num_heads=num_heads,dk=embedding_size,batch_size=batch_size)\n",
    "# multihead_attention_decoder=multihead_attention(num_heads=num_heads,dk=embedding_size,batch_size=batch_size)\n",
    "# Qc = linear_layer(embedding_size,embedding_size)\n",
    "# Kc = linear_layer(embedding_size,embedding_size)\n",
    "# Vc = linear_layer(embedding_size,embedding_size)\n",
    "# Qd = linear_layer(embedding_size,embedding_size)\n",
    "# Kd = linear_layer(embedding_size,embedding_size)\n",
    "# Vd = linear_layer(embedding_size,embedding_size)\n",
    "\n",
    "# normalization_layer_decoder_1=layer_normalization(epsilon=1e-6)\n",
    "# normalization_layer_decoder_2=layer_normalization(epsilon=1e-6)\n",
    "# normalization_layer_decoder_3=layer_normalization(epsilon=1e-6)\n",
    "\n",
    "# linear_layer_decoder_1=linear_layer(embedding_size,fl1_size)\n",
    "# linear_layer_decoder_2=linear_layer(fl1_size,embedding_size)\n",
    "# relu_layer_decoder=ReLu_layer(alpha=0.01)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "461b1c74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "36aaee8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([7.29, -4.52, 6.58, -2.67, -1.89, 1.38, 4.31, -3.22, -10.3, -1.86, -0.165, -2.48, 9.21, 0.397, 1.59, 7.13, -6.91, 4.72, -1.58, -0.172, 4.32, 3.8, 1.43, -5.23, -7.19, 1.69, 4.76, 6.03, -3.68, -4.43, -0.472, -1.38, 6.1, 2.23, 0.887, -5.05, -1.16, 0.472, 6.07, 6.63, 1.5, -2.8, -7.84, 8.3, 2.25, -13.7, 4.49, 1.6, 3.39, -6.48, -7.28, -0.184, -2.28, -3.54, -4.86, 4.98, 6.52, -3.3, 7.13, -2.69, -0.978, -2.03, -4.34, -0.0675, -4.21, 3.58, -9.06, 0.371, 0.468, 1.56, 0.745, -5.18, -5.34, -5.32, -0.718, 0.632, 1.63, 3.74, -2.94, 5.61, -0.722, -3.61, -3.2, -1.7, -5.03, -1.62, 1.89, -1.64, 6.58, 2.95, -10.6, 1.52, -6.33, -7.92, -5.77, 4.34, -0.0952, -1.63, -0.553, -0.957, 0.99, -2.37, 3.49, -3.04, 1.74, 4.95, 11.8, 0.265, -4.32, 5.9, 3.63, 0.339, 2.86, -1.67, -1.76, 0.766, -3.32, 4.36, -0.622, -1.46, 4.53, 1.96, 0.922, -0.981, 2.86, 2.27, 2.65, 1.67, 0.0472, -3.75, -6.28, -1.6, 2.4, -0.826, 4.83, -6, -0.0516, 1.95, -1.24, 4.48, -3.47, -0.587, -5.91, 3.51, -4.78, -0.428, -4.84, 0.476, 10.3, 1.23, -2.86, -1.37, -0.346, 3.42, -1.12, 4.54, -2.6, 0.17, -2.31, 5.26, 5.8, 8.39, -6.88, -3.1, -0.798, -1.88, -1.21, 7.81, 1.87, 4.69, 0.746, 7.02, 5.14, -2.88, 4.15, -2.83, 5.87, 1.71, 2.81, -3.75, 3.28, -11.9, -2.58, -5.91, 1.13, 2.79, 2.79, -2.02, -0.957, -1.8, 1.35, 4.26, 6.17, 3.6, 2.16, 3.13, -0.854, 10.2, -2.07, -9.68, 2.2, -0.799, 7.39, 8.04, 1.21, 6.69, -0.302, -4.33, 2.12, -0.99, -7.7, 0.765, 1.27, 0.75, -4.04, 8.9, -3.62, 2.47, 3.74, 4.85, 5.78, -1.26, -7.27, -3.62, 5.2, 9.79, 2.46, -6.25, 5.65, 2.95, -0.848, 2.27, 3.52, -3.82, -0.333, -5.25, 2.65, 3.7, -6.49, 0.463, 1.33, -6.31, 1.85, 0.486, 0.216, -1.13, 7.71, -3.44, 4.6, -8.98, 10.4, -3.87, -1.83, -2.21, -0.618, -1.18, -0.666, -3.63, 0.421, 5.58, 5.23, -0.541, 5.17, 4.31, -2.18, -2.06, -3.73, -0.0329, 5.45, -6.7, -0.49, 4.69, 0.407, 2.87, -1.45, -5.18, -3.36, -7.6, 5.32, -0.654, 2.86, 1.41, -1.81, 3.77, -0.498, 2.77, 4.2, 0.539, 3.52, -0.979, -2.79, 1.96, 0.588, -2.52, -1.21, -6.74, -1.87, -4.61, 0.664, 2.34], dtype=float32),\n",
       " 107)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocabulary[\"[PAD]\"]=(cp.zeros(embedding_size),vocabulary[\"[PAD]\"][1])\n",
    "vocabulary[\"[PAD]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fa5bf632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=vocabulary[\"[PAD]\"][0]\n",
    "b=vocabulary[\"[PAD]\"][0]\n",
    "a.all()==b.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3ab4c905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "words_per_phrase = num_phrases= max_v\n",
    "embedding_size=300\n",
    "fl1_size=3048\n",
    "learning_rate=0.001\n",
    "batch_size=4\n",
    "num_heads=10\n",
    "dropout_rate=0.2\n",
    "TransformerEncoder=Encoder(embedding_size,num_heads,fl1_size,dropout_rate=dropout_rate,batch_size=batch_size,words_per_phrase=max_v)\n",
    "TransformerDecoder=Decoder(embedding_size,num_heads,fl1_size,dropout_rate=dropout_rate,batch_size=batch_size,words_per_phrase=max_v)\n",
    "output_linear_layer=linear_layer(embedding_size,len(vocabulary),out=True)  \n",
    "num_batches_per_epoch = len(X_train) // batch_size\n",
    "num_epochs=550\n",
    "tot_loss_epoch=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "024879bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/550:   0%|          | 0/86 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Epoch 1/550:   9%|         | 8/86 [00:00<00:01, 49.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Epoch 1/550:  15%|        | 13/86 [00:00<00:01, 45.59it/s]Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Epoch 1/550:  29%|       | 25/86 [00:00<00:00, 68.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Epoch 1/550:  47%|     | 40/86 [00:00<00:00, 93.64it/s]Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Epoch 1/550:  66%|   | 57/86 [00:00<00:00, 114.96it/s]Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1330844885.py\", line 29, in <module>\n",
      "    Dout = TransformerDecoder.forward(target_d,Ecout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\3679559859.py\", line 123, in forward\n",
      "    Q_D=self.Qd.forward_weights_only(inputs_decoder)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_13476\\1670094454.py\", line 443, in forward_weights_only\n",
      "    Xout = cp.matmul(X, self.W)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 658, in __call__\n",
      "    args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\cupy\\_core\\_gufuncs.py\", line 515, in _get_args_transposed\n",
      "    raise ValueError(\n",
      "ValueError: Dimension k with different lengths in arrays\n",
      "Epoch 1/550:  83%| | 71/86 [00:00<00:00, 100.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n",
      "inputs_d.shape (4, 8, 300)\n",
      "inputs_e.shape (4, 8, 300)\n",
      "target_d.shape (4, 8, 1060)\n",
      "Dimension k with different lengths in arrays\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m totdLoss_dAcr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     22\u001b[0m counter_correct\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 23\u001b[0m target_d\u001b[38;5;241m=\u001b[39m\u001b[43mcreate_decoder_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43membedding_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwords_per_phrase\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_d.shape\u001b[39m\u001b[38;5;124m\"\u001b[39m,inputs_d\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_e.shape\u001b[39m\u001b[38;5;124m\"\u001b[39m,inputs_e\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[52], line 23\u001b[0m, in \u001b[0;36mlog_time.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     22\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Record start time\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Execute the wrapped function\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Record end time\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     elapsed_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[1;32mIn[52], line 158\u001b[0m, in \u001b[0;36mcreate_decoder_target\u001b[1;34m(y_train, embedding_size, max_words_per_phrase, vocabulary_decoder)\u001b[0m\n\u001b[0;32m    152\u001b[0m phrase_vectors_y \u001b[38;5;241m=\u001b[39m [i[\u001b[38;5;241m0\u001b[39m:max_words_per_phrase] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m decoder_input]\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# for sentence in phrase_vectors_y:\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m#     print(sentence)\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m#print_matrix(phrase_vectors_y)\u001b[39;00m\n\u001b[1;32m--> 158\u001b[0m yi \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39marray([[get_one_hot(word,vocabulary_decoder) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m phrase_vector] \u001b[38;5;28;01mfor\u001b[39;00m phrase_vector \u001b[38;5;129;01min\u001b[39;00m phrase_vectors_y]) \n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m yi\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(100):\n",
    "    print(\"Loss\",tot_loss_epoch/num_batches_per_epoch)\n",
    "    tot_loss_epoch=0\n",
    "    for i in tqdm(range(0,num_batches_per_epoch),desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        try: \n",
    "            start = i * batch_size\n",
    "            end = start + batch_size \n",
    "            X_batch = X_train[start:end]\n",
    "            y_batch = y_train[start:end] \n",
    "            #print(y_batch)\n",
    "            inputs_e=create_input_encoder(X_batch,vocabulary,words_per_phrase,embedding_size) \n",
    "            inputs_d=create_decoder_input(y_batch,embedding_size,words_per_phrase,vocabulary) \n",
    "            Ecout=TransformerEncoder.forward(inputs_e)\n",
    "             \n",
    "            \n",
    "            #print(\"Ecout.shape\",Ecout.shape,\"inputs_e.shape\",inputs_e.shape) \n",
    " \n",
    "            #target_d=pad_sequences(y_batch,lenght=words_per_phrase,target_type=\"target\") \n",
    "            #target_d=[re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', xx) for xx in target_d] \n",
    "            tot_loss=0\n",
    "            totdLoss_dAcr=0\n",
    "            counter_correct=0\n",
    "            target_d=create_decoder_target(y_batch,embedding_size,words_per_phrase,vocabulary) \n",
    "            print(\"inputs_d.shape\",inputs_d.shape)\n",
    "            print(\"inputs_e.shape\",inputs_e.shape)\n",
    "            print(\"target_d.shape\",target_d.shape)\n",
    "          \n",
    "                \n",
    "            Dout = TransformerDecoder.forward(target_d,Ecout)\n",
    "            #print(\"Dout\",Dout.shape,\"output_linear_layer\",output_linear_layer.W.shape)\n",
    "            Zout=output_linear_layer.forward(Dout)\n",
    "            \n",
    "            #print(\"Zout.shape\",Zout.shape)\n",
    "            SigmaZout=softmax(Zout) \n",
    "            # print(\"SigmaZout.shape\",SigmaZout.shape)\n",
    "            # print(\"target_i.shape\",target_i.shape)\n",
    "            # print(\"inputs_decoder.shape\",inputs_decoder.shape)\n",
    "        #     # for nn in SigmaZout:\n",
    "        #     #     print(np.argmax(nn)) \n",
    "            #print_vocabs(SigmaZout,vocabulary,y_batch,step,target_i,counter_correct)\n",
    "            #print_accuracy(SigmaZout, vocabulary,inputs_decoder,step,target_i,counter_correct)\n",
    "            Loss = cross_entropy_loss(SigmaZout, target_d,vocabulary)\n",
    "            #print(Loss) \n",
    "            tot_loss+=Loss \n",
    "            dLoss_dZout=SigmaZout-target_i\n",
    "            #print(\"dLoss_dZout.shape\",dLoss_dZout.shape)\n",
    "            # dLoss_dZout = SigmaZout.copy()  # Create a copy of the softmax output \n",
    "            # dLoss_dZout[np.arange(target_i.shape[0]), target_i] -= 1 \n",
    "            #clip_value = 1.0\n",
    "            #dLoss_dZout = cp.clip(dLoss_dZout, -clip_value, clip_value) \n",
    "            dLoss_Wo=cp.sum(cp.transpose(dLoss_dZout,(0,2,1))@Dout,axis=0).T\n",
    "            dLoss_bo=cp.sum(cp.sum(dLoss_dZout,axis=0),axis=0) \n",
    "            dLoss_dDout=dLoss_dZout@output_linear_layer.W.T  \n",
    "            vocabulary,dLoss_dAcr=TransformerDecoder.backpropagation(dLoss_dDout,vocabulary,y_batch,learning_rate)\n",
    "            output_linear_layer.update_weights(dLoss_Wo,dLoss_bo,learning_rate)\n",
    "            totdLoss_dAcr+=dLoss_dAcr\n",
    "\n",
    "            # #totdLoss_dAcr=totdLoss_dAcr/inputs_d.shape[0]\n",
    "            # dLoss_dAcr=totdLoss_dAcr\n",
    "            tot_loss_epoch+=tot_loss/inputs_d.shape[0] \n",
    "            dLoss_Ecout_k=TransformerDecoder.multihead_cross_attention.diffKInput(totdLoss_dAcr,TransformerDecoder.Kc.W)\n",
    "            dLoss_Ecout_v=TransformerDecoder.multihead_cross_attention.diffVInput(totdLoss_dAcr,TransformerDecoder.Vc.W)\n",
    "            dLoss_Ecout = dLoss_Ecout_k + dLoss_Ecout_v \n",
    "            \n",
    "            vocabulary=TransformerEncoder.backpropagation(dLoss_Ecout,vocabulary,X_batch,learning_rate)\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            traceback.print_exc()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dadf621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f083b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary[\"[PAD]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6303324",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', xx) for xx in y_batch][0]:\n",
    "    print(i,vocabulary[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85938a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_d[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6c18db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f38455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8754eb3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3bed4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a6ef5df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate=0.001\n",
    "\n",
    "# vocab_size=len(vocabulary)\n",
    "# batch_size=32\n",
    "\n",
    "\n",
    "# #batch_size = len(X_train)\n",
    "# words_per_phrase = num_phrases= max_v\n",
    "# dk = dv = embedding_size = 300 # constrain of transformer all embedding size both input embedding and attention embedding are the same encoder\n",
    "# num_heads=5\n",
    "# Qe = cp.random.rand(embedding_size, embedding_size) / cp.sqrt(embedding_size)\n",
    "# Ke = cp.random.rand(embedding_size, embedding_size) / cp.sqrt(embedding_size)\n",
    "# Ve = cp.random.rand(embedding_size, embedding_size) / cp.sqrt(embedding_size)\n",
    "# Qc = cp.random.rand(embedding_size, embedding_size) / cp.sqrt(embedding_size)\n",
    "# Kc = cp.random.rand(embedding_size, embedding_size) / cp.sqrt(embedding_size)\n",
    "# Vc = cp.random.rand(embedding_size, embedding_size) / cp.sqrt(embedding_size)\n",
    "# Qd = cp.random.rand(embedding_size, embedding_size) / cp.sqrt(embedding_size)\n",
    "# Kd = cp.random.rand(embedding_size, embedding_size) / cp.sqrt(embedding_size)\n",
    "# Vd = cp.random.rand(embedding_size, embedding_size) / cp.sqrt(embedding_size)\n",
    "\n",
    "# fl1_size=1024*2\n",
    "# Wfl1e=cp.random.rand(embedding_size, fl1_size)\n",
    "# bfl1e=cp.random.rand(fl1_size)\n",
    "\n",
    "# Wfl2e=cp.random.rand(fl1_size, dv)\n",
    "# bfl2e=cp.random.rand(dv)\n",
    "\n",
    "\n",
    "# Wfl1d=cp.random.rand(embedding_size, fl1_size)\n",
    "# bfl1d=cp.random.rand(fl1_size)\n",
    "\n",
    "# Wfl2d=cp.random.rand(fl1_size, dv)\n",
    "# bfl2d=cp.random.rand(dv)\n",
    "\n",
    "# Wo=cp.random.rand(embedding_size,vocab_size)* cp.sqrt(1 / (words_per_phrase * embedding_size))\n",
    "# bo=cp.random.rand(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642ffc99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a49ac739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import traceback\n",
    " \n",
    "# #X_train,y_train=load_x_y_train_plain()\n",
    "# num_batches_per_epoch = len(X_train) // batch_size\n",
    "# num_epochs=550\n",
    "# learning_rate=0.01\n",
    "# tot_loss_epoch=0\n",
    "# for epoch in range(num_epochs):\n",
    "#     print(\"Loss\",tot_loss_epoch/num_batches_per_epoch)\n",
    "#     tot_loss_epoch=0\n",
    "#     for i in tqdm(range(0,num_batches_per_epoch),desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "#         try: \n",
    "#             start = i * batch_size\n",
    "#             end = start + batch_size \n",
    "#             X_batch = X_train[start:end]\n",
    "#             y_batch = y_train[start:end] \n",
    "#             inputs_e=create_input_encoder(X_batch,vocabulary,words_per_phrase,embedding_size) \n",
    "#             inputs_d=create_decoder_input(y_batch,embedding_size,words_per_phrase,vocabulary)\n",
    "           \n",
    "#             Ae,Attention_weights_e,K_E,V_E,Q_E=forward_attention_encoder(inputs_e,Qe, Ke, Ve, num_heads, batch_size, dk)\n",
    "\n",
    "#             Ect1,Xe,mu_e,var_e,Ne=residual_and_norm(Ae,inputs_e) \n",
    "#             #print(\"Ecout.shape\",Ect1.shape)\n",
    "#             Ecout,mu_e2,var_e2,N_e2,FLe1,Xe1,Xe2=fully_connected_layers_encoder(Ect1,Wfl1e, bfl1e, Wfl2e, bfl2e)\n",
    "            \n",
    "#             K_C,V_C=cross_attention_encoder(Ecout)\n",
    "#             #K_C,V_C=cross_attention_encoder(Ect1)\n",
    "\n",
    "#             clip_value = 1500\n",
    "#             #print(\"K_C.shape\",K_C.shape)\n",
    "#             #print(\"V_C.shape\",V_C.shape)\n",
    "#             target_d=pad_sequences(y_batch,lenght=words_per_phrase,target_type=\"target\")\n",
    "            \n",
    "#             target_d=[re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', xx) for xx in target_d] \n",
    "#             #print_matrix(target_d)\n",
    "#             tot_loss=0\n",
    "#             counter_correct=0\n",
    "#             ct_max=0\n",
    "#             for step in range(0,inputs_d.shape[0]):\n",
    "#                 inputs_decoder=inputs_d[step]\n",
    "#                 #print([x[step]  for x in target_d])\n",
    "#                 target=cp.array([get_one_hot(x[step], vocabulary) for x in target_d])\n",
    "#                 #print(target)\n",
    "\n",
    "#                 A_mask,Attention_weights_masked,Q_D,K_D,V_D=forward_attention_decoder(inputs_decoder)\n",
    "\n",
    "#                 Xd,Dt1,mu_d,var_d,N_d=decoder_first_residual_and_norm(A_mask,inputs_decoder)\n",
    "\n",
    "#                 Q_C=cross_attention_decoder(Dt1)\n",
    "#                 #print(\"Q_C.shape\",Q_C.shape)\n",
    "#                 Acr,Attention_weights_cross=cross_attention(Q_C,K_C,V_C,Dt1)\n",
    "\n",
    "#                 Dt2, mu_res,var_res,N_res,Res= cross_attention_residual_and_norm(Acr ,Dt1)\n",
    "                \n",
    "#                 Dout,mu_d2,var_d2,N_d2,Xd2,Xd1,FLd1=fully_connected_layers_decoder(Dt2)\n",
    "\n",
    "#                 #Dt2=Dt2.reshape(Dt2.shape[0],Dt2.shape[1]*Dt2.shape[2])\n",
    "#                 Dout=Dout[:, step, :]\n",
    "#                 #print(Dout.shape,Dt2.shape)\n",
    "#                 SigmaZout=output_layer(Dout)\n",
    "#                 #SigmaZout=output_layer(Dt2)\n",
    "                \n",
    "#                 print(SigmaZout.shape)\n",
    "#                 #counter=print_vocabs(SigmaZout,vocabulary,y_batch,step,target,counter_correct)\n",
    "\n",
    "#                 # if counter>ct_max: \n",
    "#                 #     ct_max=counter\n",
    "#                     #print(counter)\n",
    "#                 Loss=loss_calculation(SigmaZout,target)\n",
    "\n",
    "\n",
    "#                 tot_loss+=Loss\n",
    "\n",
    "\n",
    "#                 #print(\"step\",step,\"Loss\",Loss)\n",
    "\n",
    "\n",
    "\n",
    "#                 dLoss_dZout,dLoss_Dout,dLoss_W0,dLoss_b0=derivate_dout(SigmaZout,target,Dout)\n",
    "#                 #dLoss_dZout,dLoss_Dout,dLoss_W0,dLoss_b0=derivate_dout(SigmaZout,target,Dt2)\n",
    "#                 dLoss_Wfl2d,dLoss_bfl2d,dLoss_Wfl1d,dLoss_bfl1d,DLoss_Dt2=derivate_fully_connected_layers_decoder(dLoss_Dout,Dt2,Xd2,var_d2,mu_d2,N_d2,FLd1,Xd1)\n",
    "#                 #dLoss_Qc,dLoss_Kc,dLoss_Vc,Attention_weights_cross,dLoss_Dt1_a,dLoss_Acr=derivative_cross_attention(DLoss_Dt2,Res,var_res,mu_res,N_res,Attention_weights_cross,K_C,V_C,Q_C,Ect1,Dt1)\n",
    "#                 dLoss_Qc,dLoss_Kc,dLoss_Vc,Attention_weights_cross,dLoss_Dt1_a,dLoss_Acr=derivative_cross_attention(DLoss_Dt2,Res,var_res,mu_res,N_res,Attention_weights_cross,K_C,V_C,Q_C,Ecout,Dt1)\n",
    "\n",
    "#                 dLoss_Kd,dLoss_Qd,dLoss_Vd,dLoss_inputd_a,dLoss_Amask=derivative_attention_decoder(dLoss_Acr,Attention_weights_cross,dLoss_Dt1_a,Attention_weights_masked,Q_D,V_D,K_D,K_C,V_C,Xd,var_d,mu_d,N_d,inputs_decoder)\n",
    "#                 dLoss_inputd,dLoss_dWemb_decoder=derivative_input_decoder(dLoss_Amask,Attention_weights_masked,K_D,V_D,Q_D,dLoss_inputd_a,inputs_decoder)\n",
    "\n",
    "\n",
    "#                 dLoss_W0 = cp.clip(dLoss_W0, -clip_value, clip_value)\n",
    "#                 dLoss_b0 = cp.clip(dLoss_b0, -clip_value, clip_value)\n",
    "#                 dLoss_Wfl2d = cp.clip(dLoss_Wfl2d, -clip_value, clip_value)\n",
    "#                 dLoss_bfl2d = cp.clip(dLoss_bfl2d, -clip_value, clip_value)\n",
    "#                 dLoss_Wfl1d = cp.clip(dLoss_Wfl1d, -clip_value, clip_value)\n",
    "#                 dLoss_bfl1d = cp.clip(dLoss_bfl1d, -clip_value, clip_value)\n",
    "                \n",
    "#                 dLoss_Qc = cp.clip(dLoss_Qc, -clip_value, clip_value)\n",
    "#                 dLoss_Kc = cp.clip(dLoss_Kc, -clip_value, clip_value)\n",
    "#                 dLoss_Vc = cp.clip(dLoss_Vc, -clip_value, clip_value)\n",
    "#                 dLoss_Qd = cp.clip(dLoss_Qd, -clip_value, clip_value)\n",
    "#                 dLoss_Kd = cp.clip(dLoss_Kd, -clip_value, clip_value)\n",
    "#                 dLoss_Vd = cp.clip(dLoss_Vd, -clip_value, clip_value)\n",
    "#                 dLoss_dWemb_decoder = cp.clip(dLoss_dWemb_decoder, -clip_value, clip_value)\n",
    "\n",
    "#                 Wo=Wo-learning_rate*dLoss_W0.T\n",
    "#                 bo=bo-learning_rate*dLoss_b0\n",
    "#                 Wfl2d=Wfl2d-learning_rate*dLoss_Wfl2d\n",
    "#                 bfl2d=bfl2d-learning_rate*dLoss_bfl2d\n",
    "#                 Wfl1d=Wfl1d-learning_rate*dLoss_Wfl1d\n",
    "#                 bfl1d=bfl1d-learning_rate*dLoss_bfl1d\n",
    "#                 Qc=Qc-learning_rate*dLoss_Qc\n",
    "#                 Kc=Kc-learning_rate*dLoss_Kc\n",
    "#                 Vc=Vc-learning_rate*dLoss_Vc\n",
    "#                 Qd=Qd-learning_rate*dLoss_Qd\n",
    "#                 Kd=Kd-learning_rate*dLoss_Kd\n",
    "#                 Vd=Vd-learning_rate*dLoss_Vd\n",
    "#                 inputs_decoder=inputs_decoder-learning_rate*dLoss_dWemb_decoder\n",
    "#                 vocabulary=update_wembedding_decoder(y_batch,inputs_decoder,words_per_phrase,vocabulary)\n",
    "\n",
    "#                 #print_vocabs(SigmaZout,vocabulary,y_batch,step,target,counter_correct)\n",
    "\n",
    "#             #print(\"Loss\",tot_loss/inputs_d.shape[0])\n",
    "#             tot_loss_epoch+=tot_loss/inputs_d.shape[0]\n",
    "\n",
    "\n",
    "#             dLoss_Ecout=derivative_Ecout(Attention_weights_cross,dLoss_Acr,Q_C,V_C)\n",
    "            \n",
    "#             dLoss_dWfl2e,dLoss_dbfl2e,dLoss_Wfl1e,dLoss_bfl1e,dLoss_Ect1=derivate_fully_connected_layers_encoder(dLoss_Ecout,Ect1,Xe2,var_e2,mu_e2,N_e2,FLe1,Xe1)\n",
    "\n",
    "#             #dLoss_dQe,dLoss_dKe,dLoss_dVe,dLoss_inpute_a,dLoss_Ae=derivative_attention_encoder(dLoss_Ect1,Xe,var_e,mu_e,Ne,Attention_weights_e,K_E,V_E,Q_E,inputs_e)\n",
    "#             dLoss_dQe,dLoss_dKe,dLoss_dVe,dLoss_inpute_a,dLoss_Ae=derivative_attention_encoder(dLoss_Ecout,Xe,var_e,mu_e,Ne,Attention_weights_e,K_E,V_E,Q_E,inputs_e)\n",
    "\n",
    "#             dLoss_inpute,dLoss_dWemb_encoder=derivative_input_encoder(dLoss_Ae,Attention_weights_e,K_E,V_E,Q_E,dLoss_inpute_a,inputs_e)\n",
    "\n",
    "#             dLoss_dWfl2e = cp.sum(cp.transpose(dLoss_dWfl2e ,(0,2,1)),axis=0) \n",
    "#             dLoss_dWfl2e = cp.clip(dLoss_dWfl2e, -clip_value, clip_value)\n",
    "#             dLoss_dbfl2e = cp.sum(dLoss_dbfl2e ,axis=0) \n",
    "#             dLoss_dbfl2e = cp.clip(dLoss_dbfl2e, -clip_value, clip_value)\n",
    "#             dLoss_Wfl1e = cp.sum(cp.transpose(dLoss_Wfl1e ,(0,2,1)),axis=0) \n",
    "#             dLoss_Wfl1e = cp.clip(dLoss_Wfl1e, -clip_value, clip_value)\n",
    "#             dLoss_bfl1e = cp.sum(cp.transpose(dLoss_bfl1e ,(0,2,1)),axis=0) \n",
    "#             dLoss_bfl1e = cp.clip(dLoss_bfl1e, -clip_value, clip_value)\n",
    "#             dLoss_dQe = cp.clip(dLoss_dQe, -clip_value, clip_value)\n",
    "#             dLoss_dKe = cp.clip(dLoss_dKe, -clip_value, clip_value)\n",
    "#             dLoss_dVe = cp.clip(dLoss_dVe, -clip_value, clip_value)\n",
    "#             dLoss_dWemb_encoder = cp.clip(dLoss_dWemb_encoder, -clip_value, clip_value)\n",
    "#             Wfl2e=Wfl2e-learning_rate*dLoss_dWfl2e\n",
    "#             bfl2e=bfl2e-learning_rate*dLoss_dbfl2e\n",
    "#             Wfl1e=Wfl1e-learning_rate*dLoss_Wfl1e  #print(dLoss_bfl1e.shape,cp.sum(dLoss_bfl1e,axis=0).shape,bfl1e.shape,cp.sum(cp.transpose(dLoss_bfl1e ,(0,2,1)),axis=0).shape)\n",
    "#             bfl1e=bfl1e-learning_rate*dLoss_bfl1e\n",
    "#             Qe=Qe-learning_rate*dLoss_dQe\n",
    "#             Ke=Ke-learning_rate*dLoss_dKe\n",
    "#             Ve=Ve-learning_rate*dLoss_dVe\n",
    "#             inputs_e=inputs_e-learning_rate*dLoss_dWemb_encoder\n",
    "#             vocabulary=update_wembedding_encoder(X_batch,inputs_e,vocabulary,words_per_phrase)\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#             traceback.print_exc()\n",
    "#             pass\n",
    "#     #inputs_e=inputs_e-learning_rate*dLoss_dWemb_encoder\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
