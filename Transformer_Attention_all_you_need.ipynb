{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89336dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import re\n",
    "import cupy as cp\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "np.set_printoptions(edgeitems=30, linewidth=100000, formatter=dict(float=lambda x: \"%.3g\" % x))\n",
    "\n",
    "\n",
    "def log_time(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()  # Record start time\n",
    "        result = func(*args, **kwargs)  # Execute the wrapped function\n",
    "        end_time = time.time()  # Record end time\n",
    "        elapsed_time = end_time - start_time\n",
    "        # print(f\"Function '{func.__name__}' executed in {elapsed_time:.4f} seconds\")\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "class Helper: \n",
    "    \n",
    "    def get_positional_encoding(self,seq_len, d_model):\n",
    "        \"\"\"\n",
    "        Returns a non-learnable (sinusoidal) positional encoding.\n",
    "\n",
    "\n",
    "        seq_len: Length of the input sequence.\n",
    "        d_model: Dimension of the embeddings.\n",
    "        \"\"\"\n",
    "        pos = cp.arange(seq_len)[:, cp.newaxis]  # Shape: [seq_len, 1]\n",
    "        i = cp.arange(d_model)[cp.newaxis, :]  # Shape: [1, d_model]\n",
    "\n",
    "        angle_rates = 1 / cp.power(10000, (2 * (i // 2)) / cp.float32(d_model))\n",
    "\n",
    "        # Apply sine to even indices, cosine to odd indices\n",
    "        pos_encoding = cp.zeros((seq_len, d_model))\n",
    "        pos_encoding[:, 0::2] = cp.sin(pos * angle_rates[:, 0::2])  # sine on even indices\n",
    "        pos_encoding[:, 1::2] = cp.cos(pos * angle_rates[:, 1::2])  # cosine on odd indices\n",
    "\n",
    "        return pos_encoding\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        # Subtract the max value for numerical stability\n",
    "        max_logits = cp.max(x, axis=-1, keepdims=True)\n",
    "        exp_logits = cp.exp(x - max_logits)\n",
    "        return exp_logits / cp.sum(exp_logits, axis=-1, keepdims=True)\n",
    "\n",
    " \n",
    "    # @log_time\n",
    "    def pad_sequence(self,seq, max_len, pad_value=0):\n",
    "        \"\"\"Pad a sequence with a given value up to max_len.\"\"\"\n",
    "        current_len = seq.shape[0]\n",
    "        pad_width = max_len - current_len\n",
    "        if pad_width > 0:\n",
    "            # Pad sequence with zeros (or any pad_value you provide)\n",
    "            seq = cp.pad(seq, ((0, pad_width), (0, 0)), mode='constant', constant_values=pad_value)\n",
    "        return seq\n",
    "\n",
    "\n",
    "    @log_time\n",
    "    def create_timestaped_input(self,input_d, words_per_phrase):\n",
    "        input_translation = []\n",
    "        for j in range(input_d.shape[0]):\n",
    "            # Create padded sequences\n",
    "            padded_sequences = [self.pad_sequence(input_d[j][0:i], words_per_phrase) for i in range(1, input_d.shape[1] + 1)]\n",
    "            input_translation.append(padded_sequences)\n",
    "        return cp.array(input_translation)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    " \n",
    "    def redimension(self,X):\n",
    "        return cp.concatenate(cp.swapaxes(X, 0, 1), axis=-1)\n",
    "    \n",
    "    @log_time\n",
    "    def create_vocabulary(self,complete_text, name, nlp):\n",
    "        vocabulary_file = f\"data/{name}.pkl\"\n",
    "        try:\n",
    "            with open(vocabulary_file, 'rb') as handle:\n",
    "                vocabulary = pickle.load(handle)\n",
    "                print(\"Vocabulary loaded from file.\")\n",
    "                return vocabulary\n",
    "        except FileNotFoundError:\n",
    "            print(\"Vocabulary file not found, creating a new one.\")\n",
    "        # Use re.findall to split considering punctuation\n",
    "        text = re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', complete_text)\n",
    "\n",
    "        words_list = list(set(text))\n",
    "\n",
    "        vocabulary = dict()\n",
    "\n",
    "        for i, j in enumerate(words_list):\n",
    "            # vocabulary[j]=(jax.random.uniform(jax.random.key(cp.random.randint(10000)),embedding_size),i)\n",
    "            vocabulary[j] = (cp.array(nlp(j).vector), i)\n",
    "             # print(j,len(cp.array(nlp(j).vector)))\n",
    "\n",
    "        # print(vocabulary)\n",
    "        # print(\"Vocabulary size: \", len(vocabulary))\n",
    "        with open(vocabulary_file, 'wb') as handle:\n",
    "            pickle.dump(vocabulary, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        return vocabulary\n",
    "\n",
    "\n",
    "    @log_time\n",
    "    def pad_sequences(self,sentences, lenght, pad_token='[PAD]', target_type=None):\n",
    "        \"\"\"\n",
    "        Pads the input sentences to have the same length by adding [PAD] tokens at the end.\n",
    "        \"\"\"\n",
    "        regex_str=r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n'\n",
    "\n",
    "        if target_type == \"encoder\":\n",
    "            # Split each sentence into words\n",
    "            tokenized_sentences = [[\"[START]\"] + re.findall(regex_str, sentence) + [\"[END]\"] for sentence in sentences]\n",
    "        elif target_type == \"decoder\":\n",
    "            tokenized_sentences = [[\"[START]\"] + re.findall(regex_str, sentence) for sentence in sentences]\n",
    "        elif target_type == \"target\":\n",
    "            tokenized_sentences = [re.findall(regex_str, sentence) + [\"[END]\"] for sentence in sentences]\n",
    "        # print(tokenized_sentences)\n",
    "        if lenght == 0:\n",
    "            # Find the maximum sentence length\n",
    "            max_len = max(len(sentence) for sentence in tokenized_sentences)\n",
    "        else:\n",
    "            max_len = lenght\n",
    "\n",
    "        # Pad each sentence with the [PAD] token to make them of equal length\n",
    "        padded_sentences = [\" \".join(sentence + [pad_token] * (max_len - len(sentence))) for sentence in\n",
    "                            tokenized_sentences]\n",
    "\n",
    "        return padded_sentences\n",
    "\n",
    "    def print_matrix(self,text=\"\",X=None):\n",
    "        for i in X:\n",
    "            print(text,i)\n",
    "    \n",
    "    def create_padding_mask(self,seq):\n",
    "    # Create a mask with 0 for padded tokens and 1 for real tokens\n",
    "        return (seq != 0).astype(cp.float32)[..., None]\n",
    "\n",
    "    @log_time\n",
    "    def generate_input_encoder(self,x_batch, vocabulary_encoder, max_words_per_phrase):\n",
    "\n",
    "\n",
    "        regex_str=r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n'\n",
    "        x_train = self.pad_sequences(x_batch, max_words_per_phrase, target_type=\"encoder\")# here are string\n",
    "        \n",
    "        #print_matrix(x_train) \n",
    "        \n",
    "        xi = []\n",
    "        # print(x_batch)\n",
    "        phrase_vectors_x_i = [re.findall(regex_str, x) for x in x_train]\n",
    "        \n",
    "        phrase_vectors_x = [i[0:max_words_per_phrase] for i in phrase_vectors_x_i]\n",
    "\n",
    "\n",
    "        indices = cp.array([[vocabulary_encoder[word][1] for word in phrase_vector] for phrase_vector in phrase_vectors_x])\n",
    "        mask = (indices != vocabulary_encoder[\"[PAD]\"][1]).astype(cp.float32)[..., None]\n",
    "        #print(phrase_vectors_x) \n",
    "       \n",
    "        # print(\"input_encoder:\")\n",
    "        # self.print_matrix(phrase_vectors_x)\n",
    "        xi = cp.array([[vocabulary_encoder[word][0] for word in phrase_vector] for phrase_vector in phrase_vectors_x])\n",
    "\n",
    "         \n",
    "\n",
    "\n",
    "        return xi,phrase_vectors_x_i,mask\n",
    "    \n",
    "    @log_time\n",
    "    def create_input_encoder(self,X, vocabulary_encoder, max_words_per_phrase, embedding_size):\n",
    "\n",
    "        pos_encoding = self.get_positional_encoding(max_words_per_phrase, embedding_size)\n",
    "        #print(pos_encoding)\n",
    "        inputs_e,input_e_words,mask = self.generate_input_encoder(X, vocabulary_encoder, max_words_per_phrase)\n",
    "        \n",
    "        #print(inputs_e)\n",
    "\n",
    "        inputs_e =inputs_e + pos_encoding\n",
    "        return inputs_e,input_e_words,mask\n",
    "    \n",
    "    def generate_target(self,x_batch, vocabulary, max_words_per_phrase):\n",
    "\n",
    "        regex_str=r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n'\n",
    "\n",
    "        y_target = self.pad_sequences(x_batch, max_words_per_phrase, target_type=\"target\")# here are one string with the padd\n",
    "        \n",
    "        \n",
    "        target_vector = [re.findall(regex_str, x) for x in y_target]\n",
    "\n",
    "        #print(target_vector)\n",
    "        #print_matrix(phrase_vectors_x) \n",
    "        #target_vector = [i[0:max_words_per_phrase] for i in target_vector]\n",
    "        target_vector_i = [i[0:max_words_per_phrase] for i in target_vector]\n",
    "\n",
    "        indices = cp.array([[vocabulary[word][1] for word in phrase_vector] for phrase_vector in target_vector_i])\n",
    "        mask = (indices != vocabulary[\"[PAD]\"][1]).astype(cp.float32)[..., None]\n",
    "\n",
    "        target_vector = cp.array([[self.get_one_hot(i,vocabulary) for i in phrase] for phrase in target_vector_i])\n",
    "    \n",
    "        return target_vector,target_vector_i,mask\n",
    "    \n",
    "    def generate_target_sparse_categorical(self,y_batch, vocabulary, max_words_per_phrase):\n",
    "\n",
    "        regex_str=r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n'\n",
    "\n",
    "        y_target = self.pad_sequences(y_batch, max_words_per_phrase, target_type=\"target\")# here are one string with the padd\n",
    "        \n",
    "        \n",
    "        target_vector = [re.findall(regex_str, x) for x in y_target]\n",
    "\n",
    "        #print(target_vector)\n",
    "        #print_matrix(phrase_vectors_x) \n",
    "        #target_vector = [i[0:max_words_per_phrase] for i in target_vector]\n",
    "        target_vector_i = [i[0:max_words_per_phrase] for i in target_vector]\n",
    "\n",
    "        indices = cp.array([[vocabulary[word][1] for word in phrase_vector] for phrase_vector in target_vector_i])\n",
    "        mask = (indices != vocabulary[\"[PAD]\"][1]).astype(cp.float32)[..., None]\n",
    "\n",
    "\n",
    "        target_vector = cp.array([[vocabulary[i][1] for i in phrase] for phrase in target_vector_i])\n",
    "    \n",
    "        return target_vector,target_vector_i,mask\n",
    "    \n",
    "    @log_time\n",
    "    def create_target(self,X, vocabulary_encoder, max_words_per_phrase): \n",
    "        target,target_words,mask = self.generate_target(X, vocabulary_encoder, max_words_per_phrase) \n",
    "        return target,target_words,mask\n",
    "    \n",
    "    @log_time\n",
    "    def create_target_sparse(self,X, vocabulary_encoder, max_words_per_phrase): \n",
    "        target,target_words,mask = self.generate_target_sparse_categorical(X, vocabulary_encoder, max_words_per_phrase) \n",
    "        return target,target_words,mask\n",
    "    @log_time\n",
    "    def create_decoder_input(self,y_train, embedding_size, max_words_per_phrase, vocabulary_decoder):\n",
    "\n",
    "        decoder_input = self.pad_sequences(y_train, lenght=max_words_per_phrase, target_type=\"decoder\")\n",
    "        #print_matrix(decoder_input)\n",
    "        decoder_input = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', i) for i in decoder_input]\n",
    "        \n",
    "        if max_words_per_phrase == None:\n",
    "            max_words_per_phrase = len(decoder_input[0])\n",
    "\n",
    "        phrase_vectors_y = [i[0:max_words_per_phrase] for i in decoder_input]\n",
    "        indices = cp.array([[vocabulary_decoder[word][1] for word in phrase_vector] for phrase_vector in phrase_vectors_y])\n",
    "        mask = (indices != vocabulary_decoder[\"[PAD]\"][1]).astype(cp.float32)[..., None]\n",
    "        yi = cp.array([[vocabulary_decoder[word][0] for word in phrase_vector] for phrase_vector in phrase_vectors_y])\n",
    "        \n",
    "        pos_encoding = self.get_positional_encoding(max_words_per_phrase, embedding_size)\n",
    "   \n",
    "        yi = yi + pos_encoding\n",
    "      \n",
    "        return yi,phrase_vectors_y,mask\n",
    "    @log_time\n",
    "    def create_decoder_input_teacher_forcing(self,y_train, embedding_size, max_words_per_phrase, vocabulary_decoder):\n",
    "\n",
    "        decoder_input = self.pad_sequences(y_train, lenght=max_words_per_phrase, target_type=\"decoder\")\n",
    "        #print_matrix(decoder_input)\n",
    "        decoder_input = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', i) for i in decoder_input]\n",
    "        \n",
    "        if max_words_per_phrase == None:\n",
    "            max_words_per_phrase = len(decoder_input[0])\n",
    "\n",
    "        phrase_vectors_y = [i[0:max_words_per_phrase] for i in decoder_input]\n",
    "        # for sentence in phrase_vectors_y:\n",
    "        #     print(sentence)\n",
    "        #print_matrix(phrase_vectors_y)\n",
    "        \n",
    "       \n",
    "\n",
    "        # print(\"decoder_input:\")\n",
    "        # self.print_matrix(decoder_input)\n",
    "        yi = cp.array([[vocabulary_decoder[word][0] for word in phrase_vector] for phrase_vector in phrase_vectors_y])\n",
    "        \n",
    "        pos_encoding = self.get_positional_encoding(max_words_per_phrase, embedding_size)\n",
    "        # print(pos_encoding.shape,yi.shape)\n",
    "        yi = yi + pos_encoding\n",
    "        #print_matrix(yi)\n",
    "        # decoder_inputs = cp.array(cp.swapaxes(self.create_timestaped_input(yi, max_words_per_phrase), 0, 1))\n",
    "        \n",
    "        # # decoder_inputs[zero_rows] = vocabulary_decoder[\"[PAD]\"][0]\n",
    "        # for i in range(decoder_inputs.shape[0]):\n",
    "        #     for j in range(decoder_inputs[i].shape[0]):\n",
    "        #         zero_rows = cp.all(decoder_inputs[i][j] == 0, axis=1)\n",
    "\n",
    "        #         decoder_inputs[i][j][zero_rows] = vocabulary_decoder[\"[PAD]\"][0]\n",
    "\n",
    "        # decoder_inputs = cp.array(decoder_inputs)\n",
    "        #print(decoder_inputs[2])\n",
    "        #print(decoder_inputs)\n",
    "        return yi,phrase_vectors_y\n",
    "    # @log_time\n",
    "    def update_wembedding_encoder(self,learning_rate, x_batch, dLoss_dWemb_encoder, vocabulary, max_words_per_phrase):\n",
    "        \n",
    "        x_train = self.pad_sequences(x_batch, max_words_per_phrase, target_type=\"encoder\")\n",
    "\n",
    "        phrase_vectors_x = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', x) for x in x_train]\n",
    "        phrase_vectors_x = [i[:max_words_per_phrase] for i in phrase_vectors_x]\n",
    "\n",
    "        for phrase in range(dLoss_dWemb_encoder.shape[0]):\n",
    "            for position, word in enumerate(phrase_vectors_x[phrase]):\n",
    "                # Retrieve current embedding\n",
    "                current_embedding, index = vocabulary[word]\n",
    "\n",
    "                # Calculate the updated embedding using the gradient\n",
    "                updated_embedding = current_embedding - learning_rate * dLoss_dWemb_encoder[phrase][position]\n",
    "\n",
    "                # Update the vocabulary with the new embedding\n",
    "                vocabulary[word] = (updated_embedding, index)\n",
    "\n",
    "        return vocabulary\n",
    "\n",
    "\n",
    "\n",
    "    # @log_time\n",
    "    def update_wembedding_decoder(self, learning_rate,y_batch, dLoss_dWemb_decoder, vocabulary,max_words_per_phrase):\n",
    "        decoder_input = self.pad_sequences(y_batch, lenght=max_words_per_phrase, target_type=\"decoder\")\n",
    "        decoder_input = [i.split() for i in decoder_input]\n",
    "\n",
    "        if max_words_per_phrase is None:\n",
    "            max_words_per_phrase = len(decoder_input[0])\n",
    "\n",
    "        phrase_vectors_y = [i[:max_words_per_phrase] for i in decoder_input]\n",
    "\n",
    "        for phrase in range(dLoss_dWemb_decoder.shape[0]):\n",
    "            for position, word in enumerate(phrase_vectors_y[phrase]):\n",
    "                # Retrieve current embedding for the word\n",
    "                current_embedding, index = vocabulary[word]\n",
    "\n",
    "                # Apply the gradient update\n",
    "                updated_embedding = current_embedding - learning_rate * dLoss_dWemb_decoder[phrase][position]\n",
    "\n",
    "                # Update the vocabulary with the new embedding\n",
    "                vocabulary[word] = (updated_embedding, index)\n",
    "\n",
    "        return vocabulary\n",
    "\n",
    "  \n",
    "    # @log_time\n",
    "    def get_one_hot(self,word, vocabulary_decoder):\n",
    "        # print(word)\n",
    "        vocab_size = len(vocabulary_decoder)\n",
    "        one_hot_vector = cp.zeros(vocab_size)\n",
    "        one_hot_vector[vocabulary_decoder[word][1]] = 1\n",
    "        # print(vocabulary_decoder[word][1])\n",
    "        # print(np.where(one_hot_vector== 1))\n",
    "        # print(cp.sum(one_hot_vector))\n",
    "        return one_hot_vector\n",
    " \n",
    "    \n",
    "    def log_sparse_entropy(self,ans,target,y_batch,step):\n",
    "        #print(\"target\",target)\n",
    "        #print(\"ans\",ans)\n",
    "        counter_found=0\n",
    "        total_lenght=len(ans)\n",
    "        print(f\"----DECODER--step {step}---\")\n",
    "        self.print_matrix(y_batch)\n",
    "        print(\"target\",target)\n",
    "        indexes=[]\n",
    "        yy=[re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', xx) for xx in y_batch] \n",
    "        for idx, values in enumerate(ans):\n",
    "            max_index = cp.argmax(values)\n",
    "            indexes.append(max_index)\n",
    "             \n",
    "            if max_index==target[idx]:\n",
    "                counter_found+=1\n",
    "            print(f\"{idx + 1} base: {' '.join(yy[idx][0:step+1])} -> {max_index}\")\n",
    "        print(\"indexes\",indexes)\n",
    "        print(\"accuracy batch:\",round(counter_found/total_lenght,2))\n",
    "        \n",
    "    def accruacy_sparse_entropy(self,ans,target):\n",
    "        counter_found=0\n",
    "        total_lenght=len(ans) \n",
    "        for idx, values in enumerate(ans):\n",
    "            max_index = cp.argmax(values) \n",
    "            if max_index==target[idx]:\n",
    "                counter_found+=1\n",
    "             \n",
    "        accuracy_batch_on_step=round(counter_found/total_lenght,2)\n",
    "        return accuracy_batch_on_step\n",
    "    \n",
    "\n",
    "    def print_target_vs_prediction_sparce_loss(self,ans,target): \n",
    "        indexes=[] \n",
    "        for idx, values in enumerate(ans):\n",
    "            max_index = np.argmax(values).item()\n",
    "            indexes.append(max_index) \n",
    "        print(\"target\",target)\n",
    "        print(\"indexes\",indexes)\n",
    "        \n",
    "  \n",
    "def clip_gradient(gradient,threshold):\n",
    "    return cp.clip(gradient, -threshold, threshold)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "class layer_dropout: \n",
    "\n",
    "    def __init__(self,dropout_rate=0.1):\n",
    "        self.dropout_rate=dropout_rate \n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self,X):   \n",
    "        self.mask = (cp.random.rand(*X.shape) > self.dropout_rate)#.astype(cp.float64)\n",
    "        result = X * self.mask \n",
    "        #print(self.mask )\n",
    "        return result\n",
    "    #\n",
    "    def grad(self, X):\n",
    "        # Only pass gradients through neurons that were not dropped out\n",
    "        grad_input = X * self.mask\n",
    "        grad_input = clip_gradient(grad_input, 1)\n",
    "        return grad_input\n",
    "\n",
    "class layer_normalization:\n",
    "    def __init__(self, threshold, epsilon=0.0001):\n",
    "        self.epsilon = epsilon\n",
    "        self.mu = 0\n",
    "        self.var = 0\n",
    "        self.N = None\n",
    "        self.beta = None\n",
    "        self.alpha = None\n",
    "        self.clipping_threshold = threshold\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.epsilonopt = 1e-8\n",
    "        self.t = 0 \n",
    "        \n",
    " \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.N = x.shape[-1]\n",
    "        \n",
    "        # Initialize parameters if not done\n",
    "        if self.alpha is None:\n",
    "            self.alpha = cp.ones(self.N)\n",
    "            self.beta = cp.zeros(self.N)\n",
    "            self.m_Wbeta = cp.zeros_like(self.beta)\n",
    "            self.v_Wbeta = cp.zeros_like(self.beta)\n",
    "            self.m_Walpha = cp.zeros_like(self.alpha)\n",
    "            self.v_Walpha = cp.zeros_like(self.alpha)\n",
    "\n",
    "        # Forward computation\n",
    "        self.mu = cp.mean(x, axis=-1, keepdims=True)\n",
    "        self.var = cp.var(x, axis=-1, keepdims=True)\n",
    "        self.std = cp.sqrt(self.var + self.epsilon)\n",
    "        self.x_norm = (x - self.mu) / self.std\n",
    "        \n",
    "        return self.alpha * self.x_norm + self.beta\n",
    "\n",
    "    def dL_dNorm(self):\n",
    "        self.dL_dnorm_ = self.dLoss_dy * self.alpha\n",
    "        return self.dL_dnorm_\n",
    "\n",
    "    def dL_dsigma(self):\n",
    "        self.dL_dsigma_ = (-0.5)*cp.sum(self.dx_norm*(self.x-self.mu)*((self.var+self.epsilon)**(-1.5)),axis=-1,keepdims=True)\n",
    "        return self.dL_dsigma_\n",
    "    \n",
    "    def dSigma_dmu(self):\n",
    "        return (-2/self.N)*(self.x-self.mu)\n",
    "    \n",
    "    def dL_dmu(self):\n",
    "        a = -1*cp.sum(self.dx_norm*(1/cp.sqrt(self.var+self.epsilon)),axis=-1,keepdims=True)\n",
    "        b = -2*self.dL_dsigma_*cp.sum((1/self.N)*(self.x-self.mu),axis=-1,keepdims=True)\n",
    "        self.dL_dmu_=a+b\n",
    "        return self.dL_dmu_\n",
    "         \n",
    "    def dL_dx(self): \n",
    "        self.dL_dx_=self.dx_norm*self.dNorm_dx()+self.dL_dsigma()*self.dSigma_dx()+self.dL_dmu()*(1/self.N)\n",
    "        return self.dL_dx_\n",
    "    \n",
    "    def dNorm_dx(self):\n",
    "        return 1/cp.sqrt(self.var+self.epsilon)\n",
    "\n",
    "    def dSigma_dx(self):\n",
    "        return (2/self.N)*(self.x-self.mu)\n",
    "         \n",
    "    def backpropagation(self, dLoss_dy):\n",
    "        self.dLoss_dy = dLoss_dy\n",
    "        self.dx_norm = dLoss_dy * self.alpha \n",
    "        return self.dL_dx()\n",
    "    \n",
    "   \n",
    "    def dL_dalpha(self):\n",
    "        result = self.dLoss_dy * self.x_norm\n",
    "        result=cp.sum(cp.sum(result,axis=0),axis=0)\n",
    "        #print(self.dLoss_dy.shape,self.alpha.shape)\n",
    "        #result = clip_gradient(result, self.clipping_threshold)\n",
    "        return result\n",
    "\n",
    "    def dL_dbeta(self):\n",
    "        result = cp.sum(cp.sum(self.dLoss_dy,axis=0),axis=0)\n",
    "        #print(self.dLoss_dy.shape,self.beta.shape)\n",
    "        #result = clip_gradient(result, self.clipping_threshold)\n",
    "        return result\n",
    "     \n",
    "\n",
    "    \n",
    "    \n",
    "  \n",
    "    \n",
    "    def params_update(self, learning_rate):\n",
    "\n",
    "         \n",
    "        self.t += 1\n",
    "        \n",
    "        # Update beta\n",
    "        dbeta = self.dL_dbeta()\n",
    "        self.m_Wbeta = self.beta1 * self.m_Wbeta + (1 - self.beta1) * dbeta\n",
    "        self.v_Wbeta = self.beta2 * self.v_Wbeta + (1 - self.beta2) * cp.square(dbeta)\n",
    "        \n",
    "        m_W_hat = self.m_Wbeta / (1 - self.beta1 ** self.t)\n",
    "        v_W_hat = self.v_Wbeta / (1 - self.beta2 ** self.t)\n",
    "        self.beta -= learning_rate * m_W_hat / (cp.sqrt(v_W_hat) + self.epsilonopt)\n",
    "        \n",
    "        # Update alpha\n",
    "        dalpha = self.dL_dalpha()\n",
    "        self.m_Walpha = self.beta1 * self.m_Walpha + (1 - self.beta1) * dalpha\n",
    "        self.v_Walpha = self.beta2 * self.v_Walpha + (1 - self.beta2) * cp.square(dalpha)\n",
    "        \n",
    "        m_W_hat = self.m_Walpha / (1 - self.beta1 ** self.t)\n",
    "        v_W_hat = self.v_Walpha / (1 - self.beta2 ** self.t)\n",
    "        self.alpha -= learning_rate * m_W_hat / (cp.sqrt(v_W_hat) + self.epsilonopt)\n",
    "\n",
    "       \n",
    "        \n",
    "class linear_layer: \n",
    "    def __init__(self,input_size,output_size,out=False,only_weights=False,threshold=1):\n",
    "\n",
    "        variance = 2 / (input_size + output_size)  # Variance for Glorot normal initializer\n",
    "       \n",
    "        self.W = cp.random.normal(0, cp.sqrt(variance), (input_size, output_size))\n",
    "          \n",
    "        if not only_weights:\n",
    "            self.b = cp.random.normal(0, cp.sqrt(variance), (output_size,))\n",
    "        \n",
    "        self.clipping_threshold = threshold\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.epsilon = 1e-8\n",
    "        self.t = 0  # Time step for Adam updates\n",
    "        \n",
    "        # Initialize first (m) and second (v) moment vectors for weights and biases\n",
    "        self.m_W = cp.zeros_like(self.W)\n",
    "        self.v_W = cp.zeros_like(self.W)\n",
    "        if not only_weights:\n",
    "            self.m_b = cp.zeros_like(self.b)\n",
    "            self.v_b = cp.zeros_like(self.b)\n",
    "      \n",
    "    def forward(self,x): \n",
    "        self.x=x\n",
    "        Xout = cp.matmul(x, self.W) + self.b \n",
    "        return Xout\n",
    "    \n",
    "    def forward_weights_only(self,x): \n",
    "        self.x=x\n",
    "        Xout = cp.matmul(x, self.W) \n",
    "        return Xout\n",
    "     \n",
    "    def grad(self,dL_dy):\n",
    "        self.dL_dy = dL_dy\n",
    "        # print(\"self.dL_dy\",self.dL_dy)\n",
    "        return self.dL_dy@self.W.T\n",
    "    \n",
    "    def dLoss_dW(self):\n",
    "        return cp.mean(cp.transpose(self.dL_dy,(0,2,1))@self.x,axis=0).T\n",
    "    \n",
    "    def dLoss_db(self):\n",
    "        return cp.mean(cp.mean(self.dL_dy,axis=0))\n",
    "\n",
    "    def update_weights(self,learning_rate):\n",
    "        dW = self.dLoss_dW()\n",
    "        db = self.dLoss_db()  \n",
    "\n",
    "        # Increment time step\n",
    "        self.t += 1\n",
    "\n",
    "        # Update first moment (m) and second moment (v) for weights\n",
    "        self.m_W = self.beta1 * self.m_W + (1 - self.beta1) * dW\n",
    "        self.v_W = self.beta2 * self.v_W + (1 - self.beta2) * cp.square(dW)\n",
    "        # Correct bias in first and second moment for weights\n",
    "        m_W_hat = self.m_W / (1 - self.beta1 ** self.t)\n",
    "        v_W_hat = self.v_W / (1 - self.beta2 ** self.t)\n",
    "        # Update weights using Adam\n",
    "        self.W -= learning_rate * m_W_hat / (cp.sqrt(v_W_hat) + self.epsilon)\n",
    "        \n",
    "        \n",
    "        self.m_b = self.beta1 * self.m_b + (1 - self.beta1) * db\n",
    "        self.v_b = self.beta2 * self.v_b + (1 - self.beta2) * cp.square(db)\n",
    "        # Correct bias in first and second moment for biases\n",
    "        m_b_hat = self.m_b / (1 - self.beta1 ** self.t)\n",
    "        v_b_hat = self.v_b / (1 - self.beta2 ** self.t)\n",
    "        # Update biases using Adam\n",
    "         \n",
    "        #print(up.shape,self.b.shape,db.shape)\n",
    "        self.b -= learning_rate * m_b_hat / (cp.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "    def update_weights_only(self,learning_rate):\n",
    "        dW = self.dLoss_dW()\n",
    "\n",
    "        # Increment time step\n",
    "        self.t += 1\n",
    "\n",
    "        # Update first moment (m) and second moment (v) for weights only\n",
    "        self.m_W = self.beta1 * self.m_W + (1 - self.beta1) * dW\n",
    "        self.v_W = self.beta2 * self.v_W + (1 - self.beta2) * cp.square(dW)\n",
    "        # Correct bias in first and second moment for weights\n",
    "        m_W_hat = self.m_W / (1 - self.beta1 ** self.t)\n",
    "        v_W_hat = self.v_W / (1 - self.beta2 ** self.t)\n",
    "        # Update weights using Adam\n",
    "        self.W -= learning_rate * m_W_hat / (cp.sqrt(v_W_hat) + self.epsilon)\n",
    "        \n",
    "class AdamOptimize:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class fully_connected_block:\n",
    "    def __init__(self,embedding_size,hidden_size,clipping_threshold):\n",
    "        self.embedding_size=embedding_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.linear_layer_1=linear_layer(self.embedding_size,self.hidden_size,threshold=clipping_threshold)\n",
    "        self.linear_layer_2=linear_layer(self.hidden_size,self.embedding_size,threshold=clipping_threshold)\n",
    "        self.dropout=layer_dropout()\n",
    "        self.ReLu=ReLu_layer()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x_1=self.linear_layer_1.forward(x)\n",
    "        x_1_r=self.ReLu.forward(x_1)\n",
    "        #x_1_rd=self.dropout.forward(x_1_r)\n",
    "        x_2=self.linear_layer_2.forward(x_1_r)\n",
    "        return x_2\n",
    "    \n",
    "    def grad(self,dL_dy):\n",
    "        dL_dx_1_rd=self.linear_layer_2.grad(dL_dy)\n",
    "        #dL_dx_1_r=self.dropout.grad(dL_dx_1_rd)\n",
    "        dL_dx_1=self.ReLu.backward(dL_dx_1_rd)\n",
    "        dL_dx=self.linear_layer_1.grad(dL_dx_1)\n",
    "        return dL_dx\n",
    "    \n",
    "    def update_weights(self,learning_rate):\n",
    "        self.linear_layer_1.update_weights(learning_rate)\n",
    "        self.linear_layer_2.update_weights(learning_rate)\n",
    "        \n",
    " \n",
    "class ReLu_layer:\n",
    "    def __init__(self,alpha=0.0001):\n",
    "        self.alpha=alpha \n",
    "    def forward_leaky(self,X):\n",
    "        self.X=X\n",
    "        return cp.where(X > 0, X, self.alpha * X)\n",
    "\n",
    "    def forward(self,X): \n",
    "        self.X=X\n",
    "        return cp.maximum(0,self.X)\n",
    "    \n",
    "    def backward(self, dLoss): \n",
    "        # Gradient of ReLU is 1 for x > 0, else 0\n",
    "        dx = dLoss * (self.X > 0)  # Only propagate gradients for inputs > 0\n",
    "        return dx\n",
    "    \n",
    "    def backward_leaky(self, dLoss): \n",
    "        dx = dLoss * cp.where(self.X > 0, 1, self.alpha)  # Gradient: 1 for x > 0, else alpha\n",
    "        return dx\n",
    "\n",
    "class residual_layer:\n",
    "    def __init__(self,threshold):\n",
    "        self.dropout=layer_dropout()\n",
    "        self.normalization=layer_normalization(threshold=threshold)\n",
    "        self.clipping_threshold=threshold\n",
    "\n",
    "\n",
    "    def forward(self,x,sublayer_output): \n",
    "        residual=self.dropout.forward(sublayer_output)+x\n",
    "        result=self.normalization.forward(residual)\n",
    "        return result\n",
    "    \n",
    "    def grad(self, dL_dy):\n",
    "        dl_dNorm = self.normalization.backpropagation(dL_dy)\n",
    "        \n",
    "        # Optional: Scale gradients by 1/sqrt(2) to help with training stability\n",
    "        scaling_factor = 1.0 / np.sqrt(2.0)\n",
    "        sublayer_grad = self.dropout.grad(dl_dNorm) * scaling_factor\n",
    "        residual_grad = dl_dNorm * scaling_factor\n",
    "        \n",
    "        return sublayer_grad, residual_grad\n",
    "    \n",
    "    def update_weights(self,learning_rate):\n",
    "        self.normalization.params_update(learning_rate)\n",
    "\n",
    "\n",
    "      \n",
    "class multihead_attention: \n",
    "    def __init__(self,embedding_size,num_heads,batch_size,threshold):\n",
    "        self.num_heads=num_heads\n",
    "        self.dk=embedding_size//num_heads\n",
    "        self.batch_size=batch_size\n",
    "        self.embedding_size=embedding_size\n",
    "        self.q=linear_layer(self.embedding_size,self.embedding_size,only_weights=True,threshold=threshold)\n",
    "        self.k=linear_layer(self.embedding_size,self.embedding_size,only_weights=True,threshold=threshold)\n",
    "        self.v=linear_layer(self.embedding_size,self.embedding_size,only_weights=True,threshold=threshold)\n",
    "        self.projection_layer=linear_layer(self.embedding_size,self.embedding_size,only_weights=True,threshold=threshold) \n",
    "        self.helper=Helper()\n",
    "        self.clipping_threshold=threshold\n",
    "\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.epsilon = 1e-8\n",
    "        self.t = 0 \n",
    "        self.m_Wq = cp.zeros_like(self.q.W)\n",
    "        self.v_Wq = cp.zeros_like(self.q.W)\n",
    "\n",
    "        self.m_Wk = cp.zeros_like(self.k.W)\n",
    "        self.v_Wk = cp.zeros_like(self.k.W)\n",
    "\n",
    "        self.m_Wv = cp.zeros_like(self.v.W)\n",
    "        self.v_Wv = cp.zeros_like(self.v.W)\n",
    "         \n",
    "    def reshape_heads(self,Q,K,V):\n",
    "        self.Q = cp.swapaxes(cp.array(np.array_split(Q, self.num_heads, axis=2)), 0, 1)\n",
    "        # print(\"Qval.shape: \",Q_E.shape)\n",
    "        self.K = cp.swapaxes(cp.array(np.array_split(K, self.num_heads, axis=2)), 0, 1)\n",
    "        # print(\"Kval.shape: \",K_E.shape)\n",
    "        self.V = cp.swapaxes(cp.array(np.array_split(V, self.num_heads, axis=2)), 0, 1)\n",
    "        #return self.Q,self.K,self.V\n",
    "\n",
    "    def QKV(self,input_q,input_k,input_v): \n",
    "        Q=self.q.forward_weights_only(input_q)\n",
    "        K=self.k.forward_weights_only(input_k)\n",
    "        V=self.v.forward_weights_only(input_v) \n",
    "        self.reshape_heads(Q,K,V)\n",
    "        \n",
    "\n",
    "    def attention_weights(self,mask): \n",
    "         \n",
    "        QKscaled =cp.matmul(self.Q, cp.transpose(self.K, (0, 1, 3, 2))) / cp.sqrt(self.K.shape[-1])  \n",
    "        #print(QKscaled)\n",
    "        if mask is not None:\n",
    "            \n",
    "            # Ensure mask has shape [batch_size, 1, 1, seq_len] for broadcasting\n",
    "            mask = mask[:, cp.newaxis, :]  # Shape: (batch_size, 1, 1, seq_len)\n",
    "            #print(\"mask padding in attention_weights\",mask)\n",
    "            #print(\"mask.shape-------->\",mask.shape)\n",
    "            # Add a large negative value to masked positions\n",
    "            QKscaled = QKscaled + (mask - 1) * 1e9\n",
    "        self.Attention_weights = self.helper.softmax(QKscaled)\n",
    "        #print(\"attention weights\")\n",
    "        #print(self.Attention_weights)\n",
    "         \n",
    "\n",
    "    def forward_attention(self,input_q,input_k,input_v,mask_padding): \n",
    "        self.input_q=input_q\n",
    "        self.input_k=input_k\n",
    "        self.input_v=input_v\n",
    "        #print(\"input_q.shape\",input_q.shape)\n",
    "        self.QKV(input_q,input_k,input_v)\n",
    "        self.attention_weights(mask_padding)\n",
    "        Attention = cp.matmul(self.Attention_weights, self.V) \n",
    "        Attention = cp.array([cp.concatenate(Attention[i], axis=1) for i in range(self.batch_size)]) \n",
    "        Output=self.projection_layer.forward_weights_only(Attention) \n",
    "        return Output\n",
    "    \n",
    "    def forward_masked_attention(self,input_q,input_k,input_v,mask_size,mask_padding):\n",
    "        self.input_q=input_q\n",
    "        self.input_k=input_k\n",
    "        self.input_v=input_v \n",
    "        self.QKV(input_q,input_k,input_v)\n",
    "        self.attention_weights_masked(mask_size,mask_padding)\n",
    "        Attention = cp.matmul(self.Attention_weights, self.V) \n",
    "        Attention = cp.array([cp.concatenate(Attention[i], axis=1) for i in range(self.batch_size)]) \n",
    "        Output=self.projection_layer.forward_weights_only(Attention) \n",
    "        return Output\n",
    "    \n",
    "    def attention_weights_masked(self,mask_size,mask_padding):\n",
    "        #mask_size =  words_per_phrase \n",
    "\n",
    "        QKscaled = cp.matmul(self.Q, cp.transpose(self.K, (0, 1, 3, 2))) / cp.sqrt(self.K.shape[-1])\n",
    "        mask = cp.tril(cp.ones((mask_size, mask_size)))  # (9, 9) lower triangular matrix\n",
    "        mask[mask == 0]=-cp.inf  # Set future tokens to -inf\n",
    "        mask[mask == 1]=0  # Set allowed tokens to 0\n",
    "        self.mask = mask.reshape(1, 1, mask_size, mask_size)\n",
    "        if mask_padding is not None:\n",
    "            \n",
    "            # Ensure mask has shape [batch_size, 1, 1, seq_len] for broadcasting\n",
    "            maskpad = mask_padding[:, cp.newaxis, :]  # Shape: (batch_size, 1, 1, seq_len)\n",
    "            #print(\"mask padding in masked attention\",mask_padding)\n",
    "            #print(\"mask.shape-------->\",maskpad.shape)\n",
    "            # Add a large negative value to masked positions\n",
    "            QKscaled = QKscaled + (maskpad - 1) * 1e9\n",
    "        QKscaled = QKscaled + self.mask\n",
    "       \n",
    "        self.Attention_weights = self.helper.softmax(QKscaled)\n",
    "        \n",
    "    \n",
    "    def diffQi(self,dAttention):\n",
    "        dAttention_weights = self.Attention_weights * (1 - self.Attention_weights) \n",
    "        dLoss_dX=cp.transpose(dAttention, (0, 2, 1)) @ (self.helper.redimension(dAttention_weights @ (self.K * self.V) / cp.sqrt(self.K.shape[-1]))*self.input_q)\n",
    "        self.dLoss_Qi= cp.sum(dLoss_dX,axis=0)\n",
    "    \n",
    "    def diffKi(self,dAttention):\n",
    "        dAttention_weights = self.Attention_weights * (1 - self.Attention_weights) \n",
    "        X = cp.swapaxes(cp.array(cp.array_split(self.input_k, self.num_heads, axis=2)), 0, 1) \n",
    "         \n",
    "        dLoss_dX = cp.transpose(dAttention, (0, 2, 1)) @ self.helper.redimension(\n",
    "            (dAttention_weights * (self.Q @ cp.transpose(self.V, (0, 1, 3, 2))) @ X) / cp.sqrt(self.K.shape[-1])) \n",
    "        self.dLoss_Ki= cp.sum(dLoss_dX,axis=0)\n",
    "    \n",
    "    def diffVi(self,dAttention):\n",
    "        self.dLoss_Vi = cp.sum(cp.sum(cp.transpose(cp.expand_dims(dAttention, axis=1), (0, 1, 3, 2)) @ (\n",
    "                self.Attention_weights @ cp.expand_dims(self.input_v, axis=1)), axis=1), axis=0)\n",
    "       \n",
    "\n",
    "\n",
    "    def diffKInput(self,dAttention):\n",
    "        dAttention_weights = self.Attention_weights * (1 - self.Attention_weights) \n",
    "         \n",
    "        \n",
    "        A = self.helper.redimension(self.V)@self.k.W\n",
    "         \n",
    "        B = self.helper.redimension(dAttention_weights@self.K)\n",
    "         \n",
    "        C=cp.transpose(dAttention,(0,2,1))@B\n",
    "       \n",
    "        dLoss_KI=cp.transpose((C@cp.transpose(A,(0,2,1))),(0,2,1))\n",
    "        #print(\"dLoss_KI.shape\",dLoss_KI.shape)\n",
    "        \n",
    "        return dLoss_KI\n",
    "    \n",
    "    def diffVInput(self,dAttention):\n",
    "        dLoss_V_E = cp.transpose(\n",
    "        cp.mean(cp.transpose(cp.expand_dims(dAttention, axis=1), (0, 1, 3, 2)) @ self.Attention_weights, axis=1), (0, 2, 1))\n",
    "        dLossVI = dLoss_V_E @ self.v.W\n",
    "        #print(\"dLossVI.shape\",dLossVI.shape)\n",
    "        return dLossVI\n",
    "    \n",
    "    def diffQInput(self,dAttention):\n",
    "        dAttention_weights = self.Attention_weights * (1 - self.Attention_weights) \n",
    "        \n",
    "         \n",
    "        A1=self.helper.redimension(dAttention_weights @ (self.K*self.V / cp.sqrt(self.K.shape[-1])))@self.q.W\n",
    "   \n",
    "        dLoss_QI=dAttention*A1\n",
    "        #print(\"dLoss_QI.shape\",dLoss_QI.shape)\n",
    "        return dLoss_QI\n",
    "    \n",
    "    def grad(self,dL_dy): \n",
    "        self.dLoss_dAcr=self.projection_layer.grad(dL_dy)\n",
    "        self.diffQi(self.dLoss_dAcr)\n",
    "        self.diffVi(self.dLoss_dAcr)\n",
    "        self.diffKi(self.dLoss_dAcr)\n",
    "\n",
    "        dLoss_KI=self.diffKInput(self.dLoss_dAcr)\n",
    "        \n",
    "        dLoss_QI=self.diffQInput(self.dLoss_dAcr)\n",
    "        \n",
    "        dLoss_VI=self.diffVInput(self.dLoss_dAcr)\n",
    "       \n",
    "        return dLoss_QI,dLoss_KI,dLoss_VI\n",
    "\n",
    "    def update_weights(self,learning_rate):\n",
    "        \n",
    "\n",
    "        # Increment time step\n",
    "        self.t += 1\n",
    "\n",
    "        # Update first moment (m) and second moment (v) for weights\n",
    "        self.m_Wq = self.beta1 * self.m_Wq + (1 - self.beta1) * self.dLoss_Qi\n",
    "        self.v_Wq = self.beta2 * self.v_Wq + (1 - self.beta2) * cp.square(self.dLoss_Qi)\n",
    "        # Correct bias in first and second moment for weights\n",
    "        m_W_hat = self.m_Wq / (1 - self.beta1 ** self.t)\n",
    "        v_W_hat = self.v_Wq / (1 - self.beta2 ** self.t)\n",
    "        # Update weights using Adam\n",
    "        self.q.W -= learning_rate * m_W_hat / (cp.sqrt(v_W_hat) + self.epsilon)\n",
    "        \n",
    "        \n",
    "        self.m_Wk = self.beta1 * self.m_Wk + (1 - self.beta1) * self.dLoss_Ki\n",
    "        self.v_Wk = self.beta2 * self.v_Wk + (1 - self.beta2) * cp.square(self.dLoss_Ki)\n",
    "        # Correct bias in first and second moment for weights\n",
    "        m_W_hat = self.m_Wk / (1 - self.beta1 ** self.t)\n",
    "        v_W_hat = self.v_Wk / (1 - self.beta2 ** self.t)\n",
    "        # Update weights using Adam\n",
    "        self.k.W -= learning_rate * m_W_hat / (cp.sqrt(v_W_hat) + self.epsilon)\n",
    "\n",
    "        self.m_Wv = self.beta1 * self.m_Wv + (1 - self.beta1) * self.dLoss_Vi\n",
    "        self.v_Wv = self.beta2 * self.v_Wv + (1 - self.beta2) * cp.square(self.dLoss_Vi)\n",
    "        # Correct bias in first and second moment for weights\n",
    "        m_W_hat = self.m_Wv / (1 - self.beta1 ** self.t)\n",
    "        v_W_hat = self.v_Wv / (1 - self.beta2 ** self.t)\n",
    "        # Update weights using Adam\n",
    "        self.v.W -= learning_rate * m_W_hat / (cp.sqrt(v_W_hat) + self.epsilon)\n",
    "\n",
    "\n",
    "        self.projection_layer.update_weights_only(learning_rate)\n",
    "        # self.q.W= self.q.W-self.dLoss_Qi*learning_rate\n",
    "        # self.k.W= self.k.W-self.dLoss_Ki*learning_rate\n",
    "        # self.v.W= self.v.W-self.dLoss_Vi*learning_rate\n",
    "        # print(\"self.q.W\",self.q.W)\n",
    "        # print(\"self.k.W\",self.k.W)\n",
    "        # print(\"self.v.W\",self.v.W)\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_punctuation(input_str):\n",
    "    return re.sub(r'[^\\w\\s]', '', input_str)\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self,embedding_size,num_heads,linear_layer_size,learning_rate,batch_size,words_per_phrase,clipping_threshold):\n",
    "        self.batch_size=batch_size\n",
    "        self.clipping_threshold=clipping_threshold\n",
    "        self.words_per_phrase=words_per_phrase\n",
    "        self.num_heads=num_heads\n",
    "        self.linear_layer_size=linear_layer_size\n",
    "        self.embedding_size=embedding_size\n",
    "        self.fully_connected_block=fully_connected_block(self.embedding_size,linear_layer_size,clipping_threshold=clipping_threshold)\n",
    "        self.multihead_attention_encoder=multihead_attention(num_heads=2,embedding_size=embedding_size,batch_size=batch_size,threshold=clipping_threshold)\n",
    "        self.residual_layer_1=residual_layer(clipping_threshold)\n",
    "        self.residual_layer_2=residual_layer(clipping_threshold) \n",
    "        self.learning_rate=learning_rate\n",
    "         \n",
    "        self.helper=Helper()\n",
    "     \n",
    "\n",
    "    def forward(self,inputs_e,mask_e):\n",
    "        self.inputs_e=inputs_e \n",
    "\n",
    "        PrjAe=self.multihead_attention_encoder.forward_attention(inputs_e,inputs_e,inputs_e,mask_e)\n",
    "        #print(\"PrjAe\",PrjAe.shape)\n",
    "        Ect1=self.residual_layer_1.forward(PrjAe,inputs_e) \n",
    "\n",
    "        FLe2=self.fully_connected_block.forward(Ect1) \n",
    "\n",
    "        Ecout=self.residual_layer_2.forward(FLe2,Ect1)\n",
    "\n",
    "        return Ecout\n",
    "    \n",
    "    def backpropagation(self,dL_Ecout): \n",
    "        \n",
    "        dL_dFLe2,dL_dEct1_residual=self.residual_layer_2.grad(dL_Ecout)\n",
    "        \n",
    "        #print(\"dL_dFLe2\",dL_dFLe2)\n",
    "        #print(\"dL_dEct1_residual\",dL_dFLe2)\n",
    "        \n",
    "\n",
    "        dL_dEct1=self.fully_connected_block.grad(dL_dFLe2)+dL_dEct1_residual\n",
    "        #print(\"dL_dEct1\",dL_dEct1)\n",
    "        \n",
    "        dL_dPrjAe,dL_inputs_e_residual=self.residual_layer_1.grad(dL_dEct1)\n",
    "        #print(\"dL_dPrjAe\",dL_dPrjAe)\n",
    "        #print(\"dL_inputs_e_residual\",dL_inputs_e_residual)\n",
    "        dL_inputs_e_q,dL_inputs_e_k,dL_inputs_e_v=self.multihead_attention_encoder.grad(dL_dPrjAe)\n",
    "        \n",
    "        dL_inputs_e=dL_inputs_e_residual+dL_inputs_e_q+dL_inputs_e_k+dL_inputs_e_v\n",
    "\n",
    "        #dL_inputs_e=clip_gradient(dL_inputs_e,self.clipping_threshold)\n",
    "        #print(\"dL_inputs_e\",dL_inputs_e_residual)\n",
    "        dLoss_dWemb_encoder = dL_inputs_e * self.inputs_e\n",
    "        return dL_inputs_e,dLoss_dWemb_encoder\n",
    "\n",
    "\n",
    "    def update_weights(self,learning_rate,vocabulary):\n",
    "        self.residual_layer_2.update_weights(learning_rate)\n",
    "        self.fully_connected_block.update_weights(learning_rate)\n",
    "        self.residual_layer_1.update_weights(learning_rate)\n",
    "        self.multihead_attention_encoder.update_weights(learning_rate)\n",
    "        \n",
    "        # input_e=self.inputs_e-learning_rate*dLoss_dWemb_encoder\n",
    "        # vocabulary=self.helper.update_wembedding_encoder(X_batch,input_e,vocabulary,self.words_per_phrase)\n",
    "        return vocabulary\n",
    "        \n",
    "\n",
    "  \n",
    "      \n",
    "     \n",
    "class Decoder:\n",
    "    def __init__(self,embedding_size,num_heads,linear_layer_size,learning_rate,batch_size,words_per_phrase,clipping_threshold):\n",
    "        self.words_per_phrase=words_per_phrase\n",
    "        self.clipping_threshold=clipping_threshold\n",
    "        self.batch_size=batch_size\n",
    "        self.num_heads=num_heads \n",
    "        self.linear_layer_size=linear_layer_size\n",
    "        self.embedding_size=embedding_size  \n",
    "        self.multihead_cross_attention=multihead_attention(num_heads=num_heads,embedding_size=embedding_size,batch_size=batch_size,threshold=clipping_threshold)\n",
    "        self.multihead_attention_decoder=multihead_attention(num_heads=num_heads,embedding_size=embedding_size,batch_size=batch_size,threshold=clipping_threshold) \n",
    "        self.learning_rate=learning_rate\n",
    "        self.helper=Helper() \n",
    "        self.residual_layer_1=residual_layer(threshold=clipping_threshold)\n",
    "        self.residual_layer_2=residual_layer(threshold=clipping_threshold) \n",
    "        self.residual_layer_3=residual_layer(threshold=clipping_threshold) \n",
    "        self.fully_connected_block=fully_connected_block(self.embedding_size,linear_layer_size,clipping_threshold=clipping_threshold)\n",
    "        \n",
    "\n",
    "    def forward(self,inputs_decoder,Ecout,mask_d,mask_e):\n",
    "        self.inputs_decoder=inputs_decoder \n",
    "        \n",
    "        PrjA_mask=self.multihead_attention_decoder.forward_masked_attention(inputs_decoder,inputs_decoder,inputs_decoder,mask_size=inputs_decoder.shape[1],mask_padding=mask_d)\n",
    " \n",
    "        Dt1=self.residual_layer_1.forward(self.inputs_decoder,PrjA_mask)\n",
    "        #print(\"cross attention multihead calc attention weights\")\n",
    "        PrjAcr=self.multihead_cross_attention.forward_attention(Dt1,Ecout,Ecout,mask_padding=mask_e)\n",
    "   \n",
    "        Dt2=self.residual_layer_2.forward(PrjAcr,Dt1)\n",
    " \n",
    "        FLd2=self.fully_connected_block.forward(Dt2)\n",
    "      \n",
    "        Dout=self.residual_layer_3.forward(FLd2,Dt2)  \n",
    "\n",
    "        return Dout\n",
    "    \n",
    "    # def output(self,Dout):\n",
    "    #     SigmaZout=self.helper.softmax(self.final_projection_layer.forward(Dout))\n",
    "    #     return SigmaZout\n",
    "\n",
    "\n",
    "    def backpropagation(self,dL_dDout): \n",
    "\n",
    "        dL_FLd2,dL_Dt2_residual=self.residual_layer_3.grad(dL_dDout)\n",
    "\n",
    "        dL_Dt2=self.fully_connected_block.grad(dL_FLd2)+dL_Dt2_residual\n",
    "\n",
    "        dL_PrjAcr,dL_Dt1_residual=self.residual_layer_2.grad(dL_Dt2)\n",
    "\n",
    "        dL_Dt1_q,dL_DEcout_k,dL_DEcout_v=self.multihead_cross_attention.grad(dL_PrjAcr)\n",
    "\n",
    "        dL_Dt1=dL_Dt1_residual+dL_Dt1_q\n",
    "\n",
    "        dL_PrjA_mask,dL_inputs_decoder_residual=self.residual_layer_1.grad(dL_Dt1)\n",
    "\n",
    "        dL_inputs_decoder_q,dL_inputs_decoder_k,dL_inputs_decoder_v=self.multihead_attention_decoder.grad(dL_PrjA_mask)\n",
    "\n",
    "        dL_inputs_decoder=dL_inputs_decoder_residual+dL_inputs_decoder_q+dL_inputs_decoder_k+dL_inputs_decoder_v\n",
    "        \n",
    "        dL_Ecout=dL_DEcout_k+dL_DEcout_v\n",
    "        #dL_inputs_decoder=clip_gradient(dL_inputs_decoder,self.clipping_threshold)\n",
    "        dLoss_dWemb_decoder= dL_inputs_decoder * self.inputs_decoder\n",
    "        \n",
    "        return dL_Ecout,dL_inputs_decoder,dLoss_dWemb_decoder\n",
    "\n",
    "\n",
    "    def update_weights(self,learning_rate,vocabulary):\n",
    "        self.residual_layer_3.update_weights(learning_rate)\n",
    "        self.fully_connected_block.update_weights(learning_rate)\n",
    "        self.residual_layer_2.update_weights(learning_rate)\n",
    "        self.multihead_cross_attention.update_weights(learning_rate)\n",
    "        self.residual_layer_1.update_weights(learning_rate)\n",
    "        self.multihead_attention_decoder.update_weights(learning_rate)\n",
    "        #dLoss_dWemb_decoder=clip_gradient(dLoss_dWemb_decoder,self.clipping_threshold)\n",
    "        # input_d=self.inputs_decoder-learning_rate*dLoss_dWemb_decoder\n",
    "        # vocabulary=self.helper.update_wembedding_decoder(y_batch,input_d,self.words_per_phrase,vocabulary) \n",
    "        return vocabulary\n",
    "\n",
    "class output_stack:\n",
    "    def __init__(self,embedding_size,vocabulary_size,threshold,temperature=1):\n",
    "        self.final_projection_layer=linear_layer(embedding_size,vocabulary_size,threshold=threshold,out=True)\n",
    "        self.clipping_threshold=threshold\n",
    "        self.temperature=temperature\n",
    "        \n",
    "    def softmax(self, x):\n",
    "        max_logits = cp.max(x, axis=-1, keepdims=True)\n",
    "        exp_logits = cp.exp((x - max_logits) / self.temperature)  # Apply temperature\n",
    "        return exp_logits / cp.sum(exp_logits, axis=-1, keepdims=True)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        SoftmaxOutput=self.softmax(self.final_projection_layer.forward(x)) \n",
    "        return SoftmaxOutput\n",
    "\n",
    "    \n",
    "    # def cross_entropy_loss(self,SigmaZout, target):\n",
    "    #     epsilon = 1e-12  # Small constant to avoid log(0)\n",
    "    #     SigmaZout = cp.clip(SigmaZout, epsilon, 1 - epsilon)  # Clipping predictions\n",
    "    #     return -cp.sum(target * cp.log(SigmaZout), axis=1).mean() \n",
    "    def cross_entropy_loss(self, y_pred, y_true):\n",
    "        epsilon = 1e-12\n",
    "        return -np.mean(np.sum(y_true * np.log(y_pred + epsilon), axis=-1))\n",
    "    \n",
    "    def grad_cross_entropy(self,softmax_output,target):\n",
    "        dL_dZ =softmax_output-target\n",
    "        dL_dDout=self.final_projection_layer.grad(dL_dZ)  \n",
    "        #dL_dDout=clip_gradient(dL_dDout,self.clipping_threshold)\n",
    "        return dL_dDout\n",
    "    \n",
    "    def sparse_categorical_crossentropy(self, probabilities, labels): \n",
    "        #print(\"probabilities.shape\", probabilities.shape)\n",
    "        #print(\"labels.shape\", labels.shape)\n",
    "        \n",
    "        # Unpack batch and sequence dimensions\n",
    "        batch_size, seq_length = labels.shape\n",
    "        \n",
    "        # Gather correct class probabilities for each position in the batch and sequence\n",
    "        correct_class_probs = probabilities[np.arange(batch_size)[:, None], np.arange(seq_length), labels] \n",
    "        \n",
    "        # Calculate the log loss and average it\n",
    "        loss = -np.log(correct_class_probs + 1e-8)\n",
    "        return np.mean(loss)\n",
    "    \n",
    "    def grad(self,dl_dy):\n",
    "        return self.final_projection_layer.grad(dl_dy)\n",
    "\n",
    "\n",
    "    def grad_sparse_cross_entropy(self, softmax_output, target):\n",
    "        dL_dZ = softmax_output.copy()  # Create a copy of the softmax output\n",
    "        \n",
    "        # Adjust indexing to handle both batch and sequence dimensions\n",
    "        batch_size, seq_length = target.shape\n",
    "        dL_dZ[np.arange(batch_size)[:, None], np.arange(seq_length), target] -= 1\n",
    "        \n",
    "        # Compute gradient through final projection layer\n",
    "        dL_dDout = self.final_projection_layer.grad(dL_dZ)  \n",
    "        # dL_dDout = clip_gradient(dL_dDout, self.clipping_threshold)\n",
    "        return dL_dDout\n",
    " \n",
    "    def update_weights(self,learning_rate):\n",
    "        self.final_projection_layer.update_weights(learning_rate)  \n",
    "\n",
    "\n",
    "\n",
    "class Transformer:\n",
    "    def __init__(self,num_layers,embedding_size,num_heads,fl1_size,learning_rate,batch_size,words_per_phrase_encoder,words_per_phrase_decoder,clipping_threshold,vocabulary):\n",
    "        self.vocabulary=vocabulary\n",
    "        self.clipping_threshold=clipping_threshold\n",
    "        self.EncoderStack = [Encoder(embedding_size,num_heads,fl1_size,learning_rate,batch_size,words_per_phrase_encoder,clipping_threshold) for _ in range(num_layers)]\n",
    "        self.DecoderStack = [Decoder(embedding_size,num_heads,fl1_size,learning_rate,batch_size,words_per_phrase_decoder,clipping_threshold) for _ in range(num_layers)]\n",
    "        self.outout_stack=output_stack(embedding_size,len(vocabulary),threshold=clipping_threshold,temperature=2)\n",
    "\n",
    "    def forward(self,inputs_e,inputs_decoder,X_batch,y_batch,mask_e,mask_d):\n",
    "\n",
    "        self.X_batch=X_batch\n",
    "        self.y_batch=y_batch\n",
    "        #print(\"encoder stack\")\n",
    "        Ecout=self.forward_encoder(inputs_e,mask_e)\n",
    "        #print(\"Ecout.shape\",Ecout.shape)\n",
    "        #(\"decoder stack\")\n",
    "        Dout=self.forward_decoder(Ecout,inputs_decoder,mask_d,mask_e)\n",
    "        #SigmaZout=self.outout_stack.forward(Dout) \n",
    "        return Dout\n",
    "    \n",
    "    def backpropagation(self,dL_dDout):\n",
    "        dL_Ecout,dLoss_dWemb_decoder_tot=self.backpropagation_decoder(dL_dDout)\n",
    "        dL_Ecout,dLoss_dWemb_encoder_tot=self.backpropagation_encoder(dL_Ecout)\n",
    "        return dL_Ecout,dLoss_dWemb_encoder_tot,dLoss_dWemb_decoder_tot\n",
    "\n",
    "  \n",
    "\n",
    "    def forward_encoder(self,inputs_e,mask_e):\n",
    "        for encoder_i in self.EncoderStack:\n",
    "            inputs_e=encoder_i.forward(inputs_e,mask_e)\n",
    "        return inputs_e\n",
    "    \n",
    "    def forward_decoder(self,Ecout,inputs_decoder,mask_d,mask_e):\n",
    "        for decoder_i in self.DecoderStack:\n",
    "            inputs_decoder=decoder_i.forward(inputs_decoder,Ecout,mask_d,mask_e)\n",
    "        return inputs_decoder\n",
    "    \n",
    "    def backpropagation_decoder(self,dL_dDout):\n",
    "        tot_dL_dEcout=0\n",
    "        dLoss_dWemb_decoder_tot=0\n",
    "        for decoder_i in reversed(self.DecoderStack):\n",
    "            dL_Ecout_i,dL_dDout,dLoss_dWemb_decoder=decoder_i.backpropagation(dL_dDout)\n",
    "            tot_dL_dEcout+=dL_Ecout_i\n",
    "            dLoss_dWemb_decoder_tot+=dLoss_dWemb_decoder\n",
    "            self.vocabulary=decoder_i.update_weights(decoder_i.learning_rate,self.vocabulary)\n",
    "        return tot_dL_dEcout,dLoss_dWemb_decoder_tot\n",
    "    \n",
    "    def backpropagation_encoder(self,dL_Ecout): \n",
    "        dLoss_dWemb_encoder_tot=0\n",
    "        for encoder_i in reversed(self.EncoderStack):\n",
    "            dL_Ecout,dLoss_dWemb_encoder=encoder_i.backpropagation(dL_Ecout) \n",
    "            dLoss_dWemb_encoder_tot+=dLoss_dWemb_encoder \n",
    "            self.vocabulary=encoder_i.update_weights(encoder_i.learning_rate,self.vocabulary)\n",
    "        return dL_Ecout,dLoss_dWemb_encoder_tot\n",
    "   \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dae219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "978e23dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "661 Olivia and Olivier are voting for liberals in this election\n",
      "Vocabulary file not found, creating a new one.\n",
      "vocabulary size 2213\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "helper=Helper()\n",
    "def load_x_y_train_plain():\n",
    "    with open('corpus/train.json', 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            dataset = json.load(f)  # Load the JSON data\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    # #Loop through the list and process each dialogue and summary\n",
    "    for data in dataset:\n",
    "        dialogue = data['dialogue']  # Split dialogue into a list of lines\n",
    "        summary = data['summary']\n",
    "\n",
    "        X_train.append(remove_punctuation(dialogue))\n",
    "        y_train.append(remove_punctuation(summary))\n",
    "    return X_train, y_train\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "# with open('data/vocabolary_full.pkl', 'rb') as f:\n",
    "#     vocabulary=pickle.load(f)\n",
    "def filter_train_data(X_train, y_train, to_eliminate):\n",
    "    filtered_X_train = []\n",
    "    filtered_y_train = []\n",
    "\n",
    "    for x, y in zip(X_train, y_train):\n",
    "        if not any(to_eliminate_str in x for to_eliminate_str in to_eliminate):\n",
    "             \n",
    "            filtered_X_train.append(remove_punctuation(x))\n",
    "            filtered_y_train.append(remove_punctuation(y))\n",
    "\n",
    "    return filtered_X_train, filtered_y_train\n",
    "\n",
    "\n",
    "def create_complete_vocabulary(X_train, y_train,max_v,name):\n",
    "    nlp_model = spacy.load('en_core_web_lg')\n",
    "    nlp_model.disable_pipes([\"parser\", \"ner\"])\n",
    "    complete_text_target = ' '.join(y_train)\n",
    "    complete_text_origin = ' '.join(X_train)\n",
    "    complete_text = complete_text_target + \" [START] [PAD] [END] \" + complete_text_origin\n",
    "\n",
    "    vocabulary = helper.create_vocabulary(complete_text, name, nlp_model)\n",
    "    print(\"vocabulary size\", len(vocabulary))\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "X_train, y_train = load_x_y_train_plain()\n",
    "# to_eliminate = [\n",
    "#     \"[I hope I'm not coming off as rude - If I am, I'm sorry. I just thought it would be beneficial for the both of us...]\",\n",
    "#     \"[pulls back the curtain and checks out the window]\",\n",
    "#     \"[hopefully, masses of]\"]\n",
    "# X_train, y_train = filter_train_data(X_train, y_train, to_eliminate)\n",
    "\n",
    "\n",
    "sample = [i for i in range(0,len(y_train))]\n",
    "\n",
    "\n",
    "X_train = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', x) for x in X_train]\n",
    "y_train = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', y) for y in y_train]\n",
    "\n",
    "\n",
    "min_v = 10\n",
    "max_v = 10\n",
    "\n",
    "\n",
    "\n",
    "X_train = [X_train[i] for i in sample if len(y_train[i]) <= max_v and len(y_train[i]) >= min_v]\n",
    "y_train = [y_train[i] for i in sample if len(y_train[i]) <= max_v and len(y_train[i]) >= min_v]\n",
    "\n",
    "\n",
    "# Calculate lengths of the tokenized phrases\n",
    "\n",
    "\n",
    "def plot_lenghts(X_train,y_train):\n",
    "    X_lengths = [len(x) for x in X_train]\n",
    "    y_lengths = [len(y) for y in y_train]\n",
    "    # Plot histograms\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Histogram for X_train lengths\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(X_lengths, bins=20, kde=False)\n",
    "    plt.title('Histogram of X_train Phrase Lengths')\n",
    "    plt.xlabel('Length of Phrases')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Histogram for y_train lengths\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(y_lengths, bins=20, kde=False)\n",
    "    plt.title('Histogram of y_train Phrase Lengths')\n",
    "    plt.xlabel('Length of Phrases')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Display the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "X_train=[i[::-1] for i in y_train]\n",
    "plot_lenghts(X_train,y_train)\n",
    " \n",
    "\n",
    "X_train=[\" \".join(x) for x in X_train]\n",
    "y_train=[\" \".join(y) for y in y_train]\n",
    "\n",
    "print(len(y_train),y_train[0])\n",
    "name=f\"vocabolary_full5i{max_v}{min_v}\"\n",
    "vocabulary=create_complete_vocabulary(X_train, y_train,max_v,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb9a5cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('election this in liberals for voting are Olivier and Olivia',\n",
       " 'Olivia and Olivier are voting for liberals in this election')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0],y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9350b4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(661, 2213)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train),len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a882c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "embedding_size=300\n",
    "fl1_size=2048\n",
    "batch_size=64\n",
    "num_heads=10\n",
    "dropout_rate=0.1 \n",
    "num_batches_per_epoch = len(X_train) // batch_size\n",
    "num_epochs=550\n",
    "tot_loss_epoch=0\n",
    "learning_rate=0.0001\n",
    "clipping_threshold=1e10\n",
    "tot_loss_epoch=0\n",
    "temperature=1\n",
    "encoder_seq_len=50\n",
    "decoder_seq_len=max_v\n",
    "encoder_seq_len=max_v\n",
    "\n",
    "n_layers=1\n",
    "Output_stack=output_stack(embedding_size,len(vocabulary),threshold=clipping_threshold,temperature=2)\n",
    "#encoder=Encoder(embedding_size,num_heads,fl1_size,learning_rate,batch_size,words_per_phrase,clipping_threshold)\n",
    "#decoder=Decoder(embedding_size,num_heads,fl1_size,learning_rate,batch_size,words_per_phrase,clipping_threshold)\n",
    "MyTransformer=Transformer(n_layers,embedding_size,num_heads,fl1_size,learning_rate,batch_size,encoder_seq_len,decoder_seq_len,clipping_threshold,vocabulary)\n",
    "output_linear_layer=linear_layer(embedding_size,len(vocabulary),out=True) \n",
    "accuracies=[0,0]\n",
    "mean_acc=0\n",
    "\n",
    "def print_word_from_vocabulary(word_index,vocabulary): \n",
    "    for word,(vector,position), in vocabulary.items():\n",
    "        if position==word_index:\n",
    "            return word\n",
    "        \n",
    "def training_accuracy_cross_entropy(SigmaZout,target_decoder):\n",
    "    taccuracies=[]\n",
    "    for n in range(SigmaZout.shape[0]): \n",
    "            len_phrase=SigmaZout.shape[1]\n",
    "            counter_beccate=0 \n",
    "            for l in range(SigmaZout.shape[1]): \n",
    "                if np.argmax(SigmaZout[n][l])==np.argmax(target_decoder[n][l]): \n",
    "                    counter_beccate+=1\n",
    "                    #print(np.argmax(target_decoder[n][l]),np.argmax(SigmaZout[n][l]))\n",
    "            phrase_accuracy=counter_beccate/len_phrase\n",
    "            taccuracies.append(phrase_accuracy)\n",
    "    return np.mean(np.array(taccuracies))\n",
    "\n",
    "def training_accuracy_sparse_cross_entropy(SigmaZout,target_decoder,vocabulary):\n",
    "    #print(SigmaZout.shape)\n",
    "    #print(target_decoder.shape)\n",
    "    taccuracies=[]\n",
    "    for n in range(SigmaZout.shape[0]): \n",
    "            len_phrase=SigmaZout.shape[1]\n",
    "            counter_beccate=0 \n",
    "            for l in range(SigmaZout.shape[1]): \n",
    "                if np.argmax(SigmaZout[n][l])==target_decoder[n][l]: \n",
    "                    counter_beccate+=1\n",
    "                    #print(np.argmax(SigmaZout[n][l]),target_decoder[n][l],print_word_from_vocabulary(np.argmax(SigmaZout[n][l]),vocabulary))\n",
    "            #print(\"-------------------------\")\n",
    "            phrase_accuracy=counter_beccate/len_phrase\n",
    "            taccuracies.append(phrase_accuracy)\n",
    "    return np.mean(np.array(taccuracies))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1d29d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbf5a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.0 mean accuracy 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/550: 100%|| 10/10 [00:07<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 7.64039814230191 mean accuracy 0.009062500000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/550: 100%|| 10/10 [00:07<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 7.445014263031245 mean accuracy 0.03578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/550: 100%|| 10/10 [00:07<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 7.319431377683578 mean accuracy 0.04484375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/550: 100%|| 10/10 [00:07<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 7.224556523359388 mean accuracy 0.0415625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/550: 100%|| 10/10 [00:07<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 7.143733972476201 mean accuracy 0.04109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/550: 100%|| 10/10 [00:07<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 7.068418412273802 mean accuracy 0.040781250000000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/550: 100%|| 10/10 [00:07<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 6.998526337050959 mean accuracy 0.044218749999999994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/550: 100%|| 10/10 [00:07<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 6.927174221774266 mean accuracy 0.0453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/550: 100%|| 10/10 [00:07<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 6.859202939840597 mean accuracy 0.048593750000000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/550: 100%|| 10/10 [00:07<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 6.800932777197497 mean accuracy 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/550: 100%|| 10/10 [00:07<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 6.742315691942234 mean accuracy 0.053750000000000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/550: 100%|| 10/10 [00:07<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 6.685344038505361 mean accuracy 0.061562500000000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/550: 100%|| 10/10 [00:07<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 6.63465734780694 mean accuracy 0.06796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/550: 100%|| 10/10 [00:07<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 6.58864423005235 mean accuracy 0.07203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/550: 100%|| 10/10 [00:07<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 6.5449045424774805 mean accuracy 0.075625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/550: 100%|| 10/10 [00:07<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 6.509747165996984 mean accuracy 0.07453125000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/550:  70%|   | 7/10 [00:04<00:02,  1.40it/s]"
     ]
    }
   ],
   "source": [
    " \n",
    "for epoch in range(50):\n",
    "    print(\"Loss\",tot_loss_epoch/num_batches_per_epoch,\"mean accuracy\",np.mean(np.array(accuracies)))#np.mean(np.array(accuracies))\n",
    "    tot_loss_epoch=0\n",
    "    total_accuracy_epoch=0\n",
    "    mean_acc=0\n",
    "    accuracies=[]\n",
    "    for i in tqdm(range(0,num_batches_per_epoch),desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        #try: \n",
    "        start = i * batch_size\n",
    "        end = start + batch_size \n",
    "        X_batch = X_train[start:end]\n",
    "        y_batch = y_train[start:end]  \n",
    "        \n",
    "        inputs_e,input_e_words,mask_e=helper.create_input_encoder(X_batch,vocabulary,encoder_seq_len,embedding_size) \n",
    "        inputs_decoder,inputs_decoder_words,mask_d=helper.create_decoder_input(y_batch,embedding_size,decoder_seq_len,vocabulary) \n",
    "        target_decoder,target_decoder_words,mask_t=helper.create_target_sparse(y_batch,vocabulary,decoder_seq_len) \n",
    "        mask_e=cp.transpose(mask_e,(0,2,1))\n",
    "        mask_d=cp.transpose(mask_d,(0,2,1))\n",
    "        mask_t=cp.transpose(mask_t,(0,2,1))\n",
    "        # print(\"inputs_e.shape\",inputs_e.shape)\n",
    "        # helper.print_matrix(\"input_e_words\",input_e_words)\n",
    "        # helper.print_matrix(\"mask_e\",mask_e)\n",
    "        # helper.print_matrix(\"inputs_decoder_words\",inputs_decoder_words)\n",
    "        # helper.print_matrix(\"mask_d\",mask_d)\n",
    "        # helper.print_matrix(\"target_decoder_words\",target_decoder_words)\n",
    "        # helper.print_matrix(\"mask_t\",mask_t)\n",
    "        # print(\"mask_e.shape\",mask_e.shape)\n",
    "        # print(\"mask_d.shape\",mask_d.shape)\n",
    "        # print(\"mask_t.shape\",mask_t.shape)\n",
    "        #Ecout=encoder.forward(inputs_e)\n",
    "        #Dout=decoder.forward(inputs_decoder,Ecout) \n",
    "\n",
    "        Dout=MyTransformer.forward(inputs_e,inputs_decoder,X_batch,y_batch,mask_e,mask_d) \n",
    "\n",
    "        SigmaZout=Output_stack.forward(Dout)\n",
    "        #print(SigmaZout.shape)\n",
    "        batch_accuracy=training_accuracy_sparse_cross_entropy(SigmaZout,target_decoder,vocabulary)\n",
    "        accuracies.append(batch_accuracy) \n",
    "        \n",
    " \n",
    "        Loss = Output_stack.sparse_categorical_crossentropy(SigmaZout,target_decoder) \n",
    "        #print(\"Loss\",Loss,\"Batch accuracy\",batch_accuracy)\n",
    "        tot_loss_epoch+=Loss\n",
    " \n",
    "        dL_dDout = Output_stack.grad_sparse_cross_entropy(SigmaZout,target_decoder) \n",
    "        \n",
    "        dL_Ecout,dLoss_dWemb_encoder_tot,dLoss_dWemb_decoder_tot=MyTransformer.backpropagation(dL_dDout)\n",
    "        #dL_Ecout,dL_inputs_decoder,dLoss_dWemb_decoder=decoder.backpropagation(dL_dDout)\n",
    "        #dL_inputs_e,dLoss_dWemb_encoder=encoder.backpropagation(dL_Ecout)\n",
    "      \n",
    "        \n",
    "        #_=decoder.update_weights(learning_rate,vocabulary) \n",
    "        #_=encoder.update_weights(learning_rate,vocabulary) \n",
    "        Output_stack.update_weights(learning_rate)\n",
    "        #vocabulary=helper.update_wembedding_decoder(0.00001,y_batch, dLoss_dWemb_decoder_tot,vocabulary, decoder_seq_len)\n",
    "        #vocabulary=helper.update_wembedding_encoder(0.00001,X_batch, dLoss_dWemb_encoder_tot,vocabulary,encoder_seq_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a523cc",
   "metadata": {},
   "source": [
    "To identify where to place    values in   scaled QK scaled  , consider how the mask interacts with the sequence positions. Shape of   scaled QK scaled  : For a batch size of  N,   scaled QK scaled  typically has the shape (  , num_heads , seq_len , seq_len ) (N,num_heads,seq_len,seq_len), where seq_len seq_len is the length of the input sequence. Each element in   scaled [  ,  ,  ,  ] QK scaled  [i,h,j,k] represents the attention score between the  j-th query position and the  k-th key position for the  h-th attention head in the  i-th batch item. Mask Shape Alignment: The mask typically has shape (  , seq_len ) (N,seq_len) and marks each position with a 1 (for valid tokens) or a 0 (for padding tokens). To align with   scaled QK scaled  , you expand the mask to shape (  , 1 , 1 , seq_len ) (N,1,1,seq_len). After expansion, the mask will effectively broadcast over the query positions (dimension 2 of   scaled QK scaled  ) and attention heads (dimension 1). Applying    Masking: When broadcasting the mask, only key positions that correspond to padding tokens (mask=0) in the last dimension will be assigned   . This is why adding ( mask  1 )  1 0 9 (mask1)10 9 works; any position where the mask is 0 will add  1 0 9 10 9 to   scaled QK scaled  , effectively zeroing out the attention after the softmax. So, for every padding token position (where mask[i, k] = 0), every attention score   scaled [  , : , : ,  ] QK scaled  [i,:,:,k] will be set to a large negative value, ensuring no attention is paid to padding tokens across all queries and heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426995db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(4, dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp.square(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e29d6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensionmwahahahaladidadida\n"
     ]
    }
   ],
   "source": [
    "for i,(j,k) in vocabulary.items():\n",
    "    #print(i,k)\n",
    "    if k==775:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb80648f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([7.29, -4.52, 6.58, -2.67, -1.89, 1.38, 4.31, -3.22, -10.3, -1.86, -0.165, -2.48, 9.21, 0.397, 1.59, 7.13, -6.91, 4.72, -1.58, -0.172, 4.32, 3.8, 1.43, -5.23, -7.19, 1.69, 4.76, 6.03, -3.68, -4.43, -0.472, -1.38, 6.1, 2.23, 0.887, -5.05, -1.16, 0.472, 6.07, 6.63, 1.5, -2.8, -7.84, 8.3, 2.25, -13.7, 4.49, 1.6, 3.39, -6.48, -7.28, -0.184, -2.28, -3.54, -4.86, 4.98, 6.52, -3.3, 7.13, -2.69, -0.978, -2.03, -4.34, -0.0675, -4.21, 3.58, -9.06, 0.371, 0.468, 1.56, 0.745, -5.18, -5.34, -5.32, -0.718, 0.632, 1.63, 3.74, -2.94, 5.61, -0.722, -3.61, -3.2, -1.7, -5.03, -1.62, 1.89, -1.64, 6.58, 2.95, -10.6, 1.52, -6.33, -7.92, -5.77, 4.34, -0.0952, -1.63, -0.553, -0.957, 0.99, -2.37, 3.49, -3.04, 1.74, 4.95, 11.8, 0.265, -4.32, 5.9, 3.63, 0.339, 2.86, -1.67, -1.76, 0.766, -3.32, 4.36, -0.622, -1.46, 4.53, 1.96, 0.922, -0.981, 2.86, 2.27, 2.65, 1.67, 0.0472, -3.75, -6.28, -1.6, 2.4, -0.826, 4.83, -6, -0.0516, 1.95, -1.24, 4.48, -3.47, -0.587, -5.91, 3.51, -4.78, -0.428, -4.84, 0.476, 10.3, 1.23, -2.86, -1.37, -0.346, 3.42, -1.12, 4.54, -2.6, 0.17, -2.31, 5.26, 5.8, 8.39, -6.88, -3.1, -0.798, -1.88, -1.21, 7.81, 1.87, 4.69, 0.746, 7.02, 5.14, -2.88, 4.15, -2.83, 5.87, 1.71, 2.81, -3.75, 3.28, -11.9, -2.58, -5.91, 1.13, 2.79, 2.79, -2.02, -0.957, -1.8, 1.35, 4.26, 6.17, 3.6, 2.16, 3.13, -0.854, 10.2, -2.07, -9.68, 2.2, -0.799, 7.39, 8.04, 1.21, 6.69, -0.302, -4.33, 2.12, -0.99, -7.7, 0.765, 1.27, 0.75, -4.04, 8.9, -3.62, 2.47, 3.74, 4.85, 5.78, -1.26, -7.27, -3.62, 5.2, 9.79, 2.46, -6.25, 5.65, 2.95, -0.848, 2.27, 3.52, -3.82, -0.333, -5.25, 2.65, 3.7, -6.49, 0.463, 1.33, -6.31, 1.85, 0.486, 0.216, -1.13, 7.71, -3.44, 4.6, -8.98, 10.4, -3.87, -1.83, -2.21, -0.618, -1.18, -0.666, -3.63, 0.421, 5.58, 5.23, -0.541, 5.17, 4.31, -2.18, -2.06, -3.73, -0.0329, 5.45, -6.7, -0.49, 4.69, 0.407, 2.87, -1.45, -5.18, -3.36, -7.6, 5.32, -0.654, 2.86, 1.41, -1.81, 3.77, -0.498, 2.77, 4.2, 0.539, 3.52, -0.979, -2.79, 1.96, 0.588, -2.52, -1.21, -6.74, -1.87, -4.61, 0.664, 2.34], dtype=float32),\n",
       " 17595)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[\"[PAD]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b495b556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 50, 300)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3,50,300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7075bd51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3, 15, 300 #dAttention.shape \n",
    "3, 10, 15, 50 #dAttention_weights\n",
    "3, 10, 50, 30 #self.K\n",
    "3, 10, 50, 30 #self.V\n",
    "300, 300#k.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebeffac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3, 15, 300\n",
    "3, 10, 15, 50\n",
    "3, 10, 50, 30\n",
    "3, 50, 300#self.helper.redimension(self.V)\n",
    "300, 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac02a010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 50, 300)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3, 15, 300\n",
    "3, 10, 15, 50\n",
    "3, 10, 50, 30\n",
    "3, 50, 300#self.helper.redimension(self.V)@self.k.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36268ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 50, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3, 15, 300\n",
    "3, 10, 15, 30#dAttention_weights@self.K\n",
    "3, 50, 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8da8522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 50, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3, 15, 300\n",
    "3, 15, 300#self.helper.redimension(dAttention_weights@self.K)\n",
    "3, 50, 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f67c183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 50, 300)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3, 300, 300#dAttention*self.helper.redimension(dAttention_weights@self.K)\n",
    "3, 50, 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091cd000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
