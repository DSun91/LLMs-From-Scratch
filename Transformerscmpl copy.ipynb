{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9c14a08",
   "metadata": {},
   "source": [
    "# Positional encoding and other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b060598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a2ca59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import re\n",
    "import cupy as cp\n",
    "import pickle\n",
    "import time  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jax\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "cp.set_printoptions(edgeitems=30, linewidth=100000, formatter=dict(float=lambda x: \"%.3g\" % x)) \n",
    "\n",
    "\n",
    "def log_time(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()  # Record start time\n",
    "        result = func(*args, **kwargs)  # Execute the wrapped function\n",
    "        end_time = time.time()  # Record end time\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Function '{func.__name__}' executed in {elapsed_time:.4f} seconds\")\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def get_positional_encoding(seq_len, d_model):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a non-learnable (sinusoidal) positional encoding.\n",
    "    \n",
    "    \n",
    "    seq_len: Length of the input sequence.\n",
    "    d_model: Dimension of the embeddings.\n",
    "    \"\"\"\n",
    "    pos = cp.arange(seq_len)[:, cp.newaxis]  # Shape: [seq_len, 1]\n",
    "    i = cp.arange(d_model)[cp.newaxis, :]    # Shape: [1, d_model]\n",
    "\n",
    "    angle_rates = 1 / cp.power(10000, (2 * (i // 2)) / cp.float32(d_model))\n",
    "\n",
    "    # Apply sine to even indices, cosine to odd indices\n",
    "    pos_encoding = cp.zeros((seq_len, d_model))\n",
    "    pos_encoding[:, 0::2] = cp.sin(pos * angle_rates[:, 0::2])  # sine on even indices\n",
    "    pos_encoding[:, 1::2] = cp.cos(pos * angle_rates[:, 1::2])  # cosine on odd indices\n",
    "\n",
    "    return pos_encoding\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    # Subtract the max value for numerical stability\n",
    "    e_x = cp.exp(x - cp.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / cp.sum(e_x, axis=axis, keepdims=True)\n",
    "def layer_norm(x, epsilon=1e-6):\n",
    "    # Calculate the mean and variance\n",
    "        mean = cp.mean(x, axis=-1, keepdims=True)\n",
    "        var = cp.var(x, axis=-1, keepdims=True) \n",
    "        \n",
    "        # Normalize the output\n",
    "        x_norm = (x - mean) / cp.sqrt(var + epsilon) \n",
    "        #print(x)\n",
    "        #print(mean)\n",
    "        #print(\"mean\",mean.shape)\n",
    "        #print(\"x_norm.shape\",x_norm.shape)\n",
    "        return x_norm,mean,var,x.shape[-1]\n",
    "def relu(x):\n",
    "    return cp.maximum(0, x)\n",
    "#@log_time\n",
    "def pad_sequence(seq, max_len, pad_value=0):\n",
    "    \"\"\"Pad a sequence with a given value up to max_len.\"\"\"\n",
    "    current_len = seq.shape[0]\n",
    "    pad_width = max_len - current_len\n",
    "    if pad_width > 0:\n",
    "        # Pad sequence with zeros (or any pad_value you provide)\n",
    "        seq = cp.pad(seq, ((0, pad_width), (0, 0)), mode='constant', constant_values=pad_value)\n",
    "    return seq\n",
    "@log_time\n",
    "def create_timestaped_input(input_d,words_per_phrase):\n",
    "    input_translation=[]\n",
    "    for j in range(input_d.shape[0]):\n",
    "    # Create padded sequences\n",
    "        padded_sequences = [pad_sequence(input_d[j][0:i], words_per_phrase) for i in range(1, input_d.shape[1] + 1)]\n",
    "        input_translation.append(padded_sequences)\n",
    "    return cp.array(input_translation)\n",
    "\n",
    "def cross_entropy_loss(predictions, target):\n",
    "    # Cross-entropy loss for a batch of predictions and targets\n",
    "    batch_loss = -cp.sum(target * cp.log(predictions + 1e-9), axis=1)\n",
    "    return cp.mean(batch_loss)\n",
    "\n",
    "def diff_norm(X,var,mu,N):\n",
    "    epsilon=1e-6\n",
    "    AA=((1-(1/N))*(1/(cp.sqrt(var+epsilon))))\n",
    "    BB=(1/N)*((X-mu)**2)\n",
    "    CC=((var+epsilon)**(3/2))\n",
    "    result=(AA-(BB/CC)) \n",
    "    return result\n",
    "\n",
    "def redimension(X):\n",
    "    return cp.concatenate(cp.swapaxes(X,0,1),axis=-1) \n",
    "\n",
    "def diffQKV(dAttention,Attention_weights,X1,X2,X3,dk):\n",
    "    dAttention_weights=Attention_weights*(1-Attention_weights)\n",
    "    V1=redimension(dAttention_weights@X1/cp.sqrt(dk)) \n",
    "    \n",
    "    V2=redimension(X2)\n",
    "    \n",
    "    V3=V1*V2*X3\n",
    "    dLoss_dX=cp.sum(cp.transpose(dAttention,(0,2,1))@V3,axis=0)\n",
    "    return dLoss_dX\n",
    "\n",
    "\n",
    "\n",
    "@log_time\n",
    "def create_vocabulary(complete_text,name,nlp): \n",
    "        # Use re.findall to split considering punctuation\n",
    "        text = re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]', complete_text)\n",
    "        words_list = list(set(text))\n",
    "        vocabulary=dict()\n",
    "        for i,j in enumerate(words_list):\n",
    "            #vocabulary[j]=(jax.random.uniform(jax.random.key(cp.random.randint(10000)),embedding_size),i)\n",
    "            vocabulary[j]=(cp.array(nlp(j).vector),i)\n",
    "            #print(j,len(cp.array(nlp(j).vector)))\n",
    "\n",
    "        #print(vocabulary)\n",
    "        #print(\"Vocabulary size: \", len(vocabulary))\n",
    "        with open(f\"data/{name}.pkl\", 'wb') as handle:\n",
    "            pickle.dump(vocabulary, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "        return vocabulary\n",
    "\n",
    "\n",
    "\n",
    "@log_time\n",
    "def pad_sequences(sentences,lenght, pad_token='[PAD]',target_type=None):\n",
    "        \"\"\"\n",
    "        Pads the input sentences to have the same length by adding [PAD] tokens at the end.\n",
    "        \"\"\"\n",
    "        \n",
    "        if target_type==\"encoder\": \n",
    "            # Split each sentence into words\n",
    "            tokenized_sentences = [ [\"[START]\"] + re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]',sentence)+ [\"[END]\"] for sentence in sentences]\n",
    "        elif target_type==\"decoder\": \n",
    "             tokenized_sentences = [ [\"[START]\"] + re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]',sentence) for sentence in sentences]\n",
    "        elif target_type==\"target\": \n",
    "             tokenized_sentences = [ re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]',sentence) + [\"[END]\"]  for sentence in sentences]\n",
    "        #print(tokenized_sentences)\n",
    "        if lenght==0: \n",
    "            # Find the maximum sentence length\n",
    "            max_len = max(len(sentence) for sentence in tokenized_sentences)\n",
    "        else:\n",
    "            max_len=lenght\n",
    "        \n",
    "        # Pad each sentence with the [PAD] token to make them of equal length\n",
    "        padded_sentences = [\" \".join(sentence + [pad_token] * (max_len - len(sentence))) for sentence in tokenized_sentences]\n",
    "        \n",
    "        return padded_sentences\n",
    "\n",
    "@log_time\n",
    "def generate_input_encoder(x_batch,vocabulary_encoder,max_words_per_phrase): \n",
    "        \n",
    "        xi=[]\n",
    "      \n",
    "        phrase_vectors_x = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]', x) for x in x_batch]\n",
    "      \n",
    "        phrase_vectors_x= [i[0:max_words_per_phrase] for i in phrase_vectors_x] \n",
    "\n",
    "        #print(\"phrase_vectors_x:\\n\",phrase_vectors_x)\n",
    "         \n",
    "        xi=cp.array([[vocabulary_encoder[word][0] for word in phrase_vector] for phrase_vector in phrase_vectors_x]) \n",
    "        \n",
    "        return xi \n",
    "    \n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "@log_time\n",
    "def create_input_encoder(X,vocabulary_encoder,max_words_per_phrase,embedding_size): \n",
    "    pos_encoding=get_positional_encoding(max_words_per_phrase,embedding_size)\n",
    "    x_train=pad_sequences(X,max_words_per_phrase,target_type=\"encoder\")\n",
    "    \n",
    "    #vocabulary_encoder[\"[PAD]\"]=(cp.zeros(embedding_size),vocabulary_encoder[\"[PAD]\"][1])\n",
    "    inputs_e=generate_input_encoder(x_train,vocabulary_encoder,max_words_per_phrase)\n",
    "    inputs_e+=pos_encoding\n",
    "    return inputs_e\n",
    "        \n",
    "@log_time\n",
    "def create_decoder_input(y_train,embedding_size,max_words_per_phrase,vocabulary_decoder):\n",
    "    \n",
    "     \n",
    "    \n",
    "    #vocabulary_decoder[\"[PAD]\"]=(cp.zeros(embedding_size),vocabulary_decoder[\"[PAD]\"][1])\n",
    "\n",
    "    decoder_input=pad_sequences(y_train,lenght=max_words_per_phrase,target_type=\"decoder\")\n",
    "    decoder_input=[i.split() for i in decoder_input]\n",
    "    #print(max_words_per_phrase)\n",
    "    if max_words_per_phrase==None:\n",
    "        max_words_per_phrase=len(decoder_input[0])\n",
    "\n",
    "    \n",
    "    phrase_vectors_y= [i[0:max_words_per_phrase] for i in decoder_input]\n",
    "    # for sentence in phrase_vectors_y:\n",
    "    #     print(sentence)\n",
    "    yi=cp.array([ [ vocabulary_decoder[word][0]  for word in phrase_vector ] for phrase_vector in phrase_vectors_y])\n",
    "    pos_encoding=get_positional_encoding(max_words_per_phrase,embedding_size)\n",
    "    #print(pos_encoding.shape,yi.shape)\n",
    "    yi=yi+pos_encoding\n",
    "    decoder_inputs=cp.swapaxes(create_timestaped_input(yi,max_words_per_phrase),0,1)\n",
    "    return decoder_inputs\n",
    "#@log_time\n",
    "def get_one_hot(word,vocabulary_decoder): \n",
    "    #print(word)\n",
    "    vocab_size=len(vocabulary_decoder)\n",
    "    one_hot_vector = cp.zeros(vocab_size)\n",
    "    one_hot_vector[vocabulary_decoder[word][1]] = vocabulary_decoder[word][1]\n",
    "    return one_hot_vector\n",
    "\n",
    "@log_time\n",
    "def create_target(y_train,max_words_per_phrase,vocabulary_decoder):\n",
    "    target_d=pad_sequences(y_train,lenght=max_words_per_phrase,target_type=\"target\")\n",
    "    target_d=[i.split() for i in target_d]\n",
    "    target_d=[[get_one_hot(index, vocabulary_decoder) for index in phrase] for phrase in target_d]\n",
    "    target_d=cp.swapaxes(create_timestaped_input(cp.array(target_d),max_words_per_phrase),0,1) \n",
    "    targets_d=[]\n",
    "\n",
    "    for i in range(target_d.shape[0]):\n",
    "        ff=[]\n",
    "        #print(i,target_d[i].shape)\n",
    "        for j in range(target_d[i].shape[0]):\n",
    "            ff.append(target_d[i][j][i])\n",
    "            #print(targets_d[i][j][i])\n",
    "        targets_d.append(ff)\n",
    "            #print(ff) \n",
    "    targets_d=cp.array(targets_d)\n",
    "    return targets_d\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5ea491",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549bd76e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c55528b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "563e49ab",
   "metadata": {},
   "source": [
    "# Summary Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "563ccd94",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def forward_attention_encoder(inputs_e):\n",
    "    global Qe,Ke,Ve,num_heads,batch_size,dk\n",
    " \n",
    "    Q_E= cp.swapaxes(cp.array(cp.array_split(cp.matmul(inputs_e, Qe),num_heads,axis=2)), 0, 1)\n",
    "    #print(\"Qval.shape: \",Q_E.shape)\n",
    "\n",
    "    K_E = cp.swapaxes(cp.array(cp.array_split(cp.matmul(inputs_e, Ke),num_heads,axis=2)), 0, 1)\n",
    "    #print(\"Kval.shape: \",K_E.shape)\n",
    "\n",
    "\n",
    "    V_E = cp.swapaxes(cp.array(cp.array_split(cp.matmul(inputs_e,Ve),num_heads,axis=2)), 0, 1)\n",
    "    #print(\"Vval.shape: \",V_E.shape)\n",
    "\n",
    "    QKscaled = cp.matmul(Q_E, cp.transpose(K_E, (0, 1, 3, 2))) / cp.sqrt(dk)\n",
    "\n",
    "    Attention_weights_e = softmax(QKscaled)\n",
    "    #print(\"Attention_weights shape:\",Attention_weights_e.shape)\n",
    "\n",
    "\n",
    "    Ae = cp.matmul(Attention_weights_e, V_E)\n",
    "    #print(\"Attention shape:\",Ae.shape)\n",
    "\n",
    "\n",
    "    Ae=cp.array([cp.concatenate(Ae[i], axis=1) for i in range(batch_size)])\n",
    "    #print(\"Attention shape concat:\",Ae.shape)\n",
    "\n",
    "    Xe=Ae+inputs_e\n",
    "    Ect1,mu_e,var_e,Ne=layer_norm(Xe)\n",
    "    #print(\"Ect1.shape\",Ect1.shape,Ne)\n",
    "\n",
    "    return Ae,Xe,Ect1,mu_e,var_e,Ne,Attention_weights_e,K_E,V_E,Q_E\n",
    "\n",
    "\n",
    "def fully_connected_layers_encoder(Ect1):\n",
    "    \n",
    "    global Wfl1e,bfl1e,Wfl2e,bfl2e\n",
    "    \n",
    "    Xe1=cp.matmul(Ect1,Wfl1e)+bfl1e\n",
    "    FLe1=relu(Xe1)\n",
    "\n",
    "    FLe2=cp.matmul(FLe1,Wfl2e)+bfl2e\n",
    "\n",
    "    Xe2=FLe2+Ect1\n",
    "    Ecout,mu_e2,var_e2,N_e2=layer_norm(Xe2)\n",
    "    #print(\"Ecout.shape\",Ecout.shape)\n",
    "    return Ecout,mu_e2,var_e2,N_e2,FLe1,Xe1,Xe2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07246b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_attention_encoder(Ecout):\n",
    "    global Kc,Vc \n",
    "    K_C  = cp.swapaxes(cp.array(cp.array_split(cp.matmul(Ecout, Kc),num_heads,axis=2)), 0, 1)\n",
    "    #print(\"K_C.shape: \",K_C.shape)# shape is: num_phrases, numbheads, words_per_phrase, dv/num_heads \n",
    "    V_C  = cp.swapaxes(cp.array(cp.array_split(cp.matmul(Ecout,Vc),num_heads,axis=2)), 0, 1)\n",
    "    #print(\"V_C.shape: \",V_C.shape)\n",
    "    return K_C,V_C\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe75c62",
   "metadata": {},
   "source": [
    "## Decoder forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01824c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_attention_decoder(input_decoder):\n",
    "    global Qd,Kd,Vd,words_per_phrase\n",
    "    Q_D  = cp.swapaxes(cp.array(cp.array_split(cp.matmul(input_decoder, Qd),num_heads,axis=2)), 0, 1)\n",
    "    #print(\"Qval.shape: \",Q_D.shape)# numwords, num_phrases, numheads, num_words, dv/num_heads\n",
    "\n",
    "    #K_D  = cp.swapaxes(cp.swapaxes(cp.array(cp.array_split(cp.matmul(inputs_d[step], Kd),num_heads,axis=3)), 0, 1),1,2)\n",
    "    K_D  = cp.swapaxes(cp.array(cp.array_split(cp.matmul(input_decoder, Kd),num_heads,axis=2)), 0, 1)\n",
    "    #print(\"Kval.shape: \",K_D.shape)\n",
    "\n",
    "\n",
    "    #V_D  = cp.swapaxes(cp.swapaxes(cp.array(cp.array_split(cp.matmul(inputs_d[step], Vd),num_heads,axis=3)), 0, 1),1,2)\n",
    "    V_D  = cp.swapaxes(cp.array(cp.array_split(cp.matmul(input_decoder, Vd),num_heads,axis=2)), 0, 1)\n",
    "\n",
    "    QKscaled_decoder  = cp.matmul(Q_D, cp.transpose(K_D, (0, 1, 3, 2))) / cp.sqrt(dv)\n",
    "    # Step 1: Create a causal mask of shape (1, 1, 9, 9) to broadcast across heads and batch\n",
    "    mask_size=words_per_phrase\n",
    "    mask = cp.tril(cp.ones((mask_size, mask_size)))  # (9, 9) lower triangular matrix\n",
    "    mask[mask == 0]=-cp.inf  # Set future tokens to -inf\n",
    "    mask[mask == 1]=0  # Set allowed tokens to 0\n",
    "    mask = mask.reshape(1, 1, mask_size, mask_size)   \n",
    "\n",
    "    # Step 2: Apply mask to QKscaled_decoder (it will broadcast across batch and heads)\n",
    "    QKscaled_decoder = QKscaled_decoder + mask \n",
    "\n",
    "    Attention_weights_masked = softmax(QKscaled_decoder)\n",
    "\n",
    "\n",
    "    A_mask = cp.matmul(Attention_weights_masked, V_D)\n",
    "    #print(\"A_mask.shape non concat: \",A_mask.shape)\n",
    "    \n",
    "    #A_mask=cp.swapaxes(cp.concatenate(cp.swapaxes(A_mask,0,2),axis=-1),0,1)\n",
    "    A_mask= cp.concatenate(cp.swapaxes(A_mask,0,1),axis=-1) \n",
    "\n",
    "    Xd = input_decoder + A_mask\n",
    "    Dt1,mu_d,var_d,N_d = layer_norm(Xd)\n",
    "    #print(\"A_mask.shape concat: \",A_mask.shape)\n",
    "    #print(\"inputs_d.shape: \",input_decoder.shape)\n",
    "    #print(\"Dt1.shape: \",Dt1.shape)\n",
    "    return A_mask,Xd,Dt1,mu_d,var_d,N_d,Attention_weights_masked,Q_D,K_D,V_D\n",
    "    \n",
    "def cross_attention_decoder(Dt1):\n",
    "    global Qc \n",
    "    Q_C  = cp.swapaxes(cp.array(cp.array_split(cp.matmul(Dt1, Qc),num_heads,axis=2)), 0, 1)\n",
    "    #print(\"Q_C.shape: \",Q_C.shape)\n",
    "    return Q_C\n",
    "\n",
    "def cross_attention(Q_C,K_C,V_C,Dt1):\n",
    "    global dv\n",
    "    QKscaled_cross_attention  = cp.matmul(Q_C, cp.transpose(K_C , (0, 1, 3, 2)))/ cp.sqrt(dv)\n",
    "    Attention_weights_cross = softmax(QKscaled_cross_attention)\n",
    "    Acr = cp.matmul(Attention_weights_cross, V_C)\n",
    "    #print(\"Acr.shape non concat\",Acr.shape)\n",
    "    Acr=cp.concatenate(cp.swapaxes(Acr,0,1),axis=-1)\n",
    "    #print(\"Acr.shape concat\",Acr.shape)\n",
    "    Res=Acr + Dt1\n",
    "    Dt2, mu_res,var_res,N_res = layer_norm(Res)\n",
    "    return Dt2, mu_res,var_res,N_res,Res,Attention_weights_cross\n",
    "\n",
    "def fully_connected_layers_decoder(Dt2):\n",
    "    \n",
    "    global Wfl1d,bfl1d,Wfl2d,bfl2d,num_phrases\n",
    "     \n",
    "    Xd1=cp.matmul(Dt2,Wfl1d)+bfl1d\n",
    "   \n",
    "    FLd1=relu(Xd1)\n",
    "     \n",
    "    FLd2=cp.matmul(FLd1,Wfl2d)+bfl2d\n",
    "    #print(\"FLd2.shape\",FLd2.shape)\n",
    "  \n",
    "    Xd2=FLd2+Dt2\n",
    "    Dout,mu_d2,var_d2,N_d2=layer_norm(Xd2)\n",
    " \n",
    "    #print(\"Dout.shape\",Dout.shape)\n",
    "    Dout=Dout.reshape(Dout.shape[0],Dout.shape[1]*Dout.shape[2])\n",
    "    #print(\"Dout.shape\",Dout.shape)\n",
    "    return Dout,mu_d2,var_d2,N_d2,Xd2,Xd1,FLd1\n",
    "\n",
    "def output_layer(Dout):\n",
    "    \n",
    "    global Wo,bo \n",
    "    #print(Dout.shape,W0.shape)  \n",
    "    Zout=cp.matmul(Dout,Wo)+bo \n",
    "    SigmaZout = softmax(Zout) \n",
    "    #print(\"SigmaZout.shape\",SigmaZout.shape)\n",
    "    \n",
    "    return SigmaZout \n",
    "\n",
    "\n",
    "def loss_calculation(SigmaZout,target):\n",
    "    #print(\"target.shape\",cp.array(target).shape)\n",
    "    Loss=cross_entropy_loss(SigmaZout, target)\n",
    "    #print(\"Loss:\",Loss)\n",
    "    return Loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3adb753",
   "metadata": {},
   "source": [
    "## BackPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83d0bd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivate_dout(SigmaZout,target,Dout):\n",
    "    global Wo,embedding_size,batch_size,words_per_phrase\n",
    "    dLoss_dZout=SigmaZout-target\n",
    "    #print(\"dLoss_dZout.shape\",dLoss_dZout.shape)\n",
    "    dLoss_W0=cp.transpose(dLoss_dZout,(1,0))@Dout\n",
    "    #print(\"dLoss_W0.shape\",dLoss_W0.shape,\"W0.shape\",W0.shape)\n",
    "    dLoss_b0=cp.sum(dLoss_dZout, axis=0)\n",
    "    #print(\"dLoss_b0.shape\",dLoss_b0.shape,\"b0.shape\",b0.shape)\n",
    "    dLoss_Dout=dLoss_dZout@Wo.T\n",
    "    dLoss_Dout=dLoss_Dout.reshape(batch_size,words_per_phrase,embedding_size)\n",
    "    #print(\"dLoss_Dout.shape\",dLoss_Dout.shape)\n",
    "    return dLoss_Dout,dLoss_W0,dLoss_b0\n",
    "\n",
    "\n",
    "def derivate_fully_connected_layers_decoder(dLoss_Dout,Dt2,Xd2,var_d2,mu_d2,N_d2,Wfl2d,FLd1,Xd1):\n",
    " \n",
    "    dLoss_FLd2=dLoss_Dout*diff_norm(Xd2,var_d2,mu_d2,N_d2)\n",
    "    #print(\"dLoss_FLd2.shape\",dLoss_FLd2.shape) \n",
    "    dLoss_Dt2_a=dLoss_FLd2\n",
    "    #print(\"dLoss_Dt2_a.shape\",dLoss_Dt2_a.shape) \n",
    "    #print(\"Dt2.shape\",Dt2.shape) \n",
    "    dLoss_FLd1=dLoss_FLd2@cp.transpose(Wfl2d,(1,0))\n",
    "    #print(\"dLoss_FLd1.shape\",dLoss_FLd1.shape) \n",
    "    #print(\"FLd1.shape\",FLd1.shape) \n",
    "    dLoss_Wfl2d=cp.sum(cp.transpose(dLoss_FLd2,(0,2,1))@FLd1,axis=0)\n",
    "    #print(\"dLoss_Wfl2d.shape\",dLoss_Wfl2d.shape) # do the mean here over each phrase\n",
    "    #print(\"Wfl2d.shape\",Wfl2d.shape) \n",
    "    dLoss_bfl2d=cp.sum(cp.sum(dLoss_FLd2, axis=0),axis=0)\n",
    "    #print(\"dLoss_bfl2d.shape\",dLoss_bfl2d.shape) # do the mean here over each phrase\n",
    "    #print(\"bfl2d.shape\",bfl2d.shape) \n",
    "    if Xd1.all()>0:\n",
    "        DLoss_Dt2_b=dLoss_FLd1@cp.transpose(Wfl1d,(1,0))\n",
    "    else:\n",
    "        DLoss_Dt2_b=0\n",
    "    DLoss_Dt2=dLoss_Dt2_a+DLoss_Dt2_b\n",
    "    #print(\"DLoss_Dt2.shape\",DLoss_Dt2.shape) # do the mean here over each phrase\n",
    "    #print(\"Dt2.shape\",Dt2.shape) \n",
    "    if Xd1.all()>0:\n",
    "        dLoss_Wfl1d=cp.sum(cp.transpose(dLoss_FLd1,(0,2,1))@Dt2,axis=0)\n",
    "    else:\n",
    "        dLoss_Wfl1d=0\n",
    "    #print(\"dLoss_Wfl1d.shape\",dLoss_Wfl1d.shape) # do the mean here over each phrase\n",
    "    #print(\"Wfl1d.shape\",Wfl1d.shape) \n",
    "    if Xd1.all()>0:\n",
    "        dLoss_bfl1d=cp.sum(cp.sum(dLoss_FLd1,axis=0),axis=0)\n",
    "    else:\n",
    "        dLoss_bfl1d=0\n",
    "    return dLoss_Wfl2d,dLoss_bfl2d,dLoss_Wfl1d,dLoss_bfl1d,DLoss_Dt2\n",
    "\n",
    "def derivative_cross_attention(dLoss_Dt2,Res,var_res,mu_res,N_res,Attention_weights_cross,K_C,V_C,Q_C,Ecout,Dt1):\n",
    "    #print(\"dLoss_bfl1d.shape\",dLoss_bfl1d.shape) # do the mean here over each phrase\n",
    "    #print(\"bfl1d.shape\",bfl1d.shape) \n",
    "    \n",
    "    dLoss_Acr=dLoss_Dt2*diff_norm(Res,var_res,mu_res,N_res)\n",
    "    #print(\"dLoss_Acr.shape\",dLoss_Acr.shape) # do the mean here over each phrase\n",
    "    #print(\"Acr.shape\",Acr.shape) \n",
    "    dLoss_Dt1_a=dLoss_Dt2*diff_norm(Res,var_res,mu_res,N_res)\n",
    "    #print(\"dLoss_Dt1.shape\",dLoss_Dt1_a.shape) # do the mean here over each phrase\n",
    "    #print(\"Dt1.shape\",Dt1.shape) \n",
    "    dLoss_Qc=diffQKV(dLoss_Acr,Attention_weights_cross,K_C,V_C,Dt1,dk)\n",
    "    #print(\"dLoss_dQc.shape\",dLoss_Qc.shape) # do the mean here over each phrase\n",
    "    #print(\"Qc.shape\",Qc.shape) \n",
    "    dLoss_Kc=diffQKV(dLoss_Acr,Attention_weights_cross,Q_C,V_C,Ecout,dk)\n",
    "    #print(\"dLoss_dKc.shape\",dLoss_Kc.shape) # do the mean here over each phrase\n",
    "    #print(\"Kc.shape\",Kc.shape) \n",
    "    dLoss_Vc=f=cp.sum(cp.mean(cp.transpose(cp.expand_dims(dLoss_Acr, axis=1),(0,1,3,2))@(Attention_weights_cross@cp.expand_dims(Ecout, axis=1)),axis=1),axis=0)\n",
    "    #print(\"dLoss_dVc.shape\",dLoss_Vc.shape) # do the mean here over each phrase\n",
    "    #print(\"Vc.shape\",Vc.shape) \n",
    "    return dLoss_Qc,dLoss_Kc,dLoss_Vc,Attention_weights_cross,dLoss_Dt1_a,dLoss_Acr\n",
    "    \n",
    "def derivative_attention_decoder(dLoss_Acr,Attention_weights_cross,dLoss_Dt1_a,Attention_weights_masked,Q_D,V_D,K_D,K_C,V_C,Xd,var_d,mu_d,N_d,input_d):\n",
    "    global Qc\n",
    "    dAttention_weights_cross=Attention_weights_cross*(1-Attention_weights_cross)\n",
    "    V1=redimension(dAttention_weights_cross@K_C/cp.sqrt(dk)) \n",
    "\n",
    "    V2=redimension(V_C)\n",
    "\n",
    "    V3=V1*V2@Qc\n",
    "    dLoss_Dt1_b=dLoss_Acr*V3\n",
    "    #print(\"dLoss_Dt1_b.shape\",dLoss_Dt1_b.shape) # do the mean here over each phrase\n",
    "    #print(\"dLoss_Dt1_a.shape\",dLoss_Dt1_a.shape) \n",
    "    dLoss_Dt1=dLoss_Dt1_a+dLoss_Dt1_b\n",
    "    dLoss_Amask=dLoss_Dt1*diff_norm(Xd,var_d,mu_d,N_d)\n",
    "    #print(\"dLoss_DAmask.shape\",dLoss_Amask.shape)  \n",
    "    dLoss_inputd_a=dLoss_Amask\n",
    "    #print(\"dLoss_Dinputd_a.shape\",dLoss_inputd_a.shape) \n",
    "    dLoss_Kd=diffQKV(dLoss_Amask,Attention_weights_masked,Q_D,V_D,input_d,dk) \n",
    "    #print(\"dLoss_Kd.shape\",dLoss_Kd.shape) \n",
    "    dLoss_Qd=diffQKV(dLoss_Amask,Attention_weights_masked,K_D,V_D,input_d,dk) \n",
    "    #print(\"dLoss_Qd.shape\",dLoss_Qd.shape) \n",
    "    dLoss_Vd=f=cp.sum(cp.mean(cp.transpose(cp.expand_dims(dLoss_Amask, axis=1),(0,1,3,2))@(Attention_weights_masked@cp.expand_dims(input_d, axis=1)),axis=1),axis=0)\n",
    "    return dLoss_Kd,dLoss_Qd,dLoss_Vd,dLoss_inputd_a,dLoss_Amask\n",
    "\n",
    "\n",
    "def derivative_input_decoder(dLoss_Amask,Attention_weights_masked,K_D,V_D,Q_D,dLoss_inputd_a,input_d):\n",
    "    global Qd,Kd,Vd\n",
    "    dLoss_V_D=cp.transpose(cp.mean(cp.transpose(cp.expand_dims(dLoss_Amask, axis=1),(0,1,3,2))@Attention_weights_masked,axis=1),(0,2,1))\n",
    "    dLoss_V_D.shape\n",
    "    dLoss_inputd_v=dLoss_V_D@Vd\n",
    "\n",
    "    # print(\"dLoss_inputd_v.shape\",dLoss_inputd_v.shape) # do the mean here over each phrase\n",
    "    # print(\"input_d.shape\",input_d.shape) \n",
    "\n",
    "    dAttention_weights_masked=Attention_weights_masked*(1-Attention_weights_masked)\n",
    "    V1=redimension(dAttention_weights_masked@K_D/cp.sqrt(dk)) \n",
    "    V2=redimension(V_D)\n",
    "    V3=V1*V2\n",
    "    dLoss_Q_D=dLoss_Amask*V3\n",
    "    dLoss_Q_D.shape\n",
    "    dLoss_inputd_q=dLoss_Q_D@Qd\n",
    "    #print(\"dLoss_inputd_q.shape\",dLoss_inputd_q.shape)\n",
    " \n",
    "    V1=redimension(dAttention_weights_masked@Q_D/cp.sqrt(dk)) \n",
    "    V2=redimension(V_D)\n",
    "    V3=V1*V2\n",
    "    dLoss_K_D=dLoss_Amask*V3 \n",
    "    dLoss_inputd_k=dLoss_K_D@Kd\n",
    "    #print(\"dLoss_inputd_k.shape\",dLoss_inputd_k.shape)\n",
    "    dLoss_inputd=dLoss_inputd_a+dLoss_inputd_k+dLoss_inputd_q+dLoss_inputd_v\n",
    "\n",
    "    dLoss_dWemb_decoder=dLoss_inputd*input_d\n",
    "    return dLoss_inputd,dLoss_dWemb_decoder\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd9ab3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_Ecout(Attention_weights_cross,dLoss_Acr,Q_C,V_C):\n",
    "    global Kc,Vc\n",
    "    dAttention_weights_cross=Attention_weights_cross*(1-Attention_weights_cross)\n",
    "    V1=redimension(dAttention_weights_cross@Q_C/cp.sqrt(dk)) \n",
    "\n",
    "    V2=redimension(V_C)\n",
    "\n",
    "    V3=V1*V2\n",
    "\n",
    "    \n",
    "    dLoss_K_C=dLoss_Acr*V3\n",
    "    dLoss_K_C.shape\n",
    "\n",
    "    dLoss_Ecout_k=dLoss_K_C@Kc\n",
    "    #print(\"dLoss_Ecout_k.shape\",dLoss_Ecout_k.shape) \n",
    "\n",
    "    dLoss_V_C=cp.transpose(cp.mean(cp.transpose(cp.expand_dims(dLoss_Acr, axis=1),(0,1,3,2))@Attention_weights_cross,axis=1),(0,2,1))\n",
    "    dLoss_V_C.shape\n",
    "    dLoss_Ecout_v=dLoss_V_C@Vc\n",
    "\n",
    "    #print(\"dLoss_Ecout_v.shape\",dLoss_Ecout_v.shape) # do the mean here over each phrase\n",
    "    dLoss_Ecout=dLoss_Ecout_k+dLoss_Ecout_v\n",
    "    return dLoss_Ecout\n",
    "\n",
    "def derivate_fully_connected_layers_encoder(dLoss_Ecout,Ect1,Xe2,var_e2,mu_e2,N_e2,FLe1,Xe1):\n",
    "    global Wfl2e,Wfl1e\n",
    "    dLoss_dFLe2=dLoss_Ecout*diff_norm(Xe2,var_e2,mu_e2,N_e2)\n",
    "    dLoss_Ect1_a=dLoss_dFLe2\n",
    "    #print(Wfl2e.shape)\n",
    "    dLoss_dFLe1=dLoss_dFLe2@cp.transpose(Wfl2e,(1,0))\n",
    "    dLoss_dWfl2e=cp.transpose(dLoss_dFLe2,(0,2,1))@FLe1\n",
    "    #print(dLoss_dWfl2e)\n",
    "    dLoss_dbfl2e=cp.sum(dLoss_dFLe2,axis=1) \n",
    "    if Xe1.all()>0:\n",
    "        dLoss_Ect1_b=dLoss_dFLe1@cp.transpose(Wfl1e,(1,0))\n",
    "    else:\n",
    "        dLoss_Ect1_b=0\n",
    "\n",
    "    dLoss_Ect1=dLoss_Ect1_b+dLoss_Ect1_a\n",
    "    if Xe1.all()>0:\n",
    "        dLoss_Wfl1e=cp.transpose(dLoss_dFLe1,(0,2,1))@Ect1\n",
    "    else:\n",
    "        dLoss_Wfl1e=0\n",
    " \n",
    "    if Xe1.all()>0:\n",
    "        dLoss_bfl1e=cp.transpose(dLoss_dFLe1,(0,2,1)) \n",
    "    else:\n",
    "        dLoss_bfl1e=0\n",
    " \n",
    "    return dLoss_dWfl2e,dLoss_dbfl2e,dLoss_Wfl1e,dLoss_bfl1e,dLoss_Ect1\n",
    "import warnings\n",
    " \n",
    "def derivative_attention_encoder(dLoss_Ect1,Xe,var_e,mu_e,Ne,Attention_weights_e,K_E,V_E,Q_E):\n",
    "    global inputs_e\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"error\", RuntimeWarning)\n",
    "        try:\n",
    "            dLoss_Ae=dLoss_Ect1*diff_norm(Xe,var_e,mu_e,Ne)\n",
    "            \n",
    "            dLoss_inpute_a=dLoss_Ae\n",
    "            \n",
    "            dLoss_dQe=diffQKV(dLoss_Ae,Attention_weights_e,K_E,V_E,inputs_e,dk)\n",
    "            #print(\"dLoss_dQe.shape\",dLoss_dQe.shape) # do the mean here over each phrase\n",
    "            #print(\"Qe.shape\",Qe.shape) \n",
    "            dLoss_dKe=diffQKV(dLoss_Ae,Attention_weights_e,Q_E,V_E,inputs_e,dk)\n",
    "            #print(\"dLoss_dKe.shape\",dLoss_dKe.shape) # do the mean here over each phrase\n",
    "            #print(\"Ke.shape\",Ke.shape) \n",
    "            \n",
    "           \n",
    "            dLoss_dVe=cp.sum(cp.sum(cp.transpose(cp.expand_dims(dLoss_Ae, axis=1),(0,1,3,2))@(Attention_weights_e@cp.expand_dims(inputs_e, axis=1)),axis=1),axis=0)\n",
    "            #print(\"dLoss_dVe.shape\",dLoss_dVe.shape) # do the mean here over each phrase\n",
    "            return dLoss_dQe,dLoss_dKe,dLoss_dVe,dLoss_inpute_a,dLoss_Ae\n",
    "        except RuntimeWarning as rw:\n",
    "            # Check for NaN or inf values in inputs and matrices\n",
    "            print(\"dLoss_Ae check \", dLoss_Ae)\n",
    "            print(\"Attention_weights_e  \", Attention_weights_e)\n",
    "            print(\"inputs_e  \", inputs_e)\n",
    "            print(f\"Caught a RuntimeWarning: {rw}\")\n",
    "            return None  # Return None if a warning occurs\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Additional checks in case of other exceptions\n",
    "            print(\"inputs_e check \", cp.isnan(inputs_e).any(), cp.isinf(inputs_e).any())\n",
    "            print(\"Attention_weights_e check \", cp.isnan(Attention_weights_e).any(), cp.isinf(Attention_weights_e).any())\n",
    "            print(\"dLoss_Ae check \", cp.isnan(dLoss_Ae).any(), cp.isinf(dLoss_Ae).any())\n",
    "            print(f\"Caught an error: {e}\")\n",
    "            return None \n",
    "\n",
    "\n",
    "def derivative_input_encoder(dLoss_Ae,Attention_weights_e,K_E,V_E,Q_E,dLoss_inpute_a):\n",
    "    global Ve,Qe,Ke,inputs_e\n",
    "    dLoss_V_E=cp.transpose(cp.sum(cp.transpose(cp.expand_dims(dLoss_Ae, axis=1),(0,1,3,2))@Attention_weights_e,axis=1),(0,2,1))\n",
    "    dLoss_inpute_v=dLoss_V_E@Ve\n",
    "\n",
    "  \n",
    "    dAttention_weights_e=Attention_weights_e*(1-Attention_weights_e)\n",
    "    V1=redimension(dAttention_weights_e@K_E/cp.sqrt(dk))  \n",
    "    V2=redimension(V_E) \n",
    "    V3=V1*V2 \n",
    "    dLoss_Q_E=dLoss_Ae*V3  \n",
    "    dLoss_inpute_q=dLoss_Q_E@Qe\n",
    "    #print(\"dLoss_inpute_q.shape\",dLoss_inpute_q.shape)\n",
    "    \n",
    "    V1=redimension(dAttention_weights_e@Q_E/cp.sqrt(dk))  \n",
    "    V2=redimension(V_E) \n",
    "    V3=V1*V2 \n",
    "    dLoss_K_E=dLoss_Ae*V3  \n",
    "    dLoss_inpute_k=dLoss_K_E@Ke\n",
    "    #print(\"dLoss_inpute_k.shape\",dLoss_inpute_k.shape)\n",
    "    dLoss_inpute=dLoss_inpute_a+dLoss_inpute_k+dLoss_inpute_q+dLoss_inpute_v\n",
    "    dLoss_dWemb_encoder=dLoss_inpute*inputs_e\n",
    "    return dLoss_inpute,dLoss_dWemb_encoder\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6e8f9c",
   "metadata": {},
   "source": [
    "## Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c71e997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c067efa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_vocabs(ans,vocabulary): \n",
    "    for idx, values in enumerate(ans):\n",
    "        max_index = cp.argmax(values)\n",
    "        \n",
    "        # Step 2: Find the word in the vocabulary with the corresponding position\n",
    "        matched_word = None\n",
    "        for word, (_, position) in vocabulary.items():\n",
    "            if position == max_index:\n",
    "                matched_word = word\n",
    "                break\n",
    "        print(f\"List {idx + 1}: Max value index: {max_index}, Matched word: {matched_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb0c0eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['parser', 'ner']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open the file with a specific encoding\n",
    "with open('corpus/train.json', 'r', encoding='utf-8') as f:\n",
    "    try:\n",
    "        dataset = json.load(f)  # Load the JSON data\n",
    "     \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "\n",
    "X_train=[\"i love soy sauce!\", \n",
    "         \"my dog... is cute\", \n",
    "         \"you are crazy strong!\",\n",
    "         \"the friend is good, you know\"]\n",
    "\n",
    "y_train=[\"io amo la salsa di soia!\",\n",
    "         \"sei pazzo potente!\",\n",
    "        \"il cane... è tenero\",\n",
    "        \"l' arancio è buono, vero?\"\n",
    "        ]    \n",
    "\n",
    "\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# #Loop through the list and process each dialogue and summary\n",
    "for data in dataset:\n",
    "    dialogue = data['dialogue']  # Split dialogue into a list of lines\n",
    "    summary = data['summary']\n",
    "    \n",
    "    X_train.append(dialogue)\n",
    "    y_train.append(summary)\n",
    "\n",
    "nlp_encoder = spacy.load('en_core_web_lg')\n",
    "nlp_decoder = spacy.load('it_core_news_lg')\n",
    "# Disable unnecessary pipeline components\n",
    "nlp_encoder.disable_pipes([\"parser\", \"ner\"])\n",
    "nlp_decoder.disable_pipes([\"parser\", \"ner\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe966119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f62a3691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size=10\n",
    "# #batch_size = len(X_train)\n",
    "# words_per_phrase = 50\n",
    "# dk = dv = embedding_size = 300 # constrain of transformer all embedding size both input embedding and attention embedding are the same encoder\n",
    "# num_heads=10\n",
    "\n",
    "# nlp_encoder = spacy.load('en_core_web_lg')\n",
    "# nlp_decoder = spacy.load('it_core_news_lg')\n",
    "# # Disable unnecessary pipeline components\n",
    "# nlp_encoder.disable_pipes([\"parser\", \"ner\"])\n",
    "# nlp_decoder.disable_pipes([\"parser\", \"ner\"])\n",
    "\n",
    "# complete_text_target = ' '.join(y_train)\n",
    "# vocabulary_decoder=create_vocabulary(complete_text_target+\" [START] [PAD] [END] \",\"vocabulary_decoder\",nlp_decoder) \n",
    "# complete_text_origin = ' '.join(X_train)\n",
    "# vocabulary_encoder=create_vocabulary(complete_text_origin+\" [START] [PAD] [END] \",\"vocabulary_encoder\",nlp_encoder) \n",
    "\n",
    "\n",
    "\n",
    "# #inputs_e=create_input_encoder(X_train,nlp_encoder,words_per_phrase,embedding_size)\n",
    "# #inputs_d=create_decoder_input(y_train,embedding_size,words_per_phrase,nlp_decoder)\n",
    "\n",
    "# #inputs_e.shape,inputs_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43abcbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'create_vocabulary' executed in 33.3205 seconds\n",
      "Function 'create_vocabulary' executed in 46.3046 seconds\n"
     ]
    }
   ],
   "source": [
    "nlp_encoder = spacy.load('en_core_web_lg')\n",
    "nlp_decoder = spacy.load('it_core_news_lg')\n",
    "# Disable unnecessary pipeline components\n",
    "nlp_encoder.disable_pipes([\"parser\", \"ner\"])\n",
    "nlp_decoder.disable_pipes([\"parser\", \"ner\"])\n",
    "\n",
    "complete_text_target = ' '.join(y_train)\n",
    "vocabulary_decoder=create_vocabulary(complete_text_target+\" [START] [PAD] [END] \",\"vocabulary_decoder\",nlp_decoder) \n",
    "complete_text_origin = ' '.join(X_train)\n",
    "vocabulary_encoder=create_vocabulary(complete_text_origin+\" [START] [PAD] [END] \",\"vocabulary_encoder\",nlp_encoder) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c77dd7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size=5\n",
    "# num_batches_per_epoch = len(X_train) // batch_size\n",
    "\n",
    "# #batch_size = len(X_train)\n",
    "# words_per_phrase = 100\n",
    "# dk = dv = embedding_size = 300 # constrain of transformer all embedding size both input embedding and attention embedding are the same encoder\n",
    "# num_heads=10\n",
    "# for i in tqdm(range(num_batches_per_epoch)):#desc=f\"Epoch {epoch + 1}/{num_epochs}\"\n",
    "#     start = i * batch_size\n",
    "#     end = start + batch_size\n",
    "#     X_batch = X_train[start:end]\n",
    "#     y_batch = y_train[start:end]\n",
    "#     inputs_e=create_input_encoder(X_batch,vocabulary_encoder,words_per_phrase,embedding_size)\n",
    "#     print(\"inputs_e\",inputs_e.shape)\n",
    "#     inputs_d=create_decoder_input(y_batch,embedding_size,words_per_phrase,vocabulary_decoder)\n",
    "#     print(\"inputs_d\",inputs_d.shape)\n",
    "#     targets_d=create_target(y_batch,words_per_phrase,vocabulary_decoder)\n",
    "#     print(\"targets_d\",targets_d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4f6c509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets_d=create_target(y_train,words_per_phrase,vocabulary_decoder)\n",
    "# targets_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca92084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.01\n",
    "\n",
    "vocab_size=len(vocabulary_decoder)\n",
    "batch_size=5\n",
    "num_batches_per_epoch = len(X_train) // batch_size\n",
    "\n",
    "#batch_size = len(X_train)\n",
    "words_per_phrase = num_phrases=50\n",
    "dk = dv = embedding_size = 300 # constrain of transformer all embedding size both input embedding and attention embedding are the same encoder\n",
    "num_heads=10\n",
    "Qe = cp.random.rand(embedding_size, embedding_size) / cp.sqrt(embedding_size)\n",
    "Ke = cp.random.rand(embedding_size, embedding_size) / cp.sqrt(embedding_size)\n",
    "Ve = cp.random.rand(embedding_size, embedding_size) / cp.sqrt(embedding_size)\n",
    "Qc = cp.random.rand(embedding_size, embedding_size) / cp.sqrt(embedding_size)\n",
    "Kc = cp.random.rand(embedding_size, embedding_size) / cp.sqrt(embedding_size)\n",
    "Vc = cp.random.rand(embedding_size, embedding_size) / cp.sqrt(embedding_size)\n",
    "Qd = cp.random.rand(embedding_size, embedding_size) / cp.sqrt(embedding_size)\n",
    "Kd = cp.random.rand(embedding_size, embedding_size) / cp.sqrt(embedding_size)\n",
    "Vd = cp.random.rand(embedding_size, embedding_size) / cp.sqrt(embedding_size)\n",
    "\n",
    "fl1_size=100\n",
    "Wfl1e=cp.random.rand(embedding_size, fl1_size)   \n",
    "bfl1e=cp.random.rand(fl1_size)\n",
    "\n",
    "Wfl2e=cp.random.rand(fl1_size, dv)   \n",
    "bfl2e=cp.random.rand(dv)\n",
    "\n",
    "\n",
    "Wfl1d=cp.random.rand(embedding_size, fl1_size)    \n",
    "bfl1d=cp.random.rand(fl1_size)\n",
    "\n",
    "Wfl2d=cp.random.rand(fl1_size, dv)    \n",
    "bfl2d=cp.random.rand(dv)\n",
    "\n",
    "Wo=cp.random.rand((words_per_phrase)*embedding_size,vocab_size)   \n",
    "bo=cp.random.rand(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72e65056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2946 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'pad_sequences' executed in 0.0000 seconds\n",
      "Function 'generate_input_encoder' executed in 0.0012 seconds\n",
      "Function 'create_input_encoder' executed in 0.3282 seconds\n",
      "inputs_e (5, 50, 300)\n",
      "Function 'pad_sequences' executed in 0.0000 seconds\n",
      "Function 'create_timestaped_input' executed in 0.0206 seconds\n",
      "Function 'create_decoder_input' executed in 0.0226 seconds\n",
      "inputs_d (50, 5, 50, 300)\n",
      "Function 'pad_sequences' executed in 0.0000 seconds\n",
      "Function 'create_timestaped_input' executed in 0.1383 seconds\n",
      "Function 'create_target' executed in 0.1557 seconds\n",
      "targets_d (50, 5, 17533)\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 0 Loss 148672.85376742092\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 1 Loss 196875.1701041583\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 2 Loss 151598.97890359777\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 3 Loss 204886.78467672179\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 4 Loss 183943.8522219037\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 5 Loss 194413.24612272906\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 6 Loss 106583.9008525828\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 7 Loss 125914.56322528639\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 8 Loss 136611.91305031814\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 9 Loss 258112.42065233496\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 10 Loss 165106.40357611945\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 11 Loss 205077.4387224217\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 12 Loss 220010.62408452528\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 13 Loss 195461.84337407857\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 14 Loss 107325.79376954546\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 15 Loss 101713.93337863436\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 16 Loss 100474.68208045197\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 17 Loss 97511.25506576864\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 18 Loss 23852.478974926322\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 19 Loss 130270.59370081352\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 20 Loss 70421.80196371229\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 21 Loss 62580.118171011774\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 22 Loss 30098.471297048964\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 23 Loss 28631.26407579316\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 24 Loss 8272.727717577007\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 25 Loss 6295.728156732319\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 26 Loss 20615.50485006229\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 27 Loss 57034.572231911916\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 28 Loss 40323.33066099832\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 29 Loss 19604.209477219305\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 30 Loss 30098.471297048964\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 31 Loss 33720.89816534719\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 32 Loss -5.6650004658917015e-06\n",
      "(5, 50, 300) (5, 17533)\n",
      "step 33 Loss "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2946 [02:37<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#print_vocabs(SigmaZout,vocabulary_decoder)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m Loss\u001b[38;5;241m=\u001b[39mloss_calculation(SigmaZout,target)\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstep\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLoss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mLoss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m dLoss_Dout,dLoss_W0,dLoss_b0\u001b[38;5;241m=\u001b[39mderivate_dout(SigmaZout,target,Dout)\n\u001b[0;32m     35\u001b[0m dLoss_Wfl2d,dLoss_bfl2d,dLoss_Wfl1d,dLoss_bfl1d,DLoss_Dt2\u001b[38;5;241m=\u001b[39mderivate_fully_connected_layers_decoder(dLoss_Dout,Dt2,Xd2,var_d2,mu_d2,N_d2,Wfl2d,FLd1,Xd1)\n",
      "File \u001b[1;32mcupy\\_core\\core.pyx:1747\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base.__str__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i in tqdm(range(num_batches_per_epoch)):#desc=f\"Epoch {epoch + 1}/{num_epochs}\"\n",
    "    start = i * batch_size\n",
    "    end = start + batch_size\n",
    "    X_batch = X_train[start:end]\n",
    "    y_batch = y_train[start:end]\n",
    "    inputs_e=create_input_encoder(X_batch,vocabulary_encoder,words_per_phrase,embedding_size)\n",
    "    print(\"inputs_e\",inputs_e.shape)\n",
    "    inputs_d=create_decoder_input(y_batch,embedding_size,words_per_phrase,vocabulary_decoder)\n",
    "    print(\"inputs_d\",inputs_d.shape)\n",
    "    targets_d=create_target(y_batch,words_per_phrase,vocabulary_decoder)\n",
    "    print(\"targets_d\",targets_d.shape)\n",
    "\n",
    "    Ae,Xe,Ect1,mu_e,var_e,Ne,Attention_weights_e,K_E,V_E,Q_E=forward_attention_encoder(inputs_e) \n",
    "    Ecout,mu_e2,var_e2,N_e2,FLe1,Xe1,Xe2=fully_connected_layers_encoder(Ect1)\n",
    "    K_C,V_C=cross_attention_encoder(Ecout)\n",
    "\n",
    "    step=0\n",
    "    learning_rate=0.001\n",
    "    for epochs in range(0,5):\n",
    "        for step in range(inputs_d.shape[0]):\n",
    "            inputs_decoder=inputs_d[step]\n",
    "            target=targets_d[step]\n",
    "            print(inputs_decoder.shape,target.shape)\n",
    "            A_mask,Xd,Dt1,mu_d,var_d,N_d,Attention_weights_masked,Q_D,K_D,V_D=forward_attention_decoder(inputs_decoder)\n",
    "            Q_C=cross_attention_decoder(Dt1)\n",
    "            Dt2, mu_res,var_res,N_res,Res,Attention_weights_cross=cross_attention(Q_C,K_C,V_C,Dt1)\n",
    "            Dout,mu_d2,var_d2,N_d2,Xd2,Xd1,FLd1=fully_connected_layers_decoder(Dt2)\n",
    "            SigmaZout=output_layer(Dout)\n",
    "            \n",
    "            \n",
    "            #print_vocabs(SigmaZout,vocabulary_decoder)\n",
    "            Loss=loss_calculation(SigmaZout,target)\n",
    "            print(\"step\",step,\"Loss\",Loss)\n",
    "            dLoss_Dout,dLoss_W0,dLoss_b0=derivate_dout(SigmaZout,target,Dout)\n",
    "            dLoss_Wfl2d,dLoss_bfl2d,dLoss_Wfl1d,dLoss_bfl1d,DLoss_Dt2=derivate_fully_connected_layers_decoder(dLoss_Dout,Dt2,Xd2,var_d2,mu_d2,N_d2,Wfl2d,FLd1,Xd1)\n",
    "            dLoss_Qc,dLoss_Kc,dLoss_Vc,Attention_weights_cross,dLoss_Dt1_a,dLoss_Acr=derivative_cross_attention(DLoss_Dt2,Res,var_res,mu_res,N_res,Attention_weights_cross,K_C,V_C,Q_C,Ecout,Dt1)\n",
    "            dLoss_Kd,dLoss_Qd,dLoss_Vd,dLoss_inputd_a,dLoss_Amask=derivative_attention_decoder(dLoss_Acr,Attention_weights_cross,dLoss_Dt1_a,Attention_weights_masked,Q_D,V_D,K_D,K_C,V_C,Xd,var_d,mu_d,N_d,inputs_decoder)\n",
    "            dLoss_inputd,dLoss_dWemb_decoder=derivative_input_decoder(dLoss_Amask,Attention_weights_masked,K_D,V_D,Q_D,dLoss_inputd_a,inputs_decoder)\n",
    "            #print(\"dLoss_W0\",dLoss_W0[0])\n",
    "            Wo=Wo-learning_rate*dLoss_W0.T\n",
    "            \n",
    "            #print(\"W0\",W0[0])\n",
    "            bo=bo-learning_rate*dLoss_b0\n",
    "            Wfl2d=Wfl2d-learning_rate*dLoss_Wfl2d.T\n",
    "            bfl2d=bfl2d-learning_rate*dLoss_bfl2d\n",
    "            Wfl1d=Wfl1d-learning_rate*dLoss_Wfl1d.T\n",
    "            bfl1d=bfl1d-learning_rate*dLoss_bfl1d\n",
    "            Qc=Qc-learning_rate*dLoss_Qc\n",
    "            Kc=Kc-learning_rate*dLoss_Kc\n",
    "            Vc=Vc-learning_rate*dLoss_Vc\n",
    "            Qd=Qd-learning_rate*dLoss_Qd\n",
    "            Kd=Kd-learning_rate*dLoss_Kd\n",
    "            Vd=Vd-learning_rate*dLoss_Vd\n",
    "            inputs_d=inputs_d-learning_rate*dLoss_dWemb_decoder\n",
    "            #print(input_d)\n",
    "\n",
    "        dLoss_Ecout=derivative_Ecout(Attention_weights_cross,dLoss_Acr,Q_C,V_C)\n",
    "        dLoss_dWfl2e,dLoss_dbfl2e,dLoss_Wfl1e,dLoss_bfl1e,dLoss_Ect1=derivate_fully_connected_layers_encoder(dLoss_Ecout,Ect1,Xe2,var_e2,mu_e2,N_e2,FLe1,Xe1)\n",
    "        dLoss_dQe,dLoss_dKe,dLoss_dVe,dLoss_inpute_a,dLoss_Ae=derivative_attention_encoder(dLoss_Ect1,Xe,var_e,mu_e,Ne,Attention_weights_e,K_E,V_E,Q_E)\n",
    "        dLoss_inpute,dLoss_dWemb_encoder=derivative_input_encoder(dLoss_Ae,Attention_weights_e,K_E,V_E,Q_E,dLoss_inpute_a)\n",
    "        Wfl2e=Wfl2e-learning_rate*cp.sum(cp.transpose(dLoss_dWfl2e ,(0,2,1)),axis=0) \n",
    "        bfl2e=bfl2e-learning_rate*bfl2e\n",
    "        Wfl1e=Wfl1e-learning_rate*cp.sum(cp.transpose(dLoss_Wfl1e ,(0,2,1)),axis=0) \n",
    "        bfl1e=bfl1e-learning_rate*bfl1e\n",
    "        Qe=Qe-learning_rate*dLoss_dQe\n",
    "        Ke=Ke-learning_rate*dLoss_dKe\n",
    "        Ve=Ve-learning_rate*dLoss_dVe\n",
    "        #inputs_e=inputs_e-learning_rate*dLoss_dWemb_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9c4963",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cf2f47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocabulary[\"[PAD]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37619eef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
