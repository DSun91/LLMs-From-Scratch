{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b060598",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b89336dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import re\n",
    "import cupy as cp\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jax\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "np.set_printoptions(edgeitems=30, linewidth=100000, formatter=dict(float=lambda x: \"%.3g\" % x))\n",
    "\n",
    "\n",
    "def log_time(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()  # Record start time\n",
    "        result = func(*args, **kwargs)  # Execute the wrapped function\n",
    "        end_time = time.time()  # Record end time\n",
    "        elapsed_time = end_time - start_time\n",
    "        # print(f\"Function '{func.__name__}' executed in {elapsed_time:.4f} seconds\")\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "class Helper: \n",
    "    \n",
    "    def get_positional_encoding(self,seq_len, d_model):\n",
    "        \"\"\"\n",
    "        Returns a non-learnable (sinusoidal) positional encoding.\n",
    "\n",
    "\n",
    "        seq_len: Length of the input sequence.\n",
    "        d_model: Dimension of the embeddings.\n",
    "        \"\"\"\n",
    "        pos = cp.arange(seq_len)[:, cp.newaxis]  # Shape: [seq_len, 1]\n",
    "        i = cp.arange(d_model)[cp.newaxis, :]  # Shape: [1, d_model]\n",
    "\n",
    "        angle_rates = 1 / cp.power(10000, (2 * (i // 2)) / cp.float32(d_model))\n",
    "\n",
    "        # Apply sine to even indices, cosine to odd indices\n",
    "        pos_encoding = cp.zeros((seq_len, d_model))\n",
    "        pos_encoding[:, 0::2] = cp.sin(pos * angle_rates[:, 0::2])  # sine on even indices\n",
    "        pos_encoding[:, 1::2] = cp.cos(pos * angle_rates[:, 1::2])  # cosine on odd indices\n",
    "\n",
    "        return pos_encoding\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        # Subtract the max value for numerical stability\n",
    "        max_logits = cp.max(x, axis=-1, keepdims=True)\n",
    "        exp_logits = cp.exp(x - max_logits)\n",
    "        return exp_logits / cp.sum(exp_logits, axis=-1, keepdims=True)\n",
    "\n",
    " \n",
    "    # @log_time\n",
    "    def pad_sequence(self,seq, max_len, pad_value=0):\n",
    "        \"\"\"Pad a sequence with a given value up to max_len.\"\"\"\n",
    "        current_len = seq.shape[0]\n",
    "        pad_width = max_len - current_len\n",
    "        if pad_width > 0:\n",
    "            # Pad sequence with zeros (or any pad_value you provide)\n",
    "            seq = cp.pad(seq, ((0, pad_width), (0, 0)), mode='constant', constant_values=pad_value)\n",
    "        return seq\n",
    "\n",
    "\n",
    "    @log_time\n",
    "    def create_timestaped_input(self,input_d, words_per_phrase):\n",
    "        input_translation = []\n",
    "        for j in range(input_d.shape[0]):\n",
    "            # Create padded sequences\n",
    "            padded_sequences = [self.pad_sequence(input_d[j][0:i], words_per_phrase) for i in range(1, input_d.shape[1] + 1)]\n",
    "            input_translation.append(padded_sequences)\n",
    "        return cp.array(input_translation)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    " \n",
    "    def redimension(self,X):\n",
    "        return cp.concatenate(cp.swapaxes(X, 0, 1), axis=-1)\n",
    "    \n",
    "    @log_time\n",
    "    def create_vocabulary(self,complete_text, name, nlp):\n",
    "        # Use re.findall to split considering punctuation\n",
    "        text = re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', complete_text)\n",
    "\n",
    "        words_list = list(set(text))\n",
    "\n",
    "        vocabulary = dict()\n",
    "\n",
    "        for i, j in enumerate(words_list):\n",
    "            # vocabulary[j]=(jax.random.uniform(jax.random.key(cp.random.randint(10000)),embedding_size),i)\n",
    "            vocabulary[j] = (cp.array(nlp(j).vector), i)\n",
    "            # print(j,len(cp.array(nlp(j).vector)))\n",
    "\n",
    "        # print(vocabulary)\n",
    "        # print(\"Vocabulary size: \", len(vocabulary))\n",
    "        with open(f\"data/{name}.pkl\", 'wb') as handle:\n",
    "            pickle.dump(vocabulary, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        return vocabulary\n",
    "\n",
    "\n",
    "    @log_time\n",
    "    def pad_sequences(self,sentences, lenght, pad_token='[PAD]', target_type=None):\n",
    "        \"\"\"\n",
    "        Pads the input sentences to have the same length by adding [PAD] tokens at the end.\n",
    "        \"\"\"\n",
    "        regex_str=r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n'\n",
    "\n",
    "        if target_type == \"encoder\":\n",
    "            # Split each sentence into words\n",
    "            tokenized_sentences = [[\"[START]\"] + re.findall(regex_str, sentence) + [\"[END]\"] for sentence in sentences]\n",
    "        elif target_type == \"decoder\":\n",
    "            tokenized_sentences = [[\"[START]\"] + re.findall(regex_str, sentence) for sentence in sentences]\n",
    "        elif target_type == \"target\":\n",
    "            tokenized_sentences = [re.findall(regex_str, sentence) + [\"[END]\"] for sentence in sentences]\n",
    "        # print(tokenized_sentences)\n",
    "        if lenght == 0:\n",
    "            # Find the maximum sentence length\n",
    "            max_len = max(len(sentence) for sentence in tokenized_sentences)\n",
    "        else:\n",
    "            max_len = lenght\n",
    "\n",
    "        # Pad each sentence with the [PAD] token to make them of equal length\n",
    "        padded_sentences = [\" \".join(sentence + [pad_token] * (max_len - len(sentence))) for sentence in\n",
    "                            tokenized_sentences]\n",
    "\n",
    "        return padded_sentences\n",
    "\n",
    "    def print_matrix(self,X):\n",
    "        for i in X:\n",
    "            print(i)\n",
    "\n",
    "    @log_time\n",
    "    def generate_input_encoder(self,x_batch, vocabulary_encoder, max_words_per_phrase):\n",
    "\n",
    "\n",
    "        regex_str=r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n'\n",
    "        x_train = self.pad_sequences(x_batch, max_words_per_phrase, target_type=\"encoder\")# here are string\n",
    "        \n",
    "        #print_matrix(x_train) \n",
    "    \n",
    "        xi = []\n",
    "        # print(x_batch)\n",
    "        phrase_vectors_x = [re.findall(regex_str, x) for x in x_train]\n",
    "        \n",
    "        phrase_vectors_x = [i[0:max_words_per_phrase] for i in phrase_vectors_x]\n",
    "        #print(phrase_vectors_x) \n",
    "       \n",
    "        # print(\"input_encoder:\")\n",
    "        # self.print_matrix(phrase_vectors_x)\n",
    "        xi = cp.array([[vocabulary_encoder[word][0] for word in phrase_vector] for phrase_vector in phrase_vectors_x])\n",
    "\n",
    "        return xi\n",
    "    \n",
    "    @log_time\n",
    "    def create_input_encoder(self,X, vocabulary_encoder, max_words_per_phrase, embedding_size):\n",
    "\n",
    "        pos_encoding = self.get_positional_encoding(max_words_per_phrase, embedding_size)\n",
    "        #print(pos_encoding)\n",
    "        inputs_e = self.generate_input_encoder(X, vocabulary_encoder, max_words_per_phrase)\n",
    "        \n",
    "        #print(inputs_e)\n",
    "\n",
    "        inputs_e =inputs_e + pos_encoding\n",
    "        return inputs_e\n",
    "    \n",
    "    def generate_target(self,x_batch, vocabulary_encoder, max_words_per_phrase):\n",
    "\n",
    "        regex_str=r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n'\n",
    "\n",
    "        y_target = self.pad_sequences(x_batch, max_words_per_phrase, target_type=\"target\")# here are one string with the padd\n",
    "        \n",
    "        \n",
    "        target_vector = [re.findall(regex_str, x) for x in y_target]\n",
    "\n",
    "        #print(target_vector)\n",
    "        #print_matrix(phrase_vectors_x) \n",
    "        #target_vector = [i[0:max_words_per_phrase] for i in target_vector]\n",
    "        target_vector = [i[0:max_words_per_phrase] for i in target_vector]\n",
    "        target_vector = cp.array([[self.get_one_hot(i,vocabulary_encoder) for i in phrase] for phrase in target_vector])\n",
    "    \n",
    "        return target_vector\n",
    "    \n",
    "    def generate_target_sparse_categorical(self,y_batch, vocabulary_encoder, max_words_per_phrase):\n",
    "\n",
    "        regex_str=r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n'\n",
    "\n",
    "        y_target = self.pad_sequences(y_batch, max_words_per_phrase, target_type=\"target\")# here are one string with the padd\n",
    "        \n",
    "        \n",
    "        target_vector = [re.findall(regex_str, x) for x in y_target]\n",
    "\n",
    "        #print(target_vector)\n",
    "        #print_matrix(phrase_vectors_x) \n",
    "        #target_vector = [i[0:max_words_per_phrase] for i in target_vector]\n",
    "        target_vector = [i[0:max_words_per_phrase] for i in target_vector]\n",
    "        target_vector = cp.array([[vocabulary_encoder[i][1] for i in phrase] for phrase in target_vector])\n",
    "    \n",
    "        return target_vector\n",
    "    \n",
    "    @log_time\n",
    "    def create_target(self,X, vocabulary_encoder, max_words_per_phrase): \n",
    "        inputs_e = self.generate_target(X, vocabulary_encoder, max_words_per_phrase) \n",
    "        return inputs_e\n",
    "\n",
    "    @log_time\n",
    "    def create_decoder_input(self,y_train, embedding_size, max_words_per_phrase, vocabulary_decoder):\n",
    "\n",
    "        decoder_input = self.pad_sequences(y_train, lenght=max_words_per_phrase, target_type=\"decoder\")\n",
    "        #print_matrix(decoder_input)\n",
    "        decoder_input = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', i) for i in decoder_input]\n",
    "        \n",
    "        if max_words_per_phrase == None:\n",
    "            max_words_per_phrase = len(decoder_input[0])\n",
    "\n",
    "        phrase_vectors_y = [i[0:max_words_per_phrase] for i in decoder_input]\n",
    "        # for sentence in phrase_vectors_y:\n",
    "        #     print(sentence)\n",
    "        #print_matrix(phrase_vectors_y)\n",
    "        \n",
    "       \n",
    "\n",
    "        # print(\"decoder_input:\")\n",
    "        # self.print_matrix(decoder_input)\n",
    "        yi = cp.array([[vocabulary_decoder[word][0] for word in phrase_vector] for phrase_vector in phrase_vectors_y])\n",
    "        \n",
    "        pos_encoding = self.get_positional_encoding(max_words_per_phrase, embedding_size)\n",
    "        # print(pos_encoding.shape,yi.shape)\n",
    "        yi = yi + pos_encoding\n",
    "        #print_matrix(yi)\n",
    "        # decoder_inputs = cp.array(cp.swapaxes(self.create_timestaped_input(yi, max_words_per_phrase), 0, 1))\n",
    "        \n",
    "        # # decoder_inputs[zero_rows] = vocabulary_decoder[\"[PAD]\"][0]\n",
    "        # for i in range(decoder_inputs.shape[0]):\n",
    "        #     for j in range(decoder_inputs[i].shape[0]):\n",
    "        #         zero_rows = cp.all(decoder_inputs[i][j] == 0, axis=1)\n",
    "\n",
    "        #         decoder_inputs[i][j][zero_rows] = vocabulary_decoder[\"[PAD]\"][0]\n",
    "\n",
    "        # decoder_inputs = cp.array(decoder_inputs)\n",
    "        #print(decoder_inputs[2])\n",
    "        #print(decoder_inputs)\n",
    "        return yi\n",
    "    # @log_time\n",
    "    def update_wembedding_encoder(self,learning_rate, x_batch, dLoss_dWemb_encoder, vocabulary, max_words_per_phrase):\n",
    "        x_train = self.pad_sequences(x_batch, max_words_per_phrase, target_type=\"encoder\")\n",
    "\n",
    "        phrase_vectors_x = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', x) for x in x_train]\n",
    "        phrase_vectors_x = [i[:max_words_per_phrase] for i in phrase_vectors_x]\n",
    "\n",
    "        for phrase in range(dLoss_dWemb_encoder.shape[0]):\n",
    "            for position, word in enumerate(phrase_vectors_x[phrase]):\n",
    "                # Retrieve current embedding\n",
    "                current_embedding, index = vocabulary[word]\n",
    "\n",
    "                # Calculate the updated embedding using the gradient\n",
    "                updated_embedding = current_embedding - learning_rate * dLoss_dWemb_encoder[phrase][position]\n",
    "\n",
    "                # Update the vocabulary with the new embedding\n",
    "                vocabulary[word] = (updated_embedding, index)\n",
    "\n",
    "        return vocabulary\n",
    "\n",
    "\n",
    "\n",
    "    # @log_time\n",
    "    def update_wembedding_decoder(self, learning_rate,y_batch, dLoss_dWemb_decoder, vocabulary,max_words_per_phrase):\n",
    "        decoder_input = self.pad_sequences(y_batch, lenght=max_words_per_phrase, target_type=\"decoder\")\n",
    "        decoder_input = [i.split() for i in decoder_input]\n",
    "\n",
    "        if max_words_per_phrase is None:\n",
    "            max_words_per_phrase = len(decoder_input[0])\n",
    "\n",
    "        phrase_vectors_y = [i[:max_words_per_phrase] for i in decoder_input]\n",
    "\n",
    "        for phrase in range(dLoss_dWemb_decoder.shape[0]):\n",
    "            for position, word in enumerate(phrase_vectors_y[phrase]):\n",
    "                # Retrieve current embedding for the word\n",
    "                current_embedding, index = vocabulary[word]\n",
    "\n",
    "                # Apply the gradient update\n",
    "                updated_embedding = current_embedding - learning_rate * dLoss_dWemb_decoder[phrase][position]\n",
    "\n",
    "                # Update the vocabulary with the new embedding\n",
    "                vocabulary[word] = (updated_embedding, index)\n",
    "\n",
    "        return vocabulary\n",
    "\n",
    "  \n",
    "    # @log_time\n",
    "    def get_one_hot(self,word, vocabulary_decoder):\n",
    "        # print(word)\n",
    "        vocab_size = len(vocabulary_decoder)\n",
    "        one_hot_vector = cp.zeros(vocab_size)\n",
    "        one_hot_vector[vocabulary_decoder[word][1]] = 1\n",
    "        # print(vocabulary_decoder[word][1])\n",
    "        # print(np.where(one_hot_vector== 1))\n",
    "        # print(cp.sum(one_hot_vector))\n",
    "        return one_hot_vector\n",
    " \n",
    "    \n",
    "    def log_sparse_entropy(self,ans,target,y_batch,step):\n",
    "        #print(\"target\",target)\n",
    "        #print(\"ans\",ans)\n",
    "        counter_found=0\n",
    "        total_lenght=len(ans)\n",
    "        print(f\"----DECODER--step {step}---\")\n",
    "        self.print_matrix(y_batch)\n",
    "        print(\"target\",target)\n",
    "        indexes=[]\n",
    "        yy=[re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', xx) for xx in y_batch] \n",
    "        for idx, values in enumerate(ans):\n",
    "            max_index = cp.argmax(values)\n",
    "            indexes.append(max_index)\n",
    "             \n",
    "            if max_index==target[idx]:\n",
    "                counter_found+=1\n",
    "            print(f\"{idx + 1} base: {' '.join(yy[idx][0:step+1])} -> {max_index}\")\n",
    "        print(\"indexes\",indexes)\n",
    "        print(\"accuracy batch:\",round(counter_found/total_lenght,2))\n",
    "        \n",
    "    def accruacy_sparse_entropy(self,ans,target):\n",
    "        counter_found=0\n",
    "        total_lenght=len(ans) \n",
    "        for idx, values in enumerate(ans):\n",
    "            max_index = cp.argmax(values) \n",
    "            if max_index==target[idx]:\n",
    "                counter_found+=1\n",
    "             \n",
    "        accuracy_batch_on_step=round(counter_found/total_lenght,2)\n",
    "        return accuracy_batch_on_step\n",
    "    \n",
    "\n",
    "    def print_target_vs_prediction_sparce_loss(self,ans,target): \n",
    "        indexes=[] \n",
    "        for idx, values in enumerate(ans):\n",
    "            max_index = np.argmax(values).item()\n",
    "            indexes.append(max_index) \n",
    "        print(\"target\",target)\n",
    "        print(\"indexes\",indexes)\n",
    "        \n",
    "  \n",
    "def clip_gradient(gradient,threshold):\n",
    "    return cp.clip(gradient, -threshold, threshold)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "class layer_dropout: \n",
    "\n",
    "    def __init__(self,dropout_rate=0.1):\n",
    "        self.dropout_rate=dropout_rate \n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self,X):   \n",
    "        self.mask = (cp.random.rand(*X.shape) > self.dropout_rate)#.astype(cp.float64)\n",
    "        result = X * self.mask \n",
    "        #print(self.mask )\n",
    "        return result\n",
    "    #\n",
    "    def grad(self, X):\n",
    "        # Only pass gradients through neurons that were not dropped out\n",
    "        grad_input = X * self.mask\n",
    "        grad_input = clip_gradient(grad_input, 1)\n",
    "        return grad_input\n",
    "\n",
    "class layer_normalization:\n",
    "    def __init__(self, threshold, epsilon=1e-6):\n",
    "        self.epsilon = epsilon\n",
    "        self.mu = 0\n",
    "        self.var = 0\n",
    "        self.N = 0\n",
    "        self.beta = 0\n",
    "        self.alpha = 0\n",
    "        self.clipping_threshold = threshold\n",
    "\n",
    "    def grad_sigma(self):\n",
    "        result = (2/self.N) * (self.x - self.mu) * (1 - self.grad_mu())\n",
    "        return result\n",
    "\n",
    "    def grad_mu(self):\n",
    "        result = 1/self.N\n",
    "        return result\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.N = x.shape[-1]\n",
    "        self.alpha = cp.ones(self.N)\n",
    "        self.beta = cp.zeros(self.N)\n",
    "        self.mu = cp.mean(x, axis=-1, keepdims=True)\n",
    "        self.var=cp.var(x, axis=-1, keepdims=True)\n",
    "\n",
    "        x_norm = self.alpha * (x - self.mu) /cp.sqrt(self.var+self.epsilon) + self.beta\n",
    "        return x_norm\n",
    "\n",
    "    def grad_A(self):\n",
    "        a = self.alpha * cp.sqrt(self.var+self.epsilon)\n",
    "        b = self.x * self.alpha * 0.5 * ((self.var+self.epsilon)**(-0.5)) * self.grad_sigma()\n",
    "        c = (self.var+self.epsilon)\n",
    "        result = (a - b) / c\n",
    "        #result = clip_gradient(result, self.clipping_threshold)\n",
    "        return result\n",
    "\n",
    "    def grad_B(self):\n",
    "        a = self.alpha * self.grad_mu() * cp.sqrt(self.var+self.epsilon)\n",
    "        b = self.mu * self.alpha * 0.5 * ((self.var+self.epsilon)**(-0.5)) * self.grad_sigma()\n",
    "        c = (self.var+self.epsilon)\n",
    "        result = (a - b) / c\n",
    "        #result = clip_gradient(result, self.clipping_threshold)\n",
    "        return result\n",
    "\n",
    "    def dL_dalpha(self):\n",
    "        result = self.dLoss_dy * (self.x - self.mu) / cp.sqrt(self.var+self.epsilon)\n",
    "        #result = clip_gradient(result, self.clipping_threshold)\n",
    "        return result\n",
    "\n",
    "    def dL_dbeta(self):\n",
    "        result = self.dLoss_dy\n",
    "        #result = clip_gradient(result, self.clipping_threshold)\n",
    "        return result\n",
    "\n",
    "    def backpropagation(self, dLoss_dy):\n",
    "        self.dLoss_dy = dLoss_dy\n",
    "        result = self.dLoss_dy * (self.grad_A() + self.grad_B())\n",
    "        #result = clip_gradient(result, self.clipping_threshold)\n",
    "        return result\n",
    "\n",
    "    def params_update(self, learning_rate):\n",
    "        self.beta = self.beta - self.dL_dbeta() * learning_rate\n",
    "        self.alpha = self.alpha - self.dL_dalpha() * learning_rate\n",
    "\n",
    "\n",
    "\n",
    "class output_stack:\n",
    "    def __init__(self,embedding_size,vocabulary_size,threshold):\n",
    "        self.final_projection_layer=linear_layer(embedding_size,vocabulary_size,threshold=threshold,out=True)\n",
    "        self.clipping_threshold=threshold\n",
    "\n",
    "    def softmax(self,x):\n",
    "        # Subtract the max value for numerical stability\n",
    "        max_logits = cp.max(x, axis=-1, keepdims=True)\n",
    "        exp_logits = cp.exp(x - max_logits)\n",
    "        return exp_logits / cp.sum(exp_logits, axis=-1, keepdims=True)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        SoftmaxOutput=self.softmax(self.final_projection_layer.forward(x)) \n",
    "        return SoftmaxOutput\n",
    "\n",
    "    \n",
    "    def cross_entropy_loss(self,SigmaZout, target):\n",
    "        epsilon = 1e-12  # Small constant to avoid log(0)\n",
    "        SigmaZout = cp.clip(SigmaZout, epsilon, 1 - epsilon)  # Clipping predictions\n",
    "        return -cp.sum(target * cp.log(SigmaZout), axis=1).mean() \n",
    "    \n",
    "    def grad_cross_entropy(self,softmax_output,target):\n",
    "        dL_dZ = dL_dZ=softmax_output-target\n",
    "        dL_dDout=self.final_projection_layer.grad(dL_dZ)  \n",
    "        #dL_dDout=clip_gradient(dL_dDout,self.clipping_threshold)\n",
    "        return dL_dDout\n",
    "    \n",
    "    def sparse_categorical_crossentropy(self, probabilities, labels): \n",
    "        #print(\"probabilities.shape\", probabilities.shape)\n",
    "        #print(\"labels.shape\", labels.shape)\n",
    "        \n",
    "        # Unpack batch and sequence dimensions\n",
    "        batch_size, seq_length = labels.shape\n",
    "        \n",
    "        # Gather correct class probabilities for each position in the batch and sequence\n",
    "        correct_class_probs = probabilities[np.arange(batch_size)[:, None], np.arange(seq_length), labels] \n",
    "        \n",
    "        # Calculate the log loss and average it\n",
    "        loss = -np.log(correct_class_probs + 1e-8)\n",
    "        return np.mean(loss)\n",
    "    \n",
    "    def grad(self,dl_dy):\n",
    "        return self.final_projection_layer.grad(dl_dy)\n",
    "\n",
    "\n",
    "    def grad_sparse_cross_entropy(self, softmax_output, target):\n",
    "        dL_dZ = softmax_output.copy()  # Create a copy of the softmax output\n",
    "        \n",
    "        # Adjust indexing to handle both batch and sequence dimensions\n",
    "        batch_size, seq_length = target.shape\n",
    "        dL_dZ[np.arange(batch_size)[:, None], np.arange(seq_length), target] -= 1\n",
    "        \n",
    "        # Compute gradient through final projection layer\n",
    "        dL_dDout = self.final_projection_layer.grad(dL_dZ)  \n",
    "        # dL_dDout = clip_gradient(dL_dDout, self.clipping_threshold)\n",
    "        return dL_dDout\n",
    " \n",
    "    def update_weights(self,learning_rate):\n",
    "        self.final_projection_layer.update_weights(learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "class linear_layer: \n",
    "    def __init__(self,input_size,output_size,out=False,only_weights=False,threshold=1):\n",
    "        if out==True:\n",
    "            self.W=cp.random.rand(input_size, output_size)/cp.sqrt(input_size)\n",
    "        else:\n",
    "            self.W=cp.random.rand(input_size, output_size) \n",
    "        if only_weights==True:\n",
    "            pass\n",
    "        else: \n",
    "            self.b=cp.random.rand(output_size)\n",
    "        self.clipping_threshold=threshold\n",
    "      \n",
    "    def forward(self,x): \n",
    "        self.x=x\n",
    "        Xout = cp.matmul(x, self.W) + self.b \n",
    "        return Xout\n",
    "    \n",
    "    def forward_weights_only(self,x): \n",
    "        self.x=x\n",
    "        Xout = cp.matmul(x, self.W) \n",
    "        return Xout\n",
    "     \n",
    "    def grad(self,dL_dy):\n",
    "        self.dL_dy = dL_dy\n",
    "        # print(\"self.dL_dy\",self.dL_dy)\n",
    "        return self.dL_dy@self.W.T\n",
    "    \n",
    "    def dLoss_dW(self):\n",
    "        return cp.sum(cp.transpose(self.dL_dy,(0,2,1))@self.x,axis=0).T\n",
    "    \n",
    "    def dLoss_db(self):\n",
    "        return self.dL_dy \n",
    "\n",
    "    def update_weights(self,learning_rate):\n",
    "        self.W=self.W-learning_rate*self.dLoss_dW()\n",
    "        self.b=self.b-learning_rate*self.dLoss_db()\n",
    "\n",
    "    def update_weights_only(self,learning_rate):\n",
    "        self.W=self.W-learning_rate*self.dLoss_dW()\n",
    "        \n",
    "         \n",
    "class fully_connected_block:\n",
    "    def __init__(self,embedding_size,hidden_size,clipping_threshold):\n",
    "        self.embedding_size=embedding_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.linear_layer_1=linear_layer(self.embedding_size,self.hidden_size,threshold=clipping_threshold)\n",
    "        self.linear_layer_2=linear_layer(self.hidden_size,self.embedding_size,threshold=clipping_threshold)\n",
    "        self.dropout=layer_dropout()\n",
    "        self.ReLu=ReLu_layer()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x_1=self.linear_layer_1.forward(x)\n",
    "        x_1_r=self.ReLu.forward_leaky(x_1)\n",
    "        x_1_rd=self.dropout.forward(x_1_r)\n",
    "        x_2=self.linear_layer_2.forward(x_1_rd)\n",
    "        return x_2\n",
    "    \n",
    "    def grad(self,dL_dy):\n",
    "        dL_dx_1_rd=self.linear_layer_2.grad(dL_dy)\n",
    "        dL_dx_1_r=self.dropout.grad(dL_dx_1_rd)\n",
    "        dL_dx_1=self.ReLu.backward_leaky(dL_dx_1_r)\n",
    "        dL_dx=self.linear_layer_1.grad(dL_dx_1)\n",
    "        return dL_dx\n",
    "    \n",
    "    def update_weights(self,learning_rate):\n",
    "        self.linear_layer_1.update_weights(learning_rate)\n",
    "        self.linear_layer_2.update_weights(learning_rate)\n",
    "        \n",
    " \n",
    "class ReLu_layer:\n",
    "    def __init__(self,alpha=0.0001):\n",
    "        self.alpha=alpha \n",
    "    def forward_leaky(self,X):\n",
    "        self.X=X\n",
    "        return cp.where(X > 0, X, self.alpha * X)\n",
    "\n",
    "    def forward(self,X): \n",
    "        self.X=X\n",
    "        return cp.maximum(0,self.X)\n",
    "    \n",
    "    def backward(self, dLoss): \n",
    "        # Gradient of ReLU is 1 for x > 0, else 0\n",
    "        dx = dLoss * (self.X > 0)  # Only propagate gradients for inputs > 0\n",
    "        return dx\n",
    "    \n",
    "    def backward_leaky(self, dLoss): \n",
    "        dx = dLoss * cp.where(self.X > 0, 1, self.alpha)  # Gradient: 1 for x > 0, else alpha\n",
    "        return dx\n",
    "\n",
    "class residual_layer:\n",
    "    def __init__(self,threshold):\n",
    "        self.dropuot=layer_dropout()\n",
    "        self.normalization=layer_normalization(threshold=threshold)\n",
    "        self.clipping_threshold=threshold\n",
    "\n",
    "\n",
    "    def forward(self,x,sublayer_output): \n",
    "        residual=self.dropuot.forward(sublayer_output)+x\n",
    "        result=self.normalization.forward(residual)\n",
    "        return result\n",
    "    \n",
    "    def grad(self,dL_dy): \n",
    "        #dL_dy=clip_gradient(dL_dy,1)\n",
    "       \n",
    "        dl_dNorm=self.normalization.backpropagation(dL_dy)\n",
    "        #dl_dNorm=clip_gradient(dl_dNorm,1)\n",
    "\n",
    "        sublayer_grad=self.dropuot.grad(dl_dNorm)\n",
    "        residual_grad=dl_dNorm\n",
    "        return sublayer_grad,residual_grad\n",
    "    \n",
    "    def update_weights(self,learning_rate):\n",
    "        self.normalization.params_update(learning_rate)\n",
    "\n",
    "\n",
    "      \n",
    "class multihead_attention: \n",
    "    def __init__(self,embedding_size,num_heads,batch_size,threshold):\n",
    "        self.num_heads=num_heads\n",
    "        self.dk=embedding_size//num_heads\n",
    "        self.batch_size=batch_size\n",
    "        self.embedding_size=embedding_size\n",
    "        self.q=linear_layer(self.embedding_size,self.embedding_size,only_weights=True,threshold=threshold)\n",
    "        self.k=linear_layer(self.embedding_size,self.embedding_size,only_weights=True,threshold=threshold)\n",
    "        self.v=linear_layer(self.embedding_size,self.embedding_size,only_weights=True,threshold=threshold)\n",
    "        self.projection_layer=linear_layer(self.embedding_size,self.embedding_size,only_weights=True,threshold=threshold) \n",
    "        self.helper=Helper()\n",
    "        self.clipping_threshold=threshold\n",
    "        \n",
    "    def reshape_heads(self,Q,K,V):\n",
    "        self.Q = cp.swapaxes(cp.array(np.array_split(Q, self.num_heads, axis=2)), 0, 1)\n",
    "        # print(\"Qval.shape: \",Q_E.shape)\n",
    "        self.K = cp.swapaxes(cp.array(np.array_split(K, self.num_heads, axis=2)), 0, 1)\n",
    "        # print(\"Kval.shape: \",K_E.shape)\n",
    "        self.V = cp.swapaxes(cp.array(np.array_split(V, self.num_heads, axis=2)), 0, 1)\n",
    "        #return self.Q,self.K,self.V\n",
    "\n",
    "    def QKV(self,input_q,input_k,input_v): \n",
    "        Q=self.q.forward_weights_only(input_q)\n",
    "        K=self.k.forward_weights_only(input_k)\n",
    "        V=self.v.forward_weights_only(input_v) \n",
    "        self.reshape_heads(Q,K,V)\n",
    "        \n",
    "\n",
    "    def attention_weights(self): \n",
    "        QKscaled =cp.matmul(self.Q, cp.transpose(self.K, (0, 1, 3, 2))) / cp.sqrt(self.K.shape[-1])  \n",
    "        self.Attention_weights = self.helper.softmax(QKscaled)\n",
    "         \n",
    "\n",
    "    def forward_attention(self,input_q,input_k,input_v): \n",
    "        self.input_q=input_q\n",
    "        self.input_k=input_k\n",
    "        self.input_v=input_v\n",
    "        self.QKV(input_q,input_k,input_v)\n",
    "        self.attention_weights()\n",
    "        Attention = cp.matmul(self.Attention_weights, self.V) \n",
    "        Attention = cp.array([cp.concatenate(Attention[i], axis=1) for i in range(self.batch_size)]) \n",
    "        Output=self.projection_layer.forward_weights_only(Attention) \n",
    "        return Output\n",
    "    \n",
    "    def forward_masked_attention(self,input_q,input_k,input_v,mask_size):\n",
    "        self.input_q=input_q\n",
    "        self.input_k=input_k\n",
    "        self.input_v=input_v \n",
    "        self.QKV(input_q,input_k,input_v)\n",
    "        self.attention_weights_masked(mask_size)\n",
    "        Attention = cp.matmul(self.Attention_weights, self.V) \n",
    "        Attention = cp.array([cp.concatenate(Attention[i], axis=1) for i in range(self.batch_size)]) \n",
    "        Output=self.projection_layer.forward_weights_only(Attention) \n",
    "        return Output\n",
    "    \n",
    "    def attention_weights_masked(self,mask_size):\n",
    "        #mask_size =  words_per_phrase \n",
    "\n",
    "        QKscaled = cp.matmul(self.Q, cp.transpose(self.K, (0, 1, 3, 2))) / cp.sqrt(self.K.shape[-1])\n",
    "        mask = cp.tril(cp.ones((mask_size, mask_size)))  # (9, 9) lower triangular matrix\n",
    "        mask[mask == 0]=-cp.inf  # Set future tokens to -inf\n",
    "        mask[mask == 1]=0  # Set allowed tokens to 0\n",
    "        self.mask = mask.reshape(1, 1, mask_size, mask_size)\n",
    "        QKscaled = QKscaled + self.mask\n",
    "        self.Attention_weights = self.helper.softmax(QKscaled)\n",
    "        \n",
    "    \n",
    "    def diffQi(self,dAttention):\n",
    "        dAttention_weights = self.Attention_weights * (1 - self.Attention_weights) \n",
    "        dLoss_dX=cp.transpose(dAttention, (0, 2, 1)) @ (self.helper.redimension(dAttention_weights @ (self.K * self.V) / cp.sqrt(self.K.shape[-1]))*self.input_q)\n",
    "        self.dLoss_Qi= cp.sum(dLoss_dX,axis=0)\n",
    "    \n",
    "    def diffKi(self,dAttention):\n",
    "        dAttention_weights = self.Attention_weights * (1 - self.Attention_weights) \n",
    "        X = cp.swapaxes(cp.array(cp.array_split(self.input_k, self.num_heads, axis=2)), 0, 1) \n",
    "         \n",
    "        dLoss_dX = cp.transpose(dAttention, (0, 2, 1)) @ self.helper.redimension(\n",
    "            (dAttention_weights * (self.Q @ cp.transpose(self.V, (0, 1, 3, 2))) @ X) / cp.sqrt(self.K.shape[-1])) \n",
    "        self.dLoss_Ki= cp.sum(dLoss_dX,axis=0)\n",
    "    \n",
    "    def diffVi(self,dAttention):\n",
    "        self.dLoss_Vi = cp.sum(cp.sum(cp.transpose(cp.expand_dims(dAttention, axis=1), (0, 1, 3, 2)) @ (\n",
    "                self.Attention_weights @ cp.expand_dims(self.input_v, axis=1)), axis=1), axis=0)\n",
    "       \n",
    "\n",
    "\n",
    "    def diffKInput(self,dAttention):\n",
    "        dAttention_weights = self.Attention_weights * (1 - self.Attention_weights) \n",
    "        #print(\"self.k\",self.k)\n",
    "        dLoss_KI=dAttention*(self.helper.redimension(dAttention_weights @ self.Q / cp.sqrt(self.K.shape[-1]))*self.helper.redimension(self.V)@self.k.W)\n",
    "        return dLoss_KI\n",
    "    \n",
    "    def diffVInput(self,dAttention):\n",
    "        dLoss_V_E = cp.transpose(\n",
    "        cp.mean(cp.transpose(cp.expand_dims(dAttention, axis=1), (0, 1, 3, 2)) @ self.Attention_weights, axis=1), (0, 2, 1))\n",
    "        dLoss_inpute_v = dLoss_V_E @ self.v.W\n",
    "        return dLoss_inpute_v\n",
    "    \n",
    "    def diffQInput(self,dAttention):\n",
    "        dAttention_weights = self.Attention_weights * (1 - self.Attention_weights) \n",
    "        dLoss_QI=dAttention*(self.helper.redimension(dAttention_weights @ self.K / cp.sqrt(self.K.shape[-1]))*self.helper.redimension(self.V)@self.q.W)\n",
    "        return dLoss_QI\n",
    "    \n",
    "    def grad(self,dL_dy): \n",
    "        self.dLoss_dAcr=self.projection_layer.grad(dL_dy)\n",
    "        self.diffQi(self.dLoss_dAcr)\n",
    "        self.diffVi(self.dLoss_dAcr)\n",
    "        self.diffKi(self.dLoss_dAcr)\n",
    "        dLoss_KI=self.diffKInput(self.dLoss_dAcr)\n",
    "        \n",
    "        dLoss_QI=self.diffQInput(self.dLoss_dAcr)\n",
    "        \n",
    "        dLoss_VI=self.diffVInput(self.dLoss_dAcr)\n",
    "       \n",
    "        return dLoss_QI,dLoss_KI,dLoss_VI\n",
    "\n",
    "    def update_weights(self,learning_rate):\n",
    "        self.projection_layer.update_weights_only(learning_rate)\n",
    "        self.q.W= self.q.W-self.dLoss_Qi*learning_rate\n",
    "        self.k.W= self.k.W-self.dLoss_Ki*learning_rate\n",
    "        self.v.W= self.v.W-self.dLoss_Vi*learning_rate\n",
    "        # print(\"self.q.W\",self.q.W)\n",
    "        # print(\"self.k.W\",self.k.W)\n",
    "        # print(\"self.v.W\",self.v.W)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3af4c07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73351bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2333 Edward thinks he is in love with Bella . Rachel wants Edward to open his door . Rachel is outside .\n",
      "vocabulary size 5839\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "min_v = 18\n",
    "max_v = 22\n",
    "helper=Helper()\n",
    "def load_x_y_train_plain():\n",
    "    with open('corpus/train.json', 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            dataset = json.load(f)  # Load the JSON data\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    # #Loop through the list and process each dialogue and summary\n",
    "    for data in dataset:\n",
    "        dialogue = data['dialogue']  # Split dialogue into a list of lines\n",
    "        summary = data['summary']\n",
    "\n",
    "        X_train.append(dialogue)\n",
    "        y_train.append(summary)\n",
    "    return X_train, y_train\n",
    "\n",
    "\n",
    "def split_x_y_train(X_train, y_train):\n",
    "    X_train = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', x) for x in X_train]\n",
    "    y_train = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', y) for y in y_train]\n",
    "    return X_train, y_train\n",
    "\n",
    "\n",
    "# with open('data/vocabolary_full.pkl', 'rb') as f:\n",
    "#     vocabulary=pickle.load(f)\n",
    "def filter_train_data(X_train, y_train, to_eliminate):\n",
    "    filtered_X_train = []\n",
    "    filtered_y_train = []\n",
    "\n",
    "    for x, y in zip(X_train, y_train):\n",
    "        if not any(to_eliminate_str in x for to_eliminate_str in to_eliminate):\n",
    "            filtered_X_train.append(x)\n",
    "            filtered_y_train.append(y)\n",
    "\n",
    "    return filtered_X_train, filtered_y_train\n",
    "\n",
    "\n",
    "def create_complete_vocabulary(X_train, y_train):\n",
    "    nlp_model = spacy.load('en_core_web_lg')\n",
    "    nlp_model.disable_pipes([\"parser\", \"ner\"])\n",
    "    complete_text_target = ' '.join(y_train)\n",
    "    complete_text_origin = ' '.join(X_train)\n",
    "    complete_text = complete_text_target + \" [START] [PAD] [END] \" + complete_text_origin\n",
    "\n",
    "    vocabulary = helper.create_vocabulary(complete_text, \"vocabolary_full\", nlp_model)\n",
    "    print(\"vocabulary size\", len(vocabulary))\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "X_train, y_train = load_x_y_train_plain()\n",
    "# to_eliminate = [\n",
    "#     \"[I hope I'm not coming off as rude - If I am, I'm sorry. I just thought it would be beneficial for the both of us...]\",\n",
    "#     \"[pulls back the curtain and checks out the window]\",\n",
    "#     \"[hopefully, masses of]\"]\n",
    "# X_train, y_train = filter_train_data(X_train, y_train, to_eliminate)\n",
    "\n",
    "\n",
    "sample = [i for i in range(0,len(y_train))]\n",
    "\n",
    "\n",
    "X_train = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', x) for x in X_train]\n",
    "y_train = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', y) for y in y_train]\n",
    "\n",
    "\n",
    "X_train = [X_train[i] for i in sample if len(y_train[i]) <= max_v and len(y_train[i]) >= min_v]\n",
    "y_train = [y_train[i] for i in sample if len(y_train[i]) <= max_v and len(y_train[i]) >= min_v]\n",
    "\n",
    "\n",
    "# Calculate lengths of the tokenized phrases\n",
    "\n",
    "\n",
    "def plot_lenghts(X_train,y_train):\n",
    "    X_lengths = [len(x) for x in X_train]\n",
    "    y_lengths = [len(y) for y in y_train]\n",
    "    # Plot histograms\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Histogram for X_train lengths\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(X_lengths, bins=20, kde=False)\n",
    "    plt.title('Histogram of X_train Phrase Lengths')\n",
    "    plt.xlabel('Length of Phrases')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Histogram for y_train lengths\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(y_lengths, bins=20, kde=False)\n",
    "    plt.title('Histogram of y_train Phrase Lengths')\n",
    "    plt.xlabel('Length of Phrases')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Display the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "X_train=[i[::-1] for i in y_train]\n",
    "plot_lenghts(X_train,y_train)\n",
    " \n",
    "\n",
    "X_train=[\" \".join(x) for x in X_train]\n",
    "y_train=[\" \".join(y) for y in y_train]\n",
    "\n",
    "print(len(y_train),y_train[0])\n",
    "\n",
    "vocabulary=create_complete_vocabulary(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4c7d792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self,embedding_size,num_heads,linear_layer_size,learning_rate,batch_size,words_per_phrase,clipping_threshold):\n",
    "        self.batch_size=batch_size\n",
    "        self.clipping_threshold=clipping_threshold\n",
    "        self.words_per_phrase=words_per_phrase\n",
    "        self.num_heads=num_heads\n",
    "        self.linear_layer_size=linear_layer_size\n",
    "        self.embedding_size=embedding_size\n",
    "        self.fully_connected_block=fully_connected_block(self.embedding_size,linear_layer_size,clipping_threshold=clipping_threshold)\n",
    "        self.multihead_attention_encoder=multihead_attention(num_heads=2,embedding_size=embedding_size,batch_size=batch_size,threshold=clipping_threshold)\n",
    "        self.residual_layer_1=residual_layer(clipping_threshold)\n",
    "        self.residual_layer_2=residual_layer(clipping_threshold) \n",
    "        self.learning_rate=learning_rate\n",
    "         \n",
    "        self.helper=Helper()\n",
    "     \n",
    "\n",
    "    def forward(self,inputs_e):\n",
    "        self.inputs_e=inputs_e \n",
    "\n",
    "        PrjAe=self.multihead_attention_encoder.forward_attention(inputs_e,inputs_e,inputs_e)\n",
    "        \n",
    "        Ect1=self.residual_layer_1.forward(PrjAe,inputs_e) \n",
    "\n",
    "        FLe2=self.fully_connected_block.forward(Ect1) \n",
    "\n",
    "        Ecout=self.residual_layer_2.forward(FLe2,Ect1)\n",
    "\n",
    "        return Ecout\n",
    "    \n",
    "    def backpropagation(self,dL_Ecout): \n",
    "        \n",
    "        dL_dFLe2,dL_dEct1_residual=self.residual_layer_2.grad(dL_Ecout)\n",
    "        \n",
    "        #print(\"dL_dFLe2\",dL_dFLe2)\n",
    "        #print(\"dL_dEct1_residual\",dL_dFLe2)\n",
    "        \n",
    "\n",
    "        dL_dEct1=self.fully_connected_block.grad(dL_dFLe2)+dL_dEct1_residual\n",
    "        #print(\"dL_dEct1\",dL_dEct1)\n",
    "        \n",
    "        dL_dPrjAe,dL_inputs_e_residual=self.residual_layer_1.grad(dL_dEct1)\n",
    "        #print(\"dL_dPrjAe\",dL_dPrjAe)\n",
    "        #print(\"dL_inputs_e_residual\",dL_inputs_e_residual)\n",
    "        dL_inputs_e_q,dL_inputs_e_k,dL_inputs_e_v=self.multihead_attention_encoder.grad(dL_dPrjAe)\n",
    "        \n",
    "        dL_inputs_e=dL_inputs_e_residual+dL_inputs_e_q+dL_inputs_e_k+dL_inputs_e_v\n",
    "\n",
    "        #dL_inputs_e=clip_gradient(dL_inputs_e,self.clipping_threshold)\n",
    "        #print(\"dL_inputs_e\",dL_inputs_e_residual)\n",
    "        dLoss_dWemb_encoder = dL_inputs_e * self.inputs_e\n",
    "        return dL_inputs_e,dLoss_dWemb_encoder\n",
    "\n",
    "\n",
    "    def update_weights(self,learning_rate,dLoss_dWemb_encoder,X_batch,vocabulary):\n",
    "        self.residual_layer_2.update_weights(learning_rate)\n",
    "        self.fully_connected_block.update_weights(learning_rate)\n",
    "        self.residual_layer_1.update_weights(learning_rate)\n",
    "        self.multihead_attention_encoder.update_weights(learning_rate)\n",
    "        \n",
    "        # input_e=self.inputs_e-learning_rate*dLoss_dWemb_encoder\n",
    "        # vocabulary=self.helper.update_wembedding_encoder(X_batch,input_e,vocabulary,self.words_per_phrase)\n",
    "        return vocabulary\n",
    "        \n",
    "\n",
    "  \n",
    "      \n",
    "     \n",
    "class Decoder:\n",
    "    def __init__(self,embedding_size,num_heads,linear_layer_size,learning_rate,batch_size,words_per_phrase,clipping_threshold):\n",
    "        self.words_per_phrase=words_per_phrase\n",
    "        self.clipping_threshold=clipping_threshold\n",
    "        self.batch_size=batch_size\n",
    "        self.num_heads=num_heads \n",
    "        self.linear_layer_size=linear_layer_size\n",
    "        self.embedding_size=embedding_size  \n",
    "        self.multihead_cross_attention=multihead_attention(num_heads=num_heads,embedding_size=embedding_size,batch_size=batch_size,threshold=clipping_threshold)\n",
    "        self.multihead_attention_decoder=multihead_attention(num_heads=num_heads,embedding_size=embedding_size,batch_size=batch_size,threshold=clipping_threshold) \n",
    "        self.learning_rate=learning_rate\n",
    "        self.helper=Helper() \n",
    "        self.residual_layer_1=residual_layer(threshold=clipping_threshold)\n",
    "        self.residual_layer_2=residual_layer(threshold=clipping_threshold) \n",
    "        self.residual_layer_3=residual_layer(threshold=clipping_threshold) \n",
    "        self.fully_connected_block=fully_connected_block(self.embedding_size,linear_layer_size,clipping_threshold=clipping_threshold)\n",
    "        \n",
    "\n",
    "    def forward(self,inputs_decoder,Ecout):\n",
    "        self.inputs_decoder=inputs_decoder \n",
    "        \n",
    "        PrjA_mask=self.multihead_attention_decoder.forward_masked_attention(inputs_decoder,inputs_decoder,inputs_decoder,mask_size=inputs_decoder.shape[1])\n",
    " \n",
    "        Dt1=self.residual_layer_1.forward(self.inputs_decoder,PrjA_mask)\n",
    " \n",
    "        PrjAcr=self.multihead_cross_attention.forward_attention(Dt1,Ecout,Ecout)\n",
    "   \n",
    "        Dt2=self.residual_layer_2.forward(PrjAcr,Dt1)\n",
    " \n",
    "        FLd2=self.fully_connected_block.forward(Dt2)\n",
    "      \n",
    "        Dout=self.residual_layer_3.forward(FLd2,Dt2)  \n",
    "\n",
    "        return Dout\n",
    "    \n",
    "    # def output(self,Dout):\n",
    "    #     SigmaZout=self.helper.softmax(self.final_projection_layer.forward(Dout))\n",
    "    #     return SigmaZout\n",
    "\n",
    "\n",
    "    def backpropagation(self,dL_dDout): \n",
    "\n",
    "        dL_FLd2,dL_Dt2_residual=self.residual_layer_3.grad(dL_dDout)\n",
    "\n",
    "        dL_Dt2=self.fully_connected_block.grad(dL_FLd2)+dL_Dt2_residual\n",
    "\n",
    "        dL_PrjAcr,dL_Dt1_residual=self.residual_layer_2.grad(dL_Dt2)\n",
    "\n",
    "        dL_Dt1_q,dL_DEcout_k,dL_DEcout_v=self.multihead_cross_attention.grad(dL_PrjAcr)\n",
    "\n",
    "        dL_Dt1=dL_Dt1_residual+dL_Dt1_q\n",
    "\n",
    "        dL_PrjA_mask,dL_inputs_decoder_residual=self.residual_layer_1.grad(dL_Dt1)\n",
    "\n",
    "        dL_inputs_decoder_q,dL_inputs_decoder_k,dL_inputs_decoder_v=self.multihead_attention_decoder.grad(dL_PrjA_mask)\n",
    "\n",
    "        dL_inputs_decoder=dL_inputs_decoder_residual+dL_inputs_decoder_q+dL_inputs_decoder_k+dL_inputs_decoder_v\n",
    "        \n",
    "        dL_Ecout=dL_DEcout_k+dL_DEcout_v\n",
    "        #dL_inputs_decoder=clip_gradient(dL_inputs_decoder,self.clipping_threshold)\n",
    "        dLoss_dWemb_decoder= dL_inputs_decoder * self.inputs_decoder\n",
    "        \n",
    "        return dL_Ecout,dL_inputs_decoder,dLoss_dWemb_decoder\n",
    "\n",
    "\n",
    "    def update_weights(self,learning_rate,dLoss_dWemb_decoder,y_batch,vocabulary):\n",
    "        self.residual_layer_3.update_weights(learning_rate)\n",
    "        self.fully_connected_block.update_weights(learning_rate)\n",
    "        self.residual_layer_2.update_weights(learning_rate)\n",
    "        self.multihead_cross_attention.update_weights(learning_rate)\n",
    "        self.residual_layer_1.update_weights(learning_rate)\n",
    "        self.multihead_attention_decoder.update_weights(learning_rate)\n",
    "        #dLoss_dWemb_decoder=clip_gradient(dLoss_dWemb_decoder,self.clipping_threshold)\n",
    "        # input_d=self.inputs_decoder-learning_rate*dLoss_dWemb_decoder\n",
    "        # vocabulary=self.helper.update_wembedding_decoder(y_batch,input_d,self.words_per_phrase,vocabulary) \n",
    "        return vocabulary\n",
    "        \n",
    "    \n",
    "class Transformer:\n",
    "    def __init__(self,num_layers,embedding_size,num_heads,fl1_size,learning_rate,batch_size,words_per_phrase,clipping_threshold,vocabulary):\n",
    "        self.vocabulary=vocabulary\n",
    "        self.clipping_threshold=clipping_threshold\n",
    "        self.EncoderStack = [Encoder(embedding_size,num_heads,fl1_size,learning_rate,batch_size,words_per_phrase,clipping_threshold) for _ in range(num_layers)]\n",
    "        self.DecoderStack = [Decoder(embedding_size,num_heads,fl1_size,learning_rate,batch_size,words_per_phrase,clipping_threshold) for _ in range(num_layers)]\n",
    "\n",
    "\n",
    "    def forward(self,inputs_e,inputs_decoder,X_batch,y_batch):\n",
    "\n",
    "        self.X_batch=X_batch\n",
    "        self.y_batch=y_batch\n",
    "        Ecout=self.forward_encoder(inputs_e)\n",
    "        Dout=self.forward_decoder(Ecout,inputs_decoder)\n",
    "        return Dout\n",
    "    \n",
    "    def backpropagation(self,dL_dDout):\n",
    "        dL_Ecout,dLoss_dWemb_decoder_tot=self.backpropagation_decoder(dL_dDout)\n",
    "        dL_Ecout,dLoss_dWemb_encoder_tot=self.backpropagation_encoder(dL_Ecout)\n",
    "        return dL_Ecout,dLoss_dWemb_encoder_tot,dLoss_dWemb_decoder_tot\n",
    "\n",
    "    def forward_encoder(self,inputs_e):\n",
    "        for encoder_i in self.EncoderStack:\n",
    "            inputs_e=encoder_i.forward(inputs_e)\n",
    "        return inputs_e\n",
    "    \n",
    "    def forward_decoder(self,Ecout,inputs_decoder):\n",
    "        for decoder_i in self.DecoderStack:\n",
    "            inputs_decoder=decoder_i.forward(inputs_decoder,Ecout)\n",
    "        return inputs_decoder\n",
    "    \n",
    "    def backpropagation_decoder(self,dL_dDout):\n",
    "        tot_dL_dEcout=0\n",
    "        dLoss_dWemb_decoder_tot=0\n",
    "        for decoder_i in reversed(self.DecoderStack):\n",
    "            dL_Ecout_i,dL_dDout,dLoss_dWemb_decoder=decoder_i.backpropagation(dL_dDout)\n",
    "            tot_dL_dEcout+=dL_Ecout_i\n",
    "            dLoss_dWemb_decoder_tot+=dLoss_dWemb_decoder\n",
    "            self.vocabulary=decoder_i.update_weights(decoder_i.learning_rate,dLoss_dWemb_decoder,self.y_batch,self.vocabulary)\n",
    "        return tot_dL_dEcout,dLoss_dWemb_decoder_tot\n",
    "    \n",
    "    def backpropagation_encoder(self,dL_Ecout): \n",
    "        dLoss_dWemb_encoder_tot=0\n",
    "        for encoder_i in reversed(self.EncoderStack):\n",
    "            dL_Ecout,dLoss_dWemb_encoder=encoder_i.backpropagation(dL_Ecout) \n",
    "            dLoss_dWemb_encoder_tot+=dLoss_dWemb_encoder \n",
    "            self.vocabulary=encoder_i.update_weights(encoder_i.learning_rate,dLoss_dWemb_encoder,self.X_batch,self.vocabulary)\n",
    "        return dL_Ecout,dLoss_dWemb_encoder_tot\n",
    "   \n",
    " \n",
    "    #     self.inputs_decoder=self.inputs_decoder-self.learning_rate*dLoss_dWemb_decoder\n",
    "    #     vocabulary=self.helper.update_wembedding_decoder(y_batch,self.inputs_decoder,self.words_per_phrase,vocabulary) \n",
    "    #     return vocabulary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b107b0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "461b1c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import traceback\n",
    "# embedding_size=300\n",
    "# fl1_size=1000 \n",
    "# batch_size=5\n",
    "# num_heads=5\n",
    "# dropout_rate=0.2\n",
    "# words_per_phrase = num_phrases= max_v\n",
    "# num_batches_per_epoch = len(X_train) // batch_size\n",
    "# num_epochs=550\n",
    "# tot_loss_epoch=0\n",
    "# learning_rate=0.001\n",
    "\n",
    "# Output_stack=output_stack(embedding_size,len(vocabulary))\n",
    "# TransformerEncoder=Encoder(embedding_size,num_heads,fl1_size,learning_rate,batch_size=batch_size,words_per_phrase=max_v)\n",
    "\n",
    "\n",
    "\n",
    "# TransformerDecoder=Decoder(embedding_size,num_heads,fl1_size,learning_rate,batch_size=batch_size,words_per_phrase=max_v)\n",
    "# TransformerEncoder1=Encoder(embedding_size,num_heads,fl1_size,learning_rate,batch_size=batch_size,words_per_phrase=max_v)\n",
    "# TransformerEncoder2=Encoder(embedding_size,num_heads,fl1_size,learning_rate,batch_size=batch_size,words_per_phrase=max_v)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "# for epoch in range(1):\n",
    "#     print(\"Loss\",tot_loss_epoch/num_batches_per_epoch)\n",
    "#     tot_loss_epoch=0\n",
    "#     total_accuracy_epoch=0\n",
    "    \n",
    "#     for i in tqdm(range(0,5),desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "#         #try: \n",
    "#         start = i * batch_size\n",
    "#         end = start + batch_size \n",
    "#         X_batch = X_train[start:end]\n",
    "#         y_batch = y_train[start:end] \n",
    "        \n",
    "        \n",
    "#         #helper.print_matrix(y_batch)\n",
    "#         #print(\"X_batch\",X_batch)\n",
    "        \n",
    "        \n",
    "#         inputs_e=helper.create_input_encoder(X_batch,vocabulary,words_per_phrase,embedding_size) \n",
    "#         inputs_decoder=helper.create_decoder_input(y_batch,embedding_size,words_per_phrase,vocabulary) \n",
    "#         target_decoder=helper.create_target(y_batch,vocabulary,words_per_phrase)\n",
    "#         #target_decoder=helper.generate_target_sparse_categorical(y_batch,vocabulary,words_per_phrase)\n",
    "        \n",
    "#         Ecout0 = TransformerEncoder.forward(inputs_e)\n",
    "#         Ecout1=TransformerEncoder1.forward(Ecout0)\n",
    "#         Ecout=TransformerEncoder2.forward(Ecout1)\n",
    "#         Dout = TransformerDecoder.forward(inputs_decoder, Ecout)\n",
    "        \n",
    "        \n",
    "#         SigmaZout = Output_stack.forward(Dout)\n",
    "\n",
    "#         for j in range(SigmaZout.shape[0]):\n",
    "#             for k in range(SigmaZout.shape[1]):\n",
    "#                 #print(np.argmax(SigmaZout[j][k]),np.argmax(target_decoder[j][k]))\n",
    "#                 pass\n",
    "#         #print(target_decoder.shape)\n",
    "\n",
    "#         #print(\"Loss\",Output_stack.sparse_categorical_crossentropy(SigmaZout,target_decoder))\n",
    "#         print(\"Loss\",Output_stack.cross_entropy_loss(SigmaZout,target_decoder))\n",
    "#         dL_dDout = Output_stack.grad_cross_entropy(SigmaZout,target_decoder)\n",
    "\n",
    "#         print(\"dL_dDout\",dL_dDout.shape) \n",
    "         \n",
    "#         dL_Ecout,dLoss_dWemb_decoder = TransformerDecoder.backpropagation(dL_dDout)\n",
    "\n",
    "#         dLoss_dWemb_encoder0=TransformerEncoder.backpropagation(dL_Ecout) \n",
    "#         dLoss_dWemb_encoder1=TransformerEncoder1.backpropagation(dLoss_dWemb_encoder0) \n",
    "#         dLoss_dWemb_encoder2=TransformerEncoder2.backpropagation(dLoss_dWemb_encoder1) \n",
    "\n",
    "        \n",
    "#         TransformerDecoder.update_weights(learning_rate)\n",
    "#         TransformerDecoder.update_weights(learning_rate)\n",
    "#         Output_stack.update_weights(learning_rate)\n",
    "        #inputs_decoder=inputs_decoder-learning_rate*dLoss_dWemb_decoder\n",
    "        #vocabulary=helper.update_wembedding_decoder(y_batch,inputs_decoder,words_per_phrase,vocabulary) \n",
    "        #inputs_e=inputs_e-learning_rate*dLoss_dWemb_encoder\n",
    "        #vocabulary=helper.update_wembedding_encoder(X_batch,inputs_e,vocabulary,words_per_phrase)\n",
    "        #print(\"dL_Ecout\",dL_Ecout.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a903f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47caf68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import traceback\n",
    "# embedding_size=300\n",
    "# fl1_size=2048\n",
    "# batch_size=8\n",
    "# num_heads=10\n",
    "# dropout_rate=0.1\n",
    "# words_per_phrase = num_phrases= max_v\n",
    "# num_batches_per_epoch = len(X_train) // batch_size\n",
    "# num_epochs=550\n",
    "# tot_loss_epoch=0\n",
    "# learning_rate=0.0001\n",
    "\n",
    "# Output_stack=output_stack(embedding_size,len(vocabulary),threshold=1)\n",
    "# TransformerEncoder=Encoder(embedding_size,num_heads,fl1_size,learning_rate,batch_size=batch_size,words_per_phrase=max_v,clipping_threshold=1)\n",
    "# TransformerDecoder=Decoder(embedding_size,num_heads,fl1_size,learning_rate,batch_size=batch_size,words_per_phrase=max_v,clipping_threshold=1)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "# for epoch in range(1):\n",
    "#     print(\"Loss\",tot_loss_epoch/num_batches_per_epoch)\n",
    "#     tot_loss_epoch=0\n",
    "#     total_accuracy_epoch=0\n",
    "    \n",
    "#     for i in tqdm(range(0,6),desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "#         #try: \n",
    "#         start = i * batch_size\n",
    "#         end = start + batch_size \n",
    "#         X_batch = X_train[start:end]\n",
    "#         y_batch = y_train[start:end] \n",
    "        \n",
    "        \n",
    "#         #helper.print_matrix(y_batch)\n",
    "#         #print(\"X_batch\",X_batch)\n",
    "        \n",
    "        \n",
    "#         inputs_e=helper.create_input_encoder(X_batch,vocabulary,words_per_phrase,embedding_size) \n",
    "#         inputs_decoder=helper.create_decoder_input(y_batch,embedding_size,words_per_phrase,vocabulary) \n",
    "#         #target_decoder=helper.create_target(y_batch,vocabulary,words_per_phrase)\n",
    "#         target_decoder=helper.generate_target_sparse_categorical(y_batch,vocabulary,words_per_phrase)\n",
    "        \n",
    "#         Ecout = TransformerEncoder.forward(inputs_e)\n",
    "         \n",
    "#         Dout = TransformerDecoder.forward(inputs_decoder, Ecout)\n",
    "        \n",
    "#         SigmaZout = Output_stack.forward(Dout)\n",
    "        \n",
    "#         print(\"SigmaZout.shape\",SigmaZout.shape)\n",
    "#         print(\"target_decoder.shape\",target_decoder.shape)\n",
    "#         print(\"Loss\",Output_stack.sparse_categorical_crossentropy(SigmaZout,target_decoder))\n",
    "#         dL_dDout = Output_stack.grad_sparse_cross_entropy(SigmaZout,target_decoder)\n",
    "#         #print(\"Loss\",Output_stack.Loss_cross_entropy(SigmaZout,target_decoder))\n",
    "#         #dL_dDout = Output_stack.grad_cross_entropy(SigmaZout,target_decoder)\n",
    "\n",
    "#         print(\"dL_dDout\",dL_dDout.shape) \n",
    "         \n",
    "#         dL_Ecout,dLoss_dWemb_decoder = TransformerDecoder.backpropagation(dL_dDout)\n",
    "\n",
    "#         dLoss_dWemb_encoder=TransformerEncoder.backpropagation(dL_Ecout) \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "#         TransformerDecoder.update_weights(learning_rate)\n",
    "#         TransformerDecoder.update_weights(learning_rate)\n",
    "#         Output_stack.update_weights(learning_rate)\n",
    "#         #inputs_decoder=inputs_decoder-learning_rate*dLoss_dWemb_decoder\n",
    "#         #vocabulary=helper.update_wembedding_decoder(y_batch,inputs_decoder,words_per_phrase,vocabulary) \n",
    "#         #inputs_e=inputs_e-learning_rate*dLoss_dWemb_encoder\n",
    "#         #vocabulary=helper.update_wembedding_encoder(X_batch,inputs_e,vocabulary,words_per_phrase)\n",
    "#         #print(\"dL_Ecout\",dL_Ecout.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dae219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9350b4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2333, 5839)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train),len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a882c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "embedding_size=300\n",
    "fl1_size=2048\n",
    "batch_size=64\n",
    "num_heads=15\n",
    "dropout_rate=0.1\n",
    "words_per_phrase = num_phrases= max_v\n",
    "num_batches_per_epoch = len(X_train) // batch_size\n",
    "num_epochs=550\n",
    "tot_loss_epoch=0\n",
    "learning_rate=0.000005\n",
    "clipping_threshold=9000000000\n",
    "\n",
    "\n",
    "\n",
    "n_layers=7\n",
    "Output_stack=output_stack(embedding_size,len(vocabulary),threshold=clipping_threshold)\n",
    "MyTransformer=Transformer(n_layers,embedding_size,num_heads,fl1_size,learning_rate,batch_size,words_per_phrase,clipping_threshold,vocabulary)\n",
    "output_linear_layer=linear_layer(embedding_size,len(vocabulary),out=True) \n",
    "accuracies=[0,0]\n",
    "mean_acc=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fbf5a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.0006913768694590593 mean accuracy 0.08025568181818182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/550: 100%|██████████| 36/36 [02:07<00:00,  3.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.025111417499660293 mean accuracy 0.08057133838383838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/550: 100%|██████████| 36/36 [02:06<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.024913476405349536 mean accuracy 0.08031486742424243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/550: 100%|██████████| 36/36 [02:04<00:00,  3.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.024751447091849274 mean accuracy 0.08039378156565657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/550: 100%|██████████| 36/36 [02:05<00:00,  3.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss nan mean accuracy 0.043185763888888895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/550:   8%|▊         | 3/36 [00:12<02:12,  4.01s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 44\u001b[0m\n\u001b[0;32m     37\u001b[0m tot_loss_epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mLoss\n\u001b[0;32m     39\u001b[0m dL_dDout \u001b[38;5;241m=\u001b[39m Output_stack\u001b[38;5;241m.\u001b[39mgrad_cross_entropy(SigmaZout,target_decoder)\n\u001b[1;32m---> 44\u001b[0m dL_Ecout,dLoss_dWemb_encoder_tot,dLoss_dWemb_decoder_tot\u001b[38;5;241m=\u001b[39m\u001b[43mMyTransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackpropagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdL_dDout\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     45\u001b[0m Output_stack\u001b[38;5;241m.\u001b[39mupdate_weights(learning_rate)\n\u001b[0;32m     46\u001b[0m vocabulary\u001b[38;5;241m=\u001b[39mhelper\u001b[38;5;241m.\u001b[39mupdate_wembedding_decoder(learning_rate,y_batch, dLoss_dWemb_decoder_tot,vocabulary, max_v  )\n",
      "Cell \u001b[1;32mIn[3], line 164\u001b[0m, in \u001b[0;36mTransformer.backpropagation\u001b[1;34m(self, dL_dDout)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackpropagation\u001b[39m(\u001b[38;5;28mself\u001b[39m,dL_dDout):\n\u001b[1;32m--> 164\u001b[0m     dL_Ecout,dLoss_dWemb_decoder_tot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackpropagation_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdL_dDout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m     dL_Ecout,dLoss_dWemb_encoder_tot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackpropagation_encoder(dL_Ecout)\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dL_Ecout,dLoss_dWemb_encoder_tot,dLoss_dWemb_decoder_tot\n",
      "Cell \u001b[1;32mIn[3], line 182\u001b[0m, in \u001b[0;36mTransformer.backpropagation_decoder\u001b[1;34m(self, dL_dDout)\u001b[0m\n\u001b[0;32m    180\u001b[0m dLoss_dWemb_decoder_tot\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDecoderStack):\n\u001b[1;32m--> 182\u001b[0m     dL_Ecout_i,dL_dDout,dLoss_dWemb_decoder\u001b[38;5;241m=\u001b[39m\u001b[43mdecoder_i\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackpropagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdL_dDout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m     tot_dL_dEcout\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mdL_Ecout_i\n\u001b[0;32m    184\u001b[0m     dLoss_dWemb_decoder_tot\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mdLoss_dWemb_decoder\n",
      "Cell \u001b[1;32mIn[3], line 117\u001b[0m, in \u001b[0;36mDecoder.backpropagation\u001b[1;34m(self, dL_dDout)\u001b[0m\n\u001b[0;32m    113\u001b[0m dL_Dt2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfully_connected_block\u001b[38;5;241m.\u001b[39mgrad(dL_FLd2)\u001b[38;5;241m+\u001b[39mdL_Dt2_residual\n\u001b[0;32m    115\u001b[0m dL_PrjAcr,dL_Dt1_residual\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_layer_2\u001b[38;5;241m.\u001b[39mgrad(dL_Dt2)\n\u001b[1;32m--> 117\u001b[0m dL_Dt1_q,dL_DEcout_k,dL_DEcout_v\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultihead_cross_attention\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdL_PrjAcr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m dL_Dt1\u001b[38;5;241m=\u001b[39mdL_Dt1_residual\u001b[38;5;241m+\u001b[39mdL_Dt1_q\n\u001b[0;32m    121\u001b[0m dL_PrjA_mask,dL_inputs_decoder_residual\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_layer_1\u001b[38;5;241m.\u001b[39mgrad(dL_Dt1)\n",
      "Cell \u001b[1;32mIn[1], line 734\u001b[0m, in \u001b[0;36mmultihead_attention.grad\u001b[1;34m(self, dL_dy)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiffVi(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdLoss_dAcr)\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiffKi(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdLoss_dAcr)\n\u001b[1;32m--> 734\u001b[0m dLoss_KI\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiffKInput\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdLoss_dAcr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    736\u001b[0m dLoss_QI\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiffQInput(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdLoss_dAcr)\n\u001b[0;32m    738\u001b[0m dLoss_VI\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiffVInput(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdLoss_dAcr)\n",
      "Cell \u001b[1;32mIn[1], line 715\u001b[0m, in \u001b[0;36mmultihead_attention.diffKInput\u001b[1;34m(self, dAttention)\u001b[0m\n\u001b[0;32m    713\u001b[0m dAttention_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mAttention_weights \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mAttention_weights) \n\u001b[0;32m    714\u001b[0m \u001b[38;5;66;03m#print(\"self.k\",self.k)\u001b[39;00m\n\u001b[1;32m--> 715\u001b[0m dLoss_KI\u001b[38;5;241m=\u001b[39mdAttention\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhelper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mredimension\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdAttention_weights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQ\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhelper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mredimension\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mV\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;129;43m@self\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m)\n\u001b[0;32m    716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dLoss_KI\n",
      "File \u001b[1;32mcupy\\_core\\core.pyx:1295\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base.__matmul__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\_core\\_routines_linalg.pyx:1032\u001b[0m, in \u001b[0;36mcupy._core._routines_linalg.matmul\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\numpy\\core\\_internal.py:250\u001b[0m, in \u001b[0;36m_ctypes.__init__\u001b[1;34m(self, array, ptr)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_ctypes\u001b[39;00m:\n\u001b[1;32m--> 250\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, array, ptr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arr \u001b[38;5;241m=\u001b[39m array\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m ctypes:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " \n",
    " \n",
    " \n",
    "for epoch in range(550):\n",
    "    print(\"Loss\",tot_loss_epoch/num_batches_per_epoch,\"mean accuracy\",np.mean(np.array(accuracies)))#np.mean(np.array(accuracies))\n",
    "    tot_loss_epoch=0\n",
    "    total_accuracy_epoch=0\n",
    "    mean_acc=0\n",
    "    accuracies=[]\n",
    "    for i in tqdm(range(0,num_batches_per_epoch),desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        #try: \n",
    "        start = i * batch_size\n",
    "        end = start + batch_size \n",
    "        X_batch = X_train[start:end]\n",
    "        y_batch = y_train[start:end]  \n",
    "        \n",
    "        inputs_e=helper.create_input_encoder(X_batch,vocabulary,words_per_phrase,embedding_size) \n",
    "        inputs_decoder=helper.create_decoder_input(y_batch,embedding_size,words_per_phrase,vocabulary) \n",
    "        target_decoder=helper.create_target(y_batch,vocabulary,words_per_phrase) \n",
    "        Dout=MyTransformer.forward(inputs_e,inputs_decoder,X_batch,y_batch)\n",
    "        counter_beccate=0\n",
    "        counter_tot=0 \n",
    "        SigmaZout = Output_stack.forward(Dout)\n",
    "        \n",
    "        taccuracies=[]\n",
    "        for n in range(SigmaZout.shape[0]): \n",
    "            len_phrase=SigmaZout.shape[1]\n",
    "            counter_beccate=0 \n",
    "            for l in range(SigmaZout.shape[1]): \n",
    "                if np.argmax(SigmaZout[n][l])==np.argmax(target_decoder[n][l]): \n",
    "                    counter_beccate+=1\n",
    "                    #print(np.argmax(SigmaZout[n][l]))\n",
    "            phrase_accuracy=counter_beccate/len_phrase\n",
    "            taccuracies.append(phrase_accuracy)\n",
    "\n",
    "        accuracies.append(np.mean(np.array(taccuracies))) \n",
    "\n",
    "\n",
    "        Loss = Output_stack.cross_entropy_loss(SigmaZout,target_decoder) \n",
    "        tot_loss_epoch+=Loss\n",
    " \n",
    "        dL_dDout = Output_stack.grad_cross_entropy(SigmaZout,target_decoder)\n",
    "        \n",
    "         \n",
    "\n",
    "\n",
    "        dL_Ecout,dLoss_dWemb_encoder_tot,dLoss_dWemb_decoder_tot=MyTransformer.backpropagation(dL_dDout) \n",
    "        Output_stack.update_weights(learning_rate)\n",
    "        vocabulary=helper.update_wembedding_decoder(learning_rate,y_batch, dLoss_dWemb_decoder_tot,vocabulary, max_v  )\n",
    "        vocabulary=helper.update_wembedding_encoder(learning_rate,X_batch, dLoss_dWemb_encoder_tot,vocabulary,max_v)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e29d6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seeing 0\n",
      "schools 1\n",
      "released 2\n",
      "disappeared 3\n",
      "shat 4\n",
      "bath 5\n",
      "Octavia 6\n",
      "pole 7\n",
      "garage 8\n",
      "seeded 9\n",
      "should 10\n",
      "gathered 11\n",
      "promote 12\n",
      "Paulson 13\n",
      "hate 14\n",
      "o 15\n",
      "Groundhog 16\n",
      "Nat 17\n",
      "Jesus 18\n",
      "tips 19\n",
      "story 20\n",
      "cost 21\n",
      "gala 22\n",
      "Leona 23\n",
      "goalkeeper 24\n",
      "Monopoly 25\n",
      "Jimena 26\n",
      "road 27\n",
      "chase 28\n",
      "season 29\n",
      "Beyonce 30\n",
      "Samoa 31\n",
      "Josiah 32\n",
      "Gladys 33\n",
      "Aztec 34\n",
      "travels 35\n",
      "Bay 36\n",
      "favour 37\n",
      "hurry 38\n",
      "radio 39\n",
      "heart 40\n",
      "learn 41\n",
      "Cecil 42\n",
      "Carter 43\n",
      "last 44\n",
      "Tatiana 45\n",
      "expressed 46\n",
      "such 47\n",
      "Mars 48\n",
      "A 49\n",
      "Eminem 50\n",
      "6 51\n",
      "overweight 52\n",
      "loaf 53\n",
      "tenant 54\n",
      "Leon 55\n",
      "looking 56\n",
      "hair 57\n",
      "butter 58\n",
      "go 59\n",
      "pears 60\n",
      "Austin 61\n",
      "Stacy 62\n",
      "carom 63\n",
      "training 64\n",
      "notes 65\n",
      "Audley 66\n",
      "PlayStation 67\n",
      "Sue 68\n",
      "bonus 69\n",
      "Susy 70\n",
      "Lesser 71\n",
      "desperate 72\n",
      "with 73\n",
      "common 74\n",
      "quits 75\n",
      "drowned 76\n",
      "grammar 77\n",
      "sporting 78\n",
      "shout 79\n",
      "fuse 80\n",
      "suggest 81\n",
      "fingers 82\n",
      "could 83\n",
      "liquid 84\n",
      "Alexi 85\n",
      "best 86\n",
      "Amore 87\n",
      "Daddy 88\n",
      "Barca 89\n",
      "music 90\n",
      "Hamza 91\n",
      "messaged 92\n",
      "Wade 93\n",
      "box 94\n",
      "riding 95\n",
      "Damari 96\n",
      "worry 97\n",
      "funeral 98\n",
      "Wacky 99\n",
      "consequences 100\n",
      "Formula 101\n",
      "omelette 102\n",
      "Hartley 103\n",
      "size 104\n",
      "Tiffany 105\n",
      "Cevapi 106\n",
      "shortly 107\n",
      "street 108\n",
      "grade 109\n",
      "Mine 110\n",
      "granola 111\n",
      "networking 112\n",
      "economics 113\n",
      "clients 114\n",
      "Tanvi 115\n",
      "umbrella 116\n",
      "At 117\n",
      "Vegas 118\n",
      "Before 119\n",
      "cooked 120\n",
      "Nutcracker 121\n",
      "need 122\n",
      "Olga 123\n",
      "Dona 124\n",
      "pdf 125\n",
      "laryngologist 126\n",
      "warming 127\n",
      "meetings 128\n",
      "latest 129\n",
      "CAMP 130\n",
      "Odette 131\n",
      "store 132\n",
      "opened 133\n",
      "painting 134\n",
      "name 135\n",
      "degree 136\n",
      "estimate 137\n",
      "enjoys 138\n",
      "treatment 139\n",
      "Stanley 140\n",
      "bus 141\n",
      "Bart 142\n",
      "skipped 143\n",
      "off 144\n",
      "destination 145\n",
      "Viloet 146\n",
      "corner 147\n",
      "Mom 148\n",
      "document 149\n",
      "1984 150\n",
      "tries 151\n",
      "Metaphor 152\n",
      "globalization 153\n",
      "allowance 154\n",
      "Mattia 155\n",
      "rearrange 156\n",
      "cut 157\n",
      "showing 158\n",
      "2 159\n",
      "Thomas 160\n",
      "peanut 161\n",
      "Reginald 162\n",
      "Tilly 163\n",
      "tasty 164\n",
      "m 165\n",
      "engaged 166\n",
      "ministries 167\n",
      "Basque 168\n",
      "thank 169\n",
      "SPA 170\n",
      "Zuccarini 171\n",
      "trainer 172\n",
      "Ghul 173\n",
      "Haiti 174\n",
      "amaretti 175\n",
      "decisions 176\n",
      "engines 177\n",
      "order 178\n",
      "Priti 179\n",
      "Xiaomi 180\n",
      "Paddy 181\n",
      "relax 182\n",
      "hypocrites 183\n",
      "hobbies 184\n",
      "cleaned 185\n",
      "supposes 186\n",
      "untill 187\n",
      "kilometer 188\n",
      "Philippe 189\n",
      "pavement 190\n",
      "does 191\n",
      "Kaka 192\n",
      "upon 193\n",
      "dressed 194\n",
      "Alana 195\n",
      "handle 196\n",
      "overcrowded 197\n",
      "lasagne 198\n",
      "permission 199\n",
      "Apetite 200\n",
      "separately 201\n",
      "Jorge 202\n",
      "Benny 203\n",
      "Diane 204\n",
      "reminds 205\n",
      "Getting 206\n",
      "Martha 207\n",
      "gel 208\n",
      "under 209\n",
      "Paige 210\n",
      "Dog 211\n",
      "license 212\n",
      "kg 213\n",
      "ham 214\n",
      "using 215\n",
      "Dora 216\n",
      "47th 217\n",
      "supermarket 218\n",
      "latino 219\n",
      "Açucar 220\n",
      "Tom 221\n",
      "attend 222\n",
      "robe 223\n",
      "open 224\n",
      "prize 225\n",
      "beginners 226\n",
      "rude 227\n",
      "parmezan 228\n",
      "yt 229\n",
      "Dustin 230\n",
      "hungry 231\n",
      "Rubens 232\n",
      "Paris 233\n",
      "enjoy 234\n",
      "Colin 235\n",
      "stewart 236\n",
      "agency 237\n",
      "Morton 238\n",
      "Aquaman 239\n",
      "Phillip 240\n",
      "Neal 241\n",
      "email 242\n",
      "Latin 243\n",
      "coffee 244\n",
      "profiles 245\n",
      "down 246\n",
      "lecture 247\n",
      "H 248\n",
      "son 249\n",
      "series 250\n",
      "exotic 251\n",
      "difference 252\n",
      "cream 253\n",
      "uni 254\n",
      "Doreen 255\n",
      "activities 256\n",
      "vegetables 257\n",
      "Kasper 258\n",
      "usual 259\n",
      "bagpack 260\n",
      "alcohol 261\n",
      "Ali 262\n",
      "Trier 263\n",
      "Arcadia 264\n",
      "Jade 265\n",
      "loses 266\n",
      "Caron 267\n",
      "further 268\n",
      "hotel 269\n",
      "cable 270\n",
      "spend 271\n",
      "worked 272\n",
      "advises 273\n",
      "descent 274\n",
      "enhancing 275\n",
      "recorded 276\n",
      "Julianna 277\n",
      "schedules 278\n",
      "Muriel 279\n",
      "Bangkok 280\n",
      "flowers 281\n",
      "congratulates 282\n",
      "Yoko 283\n",
      "Jawie 284\n",
      "xyz 285\n",
      "Kit 286\n",
      "Sophia 287\n",
      "observe 288\n",
      "threatens 289\n",
      "countersign 290\n",
      "many 291\n",
      "brownies 292\n",
      "Meghan 293\n",
      "vocabulary 294\n",
      "term 295\n",
      "First 296\n",
      "marks 297\n",
      "Samba 298\n",
      "S 299\n",
      "substitute 300\n",
      "killed 301\n",
      "starting 302\n",
      "amazing 303\n",
      "mushroom 304\n",
      "Donna 305\n",
      "Millie 306\n",
      "contact 307\n",
      "Wes 308\n",
      "Rod 309\n",
      "elder 310\n",
      "fighting 311\n",
      "Ocean 312\n",
      "giant 313\n",
      "Finn 314\n",
      "Reyna 315\n",
      "Youssouf 316\n",
      "worth 317\n",
      "pointer 318\n",
      "fan 319\n",
      "double 320\n",
      "Marriott 321\n",
      "Milo 322\n",
      "built 323\n",
      "comedian 324\n",
      "feeling 325\n",
      "terrible 326\n",
      "dental 327\n",
      "landing 328\n",
      "interested 329\n",
      "teenth 330\n",
      "invite 331\n",
      "Akira 332\n",
      "Food 333\n",
      "an 334\n",
      "Arnold 335\n",
      "signs 336\n",
      "Brandie 337\n",
      "Nichole 338\n",
      "Lidia 339\n",
      "cooking 340\n",
      "skatepark 341\n",
      "eBay 342\n",
      "campus 343\n",
      "compensated 344\n",
      "Thames 345\n",
      "Jeremy 346\n",
      "Historical 347\n",
      "Ersin 348\n",
      "always 349\n",
      "mushrooms 350\n",
      "origin 351\n",
      "Nick 352\n",
      "didgeridoos 353\n",
      "whose 354\n",
      "His 355\n",
      "LinkedIn 356\n",
      "Bialowieza 357\n",
      "gold 358\n",
      "evening 359\n",
      "prescription 360\n",
      "leaves 361\n",
      "Adella 362\n",
      "spotted 363\n",
      "ago 364\n",
      "leftovers 365\n",
      "Daisy 366\n",
      "Orchestra 367\n",
      "Jess 368\n",
      "Freddy 369\n",
      "Hill 370\n",
      "mould 371\n",
      "safety 372\n",
      "beaver 373\n",
      "meteor 374\n",
      "Queen 375\n",
      "passed 376\n",
      "cocktail 377\n",
      "Jazmine 378\n",
      "remain 379\n",
      "kissing 380\n",
      "appreciated 381\n",
      "employees 382\n",
      "embezzlement 383\n",
      "Enrique 384\n",
      "Jojo 385\n",
      "picked 386\n",
      "fever 387\n",
      "Vic 388\n",
      "Francesca 389\n",
      "Jarvis 390\n",
      "bad 391\n",
      "singles 392\n",
      "Carson 393\n",
      "another 394\n",
      "discovered 395\n",
      "Lorenzo 396\n",
      "drill 397\n",
      "complexion 398\n",
      "her 399\n",
      "enroll 400\n",
      "damaged 401\n",
      "Bojana 402\n",
      "adult 403\n",
      "Brexit 404\n",
      "trick 405\n",
      "19th 406\n",
      "Mat 407\n",
      "Deborah 408\n",
      "engine 409\n",
      "week 410\n",
      "wannabe 411\n",
      "did 412\n",
      "Shirley 413\n",
      "Gery 414\n",
      "faulty 415\n",
      "Roxanne 416\n",
      "Anny 417\n",
      "needs 418\n",
      "Alfonso 419\n",
      "file 420\n",
      "preferably 421\n",
      "approve 422\n",
      "hills 423\n",
      "settles 424\n",
      "Bon 425\n",
      "provider 426\n",
      "bananas 427\n",
      "Coffee 428\n",
      "hybrid 429\n",
      "aunt 430\n",
      "grandfather 431\n",
      "Hulk 432\n",
      "clothes 433\n",
      "fact 434\n",
      "stress 435\n",
      "cancer 436\n",
      "dissapointed 437\n",
      "possibility 438\n",
      "Elisa 439\n",
      "Page 440\n",
      "persuaded 441\n",
      "young 442\n",
      "Getz 443\n",
      "dollars 444\n",
      "hospitalized 445\n",
      "gets 446\n",
      "there 447\n",
      "tatoo 448\n",
      "actress 449\n",
      "Theo 450\n",
      "bought 451\n",
      "Kayah 452\n",
      "asked 453\n",
      "clarify 454\n",
      "Miriam 455\n",
      "different 456\n",
      "Theresa 457\n",
      "Christa 458\n",
      "Sadie 459\n",
      "Stranger 460\n",
      "fast 461\n",
      "Ally 462\n",
      "remark 463\n",
      "coypu 464\n",
      "passes 465\n",
      "Denver 466\n",
      "sore 467\n",
      "Michelle 468\n",
      "essay 469\n",
      "factory 470\n",
      "studio 471\n",
      "poster 472\n",
      "lentils 473\n",
      "full 474\n",
      "Skinner 475\n",
      "brand 476\n",
      "apologize 477\n",
      "Arlo 478\n",
      "bill 479\n",
      "Roisin 480\n",
      "Mr 481\n",
      "recommendations 482\n",
      "stakes 483\n",
      "Janet 484\n",
      "Isaac 485\n",
      "Italy 486\n",
      "pondering 487\n",
      "Youtube 488\n",
      "know 489\n",
      "Drive 490\n",
      "Someone 491\n",
      "stuff 492\n",
      "black 493\n",
      "toy 494\n",
      "free 495\n",
      "ambassador 496\n",
      "Jell 497\n",
      "Beatrice 498\n",
      "Wayne 499\n",
      "Magdalene 500\n",
      "festival 501\n",
      "fruits 502\n",
      "owe 503\n",
      "delivery 504\n",
      "small 505\n",
      "kid 506\n",
      "Elmer 507\n",
      "July 508\n",
      "Papa 509\n",
      "afterwards 510\n",
      "gamble 511\n",
      "Zakharovs 512\n",
      "Gina 513\n",
      "confirm 514\n",
      "face 515\n",
      "organisational 516\n",
      "Simmons 517\n",
      "Lexie 518\n",
      "Station 519\n",
      "fit 520\n",
      "vermouth 521\n",
      "create 522\n",
      "Salty 523\n",
      "site 524\n",
      "dub 525\n",
      "Anita 526\n",
      "Mindy 527\n",
      "Anthony 528\n",
      "invitations 529\n",
      "keen 530\n",
      "Gigi 531\n",
      "Marge 532\n",
      "candle 533\n",
      "Omarosa 534\n",
      "laptops 535\n",
      "Marisa 536\n",
      "stopped 537\n",
      "Ellie 538\n",
      "problematic 539\n",
      "Rapper 540\n",
      "Shania 541\n",
      "Grant 542\n",
      "Make 543\n",
      "Hilda 544\n",
      "neurologist 545\n",
      "Nathaniel 546\n",
      "Las 547\n",
      "Czech 548\n",
      "lost 549\n",
      "nice 550\n",
      "Lidsay 551\n",
      "expensive 552\n",
      "defence 553\n",
      "report 554\n",
      "Rossi 555\n",
      "seller 556\n",
      "calls 557\n",
      "Rejmonta 558\n",
      "Jack 559\n",
      "marketing 560\n",
      "Italian 561\n",
      "Philips 562\n",
      "episode 563\n",
      "ticket 564\n",
      "Killing 565\n",
      "exhibition 566\n",
      "plays 567\n",
      "vegan 568\n",
      "Eliza 569\n",
      "save 570\n",
      "geometry 571\n",
      "Wish 572\n",
      "absence 573\n",
      "3pm 574\n",
      "drain 575\n",
      "suffers 576\n",
      "Rowing 577\n",
      "couple 578\n",
      "Ripple 579\n",
      "Jonah 580\n",
      "terrifying 581\n",
      "rows 582\n",
      "accents 583\n",
      "psychologist 584\n",
      "she 585\n",
      "close 586\n",
      "shown 587\n",
      "Bonnie 588\n",
      "arrivals 589\n",
      "partying 590\n",
      "appointment 591\n",
      "November 592\n",
      "Adar 593\n",
      "heavy 594\n",
      "Wales 595\n",
      "charity 596\n",
      "Favourite 597\n",
      "10 598\n",
      "There 599\n",
      "company 600\n",
      "Misha 601\n",
      "Simone 602\n",
      "American 603\n",
      "girlrfriend 604\n",
      "homeless 605\n",
      "didn 606\n",
      "vaping 607\n",
      "fall 608\n",
      "fashioned 609\n",
      "anniversary 610\n",
      "foot 611\n",
      "financial 612\n",
      "Mounir 613\n",
      "satisfying 614\n",
      "past 615\n",
      "password 616\n",
      "straight 617\n",
      "Suzan 618\n",
      "fine 619\n",
      "Maja 620\n",
      "from 621\n",
      "Vesuvius 622\n",
      "ll 623\n",
      "organisation 624\n",
      "beacuse 625\n",
      "posts 626\n",
      "Miley 627\n",
      "telescope 628\n",
      "FB 629\n",
      "pain 630\n",
      "Bojack 631\n",
      "heater 632\n",
      "Monica 633\n",
      "yesterday 634\n",
      "[PAD] 635\n",
      "India 636\n",
      "protect 637\n",
      "planned 638\n",
      "closed 639\n",
      "grandma 640\n",
      "P 641\n",
      "recommend 642\n",
      "sample 643\n",
      "Gerda 644\n",
      "8th 645\n",
      "Arnie 646\n",
      "coding 647\n",
      "pen 648\n",
      "answer 649\n",
      "so 650\n",
      "summer 651\n",
      "sleep 652\n",
      "CV 653\n",
      "kindergarten 654\n",
      "listened 655\n",
      "housewarming 656\n",
      "Heart 657\n",
      "messenger 658\n",
      "mean 659\n",
      "Ana 660\n",
      "Baba 661\n",
      "Pastor 662\n",
      "fat 663\n",
      "pumkin 664\n",
      "4pm 665\n",
      "stays 666\n",
      "Resident 667\n",
      "Studio 668\n",
      "Tessa 669\n",
      "swimming 670\n",
      "girlfriend 671\n",
      "Troy 672\n",
      "Laura 673\n",
      "practice 674\n",
      "Bill 675\n",
      "accurate 676\n",
      "Royal 677\n",
      "rubella 678\n",
      "cancellation 679\n",
      "Sid 680\n",
      "hat 681\n",
      "Ipanema 682\n",
      "bugs 683\n",
      "felt 684\n",
      "compilation 685\n",
      "service 686\n",
      "Saul 687\n",
      "Verity 688\n",
      "doubts 689\n",
      "locations 690\n",
      "Lizzy 691\n",
      "guests 692\n",
      "ZA 693\n",
      "This 694\n",
      "confirmation 695\n",
      "cryptocurrencies 696\n",
      "spending 697\n",
      "Ruth 698\n",
      "September 699\n",
      "Ricky 700\n",
      "betting 701\n",
      "INFOLEG 702\n",
      "MicroSip 703\n",
      "autograph 704\n",
      "Roxy 705\n",
      "wireless 706\n",
      "Charlotte 707\n",
      "position 708\n",
      "Axel 709\n",
      "which 710\n",
      "repeat 711\n",
      "tissues 712\n",
      "Mel 713\n",
      "argument 714\n",
      "smile 715\n",
      "mobile 716\n",
      "manage 717\n",
      "second 718\n",
      "Waytt 719\n",
      "opposed 720\n",
      "Dave 721\n",
      "following 722\n",
      "paid 723\n",
      "Michal 724\n",
      "wonders 725\n",
      "lent 726\n",
      "tomorrow 727\n",
      "deleting 728\n",
      "Allie 729\n",
      "DJ 730\n",
      "can 731\n",
      "Cardiff 732\n",
      "Earnest 733\n",
      "Nala 734\n",
      "MoMA 735\n",
      "graduation 736\n",
      "compose 737\n",
      "Persia 738\n",
      "printer 739\n",
      "Meryl 740\n",
      "manager 741\n",
      "say 742\n",
      "dedicated 743\n",
      "performed 744\n",
      "reminded 745\n",
      "tje 746\n",
      "12th 747\n",
      "regards 748\n",
      "Jimmy 749\n",
      "Beebe 750\n",
      "cashback 751\n",
      "Tabitha 752\n",
      "older 753\n",
      "Becky 754\n",
      "Edward 755\n",
      "chess 756\n",
      "Sharon 757\n",
      "scarf 758\n",
      "Cillian 759\n",
      "Kerry 760\n",
      "contract 761\n",
      "the 762\n",
      "mash 763\n",
      "available 764\n",
      "prefers 765\n",
      "headlamps 766\n",
      "Bam 767\n",
      "Mick 768\n",
      "tennis 769\n",
      "Great 770\n",
      "Masuria 771\n",
      "consults 772\n",
      "spicy 773\n",
      "arguing 774\n",
      "casino 775\n",
      "1990s 776\n",
      "Purity 777\n",
      "Joey 778\n",
      "snobby 779\n",
      "grandmother 780\n",
      "Presentations 781\n",
      "Opel 782\n",
      "places 783\n",
      "tonight 784\n",
      "well 785\n",
      "finger 786\n",
      "Luke 787\n",
      "Thom 788\n",
      "transport 789\n",
      "attending 790\n",
      "Yoseph 791\n",
      "Oliver 792\n",
      "Willard 793\n",
      "Bernie 794\n",
      "Spirit 795\n",
      "Voice 796\n",
      "management 797\n",
      "Noah 798\n",
      "his 799\n",
      "Deedee 800\n",
      "clash 801\n",
      "Gil 802\n",
      "expectations 803\n",
      "Main 804\n",
      "transmission 805\n",
      "account 806\n",
      "day 807\n",
      "Gisele 808\n",
      "Amal 809\n",
      "Jeanie 810\n",
      "half 811\n",
      "Thursday 812\n",
      "Ashlyn 813\n",
      "Barbie 814\n",
      "headphones 815\n",
      "smoke 816\n",
      "soup 817\n",
      "Matt 818\n",
      "D 819\n",
      "rehearsals 820\n",
      "encourage 821\n",
      "CEMS 822\n",
      "airpords 823\n",
      "booths 824\n",
      "youtube 825\n",
      "Terrance 826\n",
      "fears 827\n",
      "22 828\n",
      "Old 829\n",
      "backpack 830\n",
      "Mrs 831\n",
      "also 832\n",
      "doctor 833\n",
      "maybe 834\n",
      "Geoffrey 835\n",
      "notifications 836\n",
      "guy 837\n",
      "auction 838\n",
      "material 839\n",
      "setting 840\n",
      "Sayeed 841\n",
      "visitor 842\n",
      "Jackie 843\n",
      "cookbook 844\n",
      "30 845\n",
      "Mystery 846\n",
      "5 847\n",
      "Lane 848\n",
      "want 849\n",
      "remake 850\n",
      "Maria 851\n",
      "Yann 852\n",
      "Flores 853\n",
      "05 854\n",
      "man 855\n",
      "robotics 856\n",
      "shades 857\n",
      "Mathilde 858\n",
      "stalling 859\n",
      "salary 860\n",
      "Mariam 861\n",
      "Mackenzie 862\n",
      "hurts 863\n",
      "decorations 864\n",
      "Opium 865\n",
      "pranked 866\n",
      "form 867\n",
      "cellar 868\n",
      "lies 869\n",
      "address 870\n",
      "Audrey 871\n",
      "locked 872\n",
      "Scarlet 873\n",
      "Seb 874\n",
      "tacky 875\n",
      "niece 876\n",
      "cramps 877\n",
      "overtired 878\n",
      "Bieber 879\n",
      "Milan 880\n",
      "replace 881\n",
      "Gavin 882\n",
      "Darius 883\n",
      "Isabella 884\n",
      "freedoms 885\n",
      "con 886\n",
      "owes 887\n",
      "Black 888\n",
      "Menno 889\n",
      "eats 890\n",
      "Macarena 891\n",
      "kitten 892\n",
      "lands 893\n",
      "audience 894\n",
      "language 895\n",
      "demotivated 896\n",
      "superhero 897\n",
      "Predator 898\n",
      "weekend 899\n",
      "4th 900\n",
      "lobster 901\n",
      "shows 902\n",
      "Paloma 903\n",
      "NGO 904\n",
      "Frederic 905\n",
      "levels 906\n",
      "reaction 907\n",
      "Faby 908\n",
      "merge 909\n",
      "Master 910\n",
      "independent 911\n",
      "Albert 912\n",
      "about 913\n",
      "arguments 914\n",
      "request 915\n",
      "remembers 916\n",
      "shrimps 917\n",
      "recipe 918\n",
      "wants 919\n",
      "employer 920\n",
      "wth 921\n",
      "Avengers 922\n",
      "Mendes 923\n",
      "Tammie 924\n",
      "miserable 925\n",
      "Karlo 926\n",
      "variety 927\n",
      "On 928\n",
      "behaving 929\n",
      "December 930\n",
      "warm 931\n",
      "Jorim 932\n",
      "Abbey 933\n",
      "497 934\n",
      "donate 935\n",
      "drawings 936\n",
      "pharyngitis 937\n",
      "Bertil 938\n",
      "professor 939\n",
      "Daniel 940\n",
      "fly 941\n",
      "Claude 942\n",
      "preparing 943\n",
      "social 944\n",
      "nuts 945\n",
      "feelt 946\n",
      "Mark 947\n",
      "nor 948\n",
      "Mother 949\n",
      "Asus 950\n",
      "comfy 951\n",
      "Leanne 952\n",
      "tired 953\n",
      "vote 954\n",
      "big 955\n",
      "Bose 956\n",
      "spa 957\n",
      "Javier 958\n",
      "iPhone 959\n",
      "drop 960\n",
      "raining 961\n",
      "work 962\n",
      "throwing 963\n",
      "Courtney 964\n",
      "into 965\n",
      "trips 966\n",
      "landlord 967\n",
      "Dean 968\n",
      "alone 969\n",
      "links 970\n",
      "querying 971\n",
      "Sixx 972\n",
      "May 973\n",
      "thinks 974\n",
      "Granada 975\n",
      "involved 976\n",
      "ankle 977\n",
      "land 978\n",
      "practical 979\n",
      "warehouse 980\n",
      "s 981\n",
      "versions 982\n",
      "shooting 983\n",
      "plant 984\n",
      "cousin 985\n",
      "inserted 986\n",
      "Twiggy 987\n",
      "doesn 988\n",
      "managed 989\n",
      "uptown 990\n",
      "Egbert 991\n",
      "broke 992\n",
      "Spotify 993\n",
      "Sami 994\n",
      "Direct 995\n",
      "for 996\n",
      "message 997\n",
      "important 998\n",
      "Cathie 999\n",
      "Mall 1000\n",
      "Moss 1001\n",
      "Herman 1002\n",
      "translator 1003\n",
      "Kian 1004\n",
      "Connell 1005\n",
      "18 1006\n",
      "Jonas 1007\n",
      "future 1008\n",
      "Feyi 1009\n",
      "CJay 1010\n",
      "Kayne 1011\n",
      "Norah 1012\n",
      "done 1013\n",
      "out 1014\n",
      "therapy 1015\n",
      "Facebook 1016\n",
      "uncle 1017\n",
      "Gabiel 1018\n",
      "mud 1019\n",
      "frequency 1020\n",
      "[START] 1021\n",
      "quite 1022\n",
      "IT 1023\n",
      "Josephine 1024\n",
      "desperados 1025\n",
      "appendix 1026\n",
      "ideas 1027\n",
      "entire 1028\n",
      "Calvin 1029\n",
      "Like 1030\n",
      "try 1031\n",
      "childish 1032\n",
      "figure 1033\n",
      "Shaun 1034\n",
      "women 1035\n",
      "Andrea 1036\n",
      "Maryam 1037\n",
      "surfing 1038\n",
      "Dixit 1039\n",
      "brilliantly 1040\n",
      "Yorke 1041\n",
      "especially 1042\n",
      "Marin 1043\n",
      "Peggy 1044\n",
      "furious 1045\n",
      "song 1046\n",
      "Bree 1047\n",
      "Vivien 1048\n",
      "Hope 1049\n",
      "rugby 1050\n",
      "restaurants 1051\n",
      "van 1052\n",
      "End 1053\n",
      "bullied 1054\n",
      "drunk 1055\n",
      "earlier 1056\n",
      "e 1057\n",
      "Jordans 1058\n",
      "admire 1059\n",
      "Hyperion 1060\n",
      "Festival 1061\n",
      "JAckie 1062\n",
      "great 1063\n",
      "fired 1064\n",
      "Nice 1065\n",
      "Hampson 1066\n",
      "Acres 1067\n",
      "market 1068\n",
      "tradition 1069\n",
      "lawyer 1070\n",
      "article 1071\n",
      "sad 1072\n",
      "getting 1073\n",
      "Sylas 1074\n",
      "colour 1075\n",
      "shop 1076\n",
      "Rome 1077\n",
      "yelled 1078\n",
      "two 1079\n",
      "Heather 1080\n",
      "Isla 1081\n",
      "proud 1082\n",
      "2018 1083\n",
      "@ 1084\n",
      "Lorry 1085\n",
      "apply 1086\n",
      "Asperger 1087\n",
      "length 1088\n",
      "Becca 1089\n",
      "Kaya 1090\n",
      "concerned 1091\n",
      "gift 1092\n",
      "waits 1093\n",
      "Arturo 1094\n",
      "goodbye 1095\n",
      "Everything 1096\n",
      "impossible 1097\n",
      "donuts 1098\n",
      "jokes 1099\n",
      "Lindsay 1100\n",
      "intolerant 1101\n",
      "Fonda 1102\n",
      "shirts 1103\n",
      "gloves 1104\n",
      "healthy 1105\n",
      "Sunflower 1106\n",
      "connection 1107\n",
      "whether 1108\n",
      "Regina 1109\n",
      "secretary 1110\n",
      "wait 1111\n",
      "poorly 1112\n",
      "reindeer 1113\n",
      "announcing 1114\n",
      "Lexi 1115\n",
      "nutcracker 1116\n",
      "Chai 1117\n",
      "olivier 1118\n",
      "juice 1119\n",
      "planted 1120\n",
      "Debra 1121\n",
      "sledding 1122\n",
      "pregnant 1123\n",
      "Marcel 1124\n",
      "polo 1125\n",
      "favourite 1126\n",
      "handout 1127\n",
      "Siddhi 1128\n",
      "switch 1129\n",
      "Bryan 1130\n",
      "shopped 1131\n",
      "few 1132\n",
      "Hudson 1133\n",
      "assist 1134\n",
      "Jenniffer 1135\n",
      "joins 1136\n",
      "tartare 1137\n",
      "scandinavian 1138\n",
      "classy 1139\n",
      "harsh 1140\n",
      "Orwell 1141\n",
      "themed 1142\n",
      "Joziah 1143\n",
      "skills 1144\n",
      "Oleg 1145\n",
      "grainy 1146\n",
      "Amazon 1147\n",
      "poking 1148\n",
      "raising 1149\n",
      "uncomfortable 1150\n",
      "Journey 1151\n",
      "Chuck 1152\n",
      "Payton 1153\n",
      "Zoeh 1154\n",
      "Khaled 1155\n",
      "diet 1156\n",
      "Jayden 1157\n",
      "Ines 1158\n",
      "Betty 1159\n",
      "Susie 1160\n",
      "Treaty 1161\n",
      "prior 1162\n",
      "Moneverdi 1163\n",
      "Brina 1164\n",
      "along 1165\n",
      "000 1166\n",
      "contacts 1167\n",
      "papel 1168\n",
      "Francis 1169\n",
      "pens 1170\n",
      "coming 1171\n",
      "They 1172\n",
      "scientific 1173\n",
      "Kenneth 1174\n",
      "ejoying 1175\n",
      "Maisy 1176\n",
      "raisins 1177\n",
      "5th 1178\n",
      "shouting 1179\n",
      "Pipa 1180\n",
      "Helene 1181\n",
      "country 1182\n",
      "desk 1183\n",
      "skype 1184\n",
      "Scott 1185\n",
      "easy 1186\n",
      "frequently 1187\n",
      "Bawarian 1188\n",
      "excellent 1189\n",
      "cauliflower 1190\n",
      "Olaf 1191\n",
      "fooled 1192\n",
      "Best 1193\n",
      "noticed 1194\n",
      "Paisley 1195\n",
      "Wifi 1196\n",
      "break 1197\n",
      "informs 1198\n",
      "item 1199\n",
      "MATH110 1200\n",
      "serious 1201\n",
      "wrote 1202\n",
      "code 1203\n",
      "SW 1204\n",
      "Kim 1205\n",
      "Robyn 1206\n",
      "Zeppelin 1207\n",
      "Strand 1208\n",
      "March 1209\n",
      "age 1210\n",
      "It 1211\n",
      "center 1212\n",
      "parts 1213\n",
      "frustrated 1214\n",
      "while 1215\n",
      "Greta 1216\n",
      "Monty 1217\n",
      "dreams 1218\n",
      "Bessie 1219\n",
      "Grasso 1220\n",
      "beard 1221\n",
      "host 1222\n",
      "1st 1223\n",
      "drawing 1224\n",
      "Aneta 1225\n",
      "booked 1226\n",
      "english 1227\n",
      "reservation 1228\n",
      "seats 1229\n",
      "tapas 1230\n",
      "knew 1231\n",
      "route 1232\n",
      "01 1233\n",
      "grinder 1234\n",
      "impatient 1235\n",
      "T 1236\n",
      "W 1237\n",
      "Fleur 1238\n",
      "geysers 1239\n",
      "Wells 1240\n",
      "binge 1241\n",
      "apple 1242\n",
      "Felicia 1243\n",
      "Brett 1244\n",
      "42 1245\n",
      "leak 1246\n",
      "Jordan 1247\n",
      "chose 1248\n",
      "Theatre 1249\n",
      "height 1250\n",
      "Butterball 1251\n",
      "McDonalds 1252\n",
      "Holy 1253\n",
      "promised 1254\n",
      "Kas 1255\n",
      "able 1256\n",
      "Magda 1257\n",
      "living 1258\n",
      "kennels 1259\n",
      "talking 1260\n",
      "visit 1261\n",
      "sandwich 1262\n",
      "Stacey 1263\n",
      "through 1264\n",
      "Zac 1265\n",
      "police 1266\n",
      "Maybelline 1267\n",
      "finds 1268\n",
      "letter 1269\n",
      "Karla 1270\n",
      "Katie 1271\n",
      "scapegoat 1272\n",
      "pet 1273\n",
      "rehabilitation 1274\n",
      "Artur 1275\n",
      "support 1276\n",
      "antibiotics 1277\n",
      "Brooke 1278\n",
      "mattress 1279\n",
      "reports 1280\n",
      "reckons 1281\n",
      "Robert 1282\n",
      "Morty 1283\n",
      "wrapped 1284\n",
      "saving 1285\n",
      "stay 1286\n",
      "Fiasco 1287\n",
      "rock 1288\n",
      "mark 1289\n",
      "Mats 1290\n",
      "Philadelphia 1291\n",
      "Starbucks 1292\n",
      "Agatha 1293\n",
      "doing 1294\n",
      "halfway 1295\n",
      "deals 1296\n",
      "stand 1297\n",
      "believes 1298\n",
      "communicate 1299\n",
      "Rob 1300\n",
      "posted 1301\n",
      "front 1302\n",
      "once 1303\n",
      "Oti 1304\n",
      "breaks 1305\n",
      "caught 1306\n",
      "Spain 1307\n",
      "Sulawesi 1308\n",
      "informed 1309\n",
      "Niall 1310\n",
      "minute 1311\n",
      "Bahn 1312\n",
      "chip 1313\n",
      "vet 1314\n",
      "Ellen 1315\n",
      "Shining 1316\n",
      "deadline 1317\n",
      "countries 1318\n",
      "dance 1319\n",
      "Self 1320\n",
      "exchange 1321\n",
      "shift 1322\n",
      "advice 1323\n",
      "Essex 1324\n",
      "Prudential 1325\n",
      "16 1326\n",
      "Irene 1327\n",
      "beaten 1328\n",
      "drank 1329\n",
      "p 1330\n",
      "Nikos 1331\n",
      "declares 1332\n",
      "tests 1333\n",
      "Mea 1334\n",
      "highway 1335\n",
      "Darcey 1336\n",
      "Fionna 1337\n",
      "choices 1338\n",
      "Next 1339\n",
      "Guillermo 1340\n",
      "S9 1341\n",
      "brothers 1342\n",
      "Karina 1343\n",
      "reactivate 1344\n",
      "Christine 1345\n",
      "Arena 1346\n",
      "craps 1347\n",
      "Rosanne 1348\n",
      "Seamus 1349\n",
      "Gino 1350\n",
      "universities 1351\n",
      "Steffen 1352\n",
      "Marlie 1353\n",
      "router 1354\n",
      "Russ 1355\n",
      "Vincent 1356\n",
      "him 1357\n",
      "Angie 1358\n",
      "workmates 1359\n",
      "Scottish 1360\n",
      "directions 1361\n",
      "1pm 1362\n",
      "Lindsey 1363\n",
      "sausage 1364\n",
      "church 1365\n",
      "Dawid 1366\n",
      "anywas 1367\n",
      "denies 1368\n",
      "Tress 1369\n",
      "Royals 1370\n",
      "slipped 1371\n",
      "inspiration 1372\n",
      "freezing 1373\n",
      "Gareth 1374\n",
      "Leandra 1375\n",
      "already 1376\n",
      "Tay 1377\n",
      "master 1378\n",
      "Lilah 1379\n",
      "adivce 1380\n",
      "Sophie 1381\n",
      "nickname 1382\n",
      "Ruby 1383\n",
      "Chicago 1384\n",
      "democracy 1385\n",
      "Carlo 1386\n",
      "FIFA 1387\n",
      "Bridger 1388\n",
      "Stoke 1389\n",
      "Costa 1390\n",
      "Armenian 1391\n",
      "Rita 1392\n",
      "relaxed 1393\n",
      "No 1394\n",
      "considers 1395\n",
      "Broderick 1396\n",
      "Malini 1397\n",
      "Lampard 1398\n",
      "finish 1399\n",
      "coordinates 1400\n",
      "birthday 1401\n",
      "improve 1402\n",
      "Gale 1403\n",
      "history 1404\n",
      "pedicure 1405\n",
      "Clash 1406\n",
      "After 1407\n",
      "concert 1408\n",
      "substitutes 1409\n",
      "chores 1410\n",
      "Dumbo 1411\n",
      "Instead 1412\n",
      "someone 1413\n",
      "hissing 1414\n",
      "glass 1415\n",
      "comforts 1416\n",
      "Barcelona 1417\n",
      "gym 1418\n",
      "again 1419\n",
      "Scandinavian 1420\n",
      "Britney 1421\n",
      "planning 1422\n",
      "avoid 1423\n",
      "cuddle 1424\n",
      "hub 1425\n",
      "authorities 1426\n",
      "gone 1427\n",
      "Casey 1428\n",
      "surgery 1429\n",
      "fear 1430\n",
      "Melany 1431\n",
      "mat 1432\n",
      "studies 1433\n",
      "Eveline 1434\n",
      "Caren 1435\n",
      "reason 1436\n",
      "Randall 1437\n",
      "LLC 1438\n",
      "Mona 1439\n",
      "Annie 1440\n",
      "kayaking 1441\n",
      "Hunter 1442\n",
      "disappearing 1443\n",
      "Venom 1444\n",
      "basketfuls 1445\n",
      "Potter 1446\n",
      "moved 1447\n",
      "photo 1448\n",
      "forget 1449\n",
      "Karenina 1450\n",
      "Shanon 1451\n",
      "reasons 1452\n",
      "causing 1453\n",
      "Angelina 1454\n",
      "girlish 1455\n",
      "waste 1456\n",
      "Corsica 1457\n",
      "recovering 1458\n",
      "Sabine 1459\n",
      "90 1460\n",
      "on 1461\n",
      "Cece 1462\n",
      "Ashlee 1463\n",
      "revisit 1464\n",
      "confirms 1465\n",
      "Abu 1466\n",
      "having 1467\n",
      "Netflix 1468\n",
      "propose 1469\n",
      "Damien 1470\n",
      "Stan 1471\n",
      "TI 1472\n",
      "headache 1473\n",
      "themselves 1474\n",
      "hotels 1475\n",
      "Fluff 1476\n",
      "Glen 1477\n",
      "America 1478\n",
      "Spud 1479\n",
      "Death 1480\n",
      "stop 1481\n",
      "enjoying 1482\n",
      "Hawkins 1483\n",
      "according 1484\n",
      "intermeddle 1485\n",
      "chick 1486\n",
      "34 1487\n",
      "Dan 1488\n",
      "via 1489\n",
      "Jim 1490\n",
      "32 1491\n",
      "Ala 1492\n",
      "exaggerating 1493\n",
      "Elvis 1494\n",
      "bake 1495\n",
      "50 1496\n",
      "interactive 1497\n",
      "surprising 1498\n",
      "useless 1499\n",
      "Piotr 1500\n",
      "Jonna 1501\n",
      "bread 1502\n",
      "critical 1503\n",
      "Cast 1504\n",
      "fortieth 1505\n",
      "lend 1506\n",
      "Mandy 1507\n",
      "Escobar 1508\n",
      "PS4 1509\n",
      "De 1510\n",
      "behind 1511\n",
      "DVD 1512\n",
      "less 1513\n",
      "UK 1514\n",
      "11C 1515\n",
      "Pat 1516\n",
      "Gene 1517\n",
      "Puerto 1518\n",
      "Teresa 1519\n",
      "Ola 1520\n",
      "Nicole 1521\n",
      "attractive 1522\n",
      "Mai 1523\n",
      "aware 1524\n",
      "Danuta 1525\n",
      "covered 1526\n",
      "Airport 1527\n",
      "cannot 1528\n",
      "Louisa 1529\n",
      "grateful 1530\n",
      "client 1531\n",
      "shake 1532\n",
      "Outlaw 1533\n",
      "Teddy 1534\n",
      "tire 1535\n",
      "calendars 1536\n",
      "Clinic 1537\n",
      "Kaitlyn 1538\n",
      "slows 1539\n",
      "Aya 1540\n",
      "Pierre 1541\n",
      "Eden 1542\n",
      "Arkia 1543\n",
      "around 1544\n",
      "memphis 1545\n",
      "quality 1546\n",
      "wedidng 1547\n",
      "Walter 1548\n",
      "Barba 1549\n",
      "Astra 1550\n",
      "buying 1551\n",
      "amount 1552\n",
      "reply 1553\n",
      "retake 1554\n",
      "Beaujolais 1555\n",
      "Byblos 1556\n",
      "dumping 1557\n",
      "volcano 1558\n",
      "questions 1559\n",
      "75 1560\n",
      "reach 1561\n",
      "accommodating 1562\n",
      "Yani 1563\n",
      "Ukraine 1564\n",
      "six 1565\n",
      "flavours 1566\n",
      "Megan 1567\n",
      "bucks 1568\n",
      "luck 1569\n",
      "schedule 1570\n",
      "tricks 1571\n",
      "Valery 1572\n",
      "dish 1573\n",
      "health 1574\n",
      "legend 1575\n",
      "Uber 1576\n",
      "8pm 1577\n",
      "collecting 1578\n",
      "chocolates 1579\n",
      "- 1580\n",
      "violence 1581\n",
      "against 1582\n",
      "hello 1583\n",
      "textbook 1584\n",
      "Jelmer 1585\n",
      "Darline 1586\n",
      "biked 1587\n",
      "Rosa 1588\n",
      "screenshot 1589\n",
      "Yo 1590\n",
      "brainstorming 1591\n",
      "Pret 1592\n",
      "OK 1593\n",
      "compliment 1594\n",
      "thousand 1595\n",
      "Telma 1596\n",
      "painkillers 1597\n",
      "piece 1598\n",
      "towards 1599\n",
      "parents 1600\n",
      "Vistula 1601\n",
      "lot 1602\n",
      "Gael 1603\n",
      "bleeding 1604\n",
      "lotion 1605\n",
      "Harper 1606\n",
      "eve 1607\n",
      "he 1608\n",
      "Mathews 1609\n",
      "himself 1610\n",
      "Bohemian 1611\n",
      "Karan 1612\n",
      "snow 1613\n",
      "respond 1614\n",
      "decide 1615\n",
      "receives 1616\n",
      "Kenna 1617\n",
      "improves 1618\n",
      "watch 1619\n",
      "Led 1620\n",
      "choir 1621\n",
      "Ivy 1622\n",
      "Marlene 1623\n",
      "apples 1624\n",
      "image 1625\n",
      "feelings 1626\n",
      "pursue 1627\n",
      "Conrad 1628\n",
      "modern 1629\n",
      "Ost 1630\n",
      "agreed 1631\n",
      "clubbing 1632\n",
      "Blackett 1633\n",
      "tutor 1634\n",
      "real 1635\n",
      "suspects 1636\n",
      "hers 1637\n",
      "Blainville 1638\n",
      "jars 1639\n",
      "Evil 1640\n",
      "hit 1641\n",
      "sit 1642\n",
      "Jared 1643\n",
      "features 1644\n",
      "vacancies 1645\n",
      "connected 1646\n",
      "outage 1647\n",
      "Anastasia 1648\n",
      "DQ 1649\n",
      "Cuba 1650\n",
      "Yasmina 1651\n",
      "Korean 1652\n",
      "Meir 1653\n",
      "psychotherapy 1654\n",
      "junk 1655\n",
      "Tess 1656\n",
      "Day 1657\n",
      "skating 1658\n",
      "Melanie 1659\n",
      "Mondays 1660\n",
      "advert 1661\n",
      "really 1662\n",
      "dry 1663\n",
      "Nag 1664\n",
      "protesting 1665\n",
      "attacking 1666\n",
      "abandon 1667\n",
      "coats 1668\n",
      "sometimes 1669\n",
      "even 1670\n",
      "Derick 1671\n",
      "San 1672\n",
      "slip 1673\n",
      "cleaner 1674\n",
      "autographs 1675\n",
      "copy 1676\n",
      "Dom 1677\n",
      "making 1678\n",
      "worried 1679\n",
      "2k 1680\n",
      "more 1681\n",
      "rental 1682\n",
      "Louise 1683\n",
      "Abrielle 1684\n",
      "Marsha 1685\n",
      "Rich 1686\n",
      "Coca 1687\n",
      "Dead 1688\n",
      "till 1689\n",
      "some 1690\n",
      "pound 1691\n",
      "goals 1692\n",
      "picks 1693\n",
      "Margo 1694\n",
      "Natalia 1695\n",
      "belong 1696\n",
      "texts 1697\n",
      "explaining 1698\n",
      "started 1699\n",
      "city 1700\n",
      "Intel 1701\n",
      "Luca 1702\n",
      "Myra 1703\n",
      "whereas 1704\n",
      "leave 1705\n",
      "Afhaam 1706\n",
      "Kowalsky 1707\n",
      "emergency 1708\n",
      "get 1709\n",
      "2019 1710\n",
      "Noyce 1711\n",
      "Otherwise 1712\n",
      "bibliography 1713\n",
      "Algarve 1714\n",
      "dryer 1715\n",
      "Hee 1716\n",
      "Central 1717\n",
      "7000 1718\n",
      "Escape 1719\n",
      "passport 1720\n",
      "ibuprofen 1721\n",
      "Kai 1722\n",
      "laptop 1723\n",
      "Lacey 1724\n",
      "lamps 1725\n",
      "visits 1726\n",
      "Ada 1727\n",
      "Maryann 1728\n",
      "Lola 1729\n",
      "were 1730\n",
      "write 1731\n",
      "European 1732\n",
      "vouchers 1733\n",
      "strangely 1734\n",
      "nevertheless 1735\n",
      "ready 1736\n",
      "failed 1737\n",
      "hurt 1738\n",
      "min 1739\n",
      "how 1740\n",
      "Kora 1741\n",
      "Benedict 1742\n",
      "instead 1743\n",
      "enjoyed 1744\n",
      "Royale 1745\n",
      "jets 1746\n",
      "Angola 1747\n",
      "wakes 1748\n",
      "Roma 1749\n",
      "enjoyable 1750\n",
      "crapped 1751\n",
      "transferred 1752\n",
      "joke 1753\n",
      "checkout 1754\n",
      "Iris 1755\n",
      "Carolyn 1756\n",
      "channel 1757\n",
      "am 1758\n",
      "lose 1759\n",
      "heating 1760\n",
      "Pro 1761\n",
      "step 1762\n",
      "wolf 1763\n",
      "salad 1764\n",
      "deposit 1765\n",
      "waited 1766\n",
      "frends 1767\n",
      "documenting 1768\n",
      "goat 1769\n",
      "asking 1770\n",
      "Kellie 1771\n",
      "beginning 1772\n",
      "expects 1773\n",
      "Truck 1774\n",
      "habits 1775\n",
      "poked 1776\n",
      "Sodastream 1777\n",
      "aggresive 1778\n",
      "Oz 1779\n",
      "helpdesk 1780\n",
      "procedure 1781\n",
      "offers 1782\n",
      "temporarily 1783\n",
      "Ra 1784\n",
      "cabinet 1785\n",
      "Serena 1786\n",
      "cancels 1787\n",
      "either 1788\n",
      "Canada 1789\n",
      "cheapest 1790\n",
      "hill 1791\n",
      "O2 1792\n",
      "invitation 1793\n",
      "Museum 1794\n",
      "Hotel 1795\n",
      "Randy 1796\n",
      "noises 1797\n",
      "handsome 1798\n",
      "says 1799\n",
      "Mary 1800\n",
      "Cameron 1801\n",
      "girl 1802\n",
      "Marzena 1803\n",
      "however 1804\n",
      "Lennie 1805\n",
      "Paco 1806\n",
      "Kyle 1807\n",
      "referral 1808\n",
      "Lilian 1809\n",
      "Tyson 1810\n",
      "stories 1811\n",
      "surprised 1812\n",
      "Maddie 1813\n",
      "Elijah 1814\n",
      "intentions 1815\n",
      "spot 1816\n",
      "battery 1817\n",
      "Nothing 1818\n",
      "yoga 1819\n",
      "whatsoever 1820\n",
      "volunteers 1821\n",
      "fitness 1822\n",
      "Tara 1823\n",
      "Clark 1824\n",
      "abroad 1825\n",
      "telling 1826\n",
      "thirty 1827\n",
      "Luka 1828\n",
      "hi 1829\n",
      "alright 1830\n",
      "Roger 1831\n",
      "band 1832\n",
      "authenticate 1833\n",
      "Robbie 1834\n",
      "cute 1835\n",
      "buffet 1836\n",
      "Cynthia 1837\n",
      "forwarded 1838\n",
      "shared 1839\n",
      "doodle 1840\n",
      "Now 1841\n",
      "Flora 1842\n",
      "attended 1843\n",
      "Tony 1844\n",
      "opposition 1845\n",
      "Hyde 1846\n",
      "tipping 1847\n",
      "Tommy 1848\n",
      "PM 1849\n",
      "bets 1850\n",
      "If 1851\n",
      "Fai 1852\n",
      "help 1853\n",
      "wake 1854\n",
      "New 1855\n",
      "Dragons 1856\n",
      "Naomi 1857\n",
      "Field 1858\n",
      "closer 1859\n",
      ", 1860\n",
      "Penny 1861\n",
      "closet 1862\n",
      "Isabelle 1863\n",
      "tall 1864\n",
      "Niles 1865\n",
      "task 1866\n",
      "witch 1867\n",
      "Diggle 1868\n",
      "arrange 1869\n",
      "cheese 1870\n",
      "urgently 1871\n",
      "paint 1872\n",
      "stunned 1873\n",
      "large 1874\n",
      "Cristofer 1875\n",
      "Han 1876\n",
      "Klara 1877\n",
      "slides 1878\n",
      "skip 1879\n",
      "Eugene 1880\n",
      "impressed 1881\n",
      "bulbs 1882\n",
      "Nata 1883\n",
      "Bud 1884\n",
      "bathroom 1885\n",
      "airpods 1886\n",
      "Piyush 1887\n",
      "London 1888\n",
      "Gallery 1889\n",
      "Tobias 1890\n",
      "Tawny 1891\n",
      "Extra 1892\n",
      "hist 1893\n",
      "Lennart 1894\n",
      "204 1895\n",
      "hectic 1896\n",
      "horrible 1897\n",
      "handbag 1898\n",
      "strongly 1899\n",
      "searched 1900\n",
      "bibles 1901\n",
      "Nora 1902\n",
      "replacing 1903\n",
      "keeping 1904\n",
      "Donte 1905\n",
      "toasts 1906\n",
      "Southern 1907\n",
      "forgotten 1908\n",
      "Nino 1909\n",
      "rum 1910\n",
      "Year 1911\n",
      "nan 1912\n",
      "Early 1913\n",
      "invoice 1914\n",
      "Dungeons 1915\n",
      "death 1916\n",
      "Amari 1917\n",
      "Restarting 1918\n",
      "Alex 1919\n",
      "throws 1920\n",
      "Cross 1921\n",
      "Taylor 1922\n",
      "carbon 1923\n",
      "Christmas 1924\n",
      "job 1925\n",
      "months 1926\n",
      "Kuwait 1927\n",
      "Ursula 1928\n",
      "Kento 1929\n",
      "toolbox 1930\n",
      "royale 1931\n",
      "attention 1932\n",
      "Esme 1933\n",
      "Lou 1934\n",
      "laps 1935\n",
      "Ike 1936\n",
      "fenugreek 1937\n",
      "seen 1938\n",
      "Princess 1939\n",
      "joined 1940\n",
      "dissatisfied 1941\n",
      "Yuri 1942\n",
      "village 1943\n",
      "navigation 1944\n",
      "font 1945\n",
      "experienced 1946\n",
      "release 1947\n",
      "style 1948\n",
      "offered 1949\n",
      "Rosemary 1950\n",
      "being 1951\n",
      "Lupe 1952\n",
      "theater 1953\n",
      "McGregor 1954\n",
      "playing 1955\n",
      "Cauliflower 1956\n",
      "water 1957\n",
      "smokes 1958\n",
      "brown 1959\n",
      "Kristen 1960\n",
      "lends 1961\n",
      "playground 1962\n",
      "formulas 1963\n",
      "Mateusz 1964\n",
      "sensible 1965\n",
      "seek 1966\n",
      "Polly 1967\n",
      "Garrett 1968\n",
      "Donations 1969\n",
      "Kat 1970\n",
      "borrow 1971\n",
      "reduced 1972\n",
      "Willow 1973\n",
      "rip 1974\n",
      "house 1975\n",
      "Dexter 1976\n",
      "consulting 1977\n",
      "drinking 1978\n",
      "Yate 1979\n",
      "fracture 1980\n",
      "fp 1981\n",
      "crate 1982\n",
      "sent 1983\n",
      "joking 1984\n",
      "gay 1985\n",
      "Kev 1986\n",
      "star 1987\n",
      "Woodson 1988\n",
      "Messenger 1989\n",
      "round 1990\n",
      "Lina 1991\n",
      "Diablo 1992\n",
      "platform 1993\n",
      "Patty 1994\n",
      "pirate 1995\n",
      "Sheila 1996\n",
      "studied 1997\n",
      "parrot 1998\n",
      "political 1999\n",
      "prvate 2000\n",
      "1780 2001\n",
      "swim 2002\n",
      "hamster 2003\n",
      "goint 2004\n",
      "Hailey 2005\n",
      "reyurn 2006\n",
      "limiting 2007\n",
      "Leo 2008\n",
      "Park 2009\n",
      "organize 2010\n",
      "supper 2011\n",
      "Ghost 2012\n",
      "Cristina 2013\n",
      "perceive 2014\n",
      "their 2015\n",
      "intorelant 2016\n",
      "helpful 2017\n",
      "Mirror 2018\n",
      "Misunderstood 2019\n",
      "journey 2020\n",
      "Poly 2021\n",
      "excluded 2022\n",
      "Yesterday 2023\n",
      "395 2024\n",
      "Xenia 2025\n",
      "Gotti 2026\n",
      "Gabe 2027\n",
      "98765432 2028\n",
      "Leticia 2029\n",
      "lovers 2030\n",
      "boys 2031\n",
      "Gdynia 2032\n",
      "delay 2033\n",
      "end 2034\n",
      "upload 2035\n",
      "Hans 2036\n",
      "Marco 2037\n",
      "Tomasz 2038\n",
      "Warren 2039\n",
      "five 2040\n",
      "Joyce 2041\n",
      "Agnes 2042\n",
      "Alain 2043\n",
      "neighbours 2044\n",
      "Cola 2045\n",
      "points 2046\n",
      "handmaidens 2047\n",
      "would 2048\n",
      "assignment 2049\n",
      "Ari 2050\n",
      "cousins 2051\n",
      "Julies 2052\n",
      "Stones 2053\n",
      "study 2054\n",
      "titled 2055\n",
      "Miranda 2056\n",
      "behaviourist 2057\n",
      "Luxembourg 2058\n",
      "percent 2059\n",
      "Giovanni 2060\n",
      "Ian 2061\n",
      "Snape 2062\n",
      "wishes 2063\n",
      "taxify 2064\n",
      "prevent 2065\n",
      "emailed 2066\n",
      "solo 2067\n",
      "somebody 2068\n",
      "Travis 2069\n",
      "believe 2070\n",
      "Isobel 2071\n",
      "only 2072\n",
      "' 2073\n",
      "Howard 2074\n",
      "Kubica 2075\n",
      "Chloe 2076\n",
      "scratching 2077\n",
      "wool 2078\n",
      "SH 2079\n",
      "cheating 2080\n",
      "cloister 2081\n",
      "Gotham 2082\n",
      "Emmanuel 2083\n",
      "Tallinn 2084\n",
      "MRI 2085\n",
      "chemical 2086\n",
      "of 2087\n",
      "pay 2088\n",
      "Jo 2089\n",
      "most 2090\n",
      "Nicholas 2091\n",
      "quiet 2092\n",
      "Her 2093\n",
      "success 2094\n",
      "painted 2095\n",
      "Franck 2096\n",
      "Elliott 2097\n",
      "information 2098\n",
      "IB 2099\n",
      "own 2100\n",
      "earphones 2101\n",
      "students 2102\n",
      "Luna 2103\n",
      "Elm 2104\n",
      "lockers 2105\n",
      "euro 2106\n",
      "Ralph 2107\n",
      "behavior 2108\n",
      "Will 2109\n",
      "neither 2110\n",
      "professional 2111\n",
      "ignored 2112\n",
      "Basinger 2113\n",
      "Dixie 2114\n",
      "Cole 2115\n",
      "notary 2116\n",
      "looses 2117\n",
      "Demi 2118\n",
      "bringing 2119\n",
      "Player 2120\n",
      "argued 2121\n",
      "95 2122\n",
      "at 2123\n",
      "prepare 2124\n",
      "similar 2125\n",
      "Brandon 2126\n",
      "accidentally 2127\n",
      "Cara 2128\n",
      "wasted 2129\n",
      "Vince 2130\n",
      "uniforms 2131\n",
      "orders 2132\n",
      "Doylan 2133\n",
      "% 2134\n",
      "Raphael 2135\n",
      "3sat 2136\n",
      "bakery 2137\n",
      "diamond 2138\n",
      "Decathlon 2139\n",
      "com 2140\n",
      "katie 2141\n",
      "food 2142\n",
      "committee 2143\n",
      "Today 2144\n",
      "Rose 2145\n",
      "extreme 2146\n",
      "player 2147\n",
      "Evelyn 2148\n",
      "Danny 2149\n",
      "melancholic 2150\n",
      "Oslo 2151\n",
      "paper 2152\n",
      "Stuart 2153\n",
      "applying 2154\n",
      "Tim 2155\n",
      "retirement 2156\n",
      "Fridhemsplan 2157\n",
      "almost 2158\n",
      "123 2159\n",
      "reunion 2160\n",
      "jobs 2161\n",
      "Chelsea 2162\n",
      "diagnosis 2163\n",
      "Martins 2164\n",
      "names 2165\n",
      "Republic 2166\n",
      "times 2167\n",
      "suggestions 2168\n",
      "up 2169\n",
      "Wojtek 2170\n",
      "J 2171\n",
      "clean 2172\n",
      "Joseph 2173\n",
      "trucks 2174\n",
      "Marcin 2175\n",
      "Callie 2176\n",
      "ashamed 2177\n",
      "bike 2178\n",
      "handcrafts 2179\n",
      "anyway 2180\n",
      "25 2181\n",
      "103 2182\n",
      "selecting 2183\n",
      "crowds 2184\n",
      "Summer 2185\n",
      "trying 2186\n",
      "race 2187\n",
      "wi 2188\n",
      "Sinclair 2189\n",
      "Elian 2190\n",
      "Friends 2191\n",
      "reduce 2192\n",
      "Sephora 2193\n",
      "guys 2194\n",
      "Dex 2195\n",
      "eating 2196\n",
      "Beatrix 2197\n",
      "storage 2198\n",
      "too 2199\n",
      "Johny 2200\n",
      "fridge 2201\n",
      "canteen 2202\n",
      "understood 2203\n",
      "falls 2204\n",
      "Bath 2205\n",
      "Washington 2206\n",
      "Sammy 2207\n",
      "Scofield 2208\n",
      "monkey 2209\n",
      "Shum 2210\n",
      "imagine 2211\n",
      "reschedules 2212\n",
      "neighborhood 2213\n",
      "Tonny 2214\n",
      "woken 2215\n",
      "Carey 2216\n",
      "first 2217\n",
      "voted 2218\n",
      "Cafe 2219\n",
      "influence 2220\n",
      "Hp 2221\n",
      "between 2222\n",
      "friends 2223\n",
      "syrup 2224\n",
      "Edi 2225\n",
      "lunch 2226\n",
      "choose 2227\n",
      "Bob 2228\n",
      "saw 2229\n",
      "Eaton 2230\n",
      "Fisher 2231\n",
      "Marya 2232\n",
      "sorry 2233\n",
      "sounded 2234\n",
      "Darcy 2235\n",
      "happening 2236\n",
      "date 2237\n",
      "Dolores 2238\n",
      "congested 2239\n",
      "croutons 2240\n",
      "Stephania 2241\n",
      "Denis 2242\n",
      "Jon 2243\n",
      "carrot 2244\n",
      "programme 2245\n",
      "must 2246\n",
      "Bookstore 2247\n",
      "enough 2248\n",
      "Channel 2249\n",
      "Daria 2250\n",
      "everyone 2251\n",
      "set 2252\n",
      "puzzle 2253\n",
      "convention 2254\n",
      "colours 2255\n",
      "Head 2256\n",
      "Natasha 2257\n",
      "mustard 2258\n",
      "are 2259\n",
      "Clint 2260\n",
      "startups 2261\n",
      "though 2262\n",
      "60zł 2263\n",
      "Starfish 2264\n",
      "may 2265\n",
      "Mario 2266\n",
      "Hight 2267\n",
      "floor 2268\n",
      "Dominika 2269\n",
      "classroom 2270\n",
      "sharing 2271\n",
      "Vicky 2272\n",
      "Hazel 2273\n",
      "created 2274\n",
      "gathers 2275\n",
      "embassy 2276\n",
      "Jerry 2277\n",
      "wallet 2278\n",
      "Riders 2279\n",
      "Einar 2280\n",
      "pint 2281\n",
      "play 2282\n",
      "office 2283\n",
      "cheesecake 2284\n",
      "Johnson 2285\n",
      "transfer 2286\n",
      "Sergey 2287\n",
      "glad 2288\n",
      "Isabel 2289\n",
      "verify 2290\n",
      "served 2291\n",
      "Dhabi 2292\n",
      "discounted 2293\n",
      "Hashimoto 2294\n",
      "Jodie 2295\n",
      "Berenice 2296\n",
      "iPhoneXR 2297\n",
      "winning 2298\n",
      "Gwyneth 2299\n",
      "assistant 2300\n",
      "100 2301\n",
      "Shari 2302\n",
      "hadn 2303\n",
      "dreading 2304\n",
      "Trump 2305\n",
      "President 2306\n",
      "Jasper 2307\n",
      "blue 2308\n",
      "return 2309\n",
      "mall 2310\n",
      "others 2311\n",
      "urgent 2312\n",
      "adhesive 2313\n",
      "Caleb 2314\n",
      "heel 2315\n",
      "sick 2316\n",
      "ridiculous 2317\n",
      "minus 2318\n",
      "Daniela 2319\n",
      "tracking 2320\n",
      "tech 2321\n",
      "boutique 2322\n",
      "Bale 2323\n",
      "extra 2324\n",
      "Drew 2325\n",
      "BS6 2326\n",
      "outside 2327\n",
      "Tito 2328\n",
      "afternoon 2329\n",
      "manual 2330\n",
      "seaside 2331\n",
      "Thrones 2332\n",
      "plastic 2333\n",
      "left 2334\n",
      "Rachael 2335\n",
      "Kathy 2336\n",
      "generally 2337\n",
      "threw 2338\n",
      "Connie 2339\n",
      "one 2340\n",
      "Rosie 2341\n",
      "injuries 2342\n",
      "legal 2343\n",
      "Foxi 2344\n",
      "Focus 2345\n",
      "airport 2346\n",
      ". 2347\n",
      ".\n",
      "Nish 2348\n",
      "quit 2349\n",
      "completely 2350\n",
      "until 2351\n",
      "update 2352\n",
      "Greenwood 2353\n",
      "executive 2354\n",
      "sedan 2355\n",
      "Jasmin 2356\n",
      "Japanese 2357\n",
      "championship 2358\n",
      "Drury 2359\n",
      "stairs 2360\n",
      "Phoebe 2361\n",
      "secret 2362\n",
      "translation 2363\n",
      "Asia 2364\n",
      "Svetlana 2365\n",
      "dentist 2366\n",
      "Leyla 2367\n",
      "track 2368\n",
      "Suzy 2369\n",
      "Sule 2370\n",
      "Instagram 2371\n",
      "England 2372\n",
      "statistics 2373\n",
      "refuses 2374\n",
      "Eastern 2375\n",
      "Jasmine 2376\n",
      "loved 2377\n",
      "doh 2378\n",
      "Simpsons 2379\n",
      "bracelets 2380\n",
      "uber 2381\n",
      "Stewart 2382\n",
      "sleepiness 2383\n",
      "Lawrence 2384\n",
      "zero 2385\n",
      "Allison 2386\n",
      "estate 2387\n",
      "Walmart 2388\n",
      "Pruszków 2389\n",
      "march 2390\n",
      "garden 2391\n",
      "Mila 2392\n",
      "Alice 2393\n",
      "ingredients 2394\n",
      "Jesse 2395\n",
      "Gus 2396\n",
      "Pola 2397\n",
      "Nail 2398\n",
      "However 2399\n",
      "mold 2400\n",
      "comeback 2401\n",
      "heels 2402\n",
      "Mexican 2403\n",
      "Hector 2404\n",
      "Fraser 2405\n",
      "anyone 2406\n",
      "WOK 2407\n",
      "human 2408\n",
      "mom 2409\n",
      "herb 2410\n",
      "disc 2411\n",
      "tommorrow 2412\n",
      "vector 2413\n",
      "Dilly 2414\n",
      "photography 2415\n",
      "Trace 2416\n",
      "ability 2417\n",
      "attempt 2418\n",
      "Last 2419\n",
      "claim 2420\n",
      "candles 2421\n",
      "Tancredi 2422\n",
      "sociology 2423\n",
      "beans 2424\n",
      "breaking 2425\n",
      "southeast 2426\n",
      "vlogmas 2427\n",
      "boy 2428\n",
      "Barb 2429\n",
      "waiting 2430\n",
      "Baldarelli 2431\n",
      "priorities 2432\n",
      "sightseeing 2433\n",
      "rest 2434\n",
      "content 2435\n",
      "xmas 2436\n",
      "Marty 2437\n",
      "phase 2438\n",
      "disappear 2439\n",
      "distraction 2440\n",
      "wish 2441\n",
      "PLN 2442\n",
      "Nally 2443\n",
      "Neil 2444\n",
      "Oli 2445\n",
      "Michaela 2446\n",
      "violated 2447\n",
      "Gill 2448\n",
      "homework 2449\n",
      "Sebastien 2450\n",
      "Spanish 2451\n",
      "British 2452\n",
      "Redemption 2453\n",
      "sees 2454\n",
      "hook 2455\n",
      "Iron 2456\n",
      "t 2457\n",
      "titles 2458\n",
      "costs 2459\n",
      "Portable 2460\n",
      "mike 2461\n",
      "Barry 2462\n",
      "alleged 2463\n",
      "Friday 2464\n",
      "conditioner 2465\n",
      "repaired 2466\n",
      "hammer 2467\n",
      "likely 2468\n",
      "Gilmore 2469\n",
      "person 2470\n",
      "Croydon 2471\n",
      "YouTube 2472\n",
      "test 2473\n",
      "remember 2474\n",
      "returned 2475\n",
      "Break 2476\n",
      "bottom 2477\n",
      "joining 2478\n",
      "away 2479\n",
      "History 2480\n",
      "Ryan 2481\n",
      "Gera 2482\n",
      "necks 2483\n",
      "Nana 2484\n",
      "Columbia 2485\n",
      "Hannah 2486\n",
      "sofa 2487\n",
      "Charles 2488\n",
      "DHL 2489\n",
      "look 2490\n",
      "team 2491\n",
      "Kaila 2492\n",
      "charges 2493\n",
      "Marrisa 2494\n",
      "exam 2495\n",
      "Brittany 2496\n",
      "prom 2497\n",
      "next 2498\n",
      "not 2499\n",
      "9pm 2500\n",
      "Rd 2501\n",
      "noise 2502\n",
      "Russian 2503\n",
      "del 2504\n",
      "cookies 2505\n",
      "milk 2506\n",
      "Sports 2507\n",
      "Monday 2508\n",
      "got 2509\n",
      "Nines 2510\n",
      "main 2511\n",
      "Grace 2512\n",
      "chosen 2513\n",
      "Elsa 2514\n",
      "closing 2515\n",
      "comic 2516\n",
      "Angelica 2517\n",
      "least 2518\n",
      "severe 2519\n",
      "Blaire 2520\n",
      "cat 2521\n",
      "Subway 2522\n",
      "stroke 2523\n",
      "Mercedes 2524\n",
      "guitar 2525\n",
      "focusing 2526\n",
      "Ayden 2527\n",
      "undergo 2528\n",
      "Brain 2529\n",
      "Elias 2530\n",
      "pax 2531\n",
      "a 2532\n",
      "makes 2533\n",
      "Lenny 2534\n",
      "chemistry 2535\n",
      "crazy 2536\n",
      "workplace 2537\n",
      "Eve 2538\n",
      "Ivona 2539\n",
      "leg 2540\n",
      "differences 2541\n",
      "York 2542\n",
      "Dickins 2543\n",
      "dropped 2544\n",
      "Dorothy 2545\n",
      "Incredible 2546\n",
      "Jem 2547\n",
      "High 2548\n",
      "brawl 2549\n",
      "burgers 2550\n",
      "cart 2551\n",
      "people 2552\n",
      "performances 2553\n",
      "head 2554\n",
      "boiler 2555\n",
      "Vail 2556\n",
      "150 2557\n",
      "6am 2558\n",
      "ahead 2559\n",
      "K 2560\n",
      "Aaron 2561\n",
      "crisis 2562\n",
      "piles 2563\n",
      "attack 2564\n",
      "15th 2565\n",
      "Jenny 2566\n",
      "take 2567\n",
      "met 2568\n",
      "friend 2569\n",
      "Kristie 2570\n",
      "writes 2571\n",
      "rather 2572\n",
      "geography 2573\n",
      "mad 2574\n",
      "scary 2575\n",
      "serving 2576\n",
      "Ivan 2577\n",
      "complained 2578\n",
      "improved 2579\n",
      "Amy 2580\n",
      "convey 2581\n",
      "Hortons 2582\n",
      "Rebecca 2583\n",
      "Kevin 2584\n",
      "Gala 2585\n",
      "cache 2586\n",
      "room 2587\n",
      "wearing 2588\n",
      "Omar 2589\n",
      "Vienna 2590\n",
      "Arabella 2591\n",
      "wash 2592\n",
      "baked 2593\n",
      "2016 2594\n",
      "win 2595\n",
      "fruit 2596\n",
      "Khabib 2597\n",
      "regaining 2598\n",
      "Love 2599\n",
      "re 2600\n",
      "grandmas 2601\n",
      "banana 2602\n",
      "Jessica 2603\n",
      "traffic 2604\n",
      "Doctor 2605\n",
      "Julio 2606\n",
      "1 2607\n",
      "years 2608\n",
      "today 2609\n",
      "where 2610\n",
      "Chez 2611\n",
      "visa 2612\n",
      "hunting 2613\n",
      "Enter 2614\n",
      "Loretta 2615\n",
      "allergic 2616\n",
      "property 2617\n",
      "yoghurt 2618\n",
      "stationery 2619\n",
      "payment 2620\n",
      "perfume 2621\n",
      "perfect 2622\n",
      "baking 2623\n",
      "comment 2624\n",
      "volunteering 2625\n",
      "vacant 2626\n",
      "change 2627\n",
      "Woof 2628\n",
      "Xander 2629\n",
      "Simon 2630\n",
      "dating 2631\n",
      "Chris 2632\n",
      "Victoria 2633\n",
      "helped 2634\n",
      "Sally 2635\n",
      "parties 2636\n",
      "pages 2637\n",
      "Szymon 2638\n",
      "excited 2639\n",
      "Lucy 2640\n",
      "Selvik 2641\n",
      "Viola 2642\n",
      "games 2643\n",
      "Katya 2644\n",
      "Marshall 2645\n",
      "silver 2646\n",
      "bottle 2647\n",
      "Felicity 2648\n",
      "Doris 2649\n",
      "Adrian 2650\n",
      "Steve 2651\n",
      "earned 2652\n",
      "Pamela 2653\n",
      "Zara 2654\n",
      "Rhonda 2655\n",
      "Elisabeth 2656\n",
      "hear 2657\n",
      "gives 2658\n",
      "shall 2659\n",
      "disease 2660\n",
      "Ivonne 2661\n",
      "productively 2662\n",
      "cheap 2663\n",
      "turns 2664\n",
      "Brown 2665\n",
      "Finland 2666\n",
      "Lee 2667\n",
      "same 2668\n",
      "victory 2669\n",
      "Sherry 2670\n",
      "twins 2671\n",
      "to 2672\n",
      "suitcase 2673\n",
      "eat 2674\n",
      "Orhan 2675\n",
      "favorite 2676\n",
      "hiking 2677\n",
      "was 2678\n",
      "Lufthansa 2679\n",
      "introduce 2680\n",
      "unpleasant 2681\n",
      "graduated 2682\n",
      "Holly 2683\n",
      "supposed 2684\n",
      "Spencer 2685\n",
      "Google 2686\n",
      "driver 2687\n",
      "responsible 2688\n",
      "raving 2689\n",
      "Smith 2690\n",
      "Renee 2691\n",
      "dismissed 2692\n",
      "burger 2693\n",
      "Bonny 2694\n",
      "Thailand 2695\n",
      "rare 2696\n",
      "Frequency 2697\n",
      "hired 2698\n",
      "recommended 2699\n",
      "stadium 2700\n",
      "Prince 2701\n",
      "stole 2702\n",
      "Larry 2703\n",
      "landed 2704\n",
      "Rory 2705\n",
      "Justine 2706\n",
      "Europe 2707\n",
      "robots 2708\n",
      "mentions 2709\n",
      "radiators 2710\n",
      "tie 2711\n",
      "charged 2712\n",
      "listen 2713\n",
      "Ronaldo 2714\n",
      "facebook 2715\n",
      "Envy 2716\n",
      "saying 2717\n",
      "Tesla 2718\n",
      "recently 2719\n",
      "Tereza 2720\n",
      "Valentia 2721\n",
      "Calleigh 2722\n",
      "Grey 2723\n",
      "place 2724\n",
      "RPG 2725\n",
      "Hedge 2726\n",
      "Casa 2727\n",
      "pear 2728\n",
      "operating 2729\n",
      "parliament 2730\n",
      "amazed 2731\n",
      "Julie 2732\n",
      "Macy 2733\n",
      "St 2734\n",
      "Suzanne 2735\n",
      "presentations 2736\n",
      "uses 2737\n",
      "Debora 2738\n",
      "Anna 2739\n",
      "signed 2740\n",
      "wondering 2741\n",
      "For 2742\n",
      "integration 2743\n",
      "Trevor 2744\n",
      "share 2745\n",
      "Alba 2746\n",
      "Felix 2747\n",
      "odd 2748\n",
      "session 2749\n",
      "Hayley 2750\n",
      "grades 2751\n",
      "spices 2752\n",
      "card 2753\n",
      "Susan 2754\n",
      "April 2755\n",
      "timing 2756\n",
      "Cooler 2757\n",
      "Brenda 2758\n",
      "shampoo 2759\n",
      "pink 2760\n",
      "Hugo 2761\n",
      "registered 2762\n",
      "Martina 2763\n",
      "don 2764\n",
      "Jada 2765\n",
      "V 2766\n",
      "departing 2767\n",
      "kisser 2768\n",
      "Snow 2769\n",
      "show 2770\n",
      "wet 2771\n",
      "Avery 2772\n",
      "relief 2773\n",
      "incorrect 2774\n",
      "app 2775\n",
      "mountains 2776\n",
      "island 2777\n",
      "afford 2778\n",
      "Nate 2779\n",
      "Juventus 2780\n",
      "holidays 2781\n",
      "speed 2782\n",
      "ajvar 2783\n",
      "proper 2784\n",
      "often 2785\n",
      "laundry 2786\n",
      "learning 2787\n",
      "missing 2788\n",
      "Chinese 2789\n",
      "Braden 2790\n",
      "acid 2791\n",
      "online 2792\n",
      "lady 2793\n",
      "congratulate 2794\n",
      "Naima 2795\n",
      "Poppy 2796\n",
      "Connor 2797\n",
      "forehead 2798\n",
      "Gabrial 2799\n",
      "Maurice 2800\n",
      "Ani 2801\n",
      "Sweden 2802\n",
      "House 2803\n",
      "Hermosa 2804\n",
      "photos 2805\n",
      "plenty 2806\n",
      "pattern 2807\n",
      "Jay 2808\n",
      "interesting 2809\n",
      "$ 2810\n",
      "Poppins 2811\n",
      "Keira 2812\n",
      "revealed 2813\n",
      "Steven 2814\n",
      "indulge 2815\n",
      "lived 2816\n",
      "discount 2817\n",
      "and 2818\n",
      "accepted 2819\n",
      "wanted 2820\n",
      "rubbish 2821\n",
      "ASAP 2822\n",
      "Lynn 2823\n",
      "She 2824\n",
      "upset 2825\n",
      "flights 2826\n",
      "checked 2827\n",
      "consider 2828\n",
      "ex 2829\n",
      "successful 2830\n",
      "Cady 2831\n",
      "Jemma 2832\n",
      "new 2833\n",
      "Jonny 2834\n",
      "scan 2835\n",
      "Adrienne 2836\n",
      "found 2837\n",
      "Ceaser 2838\n",
      "movie 2839\n",
      "rents 2840\n",
      "sending 2841\n",
      "visible 2842\n",
      "calms 2843\n",
      "lift 2844\n",
      "Red 2845\n",
      "Riverdale 2846\n",
      "wineries 2847\n",
      "minuted 2848\n",
      "largely 2849\n",
      "spider 2850\n",
      "bikes 2851\n",
      "sandwiches 2852\n",
      "Tina 2853\n",
      "But 2854\n",
      "Kris 2855\n",
      "Pep 2856\n",
      "window 2857\n",
      "lactose 2858\n",
      "Veronica 2859\n",
      "number 2860\n",
      "Cyrus 2861\n",
      "maching 2862\n",
      "dye 2863\n",
      "texted 2864\n",
      "Apollo 2865\n",
      "Joanna 2866\n",
      "bring 2867\n",
      "sparkly 2868\n",
      "mood 2869\n",
      "clothing 2870\n",
      "washing 2871\n",
      "Harley 2872\n",
      "Bianca 2873\n",
      "Liza 2874\n",
      "became 2875\n",
      "7 2876\n",
      "membership 2877\n",
      "seat 2878\n",
      "arranged 2879\n",
      "finale 2880\n",
      "Hamilton 2881\n",
      "dried 2882\n",
      "presidential 2883\n",
      "20 2884\n",
      "meet 2885\n",
      "fire 2886\n",
      "selfie 2887\n",
      "rid 2888\n",
      "but 2889\n",
      "Smiths 2890\n",
      "classmates 2891\n",
      "unhappy 2892\n",
      "Greg 2893\n",
      "4AB 2894\n",
      "Sash 2895\n",
      "accordance 2896\n",
      "Hut 2897\n",
      "Idea 2898\n",
      "Dharma 2899\n",
      "official 2900\n",
      "do 2901\n",
      "ve 2902\n",
      "Ezra 2903\n",
      "couch 2904\n",
      "believing 2905\n",
      "sleeve 2906\n",
      "breath 2907\n",
      "Roxanna 2908\n",
      "Johnathan 2909\n",
      "Logan 2910\n",
      "filtered 2911\n",
      "burnt 2912\n",
      "each 2913\n",
      "smaller 2914\n",
      "Valentina 2915\n",
      "Belgium 2916\n",
      "assignments 2917\n",
      "performance 2918\n",
      "21st 2919\n",
      "cleaning 2920\n",
      "starving 2921\n",
      "keeps 2922\n",
      "documentary 2923\n",
      "upstairs 2924\n",
      "interviews 2925\n",
      "Cate 2926\n",
      "sound 2927\n",
      "Evan 2928\n",
      "fireman 2929\n",
      "twice 2930\n",
      "have 2931\n",
      "Co 2932\n",
      "Zilda 2933\n",
      "Kenya 2934\n",
      "Things 2935\n",
      "year 2936\n",
      "smarter 2937\n",
      "chapter 2938\n",
      "Eric 2939\n",
      "Isaiah 2940\n",
      "Wishful 2941\n",
      "Wisconsin 2942\n",
      "you 2943\n",
      "Viktoria 2944\n",
      "like 2945\n",
      "recognise 2946\n",
      "screaming 2947\n",
      "Moira 2948\n",
      "Stu 2949\n",
      "twist 2950\n",
      "fries 2951\n",
      "immediately 2952\n",
      "awkward 2953\n",
      "consent 2954\n",
      "crusty 2955\n",
      "Audi 2956\n",
      "Kings 2957\n",
      "undergone 2958\n",
      "reading 2959\n",
      "Ring 2960\n",
      "Gap 2961\n",
      "Coimbra 2962\n",
      "Venezuela 2963\n",
      "Damian 2964\n",
      "Monika 2965\n",
      "Three 2966\n",
      "yet 2967\n",
      "According 2968\n",
      "Android 2969\n",
      "mistakenly 2970\n",
      "terms 2971\n",
      "cappuccino 2972\n",
      "Raiola 2973\n",
      "boring 2974\n",
      "bored 2975\n",
      "1980s 2976\n",
      "Samsung 2977\n",
      "Ray 2978\n",
      "poor 2979\n",
      "potato 2980\n",
      "Rumer 2981\n",
      "incidents 2982\n",
      "Hilary 2983\n",
      "apartments 2984\n",
      "Excel 2985\n",
      "manifestations 2986\n",
      "zloty 2987\n",
      "stapler 2988\n",
      "delayed 2989\n",
      "Janine 2990\n",
      "Hall 2991\n",
      "rushes 2992\n",
      "cinema 2993\n",
      "exercise 2994\n",
      "BSB 2995\n",
      "hoops 2996\n",
      "near 2997\n",
      "Peter 2998\n",
      "Alison 2999\n",
      "stuck 3000\n",
      "everything 3001\n",
      "Hallie 3002\n",
      "viral 3003\n",
      "Klaudia 3004\n",
      "Marcela 3005\n",
      "background 3006\n",
      "Plaza 3007\n",
      "Pam 3008\n",
      "Reece 3009\n",
      "Candy 3010\n",
      "rolling 3011\n",
      "Eva 3012\n",
      "RS 3013\n",
      "centre 3014\n",
      "complete 3015\n",
      "jogging 3016\n",
      "medication 3017\n",
      "naan 3018\n",
      "turned 3019\n",
      "destroyed 3020\n",
      "distance 3021\n",
      "Isa 3022\n",
      "Macedonia 3023\n",
      "dirty 3024\n",
      "text 3025\n",
      "Lanzarote 3026\n",
      "8ish 3027\n",
      "Weapon 3028\n",
      "Marry 3029\n",
      "taxi 3030\n",
      "3am 3031\n",
      "Sydney 3032\n",
      "Rachel 3033\n",
      "Della 3034\n",
      "Hudgens 3035\n",
      "love 3036\n",
      "45 3037\n",
      "Sven 3038\n",
      "level 3039\n",
      "George 3040\n",
      ": 3041\n",
      "chest 3042\n",
      "providing 3043\n",
      "USA 3044\n",
      "temperature 3045\n",
      "Bronn 3046\n",
      "gravity 3047\n",
      "Brasil 3048\n",
      "Jackson 3049\n",
      "suspicion 3050\n",
      "climbing 3051\n",
      "Walt 3052\n",
      "gig 3053\n",
      "adorable 3054\n",
      "science 3055\n",
      "Globe 3056\n",
      "zoo 3057\n",
      "Giles 3058\n",
      "Austria 3059\n",
      "Jan 3060\n",
      "spath 3061\n",
      "Penelope 3062\n",
      "Brigitte 3063\n",
      "Sam 3064\n",
      "decides 3065\n",
      "Dimitri 3066\n",
      "Good 3067\n",
      "Lester 3068\n",
      "every 3069\n",
      "chicken 3070\n",
      "dresses 3071\n",
      "hygiene 3072\n",
      "Indian 3073\n",
      "km 3074\n",
      "Williams 3075\n",
      "scared 3076\n",
      "accompany 3077\n",
      "both 3078\n",
      "IQ 3079\n",
      "Ava 3080\n",
      "sitting 3081\n",
      "plan 3082\n",
      "11 3083\n",
      "Jill 3084\n",
      "reschedule 3085\n",
      "lemon 3086\n",
      "prank 3087\n",
      "flakes 3088\n",
      "Caribbean 3089\n",
      "pho 3090\n",
      "installed 3091\n",
      "Ms 3092\n",
      "body 3093\n",
      "comes 3094\n",
      "lower 3095\n",
      "1200 3096\n",
      "prices 3097\n",
      "opera 3098\n",
      "samples 3099\n",
      "tutorial 3100\n",
      "forrest 3101\n",
      "Syah 3102\n",
      "interviewing 3103\n",
      "answering 3104\n",
      "group 3105\n",
      "things 3106\n",
      "soda 3107\n",
      "suddenly 3108\n",
      "Darren 3109\n",
      "taller 3110\n",
      "Helen 3111\n",
      "haven 3112\n",
      "videos 3113\n",
      "blog 3114\n",
      "Bodyguard 3115\n",
      "Kendra 3116\n",
      "A5 3117\n",
      "Harissa 3118\n",
      "Erica 3119\n",
      "js 3120\n",
      "coctails 3121\n",
      "give 3122\n",
      "hamburgers 3123\n",
      "machine 3124\n",
      "Mickey 3125\n",
      "Ford 3126\n",
      "ordered 3127\n",
      "mum 3128\n",
      "Cooper 3129\n",
      "Micheal 3130\n",
      "funny 3131\n",
      "stocked 3132\n",
      "Eileen 3133\n",
      "steak 3134\n",
      "club 3135\n",
      "concept 3136\n",
      "14 3137\n",
      "Kamil 3138\n",
      "Portugal 3139\n",
      "Jane 3140\n",
      "browsing 3141\n",
      "sign 3142\n",
      "reliable 3143\n",
      "Gaga 3144\n",
      "Normandy 3145\n",
      "Essie 3146\n",
      "Glenn 3147\n",
      "Eli 3148\n",
      "Enid 3149\n",
      "sweet 3150\n",
      "dinner 3151\n",
      "Rowan 3152\n",
      "sunflowers 3153\n",
      "spreadsheet 3154\n",
      "chips 3155\n",
      "bottled 3156\n",
      "possessions 3157\n",
      "Viki 3158\n",
      "brutal 3159\n",
      "Melania 3160\n",
      "Alie 3161\n",
      "stream 3162\n",
      "Fred 3163\n",
      "maps 3164\n",
      "blocking 3165\n",
      "Cass 3166\n",
      "Church 3167\n",
      "Shayla 3168\n",
      "film 3169\n",
      "Camila 3170\n",
      "forgot 3171\n",
      "Lydia 3172\n",
      "named 3173\n",
      "Stop 3174\n",
      "pounds 3175\n",
      "sauce 3176\n",
      "R 3177\n",
      "accuses 3178\n",
      "surprise 3179\n",
      "arrive 3180\n",
      "redecorating 3181\n",
      "BigBuy 3182\n",
      "whenever 3183\n",
      "regrets 3184\n",
      "movies 3185\n",
      "Cheryl 3186\n",
      "misspelling 3187\n",
      "allows 3188\n",
      "Terry 3189\n",
      "destroy 3190\n",
      "babysit 3191\n",
      "judged 3192\n",
      "60 3193\n",
      "premiere 3194\n",
      "politically 3195\n",
      "Don 3196\n",
      "hungover 3197\n",
      "pickle 3198\n",
      "news 3199\n",
      "meeting 3200\n",
      "jacket 3201\n",
      "Gainsbourg 3202\n",
      "Adelaide 3203\n",
      "Sunday 3204\n",
      "convenient 3205\n",
      "Todd 3206\n",
      "crossed 3207\n",
      "just 3208\n",
      "Hakuna 3209\n",
      "me 3210\n",
      "freaking 3211\n",
      "podcast 3212\n",
      "SUV 3213\n",
      "Blackwell 3214\n",
      "Lewis 3215\n",
      "Ken 3216\n",
      "Heidi 3217\n",
      "Luis 3218\n",
      "model 3219\n",
      "12 3220\n",
      "Flint 3221\n",
      "tutoring 3222\n",
      "winter 3223\n",
      "Battle 3224\n",
      "hard 3225\n",
      "listening 3226\n",
      "necessary 3227\n",
      "boss 3228\n",
      "HP 3229\n",
      "Arthut 3230\n",
      "tight 3231\n",
      "breakfast 3232\n",
      "Benjamin 3233\n",
      "Judith 3234\n",
      "dress 3235\n",
      "negotiations 3236\n",
      "move 3237\n",
      "Tuesdays 3238\n",
      "Switzerland 3239\n",
      "embroider 3240\n",
      "Cait 3241\n",
      "promoted 3242\n",
      "ECON 3243\n",
      "plane 3244\n",
      "Johnny 3245\n",
      "Yannick 3246\n",
      "holiday 3247\n",
      "Manpreet 3248\n",
      "Izayah 3249\n",
      "Nestor 3250\n",
      "debating 3251\n",
      "effective 3252\n",
      "carbonara 3253\n",
      "Nelly 3254\n",
      "Ross 3255\n",
      "Ananya 3256\n",
      "followed 3257\n",
      "element 3258\n",
      "Mateo 3259\n",
      "Roberts 3260\n",
      "moody 3261\n",
      "excuse 3262\n",
      "mail 3263\n",
      "iceskating 3264\n",
      "autumn 3265\n",
      "owner 3266\n",
      "Koshy 3267\n",
      "hours 3268\n",
      "when 3269\n",
      "resign 3270\n",
      "Maggie 3271\n",
      "pharmacy 3272\n",
      "white 3273\n",
      "zodiac 3274\n",
      "Preston 3275\n",
      "tattoo 3276\n",
      "special 3277\n",
      "competition 3278\n",
      "Road 3279\n",
      "Liam 3280\n",
      "beauty 3281\n",
      "Art 3282\n",
      "tastes 3283\n",
      "hesitated 3284\n",
      "falling 3285\n",
      "Gemini 3286\n",
      "Trinity 3287\n",
      "hedgehog 3288\n",
      "grounded 3289\n",
      "Indians 3290\n",
      "scored 3291\n",
      "Janelle 3292\n",
      "department 3293\n",
      "pencil 3294\n",
      "February 3295\n",
      "submit 3296\n",
      "demand 3297\n",
      "Saskatchewan 3298\n",
      "Imagine 3299\n",
      "bullet 3300\n",
      "Kurt 3301\n",
      "Emanuel 3302\n",
      "array 3303\n",
      "sales 3304\n",
      "Dennis 3305\n",
      "library 3306\n",
      "Cape 3307\n",
      "Booba 3308\n",
      "Calgary 3309\n",
      "inside 3310\n",
      "petition 3311\n",
      "caused 3312\n",
      "coffe 3313\n",
      "father 3314\n",
      "lesson 3315\n",
      "properly 3316\n",
      "coupon 3317\n",
      "Jacob 3318\n",
      "Katy 3319\n",
      "Brook 3320\n",
      "City 3321\n",
      "Hummer 3322\n",
      "Avi 3323\n",
      "this 3324\n",
      "uric 3325\n",
      "cell 3326\n",
      "Neither 3327\n",
      "digital 3328\n",
      "Guy 3329\n",
      "Declan 3330\n",
      "insult 3331\n",
      "3 3332\n",
      "building 3333\n",
      "weather 3334\n",
      "Nikolas 3335\n",
      "Monrovia 3336\n",
      "damage 3337\n",
      "short 3338\n",
      "pup 3339\n",
      "aidy 3340\n",
      "bandage 3341\n",
      "Nina 3342\n",
      "Mitch 3343\n",
      "price 3344\n",
      "married 3345\n",
      "problem 3346\n",
      "Dayna 3347\n",
      "teacher 3348\n",
      "displayed 3349\n",
      "episodes 3350\n",
      "aks 3351\n",
      "atmosphere 3352\n",
      "marathon 3353\n",
      "165 3354\n",
      "within 3355\n",
      "Sainsbury 3356\n",
      "exorcist 3357\n",
      "calling 3358\n",
      "Sylvia 3359\n",
      "Maya 3360\n",
      "shouldn 3361\n",
      "Armani 3362\n",
      "All 3363\n",
      "trailer 3364\n",
      "500 3365\n",
      "Bieszczady 3366\n",
      "Reese 3367\n",
      "pillow 3368\n",
      "guard 3369\n",
      "moment 3370\n",
      "Charlie 3371\n",
      "behalf 3372\n",
      "trip 3373\n",
      "paycheck 3374\n",
      "devastated 3375\n",
      "puppy 3376\n",
      "isn 3377\n",
      "addresses 3378\n",
      "drinks 3379\n",
      "then 3380\n",
      "Khadija 3381\n",
      "wasn 3382\n",
      "Colton 3383\n",
      "nobody 3384\n",
      "Shuhui 3385\n",
      "ancestors 3386\n",
      "McDonald 3387\n",
      "Frank 3388\n",
      "Scolt 3389\n",
      "cards 3390\n",
      "Adrianna 3391\n",
      "Gary 3392\n",
      "silly 3393\n",
      "Baltimore 3394\n",
      "Rebeca 3395\n",
      "pop 3396\n",
      "Tibi 3397\n",
      "Eleanor 3398\n",
      "Saint 3399\n",
      "Oscar 3400\n",
      "teriyaki 3401\n",
      "sell 3402\n",
      "salsa 3403\n",
      "earring 3404\n",
      "option 3405\n",
      "commercial 3406\n",
      "Japan 3407\n",
      "herself 3408\n",
      "proposed 3409\n",
      "these 3410\n",
      "hide 3411\n",
      "grocery 3412\n",
      "loose 3413\n",
      "Restaurant 3414\n",
      "Addie 3415\n",
      "Xavier 3416\n",
      "itch 3417\n",
      "Dec 3418\n",
      "gossiping 3419\n",
      "Vanessa 3420\n",
      "Rella 3421\n",
      "scolded 3422\n",
      "Samuel 3423\n",
      "redecorate 3424\n",
      "outdoor 3425\n",
      "piano 3426\n",
      "classes 3427\n",
      "Shoppers 3428\n",
      "Suzie 3429\n",
      "Fedora 3430\n",
      "misses 3431\n",
      "running 3432\n",
      "bars 3433\n",
      "class 3434\n",
      "version 3435\n",
      "Sebastian 3436\n",
      "accommodation 3437\n",
      "Cheyanne 3438\n",
      "Tasha 3439\n",
      "word 3440\n",
      "died 3441\n",
      "announces 3442\n",
      "Germany 3443\n",
      "Deal 3444\n",
      "applied 3445\n",
      "Lea 3446\n",
      "cover 3447\n",
      "WhatsApp 3448\n",
      "Amalia 3449\n",
      "Payne 3450\n",
      "entrance 3451\n",
      "stew 3452\n",
      "Vesna 3453\n",
      "torn 3454\n",
      "Kinga 3455\n",
      "right 3456\n",
      "Baltoro 3457\n",
      "celeb 3458\n",
      "minimalist 3459\n",
      "24 3460\n",
      "fare 3461\n",
      "Mill 3462\n",
      "Panamanian 3463\n",
      "personal 3464\n",
      "Ashleen 3465\n",
      "Tree 3466\n",
      "elbow 3467\n",
      "Arianna 3468\n",
      "Jason 3469\n",
      "student 3470\n",
      "Sixty 3471\n",
      "come 3472\n",
      "apologizes 3473\n",
      "warlike 3474\n",
      "Hervé 3475\n",
      "called 3476\n",
      "Jakub 3477\n",
      "To 3478\n",
      "Olly 3479\n",
      "Bartek 3480\n",
      "Reynold 3481\n",
      "neck 3482\n",
      "Riri 3483\n",
      "concerts 3484\n",
      "winner 3485\n",
      "sausages 3486\n",
      "21 3487\n",
      "before 3488\n",
      "Kimberly 3489\n",
      "blocked 3490\n",
      "Maciej 3491\n",
      "college 3492\n",
      "sugar 3493\n",
      "Joe 3494\n",
      "camping 3495\n",
      "meets 3496\n",
      "miniature 3497\n",
      "Man 3498\n",
      "twenty 3499\n",
      "pump 3500\n",
      "Abbie 3501\n",
      "download 3502\n",
      "google 3503\n",
      "find 3504\n",
      "south 3505\n",
      "travelling 3506\n",
      "Kaja 3507\n",
      "results 3508\n",
      "package 3509\n",
      "Florence 3510\n",
      "Kamila 3511\n",
      "access 3512\n",
      "invites 3513\n",
      "crocodile 3514\n",
      "Lucas 3515\n",
      "Asha 3516\n",
      "07 3517\n",
      "Sarah 3518\n",
      "gate 3519\n",
      "Peppa 3520\n",
      "Tinley 3521\n",
      "Mum 3522\n",
      "football 3523\n",
      "Val 3524\n",
      "honor 3525\n",
      "Solana 3526\n",
      "Andrew 3527\n",
      "low 3528\n",
      "Jaron 3529\n",
      "Przemek 3530\n",
      "awful 3531\n",
      "fish 3532\n",
      "internet 3533\n",
      "citizen 3534\n",
      "borrowed 3535\n",
      "products 3536\n",
      "recommends 3537\n",
      "pubs 3538\n",
      "cars 3539\n",
      "went 3540\n",
      "Switch 3541\n",
      "kitchen 3542\n",
      "influencers 3543\n",
      "Clara 3544\n",
      "Spiderman 3545\n",
      "hyped 3546\n",
      "pepperoni 3547\n",
      "101 3548\n",
      "Emily 3549\n",
      "looks 3550\n",
      "BBC 3551\n",
      "Adam 3552\n",
      "pair 3553\n",
      "Norma 3554\n",
      "SWAT 3555\n",
      "Golden 3556\n",
      "mother 3557\n",
      "send 3558\n",
      "tommorow 3559\n",
      "home 3560\n",
      "bailed 3561\n",
      "fascists 3562\n",
      "extraordinary 3563\n",
      "Thanksgiving 3564\n",
      "Ritz 3565\n",
      "Rudy 3566\n",
      "boyfriend 3567\n",
      "Melvin 3568\n",
      "inform 3569\n",
      "leaking 3570\n",
      "California 3571\n",
      "nominate 3572\n",
      "Toby 3573\n",
      "accept 3574\n",
      "matters 3575\n",
      "delays 3576\n",
      "very 3577\n",
      "TUI 3578\n",
      "any 3579\n",
      "Danie 3580\n",
      "stayed 3581\n",
      "Otto 3582\n",
      "Odo 3583\n",
      "Georgia 3584\n",
      "3rd 3585\n",
      "goes 3586\n",
      "ceiling 3587\n",
      "ignoring 3588\n",
      "Affairs 3589\n",
      "£ 3590\n",
      "Javion 3591\n",
      "Sandra 3592\n",
      "Pete 3593\n",
      "halloween 3594\n",
      "video 3595\n",
      "Debbie 3596\n",
      "discussion 3597\n",
      "Election 3598\n",
      "Nettie 3599\n",
      "inexpensive 3600\n",
      "White 3601\n",
      "Linn 3602\n",
      "Aiden 3603\n",
      "October 3604\n",
      "Olivia 3605\n",
      "26 3606\n",
      "desktop 3607\n",
      "Sheeran 3608\n",
      "companions 3609\n",
      "difficulty 3610\n",
      "courses 3611\n",
      "shopping 3612\n",
      "Sienna 3613\n",
      "cage 3614\n",
      "euros 3615\n",
      "Aria 3616\n",
      "other 3617\n",
      "serum 3618\n",
      "Justin 3619\n",
      "glasses 3620\n",
      "husky 3621\n",
      "Antifa 3622\n",
      "gather 3623\n",
      "wantsto 3624\n",
      "pizza 3625\n",
      "Robin 3626\n",
      "baptise 3627\n",
      "doorbell 3628\n",
      "Brazilian 3629\n",
      "Pass 3630\n",
      "cigarettes 3631\n",
      "Jenna 3632\n",
      "made 3633\n",
      "70 3634\n",
      "night 3635\n",
      "Ollie 3636\n",
      "wear 3637\n",
      "Shabab 3638\n",
      "Clare 3639\n",
      "stall 3640\n",
      "starts 3641\n",
      "later 3642\n",
      "born 3643\n",
      "Wetransfer 3644\n",
      "Jax 3645\n",
      "smoking 3646\n",
      "28 3647\n",
      "Son 3648\n",
      "fi 3649\n",
      "endorse 3650\n",
      "describes 3651\n",
      "Edgar 3652\n",
      "Jose 3653\n",
      "tyres 3654\n",
      "ProtonMail 3655\n",
      "Elena 3656\n",
      "stinks 3657\n",
      "Omer 3658\n",
      "Lower 3659\n",
      "buys 3660\n",
      "cafetaria 3661\n",
      "way 3662\n",
      "updating 3663\n",
      "Norway 3664\n",
      "beach 3665\n",
      "Sandy 3666\n",
      "McKlaren 3667\n",
      "Brave 3668\n",
      "Neo 3669\n",
      "arrested 3670\n",
      "existence 3671\n",
      "stains 3672\n",
      "orange 3673\n",
      "Russia 3674\n",
      "General 3675\n",
      "Juliet 3676\n",
      "AI 3677\n",
      "sweater 3678\n",
      "Tonight 3679\n",
      "decision 3680\n",
      "Baldwin 3681\n",
      "28th 3682\n",
      "manicure 3683\n",
      "documents 3684\n",
      "better 3685\n",
      "Portia 3686\n",
      "wrong 3687\n",
      "Somebody 3688\n",
      "38 3689\n",
      "d 3690\n",
      "Dima 3691\n",
      "fulfil 3692\n",
      "jewelry 3693\n",
      "Mariah 3694\n",
      "takes 3695\n",
      "pc 3696\n",
      "fits 3697\n",
      "Marianne 3698\n",
      "stuffed 3699\n",
      "Edwin 3700\n",
      "album 3701\n",
      "Lora 3702\n",
      "accordingly 3703\n",
      "48 3704\n",
      "walls 3705\n",
      "Amir 3706\n",
      "appeared 3707\n",
      "organises 3708\n",
      "accident 3709\n",
      "Philip 3710\n",
      "Thelma 3711\n",
      "Zane 3712\n",
      "Sage 3713\n",
      "actors 3714\n",
      "wind 3715\n",
      "Common 3716\n",
      "third 3717\n",
      "Elaine 3718\n",
      "organization 3719\n",
      "Alexandra 3720\n",
      "Void 3721\n",
      "Minds 3722\n",
      "Mogens 3723\n",
      "print 3724\n",
      "comfortable 3725\n",
      "Mari 3726\n",
      "Andy 3727\n",
      "Amor 3728\n",
      "gong 3729\n",
      "rehab 3730\n",
      "strange 3731\n",
      "pick 3732\n",
      "Bridget 3733\n",
      "61 3734\n",
      "Lisbon 3735\n",
      "jam 3736\n",
      "unavailable 3737\n",
      "Queensland 3738\n",
      "staying 3739\n",
      "watching 3740\n",
      "district 3741\n",
      "Gabi 3742\n",
      "Odin 3743\n",
      "case 3744\n",
      "let 3745\n",
      "Yanis 3746\n",
      "included 3747\n",
      "declines 3748\n",
      "Steph 3749\n",
      "corrections 3750\n",
      "midnight 3751\n",
      "mentioned 3752\n",
      "correspondence 3753\n",
      "Lena 3754\n",
      "matches 3755\n",
      "O 3756\n",
      "mixed 3757\n",
      "Talia 3758\n",
      "medical 3759\n",
      "Ella 3760\n",
      "recognition 3761\n",
      "Alexa 3762\n",
      "bills 3763\n",
      "far 3764\n",
      "public 3765\n",
      "pharmacies 3766\n",
      "Horacy 3767\n",
      "XXXL 3768\n",
      "hour 3769\n",
      "Abby 3770\n",
      "torchlight 3771\n",
      "David 3772\n",
      "top 3773\n",
      "North 3774\n",
      "Jonathan 3775\n",
      "Lukas 3776\n",
      "lots 3777\n",
      "finished 3778\n",
      "Apple 3779\n",
      "Nationalists 3780\n",
      "Julien 3781\n",
      "Missy 3782\n",
      "strawberry 3783\n",
      "employers 3784\n",
      "electricity 3785\n",
      "Barbara 3786\n",
      "resulted 3787\n",
      "Nancy 3788\n",
      "Hank 3789\n",
      "Carl 3790\n",
      "Mavis 3791\n",
      "traditional 3792\n",
      "disagree 3793\n",
      "happy 3794\n",
      "Dianne 3795\n",
      "relations 3796\n",
      "Emma 3797\n",
      "overqualified 3798\n",
      "cooker 3799\n",
      "Following 3800\n",
      "grandpa 3801\n",
      "US 3802\n",
      "opinion 3803\n",
      "Sasha 3804\n",
      "dangerous 3805\n",
      "rescued 3806\n",
      "Ives 3807\n",
      "literature 3808\n",
      "Nazi 3809\n",
      "brother 3810\n",
      "mikey 3811\n",
      "Wizz 3812\n",
      "watched 3813\n",
      "conflict 3814\n",
      "agree 3815\n",
      "Boris 3816\n",
      "Briar 3817\n",
      "suggested 3818\n",
      "Shades 3819\n",
      "organizes 3820\n",
      "Joselyn 3821\n",
      "carefully 3822\n",
      "Gloria 3823\n",
      "Dakota 3824\n",
      "recipes 3825\n",
      "palette 3826\n",
      "Lance 3827\n",
      "Molly 3828\n",
      "Bus 3829\n",
      "Quarter 3830\n",
      "multiple 3831\n",
      "design 3832\n",
      "final 3833\n",
      "Breaks 3834\n",
      "software 3835\n",
      "Hana 3836\n",
      "Franco 3837\n",
      "log 3838\n",
      "demanding 3839\n",
      "Tracy 3840\n",
      "By 3841\n",
      "meme 3842\n",
      "seduce 3843\n",
      "distrubed 3844\n",
      "classico 3845\n",
      "system 3846\n",
      "Beatles 3847\n",
      "sport 3848\n",
      "scammed 3849\n",
      "cricket 3850\n",
      "Sender 3851\n",
      "dishes 3852\n",
      "Raymond 3853\n",
      "Bert 3854\n",
      "warmer 3855\n",
      "Stefan 3856\n",
      "wedding 3857\n",
      "Joan 3858\n",
      "east 3859\n",
      "mini 3860\n",
      "pissed 3861\n",
      "ate 3862\n",
      "longer 3863\n",
      "convinces 3864\n",
      "fixing 3865\n",
      "absent 3866\n",
      "Lebanese 3867\n",
      "Micah 3868\n",
      "world 3869\n",
      "Matthias 3870\n",
      "post 3871\n",
      "previous 3872\n",
      "Gilda 3873\n",
      "Pippo 3874\n",
      "ladies 3875\n",
      "Manchester 3876\n",
      "relationship 3877\n",
      "shops 3878\n",
      "Markus 3879\n",
      "Presidential 3880\n",
      "read 3881\n",
      "protests 3882\n",
      "appreciates 3883\n",
      "lovely 3884\n",
      "Oh 3885\n",
      "Layla 3886\n",
      "Christopher 3887\n",
      "reasonable 3888\n",
      "keyed 3889\n",
      "la 3890\n",
      "massage 3891\n",
      "Rufus 3892\n",
      "Geri 3893\n",
      "rice 3894\n",
      "Zachary 3895\n",
      "minutes 3896\n",
      "potted 3897\n",
      "wishing 3898\n",
      "Wild 3899\n",
      "Due 3900\n",
      "files 3901\n",
      "cry 3902\n",
      "exercises 3903\n",
      "cash 3904\n",
      "Trish 3905\n",
      "Handmaid 3906\n",
      "Glasgow 3907\n",
      "Josh 3908\n",
      "Easter 3909\n",
      "stained 3910\n",
      "school 3911\n",
      "toilet 3912\n",
      "calluses 3913\n",
      "thrown 3914\n",
      "crashing 3915\n",
      "Cody 3916\n",
      "Derek 3917\n",
      "organising 3918\n",
      "lectures 3919\n",
      "Skylar 3920\n",
      "shelter 3921\n",
      "Canadian 3922\n",
      "Wroclaw 3923\n",
      "selling 3924\n",
      "supervisor 3925\n",
      "gossip 3926\n",
      "Tamara 3927\n",
      "Doll 3928\n",
      "100m 3929\n",
      "Marilyn 3930\n",
      "Chandler 3931\n",
      "Pietro 3932\n",
      "classical 3933\n",
      "kids 3934\n",
      "plumber 3935\n",
      "casa 3936\n",
      "booking 3937\n",
      "sex 3938\n",
      "Caleta 3939\n",
      "Treasure 3940\n",
      "120 3941\n",
      "Henry 3942\n",
      "escape 3943\n",
      "Aubrey 3944\n",
      "Mathew 3945\n",
      "La 3946\n",
      "Keiran 3947\n",
      "Joel 3948\n",
      "understanding 3949\n",
      "comfort 3950\n",
      "Tuscany 3951\n",
      "Susannah 3952\n",
      "list 3953\n",
      "camera 3954\n",
      "aren 3955\n",
      "Leslie 3956\n",
      "Two 3957\n",
      "CRM 3958\n",
      "Amelia 3959\n",
      "fallen 3960\n",
      "Thursdays 3961\n",
      "convince 3962\n",
      "Swedish 3963\n",
      "nocciolato 3964\n",
      "grey 3965\n",
      "cold 3966\n",
      "towel 3967\n",
      "Laos 3968\n",
      "Island 3969\n",
      "congratulating 3970\n",
      "during 3971\n",
      "Alonzo 3972\n",
      "Crystal 3973\n",
      "aired 3974\n",
      "election 3975\n",
      "consult 3976\n",
      "Rowley 3977\n",
      "Steakhouse 3978\n",
      "Jones 3979\n",
      "cough 3980\n",
      "roommate 3981\n",
      "Castle 3982\n",
      "slow 3983\n",
      "abortion 3984\n",
      "tables 3985\n",
      "Mina 3986\n",
      "care 3987\n",
      "resembled 3988\n",
      "beer 3989\n",
      "Chapel 3990\n",
      "runs 3991\n",
      "outdoors 3992\n",
      "lunchtime 3993\n",
      "Afterwards 3994\n",
      "flower 3995\n",
      "has 3996\n",
      "denial 3997\n",
      "homemade 3998\n",
      "formal 3999\n",
      "notify 4000\n",
      "revieved 4001\n",
      "playlist 4002\n",
      "messages 4003\n",
      "Libby 4004\n",
      "Barney 4005\n",
      "given 4006\n",
      "considering 4007\n",
      "Neomi 4008\n",
      "Derrick 4009\n",
      "Gustav 4010\n",
      "Reykjavik 4011\n",
      "research 4012\n",
      "grow 4013\n",
      "Janette 4014\n",
      "Trudy 4015\n",
      "event 4016\n",
      "ten 4017\n",
      "intermediate 4018\n",
      "Ida 4019\n",
      "fancy 4020\n",
      "Freda 4021\n",
      "location 4022\n",
      "finding 4023\n",
      "whom 4024\n",
      "Thanks 4025\n",
      "services 4026\n",
      "policy 4027\n",
      "Lavender 4028\n",
      "almond 4029\n",
      "split 4030\n",
      "works 4031\n",
      "rash 4032\n",
      "appeal 4033\n",
      "wallpapering 4034\n",
      "prudential 4035\n",
      "month 4036\n",
      "I 4037\n",
      "volunteer 4038\n",
      "bet 4039\n",
      "Queens 4040\n",
      "Mike 4041\n",
      "substance 4042\n",
      "Bitcoin 4043\n",
      "fond 4044\n",
      "old 4045\n",
      "Irwin 4046\n",
      "pie 4047\n",
      "call 4048\n",
      "Helmut 4049\n",
      "intermission 4050\n",
      "lenses 4051\n",
      "Pump 4052\n",
      "Irma 4053\n",
      "cheated 4054\n",
      "turtle 4055\n",
      "Dostoevsky 4056\n",
      "been 4057\n",
      "packs 4058\n",
      "teach 4059\n",
      "appreciate 4060\n",
      "apart 4061\n",
      "admit 4062\n",
      "advise 4063\n",
      "instructor 4064\n",
      "rolls 4065\n",
      "Garden 4066\n",
      "link 4067\n",
      "Harriet 4068\n",
      "Freya 4069\n",
      "karaoke 4070\n",
      "per 4071\n",
      "Ed 4072\n",
      "anymore 4073\n",
      "Store 4074\n",
      "Westin 4075\n",
      "Ant 4076\n",
      "theatre 4077\n",
      "frame 4078\n",
      "tickets 4079\n",
      "infant 4080\n",
      "came 4081\n",
      "Doug 4082\n",
      "prepared 4083\n",
      "facts 4084\n",
      "Gosling 4085\n",
      "families 4086\n",
      "Uncle 4087\n",
      "Lisabeth 4088\n",
      "choc 4089\n",
      "grab 4090\n",
      "Janek 4091\n",
      "cosmetics 4092\n",
      "cabin 4093\n",
      "condition 4094\n",
      "Natty 4095\n",
      "rumours 4096\n",
      "use 4097\n",
      "stupid 4098\n",
      "versatile 4099\n",
      "offer 4100\n",
      "pass 4101\n",
      "Cane 4102\n",
      "Clayton 4103\n",
      "Marc 4104\n",
      "stick 4105\n",
      "He 4106\n",
      "lately 4107\n",
      "Slovakia 4108\n",
      "Sontag 4109\n",
      "ferry 4110\n",
      "invited 4111\n",
      "fed 4112\n",
      "see 4113\n",
      "director 4114\n",
      "Brangelina 4115\n",
      "kissed 4116\n",
      "spreading 4117\n",
      "giving 4118\n",
      "Cristian 4119\n",
      "Darlene 4120\n",
      "quarter 4121\n",
      "login 4122\n",
      "Something 4123\n",
      "Mella 4124\n",
      "duck 4125\n",
      "stretching 4126\n",
      "Gunner 4127\n",
      "Liz 4128\n",
      "Boston 4129\n",
      "biography 4130\n",
      "Marika 4131\n",
      "database 4132\n",
      "dripping 4133\n",
      "alarm 4134\n",
      "Professor 4135\n",
      "breathing 4136\n",
      "realms 4137\n",
      "said 4138\n",
      "hand 4139\n",
      "Brooklyn 4140\n",
      "whisk 4141\n",
      "insurance 4142\n",
      "pyjamas 4143\n",
      "Anne 4144\n",
      "Sapo 4145\n",
      "windowpane 4146\n",
      "Frederick 4147\n",
      "Nando 4148\n",
      "Fay 4149\n",
      "lives 4150\n",
      "took 4151\n",
      "ill 4152\n",
      "triple 4153\n",
      "back 4154\n",
      "Jody 4155\n",
      "mechanic 4156\n",
      "Phil 4157\n",
      "solve 4158\n",
      "Gabrielle 4159\n",
      "impression 4160\n",
      "stolen 4161\n",
      "Brussels 4162\n",
      "Wednesday 4163\n",
      "visiting 4164\n",
      "Coldplay 4165\n",
      "Madison 4166\n",
      "nails 4167\n",
      "232 4168\n",
      "additional 4169\n",
      "Winston 4170\n",
      "sports 4171\n",
      "fault 4172\n",
      "sweets 4173\n",
      "HR 4174\n",
      "Quinn 4175\n",
      "horror 4176\n",
      "7pm 4177\n",
      "Tesco 4178\n",
      "ironing 4179\n",
      "Crush 4180\n",
      "law 4181\n",
      "players 4182\n",
      "faver 4183\n",
      "Cassie 4184\n",
      "knows 4185\n",
      "drawer 4186\n",
      "mailbox 4187\n",
      "recognised 4188\n",
      "comments 4189\n",
      "Wera 4190\n",
      "porch 4191\n",
      "Franklin 4192\n",
      "Still 4193\n",
      "inviting 4194\n",
      "learned 4195\n",
      "passing 4196\n",
      "Zuza 4197\n",
      "played 4198\n",
      "elections 4199\n",
      "writing 4200\n",
      "Prison 4201\n",
      "because 4202\n",
      "matching 4203\n",
      "had 4204\n",
      "Kin 4205\n",
      "\" 4206\n",
      "dead 4207\n",
      "weight 4208\n",
      "restaurant 4209\n",
      "Samantha 4210\n",
      "Camille 4211\n",
      "family 4212\n",
      "’ 4213\n",
      "journal 4214\n",
      "struggling 4215\n",
      "elevator 4216\n",
      "35 4217\n",
      "asleep 4218\n",
      "Patricia 4219\n",
      "eclipse 4220\n",
      "rocky 4221\n",
      "al 4222\n",
      "Melody 4223\n",
      "; 4224\n",
      "Anton 4225\n",
      "notices 4226\n",
      "framing 4227\n",
      "Brian 4228\n",
      "Benton 4229\n",
      "view 4230\n",
      "challenging 4231\n",
      "disco 4232\n",
      "Muse 4233\n",
      "Mae 4234\n",
      "211 4235\n",
      "gussied 4236\n",
      "elements 4237\n",
      "explanations 4238\n",
      "Amber 4239\n",
      "problems 4240\n",
      "Mason 4241\n",
      "bins 4242\n",
      "locks 4243\n",
      "without 4244\n",
      "Lyx 4245\n",
      "Julian 4246\n",
      "Sonia 4247\n",
      "reassure 4248\n",
      "Matata 4249\n",
      "mandarins 4250\n",
      "flatmate 4251\n",
      "fake 4252\n",
      "understand 4253\n",
      "Nicolas 4254\n",
      "relieved 4255\n",
      "gadgets 4256\n",
      "Samara 4257\n",
      "reception 4258\n",
      "pm 4259\n",
      "private 4260\n",
      "arrival 4261\n",
      "Hanks 4262\n",
      "mess 4263\n",
      "switched 4264\n",
      "oven 4265\n",
      "BL 4266\n",
      "Violet 4267\n",
      "Liv 4268\n",
      "Marton 4269\n",
      "join 4270\n",
      "ABC 4271\n",
      "stressful 4272\n",
      "usually 4273\n",
      "suit 4274\n",
      "bookstore 4275\n",
      "miracle 4276\n",
      "Karl 4277\n",
      "hypocrisy 4278\n",
      "Argentinian 4279\n",
      "Gizzi 4280\n",
      "Bella 4281\n",
      "Ewa 4282\n",
      "pregnancy 4283\n",
      "morning 4284\n",
      "Cork 4285\n",
      "Ron 4286\n",
      "regions 4287\n",
      "Polish 4288\n",
      "finishes 4289\n",
      "\n",
      " 4290\n",
      "Zuri 4291\n",
      "Practitioner 4292\n",
      "Edna 4293\n",
      "pretty 4294\n",
      "Flynn 4295\n",
      "Juan 4296\n",
      "Dr 4297\n",
      "Gabriella 4298\n",
      "Rodger 4299\n",
      "100k 4300\n",
      "Dalia 4301\n",
      "Sara 4302\n",
      "Tuber 4303\n",
      "male 4304\n",
      "Whitney 4305\n",
      "threats 4306\n",
      "boat 4307\n",
      "Evergreen 4308\n",
      "Norfolk 4309\n",
      "forgive 4310\n",
      "Brad 4311\n",
      "speaker 4312\n",
      "wouldn 4313\n",
      "settings 4314\n",
      "Berlin 4315\n",
      "Lonnie 4316\n",
      "Sheldon 4317\n",
      "Half 4318\n",
      "pants 4319\n",
      "principal 4320\n",
      "sunbathe 4321\n",
      "chance 4322\n",
      "nothing 4323\n",
      "Mazy 4324\n",
      "involving 4325\n",
      "Hugh 4326\n",
      "matter 4327\n",
      "Passion 4328\n",
      "John 4329\n",
      "Hernandez 4330\n",
      "launch 4331\n",
      "Zack 4332\n",
      "Julia 4333\n",
      "fondue 4334\n",
      "Damir 4335\n",
      "unpolite 4336\n",
      "69 4337\n",
      "splashes 4338\n",
      "Ginnie 4339\n",
      "Latoya 4340\n",
      "Frankie 4341\n",
      "Exercises 4342\n",
      "confessed 4343\n",
      "Kerri 4344\n",
      "Craig 4345\n",
      "technical 4346\n",
      "thought 4347\n",
      "Palace 4348\n",
      "Lois 4349\n",
      "Katarina 4350\n",
      "toothache 4351\n",
      "injured 4352\n",
      "cheat 4353\n",
      "suits 4354\n",
      "Richard 4355\n",
      "announced 4356\n",
      "The 4357\n",
      "postcard 4358\n",
      "behaviour 4359\n",
      "brought 4360\n",
      "bit 4361\n",
      "lighting 4362\n",
      "Tori 4363\n",
      "popcorn 4364\n",
      "Sina 4365\n",
      "apologized 4366\n",
      "anything 4367\n",
      "Messi 4368\n",
      "siks 4369\n",
      "lake 4370\n",
      "sneak 4371\n",
      "occasion 4372\n",
      "Catherine 4373\n",
      "Illness 4374\n",
      "nap 4375\n",
      "Marston 4376\n",
      "prefer 4377\n",
      "Jayce 4378\n",
      "grown 4379\n",
      "postpone 4380\n",
      "nights 4381\n",
      "Jacek 4382\n",
      "Other 4383\n",
      "blacklisted 4384\n",
      "resistant 4385\n",
      "ecstatic 4386\n",
      "Jessie 4387\n",
      "praised 4388\n",
      "caves 4389\n",
      "6pm 4390\n",
      "blood 4391\n",
      "sold 4392\n",
      "jail 4393\n",
      "Madrid 4394\n",
      "Laurent 4395\n",
      "walking 4396\n",
      "railway 4397\n",
      "hang 4398\n",
      "sings 4399\n",
      "mute 4400\n",
      "collect 4401\n",
      "contractor 4402\n",
      "hesitates 4403\n",
      "explain 4404\n",
      "Most 4405\n",
      "Maroon 4406\n",
      "business 4407\n",
      "over 4408\n",
      "due 4409\n",
      "crush 4410\n",
      "Clarissa 4411\n",
      "cups 4412\n",
      "impostor 4413\n",
      "shoes 4414\n",
      "informing 4415\n",
      "Zach 4416\n",
      "Pub 4417\n",
      "expedient 4418\n",
      "Fanny 4419\n",
      "Pacific 4420\n",
      "Kimi 4421\n",
      "time 4422\n",
      "Petter 4423\n",
      "Scots 4424\n",
      "dishwasher 4425\n",
      "Using 4426\n",
      "art 4427\n",
      "dieting 4428\n",
      "Fashion 4429\n",
      "Lantern 4430\n",
      "won 4431\n",
      "Rosario 4432\n",
      "queueing 4433\n",
      "Keeley 4434\n",
      "calculus 4435\n",
      "is 4436\n",
      "promotion 4437\n",
      "Charity 4438\n",
      "disagrees 4439\n",
      "Internet 4440\n",
      "stressed 4441\n",
      "80 4442\n",
      "So 4443\n",
      "begs 4444\n",
      "stationary 4445\n",
      "Tinder 4446\n",
      "6th 4447\n",
      "arrived 4448\n",
      "becoming 4449\n",
      "Varadero 4450\n",
      "departures 4451\n",
      "Sabrina 4452\n",
      "promoting 4453\n",
      "accent 4454\n",
      "Entertainment 4455\n",
      "suffered 4456\n",
      "Gemma 4457\n",
      "bag 4458\n",
      "neighbor 4459\n",
      "regarding 4460\n",
      "Parker 4461\n",
      "Michael 4462\n",
      "Mommy 4463\n",
      "Radiohead 4464\n",
      "confirmed 4465\n",
      "rise 4466\n",
      "Alaina 4467\n",
      "seriously 4468\n",
      "Stephen 4469\n",
      "salon 4470\n",
      "Raul 4471\n",
      "Sicily 4472\n",
      "CA20192735641 4473\n",
      "Current 4474\n",
      "avalanche 4475\n",
      "organized 4476\n",
      "title 4477\n",
      "clan 4478\n",
      "theme 4479\n",
      "slightly 4480\n",
      "hates 4481\n",
      "Makayla 4482\n",
      "dessert 4483\n",
      "Priya 4484\n",
      "exchanged 4485\n",
      "figured 4486\n",
      "why 4487\n",
      "Aaliyah 4488\n",
      "collects 4489\n",
      "phone 4490\n",
      "App 4491\n",
      "lipstick 4492\n",
      "CD 4493\n",
      "Cold 4494\n",
      "pomodori 4495\n",
      "Erin 4496\n",
      "community 4497\n",
      "shares 4498\n",
      "bathe 4499\n",
      "else 4500\n",
      "barbecue 4501\n",
      "recommending 4502\n",
      "editing 4503\n",
      "displeased 4504\n",
      "IKEA 4505\n",
      "struggles 4506\n",
      "+ 4507\n",
      "Bladerunner 4508\n",
      "Emilia 4509\n",
      "Clarisse 4510\n",
      "Cage 4511\n",
      "Gwen 4512\n",
      "thing 4513\n",
      "Darcie 4514\n",
      "put 4515\n",
      "fixed 4516\n",
      "Joaquin 4517\n",
      "Alexander 4518\n",
      "Dad 4519\n",
      "lyrics 4520\n",
      "Nuclear 4521\n",
      "sauna 4522\n",
      "Orthodox 4523\n",
      "museum 4524\n",
      "Malcolm 4525\n",
      "meal 4526\n",
      "contacted 4527\n",
      "recovers 4528\n",
      "Roman 4529\n",
      "Nav 4530\n",
      "ring 4531\n",
      "defrost 4532\n",
      "notice 4533\n",
      "hopes 4534\n",
      "Antoine 4535\n",
      "messy 4536\n",
      "hall 4537\n",
      "boots 4538\n",
      "disconnected 4539\n",
      "Filo 4540\n",
      "refuse 4541\n",
      "Pauline 4542\n",
      "airlines 4543\n",
      "gmail 4544\n",
      "Ulrich 4545\n",
      "Kate 4546\n",
      "renovated 4547\n",
      "ambulance 4548\n",
      "smelling 4549\n",
      "workshop 4550\n",
      "st 4551\n",
      "Claire 4552\n",
      "act 4553\n",
      "France 4554\n",
      "suspended 4555\n",
      "Gracelyn 4556\n",
      "Porter 4557\n",
      "bartending 4558\n",
      "kawaii 4559\n",
      "participate 4560\n",
      "data 4561\n",
      "nervous 4562\n",
      "39 4563\n",
      "bow 4564\n",
      "6ix9ine 4565\n",
      "Nipawin 4566\n",
      "Ellis 4567\n",
      "recycling 4568\n",
      "Yvonne 4569\n",
      "Smooshy 4570\n",
      "Bernard 4571\n",
      "warned 4572\n",
      "Poland 4573\n",
      "Square 4574\n",
      "Ginny 4575\n",
      "de 4576\n",
      "Colby 4577\n",
      "D1 4578\n",
      "Antwerp 4579\n",
      "designed 4580\n",
      "Bristol 4581\n",
      "dislikes 4582\n",
      "ask 4583\n",
      "it 4584\n",
      "Granda 4585\n",
      "Cathleen 4586\n",
      "Jeffery 4587\n",
      "something 4588\n",
      "William 4589\n",
      "Diego 4590\n",
      "soccer 4591\n",
      "China 4592\n",
      "bay 4593\n",
      "buildings 4594\n",
      "wonder 4595\n",
      "Brayden 4596\n",
      "random 4597\n",
      "cm 4598\n",
      "Gaby 4599\n",
      "Elly 4600\n",
      "McKayla 4601\n",
      "loves 4602\n",
      "disgracing 4603\n",
      "Peyton 4604\n",
      "Korea 4605\n",
      "Ingrid 4606\n",
      "forever 4607\n",
      "Branson 4608\n",
      "Tomorrow 4609\n",
      "local 4610\n",
      "chained 4611\n",
      "Angela 4612\n",
      "As 4613\n",
      "Skype 4614\n",
      "Kaleigh 4615\n",
      "plants 4616\n",
      "beers 4617\n",
      "Christina 4618\n",
      "packed 4619\n",
      "question 4620\n",
      "drama 4621\n",
      "Ashley 4622\n",
      "Grinch 4623\n",
      "Ricardo 4624\n",
      "lasted 4625\n",
      "compares 4626\n",
      "nanny 4627\n",
      "Jum 4628\n",
      "dates 4629\n",
      "lawn 4630\n",
      "workout 4631\n",
      "hold 4632\n",
      "M 4633\n",
      "celebrate 4634\n",
      "looked 4635\n",
      "who 4636\n",
      "Tolstoy 4637\n",
      "During 4638\n",
      "Dominique 4639\n",
      "Beverly 4640\n",
      "royal 4641\n",
      "Pearl 4642\n",
      "guest 4643\n",
      "crying 4644\n",
      "noisy 4645\n",
      "annoyed 4646\n",
      "afraid 4647\n",
      "AM 4648\n",
      "daughter 4649\n",
      "es 4650\n",
      "Garcia 4651\n",
      "twisted 4652\n",
      "Even 4653\n",
      "point 4654\n",
      "sceptical 4655\n",
      "watches 4656\n",
      "bicycle 4657\n",
      "Hedwig 4658\n",
      "b 4659\n",
      "Rossman 4660\n",
      "negotiated 4661\n",
      "liked 4662\n",
      "taking 4663\n",
      "Tempur 4664\n",
      "Nadia 4665\n",
      "picking 4666\n",
      "Karen 4667\n",
      "Alisha 4668\n",
      "bowl 4669\n",
      "partner 4670\n",
      "Marina 4671\n",
      "responsibilities 4672\n",
      "Ital 4673\n",
      "wil 4674\n",
      "issues 4675\n",
      "Dance 4676\n",
      "all 4677\n",
      "Ned 4678\n",
      "Fernanda 4679\n",
      "think 4680\n",
      "Valentine 4681\n",
      "lamp 4682\n",
      "installs 4683\n",
      "Billie 4684\n",
      "photographed 4685\n",
      "patrols 4686\n",
      "going 4687\n",
      "Ethan 4688\n",
      "Fin 4689\n",
      "Celeb 4690\n",
      "Senza 4691\n",
      "Selma 4692\n",
      "finally 4693\n",
      "embarrassing 4694\n",
      "tax 4695\n",
      "meat 4696\n",
      "Dermi 4697\n",
      "if 4698\n",
      "cancelled 4699\n",
      "salmon 4700\n",
      "In 4701\n",
      "Anette 4702\n",
      "Roy 4703\n",
      "Tyler 4704\n",
      "monogram 4705\n",
      "shocked 4706\n",
      "wears 4707\n",
      "phones 4708\n",
      "despite 4709\n",
      "Hood 4710\n",
      "fight 4711\n",
      "Girls 4712\n",
      "Conor 4713\n",
      "cake 4714\n",
      "money 4715\n",
      "King 4716\n",
      "accepts 4717\n",
      "likes 4718\n",
      "idea 4719\n",
      "crashed 4720\n",
      "Judy 4721\n",
      "maths 4722\n",
      "snacks 4723\n",
      "persuades 4724\n",
      "Gerry 4725\n",
      "Lin 4726\n",
      "slippers 4727\n",
      "African 4728\n",
      "cheer 4729\n",
      "Hattie 4730\n",
      "TaoTao 4731\n",
      "typing 4732\n",
      "University 4733\n",
      "One 4734\n",
      "Ultra 4735\n",
      "Natalie 4736\n",
      "since 4737\n",
      "needed 4738\n",
      "couldn 4739\n",
      "somewhere 4740\n",
      "bunny 4741\n",
      "slice 4742\n",
      "Narcos 4743\n",
      "river 4744\n",
      "pierogi 4745\n",
      "Saturdays 4746\n",
      "Died 4747\n",
      "withdrawing 4748\n",
      "Bolsonaro 4749\n",
      "Pisa 4750\n",
      "noon 4751\n",
      "Sima 4752\n",
      "Brandy 4753\n",
      "jenga 4754\n",
      "Cabernet 4755\n",
      "gave 4756\n",
      "sacrificed 4757\n",
      "boost 4758\n",
      "Roberta 4759\n",
      "extremely 4760\n",
      "Lucia 4761\n",
      "Lori 4762\n",
      "downstairs 4763\n",
      "Mercy 4764\n",
      "Harmonie 4765\n",
      "conference 4766\n",
      "Guggenheim 4767\n",
      "parking 4768\n",
      "Nicky 4769\n",
      "Knight 4770\n",
      "Reich 4771\n",
      "focus 4772\n",
      "Jeff 4773\n",
      "Manuel 4774\n",
      "G 4775\n",
      "Zora 4776\n",
      "eggs 4777\n",
      "higher 4778\n",
      "Jeniffer 4779\n",
      "promises 4780\n",
      "Jaslene 4781\n",
      "visited 4782\n",
      "20k 4783\n",
      "Prompted 4784\n",
      "Keegan 4785\n",
      "trees 4786\n",
      "across 4787\n",
      "Sisi 4788\n",
      "maid 4789\n",
      "resume 4790\n",
      "station 4791\n",
      "Pepper 4792\n",
      "feels 4793\n",
      "Paulina 4794\n",
      "profile 4795\n",
      "songs 4796\n",
      "Bunny 4797\n",
      "Pembroke 4798\n",
      "Georgina 4799\n",
      "183 4800\n",
      "mind 4801\n",
      "Ariel 4802\n",
      "pill 4803\n",
      "Martin 4804\n",
      "fox 4805\n",
      "pancakes 4806\n",
      "Beasts 4807\n",
      "15x360 4808\n",
      "thanks 4809\n",
      "Warrick 4810\n",
      "clip 4811\n",
      "gaming 4812\n",
      "leaked 4813\n",
      "volume 4814\n",
      "Street 4815\n",
      "Some 4816\n",
      "Starurday 4817\n",
      "three 4818\n",
      "sale 4819\n",
      "hope 4820\n",
      "pillows 4821\n",
      "Cory 4822\n",
      "brewery 4823\n",
      "Bobby 4824\n",
      "sells 4825\n",
      "Alexis 4826\n",
      "pumpkin 4827\n",
      "Lil 4828\n",
      "Vicki 4829\n",
      "basics 4830\n",
      "Harry 4831\n",
      "Georg 4832\n",
      "Yves 4833\n",
      "that 4834\n",
      "angrier 4835\n",
      "plans 4836\n",
      "Pho 4837\n",
      "Gregory 4838\n",
      "keys 4839\n",
      "Leah 4840\n",
      "Cards 4841\n",
      "cafe 4842\n",
      "smoothies 4843\n",
      "Nathan 4844\n",
      "youth 4845\n",
      "purchase 4846\n",
      "no 4847\n",
      "Carol 4848\n",
      "initial 4849\n",
      "Jennifer 4850\n",
      "Ludo 4851\n",
      "Pamuk 4852\n",
      "apartment 4853\n",
      "nearly 4854\n",
      "arm 4855\n",
      "related 4856\n",
      "Willy 4857\n",
      "Café 4858\n",
      "corporate 4859\n",
      "lead 4860\n",
      "books 4861\n",
      "gifs 4862\n",
      "react 4863\n",
      "Konnor 4864\n",
      "fans 4865\n",
      "project 4866\n",
      "Marly 4867\n",
      "Ahad 4868\n",
      "departure 4869\n",
      "weeks 4870\n",
      "Pierrot 4871\n",
      "Raise 4872\n",
      "Disneyland 4873\n",
      "conversation 4874\n",
      "Adrien 4875\n",
      "Francine 4876\n",
      "divorce 4877\n",
      "Pilar 4878\n",
      "Rene 4879\n",
      "details 4880\n",
      "Dennic 4881\n",
      "Melissa 4882\n",
      "Meg 4883\n",
      "Alicia 4884\n",
      "run 4885\n",
      "reckon 4886\n",
      "weird 4887\n",
      "lessons 4888\n",
      "von 4889\n",
      "Blanca 4890\n",
      "ice 4891\n",
      "dog 4892\n",
      "Off 4893\n",
      "tart 4894\n",
      "voucher 4895\n",
      "receive 4896\n",
      "degrees 4897\n",
      "Dylan 4898\n",
      "they 4899\n",
      "Polynesia 4900\n",
      "Kenny 4901\n",
      "media 4902\n",
      "Stella 4903\n",
      "Anabella 4904\n",
      "badly 4905\n",
      "Riley 4906\n",
      "Terrence 4907\n",
      "Abigail 4908\n",
      "winnipeg 4909\n",
      "Influence 4910\n",
      "11pm 4911\n",
      "7th 4912\n",
      "buliding 4913\n",
      "subway 4914\n",
      "deteriorated 4915\n",
      "Temple 4916\n",
      "Lisa 4917\n",
      "talk 4918\n",
      "Flash 4919\n",
      "Mckayla 4920\n",
      "East 4921\n",
      "beautiful 4922\n",
      "exams 4923\n",
      "politicians 4924\n",
      "Actually 4925\n",
      "National 4926\n",
      "Italia 4927\n",
      "fiancee 4928\n",
      "referendum 4929\n",
      "Louie 4930\n",
      "drive 4931\n",
      "Youtuber 4932\n",
      "2000 4933\n",
      "wall 4934\n",
      "treat 4935\n",
      "Marie 4936\n",
      "essays 4937\n",
      "café 4938\n",
      "sponsorship 4939\n",
      "CPS 4940\n",
      "Mercury 4941\n",
      "Aunt 4942\n",
      "Wendani 4943\n",
      "discuss 4944\n",
      "deer 4945\n",
      "dollar 4946\n",
      "Olia 4947\n",
      "heavily 4948\n",
      "Elie 4949\n",
      "Stephanie 4950\n",
      "Olivier 4951\n",
      "Alfred 4952\n",
      "ball 4953\n",
      "turn 4954\n",
      "Carlie 4955\n",
      "Rapsody 4956\n",
      "Aretha 4957\n",
      "heading 4958\n",
      "Thai 4959\n",
      "whole 4960\n",
      "scores 4961\n",
      "Bruno 4962\n",
      "charge 4963\n",
      "World 4964\n",
      "application 4965\n",
      "1256 4966\n",
      "Toronto 4967\n",
      "UN 4968\n",
      "TA 4969\n",
      "TV 4970\n",
      "record 4971\n",
      "PR 4972\n",
      "Crane 4973\n",
      "14C 4974\n",
      "helps 4975\n",
      "housewife 4976\n",
      "Mell 4977\n",
      "fill 4978\n",
      "career 4979\n",
      "pasta 4980\n",
      "course 4981\n",
      "poll 4982\n",
      "Jean 4983\n",
      "Jeffrey 4984\n",
      "arrives 4985\n",
      "Camilla 4986\n",
      "power 4987\n",
      "as 4988\n",
      "Blizzard 4989\n",
      "Eui 4990\n",
      "unwell 4991\n",
      "Their 4992\n",
      "smog 4993\n",
      "Dana 4994\n",
      "Dominic 4995\n",
      "Jules 4996\n",
      "Kelsie 4997\n",
      "agreement 4998\n",
      "Geraldine 4999\n",
      "Warsaw 5000\n",
      "removal 5001\n",
      "asap 5002\n",
      "wardrobe 5003\n",
      "potatoes 5004\n",
      "Vikings 5005\n",
      "Seth 5006\n",
      "offence 5007\n",
      "Linda 5008\n",
      "Mirko 5009\n",
      "Eventually 5010\n",
      "talked 5011\n",
      "Billy 5012\n",
      "changed 5013\n",
      "muscles 5014\n",
      "timings 5015\n",
      "shy 5016\n",
      "provide 5017\n",
      "Timmy 5018\n",
      "bowling 5019\n",
      "miss 5020\n",
      "added 5021\n",
      "24th 5022\n",
      "sneaky 5023\n",
      "legs 5024\n",
      "positive 5025\n",
      "Nellie 5026\n",
      "4 5027\n",
      "shirt 5028\n",
      "Matilda 5029\n",
      "Adrew 5030\n",
      "foundation 5031\n",
      "decluttering 5032\n",
      "smoothie 5033\n",
      "Greenbook 5034\n",
      "expected 5035\n",
      "stamps 5036\n",
      "become 5037\n",
      "gianduja 5038\n",
      "sister 5039\n",
      "Rick 5040\n",
      "paparazzis 5041\n",
      "Aliana 5042\n",
      "crushed 5043\n",
      "board 5044\n",
      "catch 5045\n",
      "preparation 5046\n",
      "proof 5047\n",
      "socially 5048\n",
      "ads 5049\n",
      "regard 5050\n",
      "optimistic 5051\n",
      "Cambridge 5052\n",
      "interview 5053\n",
      "assessment 5054\n",
      "collapsed 5055\n",
      "together 5056\n",
      "Anderson 5057\n",
      "driving 5058\n",
      "ebooks 5059\n",
      "Verde 5060\n",
      "angry 5061\n",
      "farm 5062\n",
      "Shaz 5063\n",
      "detail 5064\n",
      "fun 5065\n",
      "Ridge 5066\n",
      "Katherine 5067\n",
      "computer 5068\n",
      "& 5069\n",
      "Armstrong 5070\n",
      "suggests 5071\n",
      "central 5072\n",
      "Mountaineering 5073\n",
      "Filipino 5074\n",
      "Ula 5075\n",
      "children 5076\n",
      "Marianna 5077\n",
      "Nel 5078\n",
      "Carrie 5079\n",
      "Venus 5080\n",
      "irritated 5081\n",
      "Jurek 5082\n",
      "sure 5083\n",
      "discussing 5084\n",
      "instruction 5085\n",
      "Brock 5086\n",
      "sort 5087\n",
      "parcel 5088\n",
      "cats 5089\n",
      "Filip 5090\n",
      "Jez 5091\n",
      "experience 5092\n",
      "Dale 5093\n",
      "assistants 5094\n",
      "feel 5095\n",
      "Hurley 5096\n",
      "lover 5097\n",
      "vinyls 5098\n",
      "disciplinary 5099\n",
      "M6 5100\n",
      "decided 5101\n",
      "involvement 5102\n",
      "Boscaiola 5103\n",
      "Zoe 5104\n",
      "ok 5105\n",
      "Donald 5106\n",
      "French 5107\n",
      "security 5108\n",
      "compared 5109\n",
      "Evie 5110\n",
      "needn 5111\n",
      "Keith 5112\n",
      "30ish 5113\n",
      "Alvin 5114\n",
      "flying 5115\n",
      "flat 5116\n",
      "positions 5117\n",
      "googling 5118\n",
      "town 5119\n",
      "dizzy 5120\n",
      "several 5121\n",
      "role 5122\n",
      "Rio 5123\n",
      "mins 5124\n",
      "Infinity 5125\n",
      "theories 5126\n",
      "Munro 5127\n",
      "Manesh 5128\n",
      "Whitby 5129\n",
      "juggling 5130\n",
      "disappointed 5131\n",
      "sexy 5132\n",
      "backyard 5133\n",
      "days 5134\n",
      "hometown 5135\n",
      "red 5136\n",
      "Theater 5137\n",
      "festive 5138\n",
      "Mountain 5139\n",
      "Hagrid 5140\n",
      "depression 5141\n",
      "Muhammad 5142\n",
      "physical 5143\n",
      "Corina 5144\n",
      "after 5145\n",
      "laugh 5146\n",
      "Paula 5147\n",
      "live 5148\n",
      "c 5149\n",
      "arriving 5150\n",
      "math 5151\n",
      "table 5152\n",
      "Giovanna 5153\n",
      "uploaded 5154\n",
      "page 5155\n",
      "spends 5156\n",
      "advised 5157\n",
      "agrees 5158\n",
      "Paul 5159\n",
      "university 5160\n",
      "tree 5161\n",
      "Rebekah 5162\n",
      "cloth 5163\n",
      "babysitter 5164\n",
      "soft 5165\n",
      "asks 5166\n",
      "Saturday 5167\n",
      "00 5168\n",
      "note 5169\n",
      "Rhys 5170\n",
      "sleepy 5171\n",
      "Society 5172\n",
      "laundromat 5173\n",
      "Venice 5174\n",
      "Melbourne 5175\n",
      "prostitute 5176\n",
      "pack 5177\n",
      "fashion 5178\n",
      "Mohamed 5179\n",
      "Kitty 5180\n",
      "explains 5181\n",
      "ethical 5182\n",
      "Alan 5183\n",
      "construction 5184\n",
      "Sandrine 5185\n",
      "stalker 5186\n",
      "spent 5187\n",
      "Margaret 5188\n",
      "Keanu 5189\n",
      "Maciek 5190\n",
      "outstanding 5191\n",
      "unexpected 5192\n",
      "flu 5193\n",
      "mistake 5194\n",
      "th 5195\n",
      "helping 5196\n",
      "8 5197\n",
      "complimenting 5198\n",
      "held 5199\n",
      "proposes 5200\n",
      "Christian 5201\n",
      "stood 5202\n",
      "haircut 5203\n",
      "soon 5204\n",
      "Branden 5205\n",
      "entered 5206\n",
      "Kylie 5207\n",
      "kind 5208\n",
      "Barrett 5209\n",
      "beef 5210\n",
      "studying 5211\n",
      "foundations 5212\n",
      "Cathy 5213\n",
      "Mirco 5214\n",
      "suggestion 5215\n",
      "130 5216\n",
      "abilities 5217\n",
      "aerobic 5218\n",
      "dad 5219\n",
      "Jennie 5220\n",
      "pizzeria 5221\n",
      "leaving 5222\n",
      "Ronny 5223\n",
      "critisizes 5224\n",
      "description 5225\n",
      "Mon 5226\n",
      "Flaming 5227\n",
      "Petra 5228\n",
      "Elizabeth 5229\n",
      "Patrick 5230\n",
      "Danielle 5231\n",
      "English 5232\n",
      "Janice 5233\n",
      "Suchecka 5234\n",
      "Marvel 5235\n",
      "glittery 5236\n",
      "Superman 5237\n",
      "thriller 5238\n",
      "part 5239\n",
      "Elsie 5240\n",
      "bank 5241\n",
      "graves 5242\n",
      "Copper 5243\n",
      "Maries 5244\n",
      "comics 5245\n",
      "Australia 5246\n",
      "Delgado 5247\n",
      "Marcus 5248\n",
      "celebrating 5249\n",
      "snowing 5250\n",
      "pressure 5251\n",
      "revenge 5252\n",
      "never 5253\n",
      "Dreamcatcher 5254\n",
      "probably 5255\n",
      "cook 5256\n",
      "midterms 5257\n",
      "its 5258\n",
      "Ben 5259\n",
      "Vera 5260\n",
      "long 5261\n",
      "plot 5262\n",
      "Fernando 5263\n",
      "discusses 5264\n",
      "Edric 5265\n",
      "awfully 5266\n",
      "overnight 5267\n",
      "republican 5268\n",
      "Liechtenstein 5269\n",
      "Welden 5270\n",
      "rent 5271\n",
      "Ronald 5272\n",
      "reintroducing 5273\n",
      "Jacky 5274\n",
      "Max 5275\n",
      "Scotland 5276\n",
      "Alisson 5277\n",
      "Fuerteventura 5278\n",
      "An 5279\n",
      "Lapa 5280\n",
      "Bev 5281\n",
      "guessing 5282\n",
      "monsters 5283\n",
      "Elle 5284\n",
      "Gethesmankirche 5285\n",
      "15 5286\n",
      "bicycles 5287\n",
      "baby 5288\n",
      "match 5289\n",
      "teenager 5290\n",
      "Tasty 5291\n",
      "overwhelmed 5292\n",
      "9 5293\n",
      "Fran 5294\n",
      "syndrome 5295\n",
      "bonds 5296\n",
      "wine 5297\n",
      "Dick 5298\n",
      "Alec 5299\n",
      "speak 5300\n",
      "check 5301\n",
      "although 5302\n",
      "Susanne 5303\n",
      "game 5304\n",
      "search 5305\n",
      "Karolina 5306\n",
      "Angeline 5307\n",
      "invested 5308\n",
      "hesitating 5309\n",
      "Ikea 5310\n",
      "fix 5311\n",
      "Margot 5312\n",
      "groceries 5313\n",
      "presents 5314\n",
      "picture 5315\n",
      "Milton 5316\n",
      "coat 5317\n",
      "Regent 5318\n",
      "lounge 5319\n",
      "Quietcomfort 5320\n",
      "Macca 5321\n",
      "Tuesday 5322\n",
      "layout 5323\n",
      "postponed 5324\n",
      "awaiting 5325\n",
      "gifts 5326\n",
      "Eurovision 5327\n",
      "Celeste 5328\n",
      "Diana 5329\n",
      "Tabby 5330\n",
      "Watson 5331\n",
      "satisfied 5332\n",
      "South 5333\n",
      "eaten 5334\n",
      "late 5335\n",
      "Lila 5336\n",
      "shower 5337\n",
      "Alger 5338\n",
      "Edith 5339\n",
      "Annette 5340\n",
      "carne 5341\n",
      "female 5342\n",
      "reads 5343\n",
      "Daredevil 5344\n",
      "moving 5345\n",
      "Cora 5346\n",
      "Brenden 5347\n",
      "Aron 5348\n",
      "29th 5349\n",
      "coach 5350\n",
      "issue 5351\n",
      "Lebron 5352\n",
      "tea 5353\n",
      "Mia 5354\n",
      "bronchitis 5355\n",
      "Phyllis 5356\n",
      "used 5357\n",
      "endorses 5358\n",
      "Haesh 5359\n",
      "Faith 5360\n",
      "fundraiser 5361\n",
      "speaking 5362\n",
      "errors 5363\n",
      "walk 5364\n",
      "wingman 5365\n",
      "sometime 5366\n",
      "husband 5367\n",
      "appalling 5368\n",
      "overtime 5369\n",
      "urged 5370\n",
      "pointed 5371\n",
      "Klay 5372\n",
      "gived 5373\n",
      "nearby 5374\n",
      "green 5375\n",
      "tour 5376\n",
      "James 5377\n",
      "Beckham 5378\n",
      "Little 5379\n",
      "cancel 5380\n",
      "arrangements 5381\n",
      "Vasily 5382\n",
      "woke 5383\n",
      "participants 5384\n",
      "told 5385\n",
      "sends 5386\n",
      "by 5387\n",
      "Ronnie 5388\n",
      "website 5389\n",
      "singing 5390\n",
      "preference 5391\n",
      "broken 5392\n",
      "claims 5393\n",
      "what 5394\n",
      "Broadway 5395\n",
      "mirror 5396\n",
      "17 5397\n",
      "sages 5398\n",
      "finishing 5399\n",
      "wished 5400\n",
      "obligations 5401\n",
      "wigs 5402\n",
      "illumination 5403\n",
      "treated 5404\n",
      "Beth 5405\n",
      "Pão 5406\n",
      "start 5407\n",
      "trousers 5408\n",
      "Celesta 5409\n",
      "gathering 5410\n",
      "woman 5411\n",
      "Freddie 5412\n",
      "bottles 5413\n",
      "Jaeden 5414\n",
      "Bee 5415\n",
      "Concentrix 5416\n",
      "installation 5417\n",
      "Boredom 5418\n",
      "mock 5419\n",
      "Ute 5420\n",
      "highschool 5421\n",
      "Dwayne 5422\n",
      "nominated 5423\n",
      "crowded 5424\n",
      "Tegan 5425\n",
      "Space 5426\n",
      "sleeping 5427\n",
      "Because 5428\n",
      "trouble 5429\n",
      "Jude 5430\n",
      "Silvia 5431\n",
      "Scarlett 5432\n",
      "Patel 5433\n",
      "Clair 5434\n",
      "Brandi 5435\n",
      "Jen 5436\n",
      "Timothy 5437\n",
      "book 5438\n",
      "Javon 5439\n",
      "26th 5440\n",
      "reservations 5441\n",
      "Stroad 5442\n",
      "deliver 5443\n",
      "30th 5444\n",
      "fell 5445\n",
      "present 5446\n",
      "Bruce 5447\n",
      "Carols 5448\n",
      "Ted 5449\n",
      "Fiona 5450\n",
      "Gabriel 5451\n",
      "life 5452\n",
      "Gralecyn 5453\n",
      "Jarod 5454\n",
      "overslept 5455\n",
      "B 5456\n",
      "voodoo 5457\n",
      "space 5458\n",
      "child 5459\n",
      "hype 5460\n",
      "dressing 5461\n",
      "GoForIt 5462\n",
      "bravio 5463\n",
      "seven 5464\n",
      "panicking 5465\n",
      "Joanne 5466\n",
      "luggage 5467\n",
      "current 5468\n",
      "Criminal 5469\n",
      "Game 5470\n",
      "holds 5471\n",
      "menstrual 5472\n",
      "Aisha 5473\n",
      "tiring 5474\n",
      "Crew 5475\n",
      "August 5476\n",
      "credit 5477\n",
      "recons 5478\n",
      "Hillary 5479\n",
      "hospital 5480\n",
      "Marta 5481\n",
      "Women 5482\n",
      "Tomas 5483\n",
      "bluetooth 5484\n",
      "Dee 5485\n",
      "Terence 5486\n",
      "Tbilisi 5487\n",
      "Fatima 5488\n",
      "men 5489\n",
      "RDR2 5490\n",
      "Podsiadlo 5491\n",
      "tell 5492\n",
      "biggest 5493\n",
      "chocolate 5494\n",
      "interface 5495\n",
      "Jefferson 5496\n",
      "Cristine 5497\n",
      "outfit 5498\n",
      "German 5499\n",
      "Kire 5500\n",
      "Galaxy 5501\n",
      "singer 5502\n",
      "Agnieszka 5503\n",
      "scoring 5504\n",
      "papers 5505\n",
      "Joy 5506\n",
      "Colombia 5507\n",
      "Market 5508\n",
      "bed 5509\n",
      "feed 5510\n",
      "missed 5511\n",
      "killers 5512\n",
      "add 5513\n",
      "drink 5514\n",
      "curious 5515\n",
      "deal 5516\n",
      "dated 5517\n",
      "Laurel 5518\n",
      "guilty 5519\n",
      "Ziggy 5520\n",
      "value 5521\n",
      "dogs 5522\n",
      "40 5523\n",
      "spooky 5524\n",
      "Lauren 5525\n",
      "physics 5526\n",
      "Cash 5527\n",
      "greet 5528\n",
      "working 5529\n",
      "Ulia 5530\n",
      "unemployed 5531\n",
      "Tale 5532\n",
      "badge 5533\n",
      "dumplings 5534\n",
      "noodle 5535\n",
      "wife 5536\n",
      "Kent 5537\n",
      "Ronson 5538\n",
      "Mines 5539\n",
      "basketball 5540\n",
      "delicious 5541\n",
      "municipal 5542\n",
      "much 5543\n",
      "Erik 5544\n",
      "Grandpa 5545\n",
      "animal 5546\n",
      "885th 5547\n",
      "fully 5548\n",
      "keep 5549\n",
      "potential 5550\n",
      "waves 5551\n",
      "Vinnie 5552\n",
      "cause 5553\n",
      "Parents 5554\n",
      "them 5555\n",
      "dictionary 5556\n",
      "Kragmortha 5557\n",
      "Shelly 5558\n",
      "normal 5559\n",
      "Harvey 5560\n",
      "dr 5561\n",
      "extension 5562\n",
      "Corralejo 5563\n",
      "repair 5564\n",
      "Joshua 5565\n",
      "When 5566\n",
      "Lia 5567\n",
      "shelf 5568\n",
      "treadmill 5569\n",
      "here 5570\n",
      "period 5571\n",
      "currently 5572\n",
      "laughing 5573\n",
      "arranging 5574\n",
      "primary 5575\n",
      "Sprite 5576\n",
      "Barbra 5577\n",
      "ones 5578\n",
      "PC 5579\n",
      "guesses 5580\n",
      "coke 5581\n",
      "presented 5582\n",
      "Marion 5583\n",
      "Promo 5584\n",
      "lets 5585\n",
      "tap 5586\n",
      "Nils 5587\n",
      "Claudia 5588\n",
      "boarding 5589\n",
      "automobiles 5590\n",
      "Away 5591\n",
      "Adrianne 5592\n",
      "Jamie 5593\n",
      "Cindy 5594\n",
      "or 5595\n",
      "Chamber 5596\n",
      "pictures 5597\n",
      "cd 5598\n",
      "Percy 5599\n",
      "Kelly 5600\n",
      "civic 5601\n",
      "funds 5602\n",
      "Dyer 5603\n",
      "waiver 5604\n",
      "received 5605\n",
      "thinking 5606\n",
      "overreacted 5607\n",
      "Russel 5608\n",
      "supportive 5609\n",
      "Owen 5610\n",
      "intense 5611\n",
      "Francisco 5612\n",
      "make 5613\n",
      "Both 5614\n",
      "Alanis 5615\n",
      "Chili 5616\n",
      "Rafal 5617\n",
      "info 5618\n",
      "Batman 5619\n",
      "Pandora 5620\n",
      "Karim 5621\n",
      "Arthur 5622\n",
      "sushi 5623\n",
      "lunar 5624\n",
      "junior 5625\n",
      "unambiguous 5626\n",
      "Sean 5627\n",
      "flirt 5628\n",
      "experiment 5629\n",
      "block 5630\n",
      "emigrate 5631\n",
      "Club 5632\n",
      "gluten 5633\n",
      "inefficient 5634\n",
      "jeff 5635\n",
      "four 5636\n",
      "bar 5637\n",
      "seemed 5638\n",
      "vodka 5639\n",
      "Nobody 5640\n",
      "8PM 5641\n",
      "January 5642\n",
      "mon 5643\n",
      "Pizza 5644\n",
      "pub 5645\n",
      "girls 5646\n",
      "scholarship 5647\n",
      "Remi 5648\n",
      "unsure 5649\n",
      "Saoirse 5650\n",
      "Daryl 5651\n",
      "rented 5652\n",
      "Long 5653\n",
      "complied 5654\n",
      "cheaper 5655\n",
      "Malmö 5656\n",
      "Chesterfield 5657\n",
      "rain 5658\n",
      "controlled 5659\n",
      "Adventures 5660\n",
      "terminal 5661\n",
      "liberal 5662\n",
      "in 5663\n",
      "judges 5664\n",
      "versus 5665\n",
      "still 5666\n",
      "Ashton 5667\n",
      "forward 5668\n",
      "Management 5669\n",
      "cries 5670\n",
      "taken 5671\n",
      "funding 5672\n",
      "Carmen 5673\n",
      "row 5674\n",
      "roles 5675\n",
      "tasks 5676\n",
      "War 5677\n",
      "situation 5678\n",
      "Aldona 5679\n",
      "car 5680\n",
      "convinced 5681\n",
      "travel 5682\n",
      "onto 5683\n",
      "tried 5684\n",
      "ride 5685\n",
      "bough 5686\n",
      "Lorrie 5687\n",
      "door 5688\n",
      "gas 5689\n",
      "explosion 5690\n",
      "early 5691\n",
      "review 5692\n",
      "dark 5693\n",
      "rooms 5694\n",
      "little 5695\n",
      "key 5696\n",
      "include 5697\n",
      "Frances 5698\n",
      "fair 5699\n",
      "Roul 5700\n",
      "Blake 5701\n",
      "updates 5702\n",
      "emigration 5703\n",
      "prospect 5704\n",
      "Sunny 5705\n",
      "Ion 5706\n",
      "Miles 5707\n",
      "fires 5708\n",
      "train 5709\n",
      "high 5710\n",
      "flight 5711\n",
      "SWJ 5712\n",
      "nouveau 5713\n",
      "than 5714\n",
      "mill 5715\n",
      "cup 5716\n",
      "Virginia 5717\n",
      "Sylwia 5718\n",
      "cafeteria 5719\n",
      "Valerie 5720\n",
      "remove 5721\n",
      "presentation 5722\n",
      "worksheet 5723\n",
      "Lottie 5724\n",
      "consideration 5725\n",
      "Ann 5726\n",
      "Victor 5727\n",
      "Cinema 5728\n",
      "park 5729\n",
      "will 5730\n",
      "pool 5731\n",
      "Amanda 5732\n",
      "checking 5733\n",
      "Katrina 5734\n",
      "frustration 5735\n",
      "party 5736\n",
      "June 5737\n",
      "Jake 5738\n",
      "changes 5739\n",
      "Wikipedia 5740\n",
      "unbearable 5741\n",
      "Kathryn 5742\n",
      "buy 5743\n",
      "pockets 5744\n",
      "scrambled 5745\n",
      "friendly 5746\n",
      "Caroline 5747\n",
      "Moms 5748\n",
      "PS 5749\n",
      "laxatives 5750\n",
      "happened 5751\n",
      "Al 5752\n",
      "enter 5753\n",
      "hockey 5754\n",
      "famous 5755\n",
      "hangover 5756\n",
      "skiing 5757\n",
      "correct 5758\n",
      "Feel 5759\n",
      "Drake 5760\n",
      "worker 5761\n",
      "Lily 5762\n",
      "soy 5763\n",
      "spaghetti 5764\n",
      "Town 5765\n",
      "Spice 5766\n",
      "Wonders 5767\n",
      "Erasmus 5768\n",
      "Bea 5769\n",
      "Brice 5770\n",
      "square 5771\n",
      "soap 5772\n",
      "Iceland 5773\n",
      "sounds 5774\n",
      "unused 5775\n",
      "state 5776\n",
      "Carlos 5777\n",
      "Antilles 5778\n",
      "chat 5779\n",
      "advices 5780\n",
      "annoying 5781\n",
      "criticizing 5782\n",
      "Lilly 5783\n",
      "Erick 5784\n",
      "citizens 5785\n",
      "Cassidy 5786\n",
      "good 5787\n",
      "Allan 5788\n",
      "welcome 5789\n",
      "lottery 5790\n",
      "seems 5791\n",
      "be 5792\n",
      "dumped 5793\n",
      "Merrill 5794\n",
      "Matthew 5795\n",
      "idiot 5796\n",
      "requests 5797\n",
      "tail 5798\n",
      "Cracow 5799\n",
      "Eddie 5800\n",
      "deli 5801\n",
      "now 5802\n",
      "neighbors 5803\n",
      "envy 5804\n",
      "formatting 5805\n",
      "Sims 5806\n",
      "Alfie 5807\n",
      "colleague 5808\n",
      "repertoire 5809\n",
      "Gone 5810\n",
      "Birgit 5811\n",
      "Ophelia 5812\n",
      "perform 5813\n",
      "hasn 5814\n",
      "Christy 5815\n",
      "popping 5816\n",
      "Edlyna 5817\n",
      "PE 5818\n",
      "clock 5819\n",
      "yourself 5820\n",
      "Kenzie 5821\n",
      "moments 5822\n",
      "dislike 5823\n",
      "Faro 5824\n",
      "Wendy 5825\n",
      "greek 5826\n",
      "busy 5827\n",
      "€ 5828\n",
      "Eternity 5829\n",
      "Morgan 5830\n",
      "might 5831\n",
      "writer 5832\n",
      "Tokken 5833\n",
      "tells 5834\n",
      "Sigismund 5835\n",
      "[END] 5836\n",
      "Ottawa 5837\n",
      "heard 5838\n"
     ]
    }
   ],
   "source": [
    "for i,(j,k) in vocabulary.items():\n",
    "    print(i,k)\n",
    "    if k==2347:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b644a6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# learning_rate=0.00001\n",
    " \n",
    "# for epoch in range(550):\n",
    "#     print(\"Loss\",tot_loss_epoch/num_batches_per_epoch,\"mean accuracy\",np.mean(np.array(accuracies)))#np.mean(np.array(accuracies))\n",
    "#     tot_loss_epoch=0\n",
    "#     total_accuracy_epoch=0\n",
    "#     mean_acc=0\n",
    "#     accuracies=[]\n",
    "#     for i in tqdm(range(0,num_batches_per_epoch),desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "#         #try: \n",
    "#         start = i * batch_size\n",
    "#         end = start + batch_size \n",
    "#         X_batch = X_train[start:end]\n",
    "#         y_batch = y_train[start:end] \n",
    "        \n",
    "        \n",
    "#         #helper.print_matrix(y_batch)\n",
    "#         #print(\"X_batch\",X_batch)\n",
    "        \n",
    "        \n",
    "#         inputs_e=helper.create_input_encoder(X_batch,vocabulary,words_per_phrase,embedding_size) \n",
    "#         inputs_decoder=helper.create_decoder_input(y_batch,embedding_size,words_per_phrase,vocabulary) \n",
    "#         target_decoder=helper.create_target(y_batch,vocabulary,words_per_phrase)\n",
    "#         #target_decoder=helper.generate_target_sparse_categorical(y_batch,vocabulary,words_per_phrase)\n",
    "#         #print(\"target_decoder\",target_decoder.shape)\n",
    "#         #print(\"inputs_decoder\",inputs_decoder.shape)\n",
    "#         #target_decoder=inputs_decoder\n",
    "\n",
    "#         Dout=MyTransformer.forward(inputs_e,inputs_decoder,X_batch,y_batch)\n",
    "#         counter_beccate=0\n",
    "#         counter_tot=0\n",
    "#         #Zout = output_linear_layer.forward(Dout)\n",
    "#         #SigmaZout = helper.softmax(Zout)\n",
    "#         SigmaZout = Output_stack.forward(Dout)\n",
    "        \n",
    "#         taccuracies=[]\n",
    "#         for n in range(SigmaZout.shape[0]): \n",
    "#             len_phrase=SigmaZout.shape[1]\n",
    "#             counter_beccate=0 \n",
    "#             for l in range(SigmaZout.shape[1]): \n",
    "#                 if np.argmax(SigmaZout[n][l])==np.argmax(target_decoder[n][l]): \n",
    "#                     counter_beccate+=1\n",
    "#                     #print(np.argmax(SigmaZout[n][l]))\n",
    "#             phrase_accuracy=counter_beccate/len_phrase\n",
    "#             taccuracies.append(phrase_accuracy)\n",
    "\n",
    "#         accuracies.append(np.mean(np.array(taccuracies)))\n",
    "#         # if np.mean(np.array(accuracies))>0:\n",
    "#         #         print(np.mean(np.array(accuracies)))\n",
    "#                 #print(np.argmax(SigmaZout[n][l]),np.argmax(target_decoder[n][l]))\n",
    "#         #print(target_decoder)\n",
    "\n",
    "#         # for n in range(SigmaZout.shape[0]):\n",
    "#         #     phrase_len=SigmaZout.shape[1]\n",
    "#         #     phrase_accuracy=0\n",
    "#         #     counter_beccate=0\n",
    "#         #     for l in range(SigmaZout.shape[1]):\n",
    "#         #         if np.argmax(SigmaZout[n][l])==target_decoder[n][l]:\n",
    "#         #             counter_beccate+=1\n",
    "#         #             #print(target_decoder[n][l],np.argmax(SigmaZout[n][l]))\n",
    "#         #     phrase_accuracy=counter_beccate/phrase_len\n",
    "#         #     if(phrase_accuracy>0):\n",
    "#         #         pass\n",
    "#                 #print(\"phrase_accuracy\",phrase_accuracy)\n",
    "                \n",
    "#         #  #print(\"Dout\",Dout.shape)\n",
    "#         #print(\"SigmaZout\",SigmaZout.shape)\n",
    "#         #mean_acc+=(phrase_accuracy/counter_tot)\n",
    "#         #Loss=Output_stack.cross_entropy_loss(SigmaZout,target_decoder)\n",
    "#         #Loss=Output_stack.sparse_categorical_crossentropy(SigmaZout,target_decoder)\n",
    "#         #print(\"Loss\",Loss)\n",
    "\n",
    "\n",
    "\n",
    "#         Loss = Output_stack.cross_entropy_loss(SigmaZout,target_decoder)\n",
    "#         #dLoss_dZout = SigmaZout - target_decoder \n",
    "        \n",
    "#         #dLoss_dDout= output_linear_layer.grad(dLoss_dZout)\n",
    "#         #dL_dDout = dLoss_dZout @ output_linear_layer.W.T\n",
    "#         tot_loss_epoch+=Loss\n",
    "\n",
    "\n",
    "\n",
    "#         #dL_dDout = Output_stack.grad_sparse_cross_entropy(SigmaZout,target_decoder)\n",
    "#         dL_dDout = Output_stack.grad_cross_entropy(SigmaZout,target_decoder)\n",
    "        \n",
    "         \n",
    "\n",
    "\n",
    "#         dL_Ecout,dLoss_dWemb_encoder_tot,dLoss_dWemb_decoder_tot=MyTransformer.backpropagation(dL_dDout)\n",
    "#         #inputs_decoder=inputs_decoder-learning_rate*dLoss_dWemb_decoder\n",
    "#         #vocabulary=helper.update_wembedding_decoder(y_batch,inputs_decoder,words_per_phrase,vocabulary) \n",
    "#         #inputs_e=inputs_e-learning_rate*dLoss_dWemb_encoder\n",
    "#         #vocabulary=helper.update_wembedding_encoder(X_batch,inputs_e,vocabulary,words_per_phrase)\n",
    "#         #print(\"dL_Ecout\",dL_Ecout.shape)\n",
    "#         Output_stack.update_weights(learning_rate)\n",
    "#         vocabulary=helper.update_wembedding_decoder(learning_rate,y_batch, dLoss_dWemb_decoder_tot,vocabulary, max_v  )\n",
    "#         vocabulary=helper.update_wembedding_encoder(learning_rate,X_batch, dLoss_dWemb_encoder_tot,vocabulary,max_v)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024879bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "# inputs_e=cp.random.rand(4,11,300)\n",
    "# inputs_d=cp.random.rand(11,4,11,300)\n",
    "# target_i=cp.array([1, 2,3,4])\n",
    "# TransformerEncoder=Encoder(embedding_size,num_heads,fl1_size,learning_rate,batch_size=batch_size,words_per_phrase=max_v)\n",
    "# TransformerDecoder=Decoder(embedding_size,num_heads,fl1_size,learning_rate,batch_size=batch_size,words_per_phrase=max_v,vocabulary_size=len(vocabulary))\n",
    "# words_per_phrase = num_phrases= max_v\n",
    "# output_linear_layer=linear_layer(embedding_size,len(vocabulary),out=True)  \n",
    "# num_batches_per_epoch = len(X_train) // batch_size\n",
    "# num_epochs=550\n",
    "# tot_loss_epoch=0\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     print(\"Loss\",tot_loss_epoch/num_batches_per_epoch)\n",
    "#     tot_loss_epoch=0\n",
    "#     total_accuracy_epoch=0\n",
    "    \n",
    "#     for i in tqdm(range(0,num_batches_per_epoch),desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "#         #try: \n",
    "#         start = i * batch_size\n",
    "#         end = start + batch_size \n",
    "#         X_batch = X_train[start:end]\n",
    "#         y_batch = y_train[start:end] \n",
    "#         #print(\"y_batch\",y_batch)\n",
    "#         #print(X_batch)\n",
    "#         inputs_e=helper.create_input_encoder(X_batch,vocabulary,words_per_phrase,embedding_size) \n",
    "#         inputs_decoder=helper.create_decoder_input(y_batch,embedding_size,words_per_phrase,vocabulary) \n",
    "#         Ecout=TransformerEncoder.forward(inputs_e)\n",
    "        \n",
    "        \n",
    "#         #print(\"inputs_d.shape\",inputs_d.shape,\"inputs_e.shape\",inputs_e.shape) \n",
    "#         target_d=helper.create_target(y_batch,vocabulary,words_per_phrase,embedding_size)\n",
    "       \n",
    "\n",
    "#         Dout = TransformerDecoder.forward(inputs_decoder, Ecout)\n",
    "        # target_d=helper.pad_sequences(y_batch,lenght=words_per_phrase,target_type=\"target\") \n",
    "        # target_d=[re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', xx) for xx in target_d] \n",
    "        # tot_loss=0\n",
    "        # totdLoss_dAcr=0\n",
    "        # counter_correct=0\n",
    "        # total_accuracy_batch=0\n",
    "        # for step in range(0, inputs_d.shape[0]):\n",
    "        #     inputs_decoder = inputs_d[step]\n",
    "        #     target_i = cp.array([helper.get_one_hot(x[step], vocabulary) for x in target_d])\n",
    "        #     #target_i=cp.array([vocabulary[x[step]][1] for x in target_d])\n",
    "        #     Dout = TransformerDecoder.forward(inputs_decoder, Ecout)\n",
    "        #     Zout = output_linear_layer.forward(Dout)\n",
    "        #     SigmaZout = helper.softmax(Zout)\n",
    "        #     print(SigmaZout.shape)\n",
    "        #     # Calculate loss\n",
    "        #     Loss = helper.cross_entropy_loss(SigmaZout, target_i)\n",
    "        #     dLoss_dZout = SigmaZout - target_i \n",
    "        #     # Loss = helper.sparse_categorical_crossentropy(SigmaZout, target_i)  \n",
    "        #     # dLoss_dZout = SigmaZout.copy()  # Create a copy of the softmax output \n",
    "        #     # dLoss_dZout[np.arange(target_i.shape[0]), target_i] -= 1 \n",
    "\n",
    "        #     #helper.log_sparse_entropy(SigmaZout, target_i,y_batch,step)\n",
    "        #     print(\"Loss\",Loss)\n",
    "        #     print(\"dLoss_dZout\",dLoss_dZout.shape)\n",
    "            # Backpropagation\n",
    "            \n",
    "            # clip_value = 1.0\n",
    "            # dLoss_dZout = cp.clip(dLoss_dZout, -clip_value, clip_value)\n",
    "            #dLoss_dDout= output_linear_layer.grad(dLoss_dZout)\n",
    "            #dLoss_dDout = dLoss_dZout @ output_linear_layer.W.T\n",
    "            \n",
    "            # Backpropagation through decoder\n",
    "            # vocabulary, dLoss_dAcr = TransformerDecoder.backpropagation(dLoss_dDout, vocabulary, y_batch)\n",
    "            # tot_loss+=Loss \n",
    "            # batch_acc=helper.accruacy_sparse_entropy(SigmaZout,target_i)\n",
    "            # total_accuracy_batch+=batch_acc\n",
    "            # if batch_acc>0:\n",
    "            #     helper.print_target_vs_prediction_sparce_loss(SigmaZout,target_i)\n",
    "        #print(\"total_accuracy_batch\",total_accuracy_batch)\n",
    "        \n",
    "        # dLoss_dAcr = cp.clip(dLoss_dAcr, -clip_value, clip_value)\n",
    "        # dLoss_Ecout=cp.clip(dLoss_dAcr, -clip_value, clip_value)\n",
    "        # After decoder, calculate gradients for encoder\n",
    "        # dLoss_Ecout_k = TransformerDecoder.multihead_cross_attention.diffKInput(dLoss_dAcr, TransformerDecoder.Kc.W)\n",
    "        # dLoss_Ecout_v = TransformerDecoder.multihead_cross_attention.diffVInput(dLoss_dAcr, TransformerDecoder.Vc.W)\n",
    "        # dLoss_Ecout = dLoss_Ecout_k + dLoss_Ecout_v\n",
    "        \n",
    "        #     # Backpropagation through encoder\n",
    "        # vocabulary = TransformerEncoder.backpropagation(dLoss_Ecout, vocabulary, X_batch)\n",
    "        \n",
    "        # tot_loss_epoch+=tot_loss/inputs_d.shape[0] \n",
    "        # total_accuracy_epoch+=total_accuracy_batch\n",
    " \n",
    "        # except Exception as e:\n",
    "        #     print(e)\n",
    "        #     traceback.print_exc()  \n",
    "    # print(\"total_accuracy_epoch\",total_accuracy_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb80648f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
