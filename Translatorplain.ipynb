{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9b3f3e9",
   "metadata": {},
   "source": [
    "# Multi-Head Attention from Scratch Using CuPy\n",
    "\n",
    "In this notebook, we will implement the **Multi-Head Attention** mechanism from scratch using **CuPy**, a GPU-accelerated library similar to NumPy. The multi-head attention mechanism is a key component of the Transformer model, enabling it to attend to different parts of the input simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Overview of Multi-Head Attention\n",
    "\n",
    "Multi-head attention allows the model to have multiple attention \"heads,\" each of which focuses on different parts of the input sequence. Each head computes its own attention values, and the results are concatenated and transformed into the final output.\n",
    "\n",
    "The steps involved in multi-head attention:\n",
    "- **Linear transformations**: Apply learned weight matrices to the queries (Q), keys (K), and values (V).\n",
    "- **Scaled Dot-Product Attention**: For each head, compute the attention scores and apply them to the values.\n",
    "- **Concatenation**: Concatenate the outputs from all heads.\n",
    "- **Final Linear Transformation**: Apply a final linear transformation to the concatenated output.\n",
    "\n",
    "Mathematically, the output of the multi-head attention mechanism can be written as:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h)W^O\n",
    "$$\n",
    "where each attention head is computed as:\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$$\n",
    "---\n",
    "\n",
    "## 2. CuPy Setup\n",
    "\n",
    "Before we begin, make sure you have **CuPy** installed. You can install it via:\n",
    "\n",
    "```bash\n",
    "!pip install cupy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "db372a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 7, 10), (10, 10), (10, 10), (10, 10))"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "import numpy as np\n",
    "import re\n",
    "import cupy as cp\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np \n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jax\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "np.set_printoptions(edgeitems=30, linewidth=100000, formatter=dict(float=lambda x: \"%.3g\" % x)) \n",
    "def softmax(x, axis=-1):\n",
    "    # Subtract the max value for numerical stability\n",
    "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "num_classes=2\n",
    "word2vec_len = 10\n",
    "num_phrases = 3\n",
    "words_per_phrase = 7 \n",
    "dk = dv = 10\n",
    " \n",
    "num_heads=5\n",
    " \n",
    " \n",
    "inputs = np.random.rand(num_phrases,words_per_phrase, word2vec_len)\n",
    "target = softmax(np.random.rand(num_phrases,num_classes))\n",
    "\n",
    "Q = np.random.rand(word2vec_len, dk) / jnp.sqrt(word2vec_len)\n",
    "K = np.random.rand(word2vec_len, dk) / jnp.sqrt(word2vec_len)\n",
    "V = np.random.rand(word2vec_len, dv) / jnp.sqrt(word2vec_len)\n",
    "inputs.shape,Q.shape,K.shape,V.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26957032",
   "metadata": {},
   "source": [
    "### Consideriamo input formato da 3 frasi composte da 7 parole ciasuna ed ogni parola avente rappresentazione vettoriale di dimensione 10. Mentre vogliamo dopo l'attentione che ogni parola abbia rappresentazione vettoriale di 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "25dd2618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.218, 0.563, 0.196, 0.275, 0.568, 0.155, 0.426, 0.0049, 0.0325, 0.463],\n",
       "        [0.762, 0.183, 0.711, 0.974, 0.514, 0.945, 0.548, 0.534, 0.942, 0.313],\n",
       "        [0.999, 0.963, 0.784, 0.742, 0.302, 0.408, 0.382, 0.0891, 0.324, 0.414],\n",
       "        [0.35, 0.131, 0.818, 0.348, 0.508, 0.908, 0.41, 0.463, 0.907, 0.862],\n",
       "        [0.224, 0.312, 0.908, 0.26, 0.129, 0.0262, 0.0881, 0.778, 0.285, 0.946],\n",
       "        [0.611, 0.243, 0.21, 0.748, 0.684, 0.506, 0.42, 0.107, 0.7, 0.41],\n",
       "        [0.552, 0.159, 0.151, 0.51, 0.923, 0.934, 0.609, 0.809, 0.26, 0.0536]],\n",
       "\n",
       "       [[0.993, 0.179, 0.283, 0.982, 0.213, 0.898, 0.028, 0.295, 0.166, 0.795],\n",
       "        [0.297, 0.493, 0.0927, 0.986, 0.307, 0.0541, 0.404, 0.766, 0.0191, 0.981],\n",
       "        [0.24, 0.603, 0.928, 0.829, 0.719, 0.502, 0.742, 0.161, 0.611, 0.717],\n",
       "        [0.26, 0.547, 0.817, 0.995, 0.393, 0.628, 0.731, 0.446, 0.959, 0.918],\n",
       "        [0.641, 0.0695, 0.112, 0.784, 0.725, 0.718, 0.537, 0.0113, 0.916, 0.151],\n",
       "        [0.885, 0.336, 0.727, 0.314, 0.367, 0.152, 0.96, 0.774, 0.756, 0.0637],\n",
       "        [0.296, 0.924, 0.495, 0.444, 0.762, 0.218, 0.322, 0.335, 0.716, 0.757]],\n",
       "\n",
       "       [[0.887, 0.45, 0.738, 0.831, 0.0355, 0.357, 0.677, 0.494, 0.472, 0.709],\n",
       "        [0.107, 0.158, 0.563, 0.962, 0.173, 0.764, 0.335, 0.0469, 0.408, 0.526],\n",
       "        [0.506, 0.688, 0.019, 0.769, 0.0799, 0.896, 0.344, 0.656, 0.828, 0.45],\n",
       "        [0.821, 0.984, 0.222, 0.414, 0.941, 0.595, 0.601, 0.245, 0.839, 0.187],\n",
       "        [0.268, 0.377, 0.964, 0.779, 0.347, 0.827, 0.186, 0.974, 0.732, 0.488],\n",
       "        [0.995, 0.528, 0.856, 0.847, 0.968, 0.728, 0.116, 0.19, 0.293, 0.991],\n",
       "        [0.437, 0.813, 0.947, 0.208, 0.478, 0.267, 0.421, 0.93, 0.61, 0.181]]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs# each input phrase is made by 13 words having lenght 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de56657a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 7, 10), (10, 10))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape,Q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed92a667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[0.742, 0.839, 1.04, 0.927, 0.653, 0.579, 0.895, 0.808, 0.76, 0.755],\n",
       "        [0.661, 0.85, 1.2, 1.07, 0.849, 0.989, 0.633, 0.935, 0.788, 0.848],\n",
       "        [0.767, 0.645, 0.993, 0.99, 0.681, 0.824, 0.681, 0.756, 0.646, 0.9],\n",
       "        [1.02, 0.98, 1.34, 1.23, 1.12, 0.881, 1.03, 1.11, 0.822, 0.981],\n",
       "        [0.44, 0.514, 0.836, 0.715, 0.603, 0.705, 0.245, 0.771, 0.615, 0.659],\n",
       "        [0.563, 0.576, 0.978, 0.86, 0.807, 0.795, 0.698, 0.909, 0.614, 0.734],\n",
       "        [0.683, 0.659, 0.841, 0.931, 0.723, 0.691, 0.644, 0.795, 0.641, 0.546]],\n",
       "\n",
       "       [[0.664, 0.694, 0.943, 0.828, 0.772, 0.796, 0.384, 0.777, 0.553, 0.596],\n",
       "        [0.724, 0.785, 1.18, 1.05, 0.86, 0.949, 0.803, 1.07, 0.771, 0.818],\n",
       "        [0.928, 0.805, 1.13, 0.974, 0.959, 0.852, 0.681, 0.969, 0.696, 0.776],\n",
       "        [0.943, 0.821, 1.2, 1.01, 0.958, 0.905, 0.731, 1.07, 0.738, 0.881],\n",
       "        [0.987, 0.949, 1.42, 1.24, 0.987, 1.03, 0.98, 1.14, 0.911, 1.09],\n",
       "        [0.464, 0.583, 0.907, 0.601, 0.468, 0.562, 0.527, 0.733, 0.618, 0.734],\n",
       "        [0.818, 0.956, 1.19, 1.09, 0.881, 0.755, 0.866, 1.03, 0.953, 0.807]],\n",
       "\n",
       "       [[0.626, 0.639, 0.911, 0.861, 0.752, 0.672, 0.588, 0.701, 0.585, 0.668],\n",
       "        [0.374, 0.508, 0.821, 0.538, 0.616, 0.595, 0.311, 0.655, 0.457, 0.599],\n",
       "        [0.775, 0.743, 1.15, 0.954, 0.919, 0.876, 0.806, 0.996, 0.78, 0.887],\n",
       "        [0.871, 0.815, 1.19, 1.14, 1.03, 0.987, 0.703, 1.06, 0.751, 0.792],\n",
       "        [0.456, 0.459, 0.579, 0.597, 0.48, 0.487, 0.346, 0.514, 0.458, 0.411],\n",
       "        [0.792, 0.782, 1.13, 1.05, 0.896, 0.882, 0.676, 1.01, 0.796, 0.874],\n",
       "        [0.753, 0.926, 1.14, 1.01, 0.898, 0.713, 0.905, 0.906, 0.736, 0.724]]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.matmul(inputs, Q) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fedbac",
   "metadata": {},
   "source": [
    "### Avendo fissato il numero di teste per il attezione ogni matrice Qval, Kval, Vval viene suddivisa in 4 parti uguali di 2 colonne. Otteniamo un array di 4 elementi che per ogni frase riportano le prime due colonne come di seguito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eef75667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jnp.array_split(jnp.matmul(inputs, Q),num_heads,axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d76673db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[0.742, 0.839],\n",
       "        [0.661, 0.85],\n",
       "        [0.767, 0.645],\n",
       "        [1.02, 0.98],\n",
       "        [0.44, 0.514],\n",
       "        [0.563, 0.576],\n",
       "        [0.683, 0.659]],\n",
       "\n",
       "       [[0.664, 0.694],\n",
       "        [0.724, 0.785],\n",
       "        [0.928, 0.805],\n",
       "        [0.943, 0.821],\n",
       "        [0.987, 0.949],\n",
       "        [0.464, 0.583],\n",
       "        [0.818, 0.956]],\n",
       "\n",
       "       [[0.626, 0.639],\n",
       "        [0.374, 0.508],\n",
       "        [0.775, 0.743],\n",
       "        [0.871, 0.815],\n",
       "        [0.456, 0.459],\n",
       "        [0.792, 0.782],\n",
       "        [0.753, 0.926]]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.array_split(jnp.matmul(inputs, Q),num_heads,axis=2)[0]# so i have basically num_heads chuncks of the Qval this is a list not array structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b35421",
   "metadata": {},
   "source": [
    "### Ridimensioniamo l'array in modo che ogni frase contenga la lista dei rispettivi attention heads, ottenendo 3 frasi contententi 4 attention heads che hanno dimensione 7 (come il numero di parole per ogni frase) per 2 (fetta di embedding assegnata ad ogni head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34aa11cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5, 7, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs, Q),num_heads,axis=2)), 0, 1).shape#  # here i actually transform it to a structure Qval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd7464e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[0.742, 0.839],\n",
       "        [0.661, 0.85],\n",
       "        [0.767, 0.645],\n",
       "        [1.02, 0.98],\n",
       "        [0.44, 0.514],\n",
       "        [0.563, 0.576],\n",
       "        [0.683, 0.659]],\n",
       "\n",
       "       [[1.04, 0.927],\n",
       "        [1.2, 1.07],\n",
       "        [0.993, 0.99],\n",
       "        [1.34, 1.23],\n",
       "        [0.836, 0.715],\n",
       "        [0.978, 0.86],\n",
       "        [0.841, 0.931]],\n",
       "\n",
       "       [[0.653, 0.579],\n",
       "        [0.849, 0.989],\n",
       "        [0.681, 0.824],\n",
       "        [1.12, 0.881],\n",
       "        [0.603, 0.705],\n",
       "        [0.807, 0.795],\n",
       "        [0.723, 0.691]],\n",
       "\n",
       "       [[0.895, 0.808],\n",
       "        [0.633, 0.935],\n",
       "        [0.681, 0.756],\n",
       "        [1.03, 1.11],\n",
       "        [0.245, 0.771],\n",
       "        [0.698, 0.909],\n",
       "        [0.644, 0.795]],\n",
       "\n",
       "       [[0.76, 0.755],\n",
       "        [0.788, 0.848],\n",
       "        [0.646, 0.9],\n",
       "        [0.822, 0.981],\n",
       "        [0.615, 0.659],\n",
       "        [0.614, 0.734],\n",
       "        [0.641, 0.546]]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs, Q),num_heads,axis=2)), 0, 1)[0]# refer to cell jnp.matmul(inputs, Q) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60adc1fc-9393-47c0-a181-ed799c4d3b0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qval.shape:  (3, 5, 7, 2)\n",
      "Kval.shape:  (3, 5, 7, 2)\n",
      "Vval.shape:  (3, 5, 7, 2)\n"
     ]
    }
   ],
   "source": [
    "Qval = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs, Q),num_heads,axis=2)), 0, 1)\n",
    "print(\"Qval.shape: \",Qval.shape)\n",
    "\n",
    "Kval = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs, K),num_heads,axis=2)), 0, 1)\n",
    "print(\"Kval.shape: \",Kval.shape)\n",
    "\n",
    "\n",
    "Vval = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs,V),num_heads,axis=2)), 0, 1)\n",
    "print(\"Vval.shape: \",Vval.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b98a1d",
   "metadata": {},
   "source": [
    "### Per calcolare ora i pesi dell'attenzione applichiamo la formua \n",
    "\n",
    "$$\n",
    "  \\frac{QK^T}{\\sqrt{d_k}}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6c38c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[0.742, 0.839],\n",
       "        [0.661, 0.85],\n",
       "        [0.767, 0.645],\n",
       "        [1.02, 0.98],\n",
       "        [0.44, 0.514],\n",
       "        [0.563, 0.576],\n",
       "        [0.683, 0.659]], dtype=float32),\n",
       " Array([[0.847, 0.728],\n",
       "        [1.05, 0.979],\n",
       "        [0.868, 0.849],\n",
       "        [1.23, 1.22],\n",
       "        [0.807, 0.543],\n",
       "        [1, 0.869],\n",
       "        [0.731, 0.734]], dtype=float32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qval[0][0],Kval[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e02d8827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[0.742, 0.839],\n",
       "        [0.661, 0.85],\n",
       "        [0.767, 0.645],\n",
       "        [1.02, 0.98],\n",
       "        [0.44, 0.514],\n",
       "        [0.563, 0.576],\n",
       "        [0.683, 0.659]], dtype=float32),\n",
       " Array([[0.847, 1.05, 0.868, 1.23, 0.807, 1, 0.731],\n",
       "        [0.728, 0.979, 0.849, 1.22, 0.543, 0.869, 0.734]], dtype=float32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qval[0][0],np.transpose(Kval, (0, 1, 3, 2))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44bbb725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.392, 0.506, 0.429, 0.611, 0.333, 0.466, 0.366],\n",
       "       [0.373, 0.483, 0.41, 0.584, 0.315, 0.443, 0.35],\n",
       "       [0.354, 0.455, 0.384, 0.546, 0.307, 0.421, 0.327],\n",
       "       [0.498, 0.642, 0.542, 0.772, 0.428, 0.592, 0.463],\n",
       "       [0.236, 0.306, 0.259, 0.369, 0.201, 0.281, 0.221],\n",
       "       [0.283, 0.365, 0.309, 0.44, 0.242, 0.337, 0.264],\n",
       "       [0.335, 0.431, 0.364, 0.519, 0.288, 0.398, 0.311]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qval[0][0]@jnp.transpose(Kval, (0, 1, 3, 2))[0][0]/ jnp.sqrt(dk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35bec325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5, 7, 7)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QKscaled = jnp.matmul(Qval, jnp.transpose(Kval, (0, 1, 3, 2))) / jnp.sqrt(dk)\n",
    " \n",
    "QKscaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14f39e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.392, 0.506, 0.429, 0.611, 0.333, 0.466, 0.366],\n",
       "       [0.373, 0.483, 0.41, 0.584, 0.315, 0.443, 0.35],\n",
       "       [0.354, 0.455, 0.384, 0.546, 0.307, 0.421, 0.327],\n",
       "       [0.498, 0.642, 0.542, 0.772, 0.428, 0.592, 0.463],\n",
       "       [0.236, 0.306, 0.259, 0.369, 0.201, 0.281, 0.221],\n",
       "       [0.283, 0.365, 0.309, 0.44, 0.242, 0.337, 0.264],\n",
       "       [0.335, 0.431, 0.364, 0.519, 0.288, 0.398, 0.311]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QKscaled[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d505c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention_weights shape: (3, 5, 7, 7)\n"
     ]
    }
   ],
   "source": [
    "Attention_weights = softmax(QKscaled)\n",
    "print(\"Attention_weights shape:\",Attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52a8b7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention shape: (3, 5, 7, 2)\n"
     ]
    }
   ],
   "source": [
    "Attention = jnp.matmul(Attention_weights, Vval)\n",
    "print(\"Attention shape:\",Attention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ec19e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[0.959, 0.954],\n",
       "        [0.958, 0.954],\n",
       "        [0.956, 0.953],\n",
       "        [0.963, 0.958],\n",
       "        [0.952, 0.949],\n",
       "        [0.954, 0.95],\n",
       "        [0.956, 0.952]],\n",
       "\n",
       "       [[0.736, 0.842],\n",
       "        [0.738, 0.845],\n",
       "        [0.736, 0.843],\n",
       "        [0.739, 0.847],\n",
       "        [0.734, 0.839],\n",
       "        [0.736, 0.842],\n",
       "        [0.735, 0.841]],\n",
       "\n",
       "       [[0.996, 0.671],\n",
       "        [0.998, 0.672],\n",
       "        [0.997, 0.672],\n",
       "        [0.999, 0.673],\n",
       "        [0.996, 0.671],\n",
       "        [0.997, 0.672],\n",
       "        [0.997, 0.671]],\n",
       "\n",
       "       [[0.586, 0.927],\n",
       "        [0.585, 0.927],\n",
       "        [0.585, 0.926],\n",
       "        [0.588, 0.93],\n",
       "        [0.583, 0.923],\n",
       "        [0.585, 0.927],\n",
       "        [0.585, 0.926]],\n",
       "\n",
       "       [[0.678, 0.897],\n",
       "        [0.679, 0.898],\n",
       "        [0.678, 0.896],\n",
       "        [0.679, 0.899],\n",
       "        [0.678, 0.895],\n",
       "        [0.678, 0.895],\n",
       "        [0.677, 0.894]]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10742435",
   "metadata": {},
   "source": [
    "### Now for tetrieving the attention correct size we need to horizontaly concatenate the attention output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c3a9217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[[0.959, 0.954, 0.736, 0.842, 0.996, 0.671, 0.586, 0.927, 0.678, 0.897],\n",
       "         [0.958, 0.954, 0.738, 0.845, 0.998, 0.672, 0.585, 0.927, 0.679, 0.898],\n",
       "         [0.956, 0.953, 0.736, 0.843, 0.997, 0.672, 0.585, 0.926, 0.678, 0.896],\n",
       "         [0.963, 0.958, 0.739, 0.847, 0.999, 0.673, 0.588, 0.93, 0.679, 0.899],\n",
       "         [0.952, 0.949, 0.734, 0.839, 0.996, 0.671, 0.583, 0.923, 0.678, 0.895],\n",
       "         [0.954, 0.95, 0.736, 0.842, 0.997, 0.672, 0.585, 0.927, 0.678, 0.895],\n",
       "         [0.956, 0.952, 0.735, 0.841, 0.997, 0.671, 0.585, 0.926, 0.677, 0.894]],\n",
       " \n",
       "        [[1.02, 1.06, 0.787, 0.83, 1.09, 0.707, 0.661, 0.974, 0.779, 0.996],\n",
       "         [1.02, 1.06, 0.789, 0.834, 1.09, 0.708, 0.664, 0.978, 0.781, 0.998],\n",
       "         [1.02, 1.06, 0.789, 0.833, 1.09, 0.708, 0.663, 0.977, 0.78, 0.998],\n",
       "         [1.02, 1.06, 0.789, 0.834, 1.09, 0.708, 0.663, 0.978, 0.781, 0.998],\n",
       "         [1.02, 1.07, 0.791, 0.837, 1.09, 0.708, 0.664, 0.979, 0.782, 1],\n",
       "         [1.02, 1.06, 0.786, 0.828, 1.08, 0.706, 0.662, 0.974, 0.78, 0.997],\n",
       "         [1.02, 1.06, 0.79, 0.834, 1.09, 0.707, 0.664, 0.978, 0.781, 0.999]],\n",
       " \n",
       "        [[0.907, 0.929, 0.743, 0.772, 0.941, 0.671, 0.562, 0.888, 0.691, 0.852],\n",
       "         [0.902, 0.925, 0.74, 0.768, 0.939, 0.67, 0.56, 0.885, 0.69, 0.85],\n",
       "         [0.91, 0.932, 0.746, 0.775, 0.944, 0.673, 0.565, 0.893, 0.694, 0.856],\n",
       "         [0.911, 0.934, 0.747, 0.777, 0.946, 0.675, 0.565, 0.893, 0.693, 0.855],\n",
       "         [0.903, 0.925, 0.738, 0.766, 0.936, 0.668, 0.56, 0.884, 0.689, 0.848],\n",
       "         [0.91, 0.933, 0.746, 0.776, 0.944, 0.673, 0.565, 0.892, 0.694, 0.856],\n",
       "         [0.911, 0.934, 0.746, 0.776, 0.942, 0.672, 0.565, 0.892, 0.692, 0.854]]], dtype=float32),\n",
       " (3, 7, 10),\n",
       " (3, 7, 10))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention=jnp.array([jnp.concatenate(Attention[i], axis=1) for i in range(num_phrases)])\n",
    "Attention,Attention.shape,inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb48e5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 10, 10), (3, 1, 10))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linearlayer= np.random.rand(num_phrases,dv, word2vec_len)   \n",
    "linear_bias = np.random.rand(num_phrases,1,word2vec_len)\n",
    "linearlayer.shape,linear_bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bd1665b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.149, 0.965, 0.688, 0.605, 0.158, 0.303, 0.201, 0.959, 0.635, 0.277]],\n",
       "\n",
       "       [[0.0505, 0.2, 0.491, 0.996, 0.123, 0.332, 0.941, 0.714, 0.573, 0.0514]],\n",
       "\n",
       "       [[0.49, 0.273, 0.311, 0.749, 0.808, 0.119, 0.587, 0.472, 0.206, 0.71]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1bebf84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[1.37, -1.24, -0.374, 0.015, -0.713, 0.942, -1.28, 0.597, -0.849, 1.53],\n",
       "        [1.66, 1.28, 1.02, -0.0108, -0.612, -1.48, -0.235, -1.21, -0.647, 0.237],\n",
       "        [0.201, -0.88, 1.31, 1.53, -0.0492, -0.848, 0.789, -0.75, -1.74, 0.437],\n",
       "        [0.406, -0.243, -1.89, 0.863, 0.165, -1.44, -0.0345, 1.41, -0.353, 1.12],\n",
       "        [-0.502, 0.561, 1.41, -0.722, 1.15, -1, -1.54, 0.0923, 1.29, -0.735],\n",
       "        [0.313, 1.08, -0.648, 1.51, 0.494, -1.31, -1.89, -0.363, 0.255, 0.561],\n",
       "        [0.537, 0.639, 0.987, -0.541, -0.1, -1.06, -1.14, 1.39, -1.66, 0.955]],\n",
       "\n",
       "       [[0.842, 1.43, 0.318, -1.15, 0.988, -1.57, 0.821, 0.0974, -1.14, -0.643],\n",
       "        [1.23, 1.21, 0.241, 0.842, 0.565, -1.21, -1.94, -0.278, -0.769, 0.116],\n",
       "        [-0.277, 1.09, -0.493, -0.871, 1.69, -0.454, 0.702, 0.704, -1.91, -0.187],\n",
       "        [-0.864, 0.762, -0.412, 0.307, 1.79, -0.417, 0.0304, 1.34, -1.24, -1.3],\n",
       "        [1.35, -0.467, 0.158, 0.316, 0.889, -0.273, -0.296, -1.86, -1.22, 1.4],\n",
       "        [0.283, -0.315, -0.144, 0.476, 1.11, 1.13, -2.14, -0.907, 1.13, -0.626],\n",
       "        [0.053, 0.173, 0.425, -1.48, -0.521, 0.551, -1.93, 1.33, 0.16, 1.23]],\n",
       "\n",
       "       [[1.23, 0.26, -0.118, -0.724, 0.128, -1.47, 0.0475, -1.09, -0.368, 2.1],\n",
       "        [0.219, 1.42, -1.06, 0.092, 1.05, -1.03, -0.771, -0.68, 1.66, -0.901],\n",
       "        [-1.37, 1.53, -0.402, 1.42, 0.136, 0.687, -1.64, -0.00297, -0.605, 0.249],\n",
       "        [1.05, 1.03, 0.423, -0.934, 1.21, -1.72, -0.274, -0.511, -1.13, 0.866],\n",
       "        [-0.615, 1.07, 1.45, -0.768, -0.315, -0.463, -0.723, 1.83, -1.25, -0.218],\n",
       "        [-1.25, 0.588, 1.12, 0.24, 0.331, -1.05, -1.34, 1.79, 0.258, -0.686],\n",
       "        [1.25, 0.775, -1.56, 0.25, -1.29, -0.693, -0.596, 1.44, -0.379, 0.79]]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def layer_norm(x, epsilon=1e-6):\n",
    "    # Calculate the mean and variance\n",
    "        mean = jnp.mean(x, axis=-1, keepdims=True)\n",
    "        var = jnp.var(x, axis=-1, keepdims=True) \n",
    "        # Normalize the output\n",
    "        x_norm = (x - mean) / jnp.sqrt(var + epsilon) \n",
    "        return x_norm\n",
    "\n",
    "\n",
    "#output_sublayer_one=layer_norm((Attention@linearlayer +linear_bias)+inputs)\n",
    "output_sublayer_one=layer_norm(Attention+inputs)\n",
    "output_sublayer_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac1a52a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 7, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sublayer_one.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c41521b",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c028923",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_word_embedding_size=10\n",
    "decoder_input_number_of_words_per_phrase=9\n",
    "num_heads_decoder=5# dv=10 \n",
    "dv_decoder=10\n",
    "inputs_decoder = np.random.rand(num_phrases,decoder_input_number_of_words_per_phrase, decoder_input_word_embedding_size)# for the target language suppose the \n",
    "target_decoder = inputs_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcdc4273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.519, 0.655, 0.631, 0.843, 0.515, 0.233, 0.721, 0.719, 0.754, 0.287],\n",
       "        [0.298, 0.412, 0.777, 0.535, 0.0521, 0.413, 0.804, 0.803, 0.689, 0.6],\n",
       "        [0.705, 0.191, 0.392, 0.681, 0.416, 0.42, 0.734, 0.14, 0.159, 0.259],\n",
       "        [0.763, 0.423, 0.091, 0.569, 0.328, 0.198, 0.768, 0.434, 0.678, 0.899],\n",
       "        [0.165, 0.115, 0.582, 0.133, 0.693, 0.453, 0.243, 0.962, 0.457, 0.185],\n",
       "        [0.267, 0.0362, 0.547, 0.0591, 0.605, 0.792, 0.415, 0.299, 0.986, 0.6],\n",
       "        [0.425, 0.335, 0.154, 0.462, 0.905, 0.116, 0.595, 0.893, 0.7, 0.437],\n",
       "        [0.476, 0.295, 0.0113, 0.51, 0.949, 0.671, 0.707, 0.229, 0.652, 0.348],\n",
       "        [0.31, 0.0342, 0.732, 0.503, 0.824, 0.907, 0.793, 0.277, 0.686, 0.418]],\n",
       "\n",
       "       [[0.706, 0.76, 0.217, 0.297, 0.938, 0.124, 0.461, 0.0958, 0.584, 0.281],\n",
       "        [0.842, 0.261, 0.798, 0.453, 0.259, 0.844, 0.217, 0.543, 0.392, 0.149],\n",
       "        [0.922, 0.227, 0.287, 0.778, 0.393, 0.94, 0.199, 0.305, 0.82, 0.842],\n",
       "        [0.818, 0.673, 0.232, 0.867, 0.196, 0.603, 0.518, 0.897, 0.93, 0.104],\n",
       "        [0.483, 0.841, 0.0414, 0.286, 0.361, 0.347, 0.354, 0.962, 0.711, 0.911],\n",
       "        [0.995, 0.151, 0.902, 0.658, 0.654, 0.274, 0.731, 0.561, 0.429, 0.541],\n",
       "        [0.25, 0.92, 0.709, 0.0121, 0.407, 0.523, 0.533, 0.0563, 0.936, 0.0357],\n",
       "        [0.607, 0.967, 0.872, 0.261, 0.601, 0.336, 0.502, 0.592, 0.326, 0.211],\n",
       "        [0.671, 0.0263, 0.277, 0.388, 0.578, 0.959, 0.537, 0.857, 0.582, 0.219]],\n",
       "\n",
       "       [[0.0336, 0.815, 0.493, 0.561, 0.69, 0.44, 0.13, 0.195, 0.33, 0.492],\n",
       "        [0.617, 0.65, 0.199, 0.0799, 0.394, 0.808, 0.226, 0.392, 0.85, 0.868],\n",
       "        [0.172, 0.0728, 0.0955, 0.574, 0.0192, 0.862, 0.476, 0.645, 0.329, 0.695],\n",
       "        [0.331, 0.962, 0.122, 0.856, 0.101, 0.616, 0.616, 0.43, 0.14, 0.932],\n",
       "        [0.463, 0.109, 0.843, 0.139, 0.231, 0.999, 0.567, 0.0678, 0.622, 0.522],\n",
       "        [0.693, 0.347, 0.378, 0.248, 0.208, 0.426, 0.627, 0.326, 0.0658, 0.534],\n",
       "        [0.275, 0.569, 0.393, 0.782, 0.196, 0.925, 0.939, 0.751, 0.333, 0.83],\n",
       "        [0.0969, 0.967, 0.42, 0.43, 0.828, 0.719, 0.428, 0.223, 0.103, 0.518],\n",
       "        [0.852, 0.138, 0.946, 0.315, 0.516, 0.0162, 0.351, 0.702, 0.815, 0.806]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49030921",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "input_translation=[]\n",
    "def pad_sequence(seq, max_len, pad_value=0):\n",
    "    \"\"\"Pad a sequence with a given value up to max_len.\"\"\"\n",
    "    current_len = seq.shape[0]\n",
    "    pad_width = max_len - current_len\n",
    "    if pad_width > 0:\n",
    "        # Pad sequence with zeros (or any pad_value you provide)\n",
    "        seq = jnp.pad(seq, ((0, pad_width), (0, 0)), mode='constant', constant_values=pad_value)\n",
    "    return seq\n",
    "\n",
    "# Example usage:\n",
    "max_len = decoder_input_number_of_words_per_phrase  # Max sequence length for decoder input\n",
    " \n",
    "for j in range(inputs_decoder.shape[0]):\n",
    "    # Create padded sequences\n",
    "    padded_sequences = [pad_sequence(inputs_decoder[j][0:i], max_len) for i in range(1, inputs_decoder.shape[1] + 1)]\n",
    "    input_translation.append(padded_sequences)\n",
    "\n",
    "\n",
    "# Convert to an array for batching\n",
    "input_translation = jnp.array(input_translation)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70947e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 9, 9, 10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_translation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04e7fbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[0.519, 0.655, 0.631, 0.843, 0.515, 0.233, 0.721, 0.719, 0.754, 0.287],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0.519, 0.655, 0.631, 0.843, 0.515, 0.233, 0.721, 0.719, 0.754, 0.287],\n",
       "        [0.298, 0.412, 0.777, 0.535, 0.0521, 0.413, 0.804, 0.803, 0.689, 0.6],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0.519, 0.655, 0.631, 0.843, 0.515, 0.233, 0.721, 0.719, 0.754, 0.287],\n",
       "        [0.298, 0.412, 0.777, 0.535, 0.0521, 0.413, 0.804, 0.803, 0.689, 0.6],\n",
       "        [0.705, 0.191, 0.392, 0.681, 0.416, 0.42, 0.734, 0.14, 0.159, 0.259],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0.519, 0.655, 0.631, 0.843, 0.515, 0.233, 0.721, 0.719, 0.754, 0.287],\n",
       "        [0.298, 0.412, 0.777, 0.535, 0.0521, 0.413, 0.804, 0.803, 0.689, 0.6],\n",
       "        [0.705, 0.191, 0.392, 0.681, 0.416, 0.42, 0.734, 0.14, 0.159, 0.259],\n",
       "        [0.763, 0.423, 0.091, 0.569, 0.328, 0.198, 0.768, 0.434, 0.678, 0.899],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0.519, 0.655, 0.631, 0.843, 0.515, 0.233, 0.721, 0.719, 0.754, 0.287],\n",
       "        [0.298, 0.412, 0.777, 0.535, 0.0521, 0.413, 0.804, 0.803, 0.689, 0.6],\n",
       "        [0.705, 0.191, 0.392, 0.681, 0.416, 0.42, 0.734, 0.14, 0.159, 0.259],\n",
       "        [0.763, 0.423, 0.091, 0.569, 0.328, 0.198, 0.768, 0.434, 0.678, 0.899],\n",
       "        [0.165, 0.115, 0.582, 0.133, 0.693, 0.453, 0.243, 0.962, 0.457, 0.185],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0.519, 0.655, 0.631, 0.843, 0.515, 0.233, 0.721, 0.719, 0.754, 0.287],\n",
       "        [0.298, 0.412, 0.777, 0.535, 0.0521, 0.413, 0.804, 0.803, 0.689, 0.6],\n",
       "        [0.705, 0.191, 0.392, 0.681, 0.416, 0.42, 0.734, 0.14, 0.159, 0.259],\n",
       "        [0.763, 0.423, 0.091, 0.569, 0.328, 0.198, 0.768, 0.434, 0.678, 0.899],\n",
       "        [0.165, 0.115, 0.582, 0.133, 0.693, 0.453, 0.243, 0.962, 0.457, 0.185],\n",
       "        [0.267, 0.0362, 0.547, 0.0591, 0.605, 0.792, 0.415, 0.299, 0.986, 0.6],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0.519, 0.655, 0.631, 0.843, 0.515, 0.233, 0.721, 0.719, 0.754, 0.287],\n",
       "        [0.298, 0.412, 0.777, 0.535, 0.0521, 0.413, 0.804, 0.803, 0.689, 0.6],\n",
       "        [0.705, 0.191, 0.392, 0.681, 0.416, 0.42, 0.734, 0.14, 0.159, 0.259],\n",
       "        [0.763, 0.423, 0.091, 0.569, 0.328, 0.198, 0.768, 0.434, 0.678, 0.899],\n",
       "        [0.165, 0.115, 0.582, 0.133, 0.693, 0.453, 0.243, 0.962, 0.457, 0.185],\n",
       "        [0.267, 0.0362, 0.547, 0.0591, 0.605, 0.792, 0.415, 0.299, 0.986, 0.6],\n",
       "        [0.425, 0.335, 0.154, 0.462, 0.905, 0.116, 0.595, 0.893, 0.7, 0.437],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0.519, 0.655, 0.631, 0.843, 0.515, 0.233, 0.721, 0.719, 0.754, 0.287],\n",
       "        [0.298, 0.412, 0.777, 0.535, 0.0521, 0.413, 0.804, 0.803, 0.689, 0.6],\n",
       "        [0.705, 0.191, 0.392, 0.681, 0.416, 0.42, 0.734, 0.14, 0.159, 0.259],\n",
       "        [0.763, 0.423, 0.091, 0.569, 0.328, 0.198, 0.768, 0.434, 0.678, 0.899],\n",
       "        [0.165, 0.115, 0.582, 0.133, 0.693, 0.453, 0.243, 0.962, 0.457, 0.185],\n",
       "        [0.267, 0.0362, 0.547, 0.0591, 0.605, 0.792, 0.415, 0.299, 0.986, 0.6],\n",
       "        [0.425, 0.335, 0.154, 0.462, 0.905, 0.116, 0.595, 0.893, 0.7, 0.437],\n",
       "        [0.476, 0.295, 0.0113, 0.51, 0.949, 0.671, 0.707, 0.229, 0.652, 0.348],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0.519, 0.655, 0.631, 0.843, 0.515, 0.233, 0.721, 0.719, 0.754, 0.287],\n",
       "        [0.298, 0.412, 0.777, 0.535, 0.0521, 0.413, 0.804, 0.803, 0.689, 0.6],\n",
       "        [0.705, 0.191, 0.392, 0.681, 0.416, 0.42, 0.734, 0.14, 0.159, 0.259],\n",
       "        [0.763, 0.423, 0.091, 0.569, 0.328, 0.198, 0.768, 0.434, 0.678, 0.899],\n",
       "        [0.165, 0.115, 0.582, 0.133, 0.693, 0.453, 0.243, 0.962, 0.457, 0.185],\n",
       "        [0.267, 0.0362, 0.547, 0.0591, 0.605, 0.792, 0.415, 0.299, 0.986, 0.6],\n",
       "        [0.425, 0.335, 0.154, 0.462, 0.905, 0.116, 0.595, 0.893, 0.7, 0.437],\n",
       "        [0.476, 0.295, 0.0113, 0.51, 0.949, 0.671, 0.707, 0.229, 0.652, 0.348],\n",
       "        [0.31, 0.0342, 0.732, 0.503, 0.824, 0.907, 0.793, 0.277, 0.686, 0.418]]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_translation[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ae7156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26a41656",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_decoder=input_translation[0]\n",
    "\n",
    "Q_decoder = np.random.rand(decoder_input_word_embedding_size, dv_decoder) / jnp.sqrt(decoder_input_word_embedding_size)\n",
    "K_decoder = np.random.rand(decoder_input_word_embedding_size, dv_decoder) / jnp.sqrt(decoder_input_word_embedding_size)\n",
    "V_decoder = np.random.rand(decoder_input_word_embedding_size, dv_decoder) / jnp.sqrt(decoder_input_word_embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ec1676b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_decoder.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60a0ed3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qval.shape:  (9, 9, 10)\n"
     ]
    }
   ],
   "source": [
    "Qval_decoder=input_decoder@Q_decoder\n",
    "print(\"Qval.shape: \",Qval_decoder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5fed93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qval.shape:  (9, 5, 9, 2)\n",
      "Kval.shape:  (9, 5, 9, 2)\n",
      "Vval.shape:  (9, 5, 9, 2)\n"
     ]
    }
   ],
   "source": [
    "Qval_decoder  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(input_decoder, Q_decoder),num_heads_decoder,axis=2)), 0, 1)\n",
    "print(\"Qval.shape: \",Qval_decoder.shape)\n",
    "\n",
    "Kval_decoder  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(input_decoder, K_decoder),num_heads_decoder,axis=2)), 0, 1)\n",
    "print(\"Kval.shape: \",Kval_decoder.shape)\n",
    "\n",
    "\n",
    "Vval_decoder  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(input_decoder,V_decoder),num_heads_decoder,axis=2)), 0, 1)\n",
    "print(\"Vval.shape: \",Vval_decoder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2df93660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[-1e+09, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.445, -1e+09, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.304, 0.282, -1e+09, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.455, 0.418, 0.299, -1e+09, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, -1e+09, -inf, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, -1e+09, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, -1e+09, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, 0, -1e+09, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, -1e+09]],\n",
       "\n",
       "       [[-1e+09, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.533, -1e+09, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.352, 0.336, -1e+09, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.399, 0.383, 0.323, -1e+09, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, -1e+09, -inf, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, -1e+09, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, -1e+09, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, 0, -1e+09, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, -1e+09]],\n",
       "\n",
       "       [[-1e+09, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.478, -1e+09, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.366, 0.327, -1e+09, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.43, 0.38, 0.312, -1e+09, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, -1e+09, -inf, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, -1e+09, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, -1e+09, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, 0, -1e+09, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, -1e+09]],\n",
       "\n",
       "       [[-1e+09, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.626, -1e+09, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.462, 0.404, -1e+09, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.625, 0.547, 0.427, -1e+09, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, -1e+09, -inf, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, -1e+09, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, -1e+09, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, 0, -1e+09, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, -1e+09]],\n",
       "\n",
       "       [[-1e+09, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.682, -1e+09, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.511, 0.461, -1e+09, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0.672, 0.607, 0.406, -1e+09, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, -1e+09, -inf, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, -1e+09, -inf, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, -1e+09, -inf, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, 0, -1e+09, -inf],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, -1e+09]]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QKscaled_decoder  = jnp.matmul(Qval_decoder, jnp.transpose(Kval_decoder, (0, 1, 3, 2))) / jnp.sqrt(dk) + jnp.triu(jnp.ones((9, 9)))* -1e9 \n",
    "# Step 1: Create a causal mask of shape (1, 1, 9, 9) to broadcast across heads and batch\n",
    "mask = jnp.tril(jnp.ones((max_len, max_len)))  # (9, 9) lower triangular matrix\n",
    "mask = mask.at[mask == 0].set(-jnp.inf)  # Set future tokens to -inf\n",
    "mask = mask.at[mask == 1].set(0)  # Set allowed tokens to 0\n",
    "mask = mask.reshape(1, 1, max_len, max_len)  # Reshape to (1, 1, 9, 9)\n",
    "\n",
    "# Step 2: Apply mask to QKscaled_decoder (it will broadcast across batch and heads)\n",
    "QKscaled_decoder = QKscaled_decoder + mask \n",
    "QKscaled_decoder[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d5e0fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 5, 9, 9)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention_weights = softmax(QKscaled_decoder)\n",
    "Attention_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3feab27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[[0.939, 1.07],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.929, 1.18],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[1.08, 0.805],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.825, 0.794],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.681, 1.27],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0.939, 1.07],\n",
       "         [0.859, 1.08],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.929, 1.18],\n",
       "         [0.895, 1.01],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[1.08, 0.805],\n",
       "         [1.06, 0.918],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.825, 0.794],\n",
       "         [0.745, 0.611],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.681, 1.27],\n",
       "         [0.609, 1.23],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0.939, 1.07],\n",
       "         [0.859, 1.08],\n",
       "         [0.714, 0.877],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.929, 1.18],\n",
       "         [0.895, 1.01],\n",
       "         [0.66, 0.827],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[1.08, 0.805],\n",
       "         [1.06, 0.918],\n",
       "         [0.73, 0.546],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.825, 0.794],\n",
       "         [0.745, 0.611],\n",
       "         [0.598, 0.588],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.681, 1.27],\n",
       "         [0.609, 1.23],\n",
       "         [0.433, 0.999],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0.939, 1.07],\n",
       "         [0.859, 1.08],\n",
       "         [0.714, 0.877],\n",
       "         [0.859, 1.02],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.929, 1.18],\n",
       "         [0.895, 1.01],\n",
       "         [0.66, 0.827],\n",
       "         [0.864, 0.973],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[1.08, 0.805],\n",
       "         [1.06, 0.918],\n",
       "         [0.73, 0.546],\n",
       "         [0.864, 0.763],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.825, 0.794],\n",
       "         [0.745, 0.611],\n",
       "         [0.598, 0.588],\n",
       "         [0.606, 0.603],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.681, 1.27],\n",
       "         [0.609, 1.23],\n",
       "         [0.433, 0.999],\n",
       "         [0.478, 1.16],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0.939, 1.07],\n",
       "         [0.859, 1.08],\n",
       "         [0.714, 0.877],\n",
       "         [0.859, 1.02],\n",
       "         [0.588, 0.746],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.929, 1.18],\n",
       "         [0.895, 1.01],\n",
       "         [0.66, 0.827],\n",
       "         [0.864, 0.973],\n",
       "         [0.685, 0.869],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[1.08, 0.805],\n",
       "         [1.06, 0.918],\n",
       "         [0.73, 0.546],\n",
       "         [0.864, 0.763],\n",
       "         [0.836, 0.666],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.825, 0.794],\n",
       "         [0.745, 0.611],\n",
       "         [0.598, 0.588],\n",
       "         [0.606, 0.603],\n",
       "         [0.514, 0.425],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.681, 1.27],\n",
       "         [0.609, 1.23],\n",
       "         [0.433, 0.999],\n",
       "         [0.478, 1.16],\n",
       "         [0.585, 0.93],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0.939, 1.07],\n",
       "         [0.859, 1.08],\n",
       "         [0.714, 0.877],\n",
       "         [0.859, 1.02],\n",
       "         [0.588, 0.746],\n",
       "         [0.576, 0.808],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.929, 1.18],\n",
       "         [0.895, 1.01],\n",
       "         [0.66, 0.827],\n",
       "         [0.864, 0.973],\n",
       "         [0.685, 0.869],\n",
       "         [0.683, 0.886],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[1.08, 0.805],\n",
       "         [1.06, 0.918],\n",
       "         [0.73, 0.546],\n",
       "         [0.864, 0.763],\n",
       "         [0.836, 0.666],\n",
       "         [0.84, 0.879],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.825, 0.794],\n",
       "         [0.745, 0.611],\n",
       "         [0.598, 0.588],\n",
       "         [0.606, 0.603],\n",
       "         [0.514, 0.425],\n",
       "         [0.55, 0.413],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.681, 1.27],\n",
       "         [0.609, 1.23],\n",
       "         [0.433, 0.999],\n",
       "         [0.478, 1.16],\n",
       "         [0.585, 0.93],\n",
       "         [0.445, 1],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0.939, 1.07],\n",
       "         [0.859, 1.08],\n",
       "         [0.714, 0.877],\n",
       "         [0.859, 1.02],\n",
       "         [0.588, 0.746],\n",
       "         [0.576, 0.808],\n",
       "         [0.793, 0.905],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.929, 1.18],\n",
       "         [0.895, 1.01],\n",
       "         [0.66, 0.827],\n",
       "         [0.864, 0.973],\n",
       "         [0.685, 0.869],\n",
       "         [0.683, 0.886],\n",
       "         [0.882, 1.12],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[1.08, 0.805],\n",
       "         [1.06, 0.918],\n",
       "         [0.73, 0.546],\n",
       "         [0.864, 0.763],\n",
       "         [0.836, 0.666],\n",
       "         [0.84, 0.879],\n",
       "         [0.887, 0.725],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.825, 0.794],\n",
       "         [0.745, 0.611],\n",
       "         [0.598, 0.588],\n",
       "         [0.606, 0.603],\n",
       "         [0.514, 0.425],\n",
       "         [0.55, 0.413],\n",
       "         [0.636, 0.682],\n",
       "         [0, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.681, 1.27],\n",
       "         [0.609, 1.23],\n",
       "         [0.433, 0.999],\n",
       "         [0.478, 1.16],\n",
       "         [0.585, 0.93],\n",
       "         [0.445, 1],\n",
       "         [0.64, 1.12],\n",
       "         [0, 0],\n",
       "         [0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0.939, 1.07],\n",
       "         [0.859, 1.08],\n",
       "         [0.714, 0.877],\n",
       "         [0.859, 1.02],\n",
       "         [0.588, 0.746],\n",
       "         [0.576, 0.808],\n",
       "         [0.793, 0.905],\n",
       "         [0.713, 0.884],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.929, 1.18],\n",
       "         [0.895, 1.01],\n",
       "         [0.66, 0.827],\n",
       "         [0.864, 0.973],\n",
       "         [0.685, 0.869],\n",
       "         [0.683, 0.886],\n",
       "         [0.882, 1.12],\n",
       "         [0.756, 1.11],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[1.08, 0.805],\n",
       "         [1.06, 0.918],\n",
       "         [0.73, 0.546],\n",
       "         [0.864, 0.763],\n",
       "         [0.836, 0.666],\n",
       "         [0.84, 0.879],\n",
       "         [0.887, 0.725],\n",
       "         [0.809, 0.729],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.825, 0.794],\n",
       "         [0.745, 0.611],\n",
       "         [0.598, 0.588],\n",
       "         [0.606, 0.603],\n",
       "         [0.514, 0.425],\n",
       "         [0.55, 0.413],\n",
       "         [0.636, 0.682],\n",
       "         [0.615, 0.68],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0.681, 1.27],\n",
       "         [0.609, 1.23],\n",
       "         [0.433, 0.999],\n",
       "         [0.478, 1.16],\n",
       "         [0.585, 0.93],\n",
       "         [0.445, 1],\n",
       "         [0.64, 1.12],\n",
       "         [0.475, 1.06],\n",
       "         [0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0.939, 1.07],\n",
       "         [0.859, 1.08],\n",
       "         [0.714, 0.877],\n",
       "         [0.859, 1.02],\n",
       "         [0.588, 0.746],\n",
       "         [0.576, 0.808],\n",
       "         [0.793, 0.905],\n",
       "         [0.713, 0.884],\n",
       "         [0.76, 1.06]],\n",
       "\n",
       "        [[0.929, 1.18],\n",
       "         [0.895, 1.01],\n",
       "         [0.66, 0.827],\n",
       "         [0.864, 0.973],\n",
       "         [0.685, 0.869],\n",
       "         [0.683, 0.886],\n",
       "         [0.882, 1.12],\n",
       "         [0.756, 1.11],\n",
       "         [0.861, 1.15]],\n",
       "\n",
       "        [[1.08, 0.805],\n",
       "         [1.06, 0.918],\n",
       "         [0.73, 0.546],\n",
       "         [0.864, 0.763],\n",
       "         [0.836, 0.666],\n",
       "         [0.84, 0.879],\n",
       "         [0.887, 0.725],\n",
       "         [0.809, 0.729],\n",
       "         [0.988, 0.939]],\n",
       "\n",
       "        [[0.825, 0.794],\n",
       "         [0.745, 0.611],\n",
       "         [0.598, 0.588],\n",
       "         [0.606, 0.603],\n",
       "         [0.514, 0.425],\n",
       "         [0.55, 0.413],\n",
       "         [0.636, 0.682],\n",
       "         [0.615, 0.68],\n",
       "         [0.785, 0.7]],\n",
       "\n",
       "        [[0.681, 1.27],\n",
       "         [0.609, 1.23],\n",
       "         [0.433, 0.999],\n",
       "         [0.478, 1.16],\n",
       "         [0.585, 0.93],\n",
       "         [0.445, 1],\n",
       "         [0.64, 1.12],\n",
       "         [0.475, 1.06],\n",
       "         [0.593, 1.28]]]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vval_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76e11962",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attention = jnp.matmul(Attention_weights, Vval_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56c951ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 5, 9, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a4b7f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[0.939, 1.07],\n",
       "        [0.939, 1.07],\n",
       "        [0.47, 0.534],\n",
       "        [0.313, 0.356],\n",
       "        [0.235, 0.267],\n",
       "        [0.188, 0.213],\n",
       "        [0.157, 0.178],\n",
       "        [0.134, 0.152],\n",
       "        [0.117, 0.133]],\n",
       "\n",
       "       [[0.929, 1.18],\n",
       "        [0.929, 1.18],\n",
       "        [0.464, 0.59],\n",
       "        [0.31, 0.393],\n",
       "        [0.232, 0.295],\n",
       "        [0.186, 0.236],\n",
       "        [0.155, 0.197],\n",
       "        [0.133, 0.169],\n",
       "        [0.116, 0.147]],\n",
       "\n",
       "       [[1.08, 0.805],\n",
       "        [1.08, 0.805],\n",
       "        [0.541, 0.402],\n",
       "        [0.361, 0.268],\n",
       "        [0.27, 0.201],\n",
       "        [0.216, 0.161],\n",
       "        [0.18, 0.134],\n",
       "        [0.155, 0.115],\n",
       "        [0.135, 0.101]],\n",
       "\n",
       "       [[0.825, 0.794],\n",
       "        [0.825, 0.794],\n",
       "        [0.412, 0.397],\n",
       "        [0.275, 0.265],\n",
       "        [0.206, 0.199],\n",
       "        [0.165, 0.159],\n",
       "        [0.137, 0.132],\n",
       "        [0.118, 0.113],\n",
       "        [0.103, 0.0993]],\n",
       "\n",
       "       [[0.681, 1.27],\n",
       "        [0.681, 1.27],\n",
       "        [0.34, 0.634],\n",
       "        [0.227, 0.423],\n",
       "        [0.17, 0.317],\n",
       "        [0.136, 0.254],\n",
       "        [0.113, 0.211],\n",
       "        [0.0973, 0.181],\n",
       "        [0.0851, 0.159]]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ed38f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9, 9, 10), (9, 9, 10))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention=jnp.array([jnp.concatenate(Attention[i], axis=1) for i in range(9)])\n",
    "Attention.shape,input_decoder.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b2a3377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.939, 1.07, 0.929, 1.18, 1.08, 0.805, 0.825, 0.794, 0.681, 1.27],\n",
       "       [0.939, 1.07, 0.929, 1.18, 1.08, 0.805, 0.825, 0.794, 0.681, 1.27],\n",
       "       [0.899, 1.07, 0.912, 1.09, 1.07, 0.861, 0.785, 0.703, 0.645, 1.25],\n",
       "       [0.599, 0.716, 0.608, 0.729, 0.715, 0.574, 0.523, 0.468, 0.43, 0.833],\n",
       "       [0.45, 0.537, 0.456, 0.547, 0.536, 0.431, 0.392, 0.351, 0.322, 0.625],\n",
       "       [0.36, 0.43, 0.365, 0.437, 0.429, 0.345, 0.314, 0.281, 0.258, 0.5],\n",
       "       [0.3, 0.358, 0.304, 0.365, 0.358, 0.287, 0.262, 0.234, 0.215, 0.416],\n",
       "       [0.257, 0.307, 0.261, 0.312, 0.307, 0.246, 0.224, 0.201, 0.184, 0.357],\n",
       "       [0.225, 0.269, 0.228, 0.273, 0.268, 0.215, 0.196, 0.176, 0.161, 0.312]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "727a0303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 9, 10)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def layer_norm(x, epsilon=1e-6):\n",
    "    # Calculate the mean and variance\n",
    "        mean = jnp.mean(x, axis=-1, keepdims=True)\n",
    "        var = jnp.var(x, axis=-1, keepdims=True) \n",
    "        # Normalize the output\n",
    "        x_norm = (x - mean) / jnp.sqrt(var + epsilon) \n",
    "        return x_norm\n",
    "residual_output = layer_norm(input_decoder + Attention)\n",
    "residual_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1238b3f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "5b234398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[198, 83.1, 140, 155, 193],\n",
       "       [122, 140, 196, 197, 120],\n",
       "       [156, 215, 188, 129, 183],\n",
       "       [133, 180, 203, 148, 146],\n",
       "       [143, 203, 119, 201, 205],\n",
       "       [204, 89.6, 88.3, 170, 116]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phlenght=6\n",
    "input_t=[34,55,67,27,45,78]\n",
    "embedding_l=5\n",
    "A=np.random.rand(len(input_t), len(input_t),embedding_l)\n",
    "ins=input_t@A\n",
    "ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "4e497b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 5)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ins.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bac6f65",
   "metadata": {},
   "source": [
    "# Summary Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58bc3550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape:  (1, 3, 4)\n",
      "Qval.shape:  (1, 2, 3, 2)\n",
      "Kval.shape:  (1, 2, 3, 2)\n",
      "Vval.shape:  (1, 2, 3, 2)\n",
      "Attention_weights shape: (1, 2, 3, 3)\n",
      "Attention shape: (1, 2, 3, 2)\n",
      "Attention shape concat: (1, 3, 4)\n",
      "Ect1.shape (1, 3, 4) 4\n",
      "Xe1.shape (1, 3, 4)\n",
      "FLe1.shape (1, 3, 4)\n",
      "FLe2.shape (1, 3, 4)\n",
      "Ecout.shape (1, 3, 4) 4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "def softmax(x, axis=-1):\n",
    "    # Subtract the max value for numerical stability\n",
    "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "def layer_norm(x, epsilon=1e-6):\n",
    "    # Calculate the mean and variance\n",
    "        mean = jnp.mean(x, axis=-1, keepdims=True)\n",
    "        var = jnp.var(x, axis=-1, keepdims=True) \n",
    "        \n",
    "        # Normalize the output\n",
    "        x_norm = (x - mean) / jnp.sqrt(var + epsilon) \n",
    "        #print(x)\n",
    "        #print(mean)\n",
    "        #print(\"mean\",mean.shape)\n",
    "        #print(\"x_norm.shape\",x_norm.shape)\n",
    "        return x_norm,mean,var,x.shape[-1]\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    " \n",
    "\n",
    "num_phrases = 1\n",
    "words_per_phrase = 3 \n",
    "dk = dv = word2vec_len = 4 # constrain of transformer\n",
    " \n",
    "num_heads=2\n",
    " \n",
    " \n",
    "inputs_encoder = np.random.rand(num_phrases,words_per_phrase, word2vec_len)\n",
    "print(\"inputs.shape: \",inputs_encoder.shape)\n",
    "\n",
    "Qe = np.random.rand(word2vec_len, dk) / jnp.sqrt(word2vec_len)\n",
    "Ke = np.random.rand(word2vec_len, dk) / jnp.sqrt(word2vec_len)\n",
    "Ve = np.random.rand(word2vec_len, dv) / jnp.sqrt(word2vec_len)\n",
    "\n",
    "Q_E= jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs_encoder, Qe),num_heads,axis=2)), 0, 1)\n",
    "print(\"Qval.shape: \",Q_E.shape)\n",
    "\n",
    "K_E = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs_encoder, Ke),num_heads,axis=2)), 0, 1)\n",
    "print(\"Kval.shape: \",K_E.shape)\n",
    "\n",
    "\n",
    "V_E = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs_encoder,Ve),num_heads,axis=2)), 0, 1)\n",
    "print(\"Vval.shape: \",V_E.shape)\n",
    "\n",
    "\n",
    "QKscaled = jnp.matmul(Q_E, jnp.transpose(K_E, (0, 1, 3, 2))) / jnp.sqrt(dk)\n",
    "\n",
    "Attention_weights = softmax(QKscaled)\n",
    "print(\"Attention_weights shape:\",Attention_weights.shape)\n",
    "\n",
    "\n",
    "Attention_E = jnp.matmul(Attention_weights, V_E)\n",
    "print(\"Attention shape:\",Attention_E.shape)\n",
    "\n",
    "\n",
    "Attention_E=jnp.array([jnp.concatenate(Attention_E[i], axis=1) for i in range(num_phrases)])\n",
    "print(\"Attention shape concat:\",Attention_E.shape)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "Xe=Attention_E+inputs_encoder\n",
    "Ect1,mu_e,var_e,Ne=layer_norm(Xe)\n",
    "print(\"Ect1.shape\",Ect1.shape,Ne)\n",
    "\n",
    "fl1_size=100\n",
    "Wfl1e=np.random.rand(num_phrases,dv, dv)   \n",
    "bfl1e=np.random.rand(num_phrases,1,dv)\n",
    "Xe1=jnp.matmul(Ect1,Wfl1e)+bfl1e\n",
    "print(\"Xe1.shape\",Xe1.shape)\n",
    "\n",
    "FLe1=relu(Xe1)\n",
    "print(\"FLe1.shape\",FLe1.shape)\n",
    "\n",
    "\n",
    "fl2_size=50\n",
    "Wfl2e=np.random.rand(num_phrases,FLe1.shape[2], dv)   \n",
    "bfl2e=np.random.rand(num_phrases,1,dv)\n",
    "FLe2=jnp.matmul(FLe1,Wfl2e)+bfl2e\n",
    "print(\"FLe2.shape\",FLe2.shape)\n",
    "\n",
    "Xe2=FLe2+Xe1\n",
    "Ecout,mu_e2,var_e2,Ne2=layer_norm(Xe2)\n",
    "print(\"Ecout.shape\",Ecout.shape,Ne2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "624c29ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5782040675"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([0.78880733, 0.3270942 , 0.27765104 ,0.9192637 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf3ce05",
   "metadata": {},
   "source": [
    "# Cross Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "924b8323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kval.shape:  (1, 2, 3, 2)\n",
      "Vval.shape:  (1, 2, 3, 2)\n"
     ]
    }
   ],
   "source": [
    "dv_cross=dv\n",
    "num_heads_cross=num_heads\n",
    "#Qc = np.random.rand(Xe2.shape[-1], dv_cross) / jnp.sqrt(Xe2.shape[-1])\n",
    "Kc = np.random.rand(Ecout.shape[-1], dv_cross) / jnp.sqrt(Ecout.shape[-1])\n",
    "Vc = np.random.rand(Ecout.shape[-1], dv_cross) / jnp.sqrt(Ecout.shape[-1])\n",
    "\n",
    "\n",
    "\n",
    "K_C  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(Xe2, Kc),num_heads_cross,axis=2)), 0, 1)\n",
    "print(\"K_C.shape: \",K_C.shape)\n",
    "\n",
    "\n",
    "V_C  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(Xe2,Vc),num_heads_cross,axis=2)), 0, 1)\n",
    "print(\"K_C.shape: \",K_C.shape)\n",
    "\n",
    "K_C = K_C[0]  # Use the first phrase from the encoder output\n",
    "V_C = V_C[0]  # Use the first phrase from the encoder output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4523a2c6",
   "metadata": {},
   "source": [
    "# Summary Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4be68b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qval.shape:  (7, 2, 7, 2)\n",
      "Kval.shape:  (7, 2, 7, 2)\n",
      "Vval.shape:  (7, 2, 7, 2)\n",
      "Dt1.shape (7, 7, 4)\n",
      "Qval.shape:  (7, 2, 7, 2)\n"
     ]
    }
   ],
   "source": [
    "decoder_input_word_embedding_size=word2vec_len\n",
    "decoder_input_number_of_words_per_phrase=7\n",
    "num_heads_decoder=2# dv=10 \n",
    "dv_decoder=dv     \n",
    "input_d = np.random.rand(num_phrases,decoder_input_number_of_words_per_phrase, decoder_input_word_embedding_size)# for the target language suppose the \n",
    "target_decoder = input_d\n",
    "\n",
    "input_translation=[]\n",
    "def pad_sequence(seq, max_len, pad_value=0):\n",
    "    \"\"\"Pad a sequence with a given value up to max_len.\"\"\"\n",
    "    current_len = seq.shape[0]\n",
    "    pad_width = max_len - current_len\n",
    "    if pad_width > 0:\n",
    "        # Pad sequence with zeros (or any pad_value you provide)\n",
    "        seq = jnp.pad(seq, ((0, pad_width), (0, 0)), mode='constant', constant_values=pad_value)\n",
    "    return seq\n",
    "\n",
    "# Example usage:\n",
    "max_len = decoder_input_number_of_words_per_phrase  # Max sequence length for decoder input\n",
    " \n",
    "for j in range(input_d.shape[0]):\n",
    "    # Create padded sequences\n",
    "    padded_sequences = [pad_sequence(input_d[j][0:i], max_len) for i in range(1, input_d.shape[1] + 1)]\n",
    "    input_translation.append(padded_sequences)\n",
    "\n",
    "\n",
    "# Convert to an array for batching\n",
    "input_translation = jnp.array(input_translation)\n",
    "\n",
    "inputs_d=input_translation[0]\n",
    "\n",
    "Qd = np.random.rand(decoder_input_word_embedding_size, dv_decoder) / jnp.sqrt(decoder_input_word_embedding_size)\n",
    "Kd = np.random.rand(decoder_input_word_embedding_size, dv_decoder) / jnp.sqrt(decoder_input_word_embedding_size)\n",
    "Vd = np.random.rand(decoder_input_word_embedding_size, dv_decoder) / jnp.sqrt(decoder_input_word_embedding_size)\n",
    "\n",
    "\n",
    "Q_D  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs_d, Qd),num_heads_decoder,axis=2)), 0, 1)\n",
    "print(\"Qval.shape: \",Q_D.shape)\n",
    "\n",
    "K_D  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs_d, Kd),num_heads_decoder,axis=2)), 0, 1)\n",
    "print(\"Kval.shape: \",K_D.shape)\n",
    "\n",
    "\n",
    "V_D  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs_d,Vd),num_heads_decoder,axis=2)), 0, 1)\n",
    "print(\"Vval.shape: \",V_D.shape)\n",
    "\n",
    "\n",
    "QKscaled_decoder  = jnp.matmul(Q_D, jnp.transpose(K_D, (0, 1, 3, 2))) / jnp.sqrt(dk) + jnp.triu(jnp.ones((decoder_input_number_of_words_per_phrase, decoder_input_number_of_words_per_phrase)))* -1e9 \n",
    "# Step 1: Create a causal mask of shape (1, 1, 9, 9) to broadcast across heads and batch\n",
    "mask = jnp.tril(jnp.ones((max_len, max_len)))  # (9, 9) lower triangular matrix\n",
    "mask = mask.at[mask == 0].set(-jnp.inf)  # Set future tokens to -inf\n",
    "mask = mask.at[mask == 1].set(0)  # Set allowed tokens to 0\n",
    "mask = mask.reshape(1, 1, max_len, max_len)  # Reshape to (1, 1, 9, 9)\n",
    "\n",
    "# Step 2: Apply mask to QKscaled_decoder (it will broadcast across batch and heads)\n",
    "QKscaled_decoder = QKscaled_decoder + mask \n",
    "\n",
    "Attention_weights = softmax(QKscaled_decoder)\n",
    "\n",
    "\n",
    "A_mask = jnp.matmul(Attention_weights, V_D)\n",
    "\n",
    "\n",
    "A_mask=jnp.array([jnp.concatenate(A_mask[i], axis=1) for i in range(num_phrases)])\n",
    "\n",
    "\n",
    "\n",
    "Xd = inputs_d + A_mask\n",
    "Dt1,mu_d,var_d,N_d = layer_norm(Xd)\n",
    "print(\"Dt1.shape\",Dt1.shape)\n",
    "\n",
    "Qc = np.random.rand(Dt1.shape[-1], dv_cross) / jnp.sqrt(Dt1.shape[-1])\n",
    "Q_C  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(Dt1, Qc),num_heads_decoder,axis=2)), 0, 1)\n",
    "print(\"Qval.shape: \",Q_C.shape)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71462da9",
   "metadata": {},
   "source": [
    " # Cross Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4097a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dt2 shape: (7, 7, 4)\n",
      "Xd1.shape (7, 7, 4)\n",
      "FLe1.shape (7, 7, 4)\n",
      "FLd2.shape (7, 7, 4)\n",
      "Xd2.shape (7, 7, 4)\n",
      "Zout.shape (7, 7, 4)\n",
      "SigmaZout.shape (7, 7, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.369694  , 0.12853794, 0.27173984, 0.23002832],\n",
       "       [0.34361538, 0.20036109, 0.30039287, 0.15563065],\n",
       "       [0.343619  , 0.20036043, 0.30038974, 0.1556309 ],\n",
       "       [0.34362486, 0.20035928, 0.3003845 , 0.15563135],\n",
       "       [0.34363323, 0.20035768, 0.30037713, 0.15563194],\n",
       "       [0.3436439 , 0.20035566, 0.30036774, 0.1556327 ],\n",
       "       [0.3436569 , 0.20035319, 0.30035624, 0.15563364]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QKscaled_cross_attention  = jnp.matmul(Q_C, jnp.transpose(jnp.expand_dims(K_C, axis=0) , (0, 1, 3, 2))) / jnp.sqrt(dv_decoder)\n",
    "Attention_weights_cross = softmax(QKscaled_cross_attention)\n",
    "Acr = jnp.matmul(Attention_weights_cross, jnp.expand_dims(V_C, axis=0))\n",
    "Acr=jnp.array([jnp.concatenate(Acr[i], axis=1) for i in range(decoder_input_number_of_words_per_phrase)]) \n",
    "Res=Acr + Dt1\n",
    "Dt2, mu_res,var_res,N_res = layer_norm(Res)  # residual_output is (9, 9, 10)\n",
    "print(\"Dt2 shape:\", Dt2.shape)\n",
    "\n",
    " \n",
    "Wfl1d=np.random.rand(num_phrases,dv, dv)   \n",
    "bfl1d=np.random.rand(num_phrases,1,dv)\n",
    "Xd1=jnp.matmul(Dt2,Wfl1d)+bfl1d\n",
    "print(\"Xd1.shape\",Xd1.shape)\n",
    "\n",
    " \n",
    "FLd1=relu(Xd1)\n",
    "print(\"FLe1.shape\",FLd1.shape)\n",
    "\n",
    "\n",
    " \n",
    "Wfl2d=np.random.rand(num_phrases,FLd1.shape[2], dv)   \n",
    "bfl2d=np.random.rand(num_phrases,1,dv)\n",
    "FLd2=jnp.matmul(FLd1,Wfl2d)+bfl2d\n",
    "print(\"FLd2.shape\",FLd2.shape)\n",
    "\n",
    "Xd2=FLd2+Dt2\n",
    "Dout,mu_d2,var_d2,N_d2=layer_norm(Xd2)\n",
    "Dout.shape\n",
    "print(\"Xd2.shape\",Dout.shape)\n",
    "\n",
    "\n",
    "W0=np.random.rand(num_phrases,dv, dv)   \n",
    "b0=np.random.rand(num_phrases,1,dv)\n",
    "Zout=jnp.matmul(Dout,W0)+b0\n",
    "print(\"Zout.shape\",Zout.shape)\n",
    "SigmaZout = softmax(Zout) \n",
    "print(\"SigmaZout.shape\",SigmaZout.shape)\n",
    "SigmaZout[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44e2d23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6430086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(predictions, target):\n",
    "    # Cross-entropy loss for a batch of predictions and targets\n",
    "    batch_loss = -jnp.sum(target * jnp.log(predictions + 1e-9), axis=1)\n",
    "    return jnp.mean(batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6316d5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(6.692228, dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_loss(SigmaZout, target_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f809210c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7, 7, 4), (1, 4, 4), (1, 1, 4))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dout.shape,W0.shape,b0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff7210cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLoss_dZout.shape (7, 7, 4)\n",
      "dLoss_W0.shape (7, 4, 4)\n",
      "dLoss_b0.shape (7, 7, 4)\n"
     ]
    }
   ],
   "source": [
    "dLoss_dZout=SigmaZout-target_decoder\n",
    "print(\"dLoss_dZout.shape\",dLoss_dZout.shape)\n",
    "dLoss_W0=jnp.transpose(dLoss_dZout,(0,2,1))@Dout\n",
    "print(\"dLoss_W0.shape\",dLoss_W0.shape)\n",
    "dLoss_b0=dLoss_dZout\n",
    "print(\"dLoss_b0.shape\",dLoss_b0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b94fda3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dot_general requires contracting dimensions to have the same shape, got (4,) and (7,).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m\n\u001b[0;32m      2\u001b[0m dLoss_Dout\u001b[38;5;241m=\u001b[39mdLoss_dZout\u001b[38;5;129m@W0\u001b[39m\n\u001b[1;32m----> 3\u001b[0m dLoss_FLd2\u001b[38;5;241m=\u001b[39m\u001b[43mdLoss_Dout\u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mN_d2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_d2\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mN_d2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXd2\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mmu_d2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_d2\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\jax\\_src\\numpy\\array_methods.py:573\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    571\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[1;32m--> 573\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "    \u001b[1;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\jax\\_src\\numpy\\lax_numpy.py:6889\u001b[0m, in \u001b[0;36mmatmul\u001b[1;34m(a, b, precision, preferred_element_type)\u001b[0m\n\u001b[0;32m   6887\u001b[0m a \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39msqueeze(a, \u001b[38;5;28mtuple\u001b[39m(a_squeeze))\n\u001b[0;32m   6888\u001b[0m b \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39msqueeze(b, \u001b[38;5;28mtuple\u001b[39m(b_squeeze))\n\u001b[1;32m-> 6889\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot_general\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   6890\u001b[0m \u001b[43m  \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb_is_mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6891\u001b[0m \u001b[43m  \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6892\u001b[0m result \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mtranspose(out, perm)\n\u001b[0;32m   6893\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lax_internal\u001b[38;5;241m.\u001b[39m_convert_element_type(result, preferred_element_type, output_weak_type)\n",
      "    \u001b[1;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\jax\\_src\\lax\\lax.py:2774\u001b[0m, in \u001b[0;36m_dot_general_shape_rule\u001b[1;34m(lhs, rhs, dimension_numbers, precision, preferred_element_type)\u001b[0m\n\u001b[0;32m   2771\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m core\u001b[38;5;241m.\u001b[39mdefinitely_equal_shape(lhs_contracting_shape, rhs_contracting_shape):\n\u001b[0;32m   2772\u001b[0m   msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdot_general requires contracting dimensions to have the same \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2773\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2774\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(lhs_contracting_shape, rhs_contracting_shape))\n\u001b[0;32m   2776\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _dot_general_shape_computation(lhs\u001b[38;5;241m.\u001b[39mshape, rhs\u001b[38;5;241m.\u001b[39mshape, dimension_numbers)\n",
      "\u001b[1;31mTypeError\u001b[0m: dot_general requires contracting dimensions to have the same shape, got (4,) and (7,)."
     ]
    }
   ],
   "source": [
    "epsilon=1e-6\n",
    "dLoss_Dout=dLoss_dZout@W0\n",
    "dLoss_FLd2=dLoss_Dout@((1-(1/N_d2))*(1/(jnp.sqrt(var_d2+epsilon)))-(1/N_d2)*(((Xd2-mu_d2)**2)/((var_d2+epsilon)**(3/2))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
