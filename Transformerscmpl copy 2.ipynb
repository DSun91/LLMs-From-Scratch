{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b060598",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b89336dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import re\n",
    "import cupy as cp\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jax\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "np.set_printoptions(edgeitems=30, linewidth=100000, formatter=dict(float=lambda x: \"%.3g\" % x))\n",
    "\n",
    "\n",
    "def log_time(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()  # Record start time\n",
    "        result = func(*args, **kwargs)  # Execute the wrapped function\n",
    "        end_time = time.time()  # Record end time\n",
    "        elapsed_time = end_time - start_time\n",
    "        # print(f\"Function '{func.__name__}' executed in {elapsed_time:.4f} seconds\")\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "class Helper: \n",
    "    \n",
    "    def get_positional_encoding(self,seq_len, d_model):\n",
    "        \"\"\"\n",
    "        Returns a non-learnable (sinusoidal) positional encoding.\n",
    "\n",
    "\n",
    "        seq_len: Length of the input sequence.\n",
    "        d_model: Dimension of the embeddings.\n",
    "        \"\"\"\n",
    "        pos = cp.arange(seq_len)[:, cp.newaxis]  # Shape: [seq_len, 1]\n",
    "        i = cp.arange(d_model)[cp.newaxis, :]  # Shape: [1, d_model]\n",
    "\n",
    "        angle_rates = 1 / cp.power(10000, (2 * (i // 2)) / cp.float32(d_model))\n",
    "\n",
    "        # Apply sine to even indices, cosine to odd indices\n",
    "        pos_encoding = cp.zeros((seq_len, d_model))\n",
    "        pos_encoding[:, 0::2] = cp.sin(pos * angle_rates[:, 0::2])  # sine on even indices\n",
    "        pos_encoding[:, 1::2] = cp.cos(pos * angle_rates[:, 1::2])  # cosine on odd indices\n",
    "\n",
    "        return pos_encoding\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        # Subtract the max value for numerical stability\n",
    "        max_logits = cp.max(x, axis=-1, keepdims=True)\n",
    "        exp_logits = cp.exp(x - max_logits)\n",
    "        return exp_logits / cp.sum(exp_logits, axis=-1, keepdims=True)\n",
    "\n",
    " \n",
    "    # @log_time\n",
    "    def pad_sequence(self,seq, max_len, pad_value=0):\n",
    "        \"\"\"Pad a sequence with a given value up to max_len.\"\"\"\n",
    "        current_len = seq.shape[0]\n",
    "        pad_width = max_len - current_len\n",
    "        if pad_width > 0:\n",
    "            # Pad sequence with zeros (or any pad_value you provide)\n",
    "            seq = cp.pad(seq, ((0, pad_width), (0, 0)), mode='constant', constant_values=pad_value)\n",
    "        return seq\n",
    "\n",
    "\n",
    "    @log_time\n",
    "    def create_timestaped_input(self,input_d, words_per_phrase):\n",
    "        input_translation = []\n",
    "        for j in range(input_d.shape[0]):\n",
    "            # Create padded sequences\n",
    "            padded_sequences = [self.pad_sequence(input_d[j][0:i], words_per_phrase) for i in range(1, input_d.shape[1] + 1)]\n",
    "            input_translation.append(padded_sequences)\n",
    "        return cp.array(input_translation)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    " \n",
    "    def redimension(self,X):\n",
    "        return cp.concatenate(cp.swapaxes(X, 0, 1), axis=-1)\n",
    "    \n",
    "    @log_time\n",
    "    def create_vocabulary(self,complete_text, name, nlp):\n",
    "        # Use re.findall to split considering punctuation\n",
    "        text = re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', complete_text)\n",
    "\n",
    "        words_list = list(set(text))\n",
    "\n",
    "        vocabulary = dict()\n",
    "\n",
    "        for i, j in enumerate(words_list):\n",
    "            # vocabulary[j]=(jax.random.uniform(jax.random.key(cp.random.randint(10000)),embedding_size),i)\n",
    "            vocabulary[j] = (cp.array(nlp(j).vector), i)\n",
    "            # print(j,len(cp.array(nlp(j).vector)))\n",
    "\n",
    "        # print(vocabulary)\n",
    "        # print(\"Vocabulary size: \", len(vocabulary))\n",
    "        with open(f\"data/{name}.pkl\", 'wb') as handle:\n",
    "            pickle.dump(vocabulary, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        return vocabulary\n",
    "\n",
    "\n",
    "    @log_time\n",
    "    def pad_sequences(self,sentences, lenght, pad_token='[PAD]', target_type=None):\n",
    "        \"\"\"\n",
    "        Pads the input sentences to have the same length by adding [PAD] tokens at the end.\n",
    "        \"\"\"\n",
    "        regex_str=r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n'\n",
    "\n",
    "        if target_type == \"encoder\":\n",
    "            # Split each sentence into words\n",
    "            tokenized_sentences = [[\"[START]\"] + re.findall(regex_str, sentence) + [\"[END]\"] for sentence in sentences]\n",
    "        elif target_type == \"decoder\":\n",
    "            tokenized_sentences = [[\"[START]\"] + re.findall(regex_str, sentence) for sentence in sentences]\n",
    "        elif target_type == \"target\":\n",
    "            tokenized_sentences = [re.findall(regex_str, sentence) + [\"[END]\"] for sentence in sentences]\n",
    "        # print(tokenized_sentences)\n",
    "        if lenght == 0:\n",
    "            # Find the maximum sentence length\n",
    "            max_len = max(len(sentence) for sentence in tokenized_sentences)\n",
    "        else:\n",
    "            max_len = lenght\n",
    "\n",
    "        # Pad each sentence with the [PAD] token to make them of equal length\n",
    "        padded_sentences = [\" \".join(sentence + [pad_token] * (max_len - len(sentence))) for sentence in\n",
    "                            tokenized_sentences]\n",
    "\n",
    "        return padded_sentences\n",
    "\n",
    "    def print_matrix(self,X):\n",
    "        for i in X:\n",
    "            print(i)\n",
    "\n",
    "    @log_time\n",
    "    def generate_input_encoder(self,x_batch, vocabulary_encoder, max_words_per_phrase):\n",
    "\n",
    "\n",
    "        regex_str=r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n'\n",
    "        x_train = self.pad_sequences(x_batch, max_words_per_phrase, target_type=\"encoder\")# here are string\n",
    "        \n",
    "        #print_matrix(x_train) \n",
    "    \n",
    "        xi = []\n",
    "        # print(x_batch)\n",
    "        phrase_vectors_x = [re.findall(regex_str, x) for x in x_train]\n",
    "        \n",
    "        phrase_vectors_x = [i[0:max_words_per_phrase] for i in phrase_vectors_x]\n",
    "        #print(phrase_vectors_x) \n",
    "       \n",
    "        # print(\"input_encoder:\")\n",
    "        # self.print_matrix(phrase_vectors_x)\n",
    "        xi = cp.array([[vocabulary_encoder[word][0] for word in phrase_vector] for phrase_vector in phrase_vectors_x])\n",
    "\n",
    "        return xi\n",
    "    \n",
    "    @log_time\n",
    "    def create_input_encoder(self,X, vocabulary_encoder, max_words_per_phrase, embedding_size):\n",
    "\n",
    "        pos_encoding = self.get_positional_encoding(max_words_per_phrase, embedding_size)\n",
    "        #print(pos_encoding)\n",
    "        inputs_e = self.generate_input_encoder(X, vocabulary_encoder, max_words_per_phrase)\n",
    "        \n",
    "        #print(inputs_e)\n",
    "\n",
    "        inputs_e =inputs_e + pos_encoding\n",
    "        return inputs_e\n",
    "    \n",
    "    def generate_target(self,x_batch, vocabulary_encoder, max_words_per_phrase):\n",
    "\n",
    "        regex_str=r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n'\n",
    "\n",
    "        y_target = self.pad_sequences(x_batch, max_words_per_phrase, target_type=\"target\")# here are one string with the padd\n",
    "        \n",
    "        \n",
    "        target_vector = [re.findall(regex_str, x) for x in y_target]\n",
    "\n",
    "        #print(target_vector)\n",
    "        #print_matrix(phrase_vectors_x) \n",
    "        #target_vector = [i[0:max_words_per_phrase] for i in target_vector]\n",
    "        target_vector = [i[0:max_words_per_phrase] for i in target_vector]\n",
    "        target_vector = cp.array([[self.get_one_hot(i,vocabulary_encoder) for i in phrase] for phrase in target_vector])\n",
    "    \n",
    "        return target_vector\n",
    "    \n",
    "    def generate_target_sparse_categorical(self,y_batch, vocabulary_encoder, max_words_per_phrase):\n",
    "\n",
    "        regex_str=r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n'\n",
    "\n",
    "        y_target = self.pad_sequences(y_batch, max_words_per_phrase, target_type=\"target\")# here are one string with the padd\n",
    "        \n",
    "        \n",
    "        target_vector = [re.findall(regex_str, x) for x in y_target]\n",
    "\n",
    "        #print(target_vector)\n",
    "        #print_matrix(phrase_vectors_x) \n",
    "        #target_vector = [i[0:max_words_per_phrase] for i in target_vector]\n",
    "        target_vector = [i[0:max_words_per_phrase] for i in target_vector]\n",
    "        target_vector = cp.array([[vocabulary_encoder[i][1] for i in phrase] for phrase in target_vector])\n",
    "    \n",
    "        return target_vector\n",
    "    \n",
    "    @log_time\n",
    "    def create_target(self,X, vocabulary_encoder, max_words_per_phrase): \n",
    "        inputs_e = self.generate_target(X, vocabulary_encoder, max_words_per_phrase) \n",
    "        return inputs_e\n",
    "\n",
    "    @log_time\n",
    "    def create_decoder_input(self,y_train, embedding_size, max_words_per_phrase, vocabulary_decoder):\n",
    "\n",
    "        decoder_input = self.pad_sequences(y_train, lenght=max_words_per_phrase, target_type=\"decoder\")\n",
    "        #print_matrix(decoder_input)\n",
    "        decoder_input = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', i) for i in decoder_input]\n",
    "        \n",
    "        if max_words_per_phrase == None:\n",
    "            max_words_per_phrase = len(decoder_input[0])\n",
    "\n",
    "        phrase_vectors_y = [i[0:max_words_per_phrase] for i in decoder_input]\n",
    "        # for sentence in phrase_vectors_y:\n",
    "        #     print(sentence)\n",
    "        #print_matrix(phrase_vectors_y)\n",
    "        \n",
    "       \n",
    "\n",
    "        # print(\"decoder_input:\")\n",
    "        # self.print_matrix(decoder_input)\n",
    "        yi = cp.array([[vocabulary_decoder[word][0] for word in phrase_vector] for phrase_vector in phrase_vectors_y])\n",
    "        \n",
    "        pos_encoding = self.get_positional_encoding(max_words_per_phrase, embedding_size)\n",
    "        # print(pos_encoding.shape,yi.shape)\n",
    "        yi = yi + pos_encoding\n",
    "        #print_matrix(yi)\n",
    "        # decoder_inputs = cp.array(cp.swapaxes(self.create_timestaped_input(yi, max_words_per_phrase), 0, 1))\n",
    "        \n",
    "        # # decoder_inputs[zero_rows] = vocabulary_decoder[\"[PAD]\"][0]\n",
    "        # for i in range(decoder_inputs.shape[0]):\n",
    "        #     for j in range(decoder_inputs[i].shape[0]):\n",
    "        #         zero_rows = cp.all(decoder_inputs[i][j] == 0, axis=1)\n",
    "\n",
    "        #         decoder_inputs[i][j][zero_rows] = vocabulary_decoder[\"[PAD]\"][0]\n",
    "\n",
    "        # decoder_inputs = cp.array(decoder_inputs)\n",
    "        #print(decoder_inputs[2])\n",
    "        #print(decoder_inputs)\n",
    "        return yi\n",
    "    # @log_time\n",
    "    def update_wembedding_encoder(self,learning_rate, x_batch, dLoss_dWemb_encoder, vocabulary, max_words_per_phrase):\n",
    "        x_train = self.pad_sequences(x_batch, max_words_per_phrase, target_type=\"encoder\")\n",
    "\n",
    "        phrase_vectors_x = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', x) for x in x_train]\n",
    "        phrase_vectors_x = [i[:max_words_per_phrase] for i in phrase_vectors_x]\n",
    "\n",
    "        for phrase in range(dLoss_dWemb_encoder.shape[0]):\n",
    "            for position, word in enumerate(phrase_vectors_x[phrase]):\n",
    "                # Retrieve current embedding\n",
    "                current_embedding, index = vocabulary[word]\n",
    "\n",
    "                # Calculate the updated embedding using the gradient\n",
    "                updated_embedding = current_embedding - learning_rate * dLoss_dWemb_encoder[phrase][position]\n",
    "\n",
    "                # Update the vocabulary with the new embedding\n",
    "                vocabulary[word] = (updated_embedding, index)\n",
    "\n",
    "        return vocabulary\n",
    "\n",
    "\n",
    "\n",
    "    # @log_time\n",
    "    def update_wembedding_decoder(self, learning_rate,y_batch, dLoss_dWemb_decoder, vocabulary,max_words_per_phrase):\n",
    "        decoder_input = self.pad_sequences(y_batch, lenght=max_words_per_phrase, target_type=\"decoder\")\n",
    "        decoder_input = [i.split() for i in decoder_input]\n",
    "\n",
    "        if max_words_per_phrase is None:\n",
    "            max_words_per_phrase = len(decoder_input[0])\n",
    "\n",
    "        phrase_vectors_y = [i[:max_words_per_phrase] for i in decoder_input]\n",
    "\n",
    "        for phrase in range(dLoss_dWemb_decoder.shape[0]):\n",
    "            for position, word in enumerate(phrase_vectors_y[phrase]):\n",
    "                # Retrieve current embedding for the word\n",
    "                current_embedding, index = vocabulary[word]\n",
    "\n",
    "                # Apply the gradient update\n",
    "                updated_embedding = current_embedding - learning_rate * dLoss_dWemb_decoder[phrase][position]\n",
    "\n",
    "                # Update the vocabulary with the new embedding\n",
    "                vocabulary[word] = (updated_embedding, index)\n",
    "\n",
    "        return vocabulary\n",
    "\n",
    "  \n",
    "    # @log_time\n",
    "    def get_one_hot(self,word, vocabulary_decoder):\n",
    "        # print(word)\n",
    "        vocab_size = len(vocabulary_decoder)\n",
    "        one_hot_vector = cp.zeros(vocab_size)\n",
    "        one_hot_vector[vocabulary_decoder[word][1]] = 1\n",
    "        # print(vocabulary_decoder[word][1])\n",
    "        # print(np.where(one_hot_vector== 1))\n",
    "        # print(cp.sum(one_hot_vector))\n",
    "        return one_hot_vector\n",
    " \n",
    "    \n",
    "    def log_sparse_entropy(self,ans,target,y_batch,step):\n",
    "        #print(\"target\",target)\n",
    "        #print(\"ans\",ans)\n",
    "        counter_found=0\n",
    "        total_lenght=len(ans)\n",
    "        print(f\"----DECODER--step {step}---\")\n",
    "        self.print_matrix(y_batch)\n",
    "        print(\"target\",target)\n",
    "        indexes=[]\n",
    "        yy=[re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', xx) for xx in y_batch] \n",
    "        for idx, values in enumerate(ans):\n",
    "            max_index = cp.argmax(values)\n",
    "            indexes.append(max_index)\n",
    "             \n",
    "            if max_index==target[idx]:\n",
    "                counter_found+=1\n",
    "            print(f\"{idx + 1} base: {' '.join(yy[idx][0:step+1])} -> {max_index}\")\n",
    "        print(\"indexes\",indexes)\n",
    "        print(\"accuracy batch:\",round(counter_found/total_lenght,2))\n",
    "        \n",
    "    def accruacy_sparse_entropy(self,ans,target):\n",
    "        counter_found=0\n",
    "        total_lenght=len(ans) \n",
    "        for idx, values in enumerate(ans):\n",
    "            max_index = cp.argmax(values) \n",
    "            if max_index==target[idx]:\n",
    "                counter_found+=1\n",
    "             \n",
    "        accuracy_batch_on_step=round(counter_found/total_lenght,2)\n",
    "        return accuracy_batch_on_step\n",
    "    \n",
    "\n",
    "    def print_target_vs_prediction_sparce_loss(self,ans,target): \n",
    "        indexes=[] \n",
    "        for idx, values in enumerate(ans):\n",
    "            max_index = np.argmax(values).item()\n",
    "            indexes.append(max_index) \n",
    "        print(\"target\",target)\n",
    "        print(\"indexes\",indexes)\n",
    "        \n",
    "  \n",
    "def clip_gradient(gradient,threshold):\n",
    "    return cp.clip(gradient, -threshold, threshold)\n",
    "\n",
    "\n",
    "    \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "class output_stack:\n",
    "    def __init__(self,embedding_size,vocabulary_size,threshold,temperature=1):\n",
    "        self.final_projection_layer=linear_layer(embedding_size,vocabulary_size,threshold=threshold,out=True)\n",
    "        self.clipping_threshold=threshold\n",
    "        self.temperature=temperature\n",
    "        \n",
    "    def softmax(self, x):\n",
    "        max_logits = cp.max(x, axis=-1, keepdims=True)\n",
    "        exp_logits = cp.exp((x - max_logits) / self.temperature)  # Apply temperature\n",
    "        return exp_logits / cp.sum(exp_logits, axis=-1, keepdims=True)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        SoftmaxOutput=self.softmax(self.final_projection_layer.forward(x)) \n",
    "        return SoftmaxOutput\n",
    "\n",
    "    \n",
    "    # def cross_entropy_loss(self,SigmaZout, target):\n",
    "    #     epsilon = 1e-12  # Small constant to avoid log(0)\n",
    "    #     SigmaZout = cp.clip(SigmaZout, epsilon, 1 - epsilon)  # Clipping predictions\n",
    "    #     return -cp.sum(target * cp.log(SigmaZout), axis=1).mean() \n",
    "    def cross_entropy_loss(self, y_pred, y_true):\n",
    "        epsilon = 1e-12\n",
    "        return -np.mean(np.sum(y_true * np.log(y_pred + epsilon), axis=-1))\n",
    "    \n",
    "    def grad_cross_entropy(self,softmax_output,target):\n",
    "        dL_dZ =softmax_output-target\n",
    "        dL_dDout=self.final_projection_layer.grad(dL_dZ)  \n",
    "        #dL_dDout=clip_gradient(dL_dDout,self.clipping_threshold)\n",
    "        return dL_dDout\n",
    "    \n",
    "    def sparse_categorical_crossentropy(self, probabilities, labels,padding_mask): \n",
    "        #print(\"probabilities.shape\", probabilities.shape)\n",
    "        #print(\"labels.shape\", labels.shape)\n",
    "        \n",
    "        # Unpack batch and sequence dimensions\n",
    "        batch_size, seq_length = labels.shape\n",
    "        \n",
    "        # Gather correct class probabilities for each position in the batch and sequence\n",
    "        correct_class_probs = probabilities[np.arange(batch_size)[:, None], np.arange(seq_length), labels] \n",
    "        \n",
    "        # Calculate the log loss and average it\n",
    "        loss = -np.log(correct_class_probs + 1e-8)\n",
    "\n",
    "        masked_loss = loss * padding_mask\n",
    "        print(\"loss shape\",loss.shape,\"mask shape\",padding_mask.shape)\n",
    "\n",
    "        return np.sum(masked_loss) / np.sum(padding_mask)#np.mean(loss)\n",
    "    \n",
    "    def grad(self,dl_dy):\n",
    "        return self.final_projection_layer.grad(dl_dy)\n",
    "\n",
    "\n",
    "    def grad_sparse_cross_entropy(self, softmax_output, target,mask):\n",
    "        dL_dZ = softmax_output.copy()  # Create a copy of the softmax output\n",
    "        \n",
    "        # Adjust indexing to handle both batch and sequence dimensions\n",
    "        batch_size, seq_length = target.shape\n",
    "        dL_dZ[np.arange(batch_size)[:, None], np.arange(seq_length), target] -= 1\n",
    "        expanded_mask = mask.reshape(batch_size, seq_length, 1)\n",
    "        dL_dZ *= expanded_mask\n",
    "        #print(\"expanded_mask\",expanded_mask.shape)\n",
    "        # Compute gradient through final projection layer\n",
    "        dL_dDout = self.final_projection_layer.grad(dL_dZ)  \n",
    "        # dL_dDout = clip_gradient(dL_dDout, self.clipping_threshold)\n",
    "        return dL_dDout\n",
    " \n",
    "    def update_weights(self,learning_rate):\n",
    "        self.final_projection_layer.update_weights(learning_rate)  \n",
    "\n",
    "\n",
    "    def __init__(self,embedding_size,vocabulary_size,threshold):\n",
    "        self.final_projection_layer=linear_layer(embedding_size,vocabulary_size,threshold=threshold,out=True)\n",
    "        self.clipping_threshold=threshold\n",
    "\n",
    "    def softmax(self,x):\n",
    "        # Subtract the max value for numerical stability\n",
    "        max_logits = cp.max(x, axis=-1, keepdims=True)\n",
    "        exp_logits = cp.exp(x - max_logits)\n",
    "        return exp_logits / cp.sum(exp_logits, axis=-1, keepdims=True)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        SoftmaxOutput=self.softmax(self.final_projection_layer.forward(x)) \n",
    "        return SoftmaxOutput\n",
    "\n",
    "    \n",
    "    def cross_entropy_loss(self,SigmaZout, target):\n",
    "        epsilon = 1e-12  # Small constant to avoid log(0)\n",
    "        SigmaZout = cp.clip(SigmaZout, epsilon, 1 - epsilon)  # Clipping predictions\n",
    "        return -cp.sum(target * cp.log(SigmaZout), axis=1).mean() \n",
    "    \n",
    "    def grad_cross_entropy(self,softmax_output,target):\n",
    "        dL_dZ = dL_dZ=softmax_output-target\n",
    "        dL_dDout=self.final_projection_layer.grad(dL_dZ)  \n",
    "        #dL_dDout=clip_gradient(dL_dDout,self.clipping_threshold)\n",
    "        return dL_dDout\n",
    "    \n",
    "    def sparse_categorical_crossentropy(self, probabilities, labels): \n",
    "        #print(\"probabilities.shape\", probabilities.shape)\n",
    "        #print(\"labels.shape\", labels.shape)\n",
    "        \n",
    "        # Unpack batch and sequence dimensions\n",
    "        batch_size, seq_length = labels.shape\n",
    "        \n",
    "        # Gather correct class probabilities for each position in the batch and sequence\n",
    "        correct_class_probs = probabilities[np.arange(batch_size)[:, None], np.arange(seq_length), labels] \n",
    "        \n",
    "        # Calculate the log loss and average it\n",
    "        loss = -np.log(correct_class_probs + 1e-8)\n",
    "        return np.mean(loss)\n",
    "    \n",
    "    def grad(self,dl_dy):\n",
    "        return self.final_projection_layer.grad(dl_dy)\n",
    "\n",
    "\n",
    "    def grad_sparse_cross_entropy(self, softmax_output, target):\n",
    "        dL_dZ = softmax_output.copy()  # Create a copy of the softmax output\n",
    "        \n",
    "        # Adjust indexing to handle both batch and sequence dimensions\n",
    "        batch_size, seq_length = target.shape\n",
    "        dL_dZ[np.arange(batch_size)[:, None], np.arange(seq_length), target] -= 1\n",
    "        \n",
    "        # Compute gradient through final projection layer\n",
    "        dL_dDout = self.final_projection_layer.grad(dL_dZ)  \n",
    "        # dL_dDout = clip_gradient(dL_dDout, self.clipping_threshold)\n",
    "        return dL_dDout\n",
    " \n",
    "    def update_weights(self,learning_rate):\n",
    "        self.final_projection_layer.update_weights(learning_rate)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "class layer_dropout: \n",
    "\n",
    "    def __init__(self,dropout_rate=0.1):\n",
    "        self.dropout_rate=dropout_rate \n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self,X):   \n",
    "        self.mask = (cp.random.rand(*X.shape) > self.dropout_rate)#.astype(cp.float64)\n",
    "        result = X * self.mask \n",
    "        #print(self.mask )\n",
    "        return result\n",
    "    #\n",
    "    def grad(self, X):\n",
    "        # Only pass gradients through neurons that were not dropped out\n",
    "        grad_input = X * self.mask\n",
    "        grad_input = clip_gradient(grad_input, 1)\n",
    "        return grad_input\n",
    "\n",
    "class layer_normalization:\n",
    "    def __init__(self, threshold, epsilon=0.0001):\n",
    "        self.epsilon = epsilon\n",
    "        self.mu = 0\n",
    "        self.var = 0\n",
    "        self.N = None\n",
    "        self.beta = None\n",
    "        self.alpha = None\n",
    "        self.clipping_threshold = threshold\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.epsilonopt = 1e-8\n",
    "        self.t = 0 \n",
    "        \n",
    " \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.N = x.shape[-1]\n",
    "        \n",
    "        # Initialize parameters if not done\n",
    "        if self.alpha is None:\n",
    "            self.alpha = cp.ones(self.N)\n",
    "            self.beta = cp.zeros(self.N)\n",
    "            self.m_Wbeta = cp.zeros_like(self.beta)\n",
    "            self.v_Wbeta = cp.zeros_like(self.beta)\n",
    "            self.m_Walpha = cp.zeros_like(self.alpha)\n",
    "            self.v_Walpha = cp.zeros_like(self.alpha)\n",
    "\n",
    "        # Forward computation\n",
    "        self.mu = cp.mean(x, axis=-1, keepdims=True)\n",
    "        self.var = cp.var(x, axis=-1, keepdims=True)\n",
    "        self.std = cp.sqrt(self.var + self.epsilon)\n",
    "        self.x_norm = (x - self.mu) / self.std\n",
    "        \n",
    "        return self.alpha * self.x_norm + self.beta\n",
    "\n",
    "    def dL_dNorm(self):\n",
    "        self.dL_dnorm_ = self.dLoss_dy * self.alpha\n",
    "        return self.dL_dnorm_\n",
    "\n",
    "    def dL_dsigma(self):\n",
    "        self.dL_dsigma_ = (-0.5)*cp.sum(self.dx_norm*(self.x-self.mu)*((self.var+self.epsilon)**(-1.5)),axis=-1,keepdims=True)\n",
    "        return self.dL_dsigma_\n",
    "    \n",
    "    def dSigma_dmu(self):\n",
    "        return (-2/self.N)*(self.x-self.mu)\n",
    "    \n",
    "    def dL_dmu(self):\n",
    "        a = -1*cp.sum(self.dx_norm*(1/cp.sqrt(self.var+self.epsilon)),axis=-1,keepdims=True)\n",
    "        b = -2*self.dL_dsigma_*cp.sum((1/self.N)*(self.x-self.mu),axis=-1,keepdims=True)\n",
    "        self.dL_dmu_=a+b\n",
    "        return self.dL_dmu_\n",
    "         \n",
    "    def dL_dx(self): \n",
    "        self.dL_dx_=self.dx_norm*self.dNorm_dx()+self.dL_dsigma()*self.dSigma_dx()+self.dL_dmu()*(1/self.N)\n",
    "        return self.dL_dx_\n",
    "    \n",
    "    def dNorm_dx(self):\n",
    "        return 1/cp.sqrt(self.var+self.epsilon)\n",
    "\n",
    "    def dSigma_dx(self):\n",
    "        return (2/self.N)*(self.x-self.mu)\n",
    "         \n",
    "    def backpropagation(self, dLoss_dy):\n",
    "        self.dLoss_dy = dLoss_dy\n",
    "        self.dx_norm = dLoss_dy * self.alpha \n",
    "        return self.dL_dx()\n",
    "    \n",
    "   \n",
    "    def dL_dalpha(self):\n",
    "        result = self.dLoss_dy * self.x_norm\n",
    "        result=cp.sum(cp.sum(result,axis=0),axis=0)\n",
    "        #print(self.dLoss_dy.shape,self.alpha.shape)\n",
    "        #result = clip_gradient(result, self.clipping_threshold)\n",
    "        return result\n",
    "\n",
    "    def dL_dbeta(self):\n",
    "        result = cp.sum(cp.sum(self.dLoss_dy,axis=0),axis=0)\n",
    "        #print(self.dLoss_dy.shape,self.beta.shape)\n",
    "        #result = clip_gradient(result, self.clipping_threshold)\n",
    "        return result\n",
    "     \n",
    "\n",
    "    \n",
    "    \n",
    "  \n",
    "    \n",
    "    def params_update(self, learning_rate):\n",
    "\n",
    "         \n",
    "        self.t += 1\n",
    "        \n",
    "        # Update beta\n",
    "        dbeta = self.dL_dbeta()\n",
    "        self.m_Wbeta = self.beta1 * self.m_Wbeta + (1 - self.beta1) * dbeta\n",
    "        self.v_Wbeta = self.beta2 * self.v_Wbeta + (1 - self.beta2) * cp.square(dbeta)\n",
    "        \n",
    "        m_W_hat = self.m_Wbeta / (1 - self.beta1 ** self.t)\n",
    "        v_W_hat = self.v_Wbeta / (1 - self.beta2 ** self.t)\n",
    "        self.beta -= learning_rate * m_W_hat / (cp.sqrt(v_W_hat) + self.epsilonopt)\n",
    "        \n",
    "        # Update alpha\n",
    "        dalpha = self.dL_dalpha()\n",
    "        self.m_Walpha = self.beta1 * self.m_Walpha + (1 - self.beta1) * dalpha\n",
    "        self.v_Walpha = self.beta2 * self.v_Walpha + (1 - self.beta2) * cp.square(dalpha)\n",
    "        \n",
    "        m_W_hat = self.m_Walpha / (1 - self.beta1 ** self.t)\n",
    "        v_W_hat = self.v_Walpha / (1 - self.beta2 ** self.t)\n",
    "        self.alpha -= learning_rate * m_W_hat / (cp.sqrt(v_W_hat) + self.epsilonopt)\n",
    "\n",
    "       \n",
    "        \n",
    "class linear_layer: \n",
    "    def __init__(self,input_size,output_size,out=False,only_weights=False,threshold=1):\n",
    "\n",
    "        variance = 2 / (input_size + output_size)  # Variance for Glorot normal initializer\n",
    "       \n",
    "        self.W = cp.random.normal(0, cp.sqrt(variance), (input_size, output_size))\n",
    "          \n",
    "        if not only_weights:\n",
    "            self.b = cp.random.normal(0, cp.sqrt(variance), (output_size,))\n",
    "        \n",
    "        self.clipping_threshold = threshold\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.epsilon = 1e-8\n",
    "        self.t = 0  # Time step for Adam updates\n",
    "        \n",
    "        # Initialize first (m) and second (v) moment vectors for weights and biases\n",
    "        self.m_W = cp.zeros_like(self.W)\n",
    "        self.v_W = cp.zeros_like(self.W)\n",
    "        if not only_weights:\n",
    "            self.m_b = cp.zeros_like(self.b)\n",
    "            self.v_b = cp.zeros_like(self.b)\n",
    "      \n",
    "    def forward(self,x): \n",
    "        self.x=x\n",
    "        Xout = cp.matmul(x, self.W) + self.b \n",
    "        return Xout\n",
    "    \n",
    "    def forward_weights_only(self,x): \n",
    "        self.x=x\n",
    "        Xout = cp.matmul(x, self.W) \n",
    "        return Xout\n",
    "     \n",
    "    def grad(self,dL_dy):\n",
    "        self.dL_dy = dL_dy\n",
    "        # print(\"self.dL_dy\",self.dL_dy)\n",
    "        return self.dL_dy@self.W.T\n",
    "    \n",
    "    def dLoss_dW(self):\n",
    "        return cp.mean(cp.transpose(self.dL_dy,(0,2,1))@self.x,axis=0).T\n",
    "    \n",
    "    def dLoss_db(self):\n",
    "        return cp.mean(cp.mean(self.dL_dy,axis=0))\n",
    "\n",
    "    def update_weights(self,learning_rate):\n",
    "        dW = self.dLoss_dW()\n",
    "        db = self.dLoss_db()  \n",
    "\n",
    "        # Increment time step\n",
    "        self.t += 1\n",
    "\n",
    "        # Update first moment (m) and second moment (v) for weights\n",
    "        self.m_W = self.beta1 * self.m_W + (1 - self.beta1) * dW\n",
    "        self.v_W = self.beta2 * self.v_W + (1 - self.beta2) * cp.square(dW)\n",
    "        # Correct bias in first and second moment for weights\n",
    "        m_W_hat = self.m_W / (1 - self.beta1 ** self.t)\n",
    "        v_W_hat = self.v_W / (1 - self.beta2 ** self.t)\n",
    "        # Update weights using Adam\n",
    "        self.W -= learning_rate * m_W_hat / (cp.sqrt(v_W_hat) + self.epsilon)\n",
    "        \n",
    "        \n",
    "        self.m_b = self.beta1 * self.m_b + (1 - self.beta1) * db\n",
    "        self.v_b = self.beta2 * self.v_b + (1 - self.beta2) * cp.square(db)\n",
    "        # Correct bias in first and second moment for biases\n",
    "        m_b_hat = self.m_b / (1 - self.beta1 ** self.t)\n",
    "        v_b_hat = self.v_b / (1 - self.beta2 ** self.t)\n",
    "        # Update biases using Adam\n",
    "         \n",
    "        #print(up.shape,self.b.shape,db.shape)\n",
    "        self.b -= learning_rate * m_b_hat / (cp.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "    def update_weights_only(self,learning_rate):\n",
    "        dW = self.dLoss_dW()\n",
    "\n",
    "        # Increment time step\n",
    "        self.t += 1\n",
    "\n",
    "        # Update first moment (m) and second moment (v) for weights only\n",
    "        self.m_W = self.beta1 * self.m_W + (1 - self.beta1) * dW\n",
    "        self.v_W = self.beta2 * self.v_W + (1 - self.beta2) * cp.square(dW)\n",
    "        # Correct bias in first and second moment for weights\n",
    "        m_W_hat = self.m_W / (1 - self.beta1 ** self.t)\n",
    "        v_W_hat = self.v_W / (1 - self.beta2 ** self.t)\n",
    "        # Update weights using Adam\n",
    "        self.W -= learning_rate * m_W_hat / (cp.sqrt(v_W_hat) + self.epsilon)\n",
    "        \n",
    "class AdamOptimize:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class fully_connected_block:\n",
    "    def __init__(self,embedding_size,hidden_size,clipping_threshold):\n",
    "        self.embedding_size=embedding_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.linear_layer_1=linear_layer(self.embedding_size,self.hidden_size,threshold=clipping_threshold)\n",
    "        self.linear_layer_2=linear_layer(self.hidden_size,self.embedding_size,threshold=clipping_threshold)\n",
    "        self.dropout=layer_dropout()\n",
    "        self.ReLu=ReLu_layer()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x_1=self.linear_layer_1.forward(x)\n",
    "        x_1_r=self.ReLu.forward(x_1)\n",
    "        #x_1_rd=self.dropout.forward(x_1_r)\n",
    "        x_2=self.linear_layer_2.forward(x_1_r)\n",
    "        return x_2\n",
    "    \n",
    "    def grad(self,dL_dy):\n",
    "        dL_dx_1_rd=self.linear_layer_2.grad(dL_dy)\n",
    "        #dL_dx_1_r=self.dropout.grad(dL_dx_1_rd)\n",
    "        dL_dx_1=self.ReLu.backward(dL_dx_1_rd)\n",
    "        dL_dx=self.linear_layer_1.grad(dL_dx_1)\n",
    "        return dL_dx\n",
    "    \n",
    "    def update_weights(self,learning_rate):\n",
    "        self.linear_layer_1.update_weights(learning_rate)\n",
    "        self.linear_layer_2.update_weights(learning_rate)\n",
    "        \n",
    " \n",
    "class ReLu_layer:\n",
    "    def __init__(self,alpha=0.0001):\n",
    "        self.alpha=alpha \n",
    "    def forward_leaky(self,X):\n",
    "        self.X=X\n",
    "        return cp.where(X > 0, X, self.alpha * X)\n",
    "\n",
    "    def forward(self,X): \n",
    "        self.X=X\n",
    "        return cp.maximum(0,self.X)\n",
    "    \n",
    "    def backward(self, dLoss): \n",
    "        # Gradient of ReLU is 1 for x > 0, else 0\n",
    "        dx = dLoss * (self.X > 0)  # Only propagate gradients for inputs > 0\n",
    "        return dx\n",
    "    \n",
    "    def backward_leaky(self, dLoss): \n",
    "        dx = dLoss * cp.where(self.X > 0, 1, self.alpha)  # Gradient: 1 for x > 0, else alpha\n",
    "        return dx\n",
    "\n",
    "class residual_layer:\n",
    "    def __init__(self,threshold):\n",
    "        self.dropout=layer_dropout()\n",
    "        self.normalization=layer_normalization(threshold=threshold)\n",
    "        self.clipping_threshold=threshold\n",
    "\n",
    "\n",
    "    def forward(self,x,sublayer_output): \n",
    "        residual=self.dropout.forward(sublayer_output)+x\n",
    "        result=self.normalization.forward(residual)\n",
    "        return result\n",
    "    \n",
    "    def grad(self, dL_dy):\n",
    "        dl_dNorm = self.normalization.backpropagation(dL_dy) \n",
    "        scaling_factor = 1.0 #/ np.sqrt(2.0)\n",
    "        sublayer_grad = self.dropout.grad(dl_dNorm) * scaling_factor\n",
    "        residual_grad = dl_dNorm * scaling_factor\n",
    "        \n",
    "        return sublayer_grad, residual_grad\n",
    "    \n",
    "    def update_weights(self,learning_rate):\n",
    "        self.normalization.params_update(learning_rate)\n",
    "\n",
    "\n",
    "      \n",
    "class multihead_attention: \n",
    "    def __init__(self,embedding_size,num_heads,batch_size,threshold):\n",
    "        self.num_heads=num_heads\n",
    "        self.dk=embedding_size//num_heads\n",
    "        self.batch_size=batch_size\n",
    "        self.embedding_size=embedding_size\n",
    "        self.q=linear_layer(self.embedding_size,self.embedding_size,only_weights=True,threshold=threshold)\n",
    "        self.k=linear_layer(self.embedding_size,self.embedding_size,only_weights=True,threshold=threshold)\n",
    "        self.v=linear_layer(self.embedding_size,self.embedding_size,only_weights=True,threshold=threshold)\n",
    "        self.projection_layer=linear_layer(self.embedding_size,self.embedding_size,only_weights=True,threshold=threshold) \n",
    "        self.helper=Helper()\n",
    "        self.clipping_threshold=threshold\n",
    "\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.epsilon = 1e-8\n",
    "        self.t = 0 \n",
    "        self.m_Wq = cp.zeros_like(self.q.W)\n",
    "        self.v_Wq = cp.zeros_like(self.q.W)\n",
    "\n",
    "        self.m_Wk = cp.zeros_like(self.k.W)\n",
    "        self.v_Wk = cp.zeros_like(self.k.W)\n",
    "\n",
    "        self.m_Wv = cp.zeros_like(self.v.W)\n",
    "        self.v_Wv = cp.zeros_like(self.v.W)\n",
    "         \n",
    "    def reshape_heads(self,Q,K,V):\n",
    "        self.Q = cp.swapaxes(cp.array(np.array_split(Q, self.num_heads, axis=2)), 0, 1)\n",
    "        # print(\"Qval.shape: \",Q_E.shape)\n",
    "        self.K = cp.swapaxes(cp.array(np.array_split(K, self.num_heads, axis=2)), 0, 1)\n",
    "        # print(\"Kval.shape: \",K_E.shape)\n",
    "        self.V = cp.swapaxes(cp.array(np.array_split(V, self.num_heads, axis=2)), 0, 1)\n",
    "        #return self.Q,self.K,self.V\n",
    "\n",
    "    def QKV(self,input_q,input_k,input_v): \n",
    "        Q=self.q.forward_weights_only(input_q)\n",
    "        K=self.k.forward_weights_only(input_k)\n",
    "        V=self.v.forward_weights_only(input_v) \n",
    "        self.reshape_heads(Q,K,V)\n",
    "        \n",
    "\n",
    "    def attention_weights(self,mask): \n",
    "         \n",
    "        QKscaled =cp.matmul(self.Q, cp.transpose(self.K, (0, 1, 3, 2))) / cp.sqrt(self.K.shape[-1])  \n",
    "        #print(QKscaled)\n",
    "        if mask is not None:\n",
    "            \n",
    "            # Ensure mask has shape [batch_size, 1, 1, seq_len] for broadcasting\n",
    "            mask = mask[:, cp.newaxis, :]  # Shape: (batch_size, 1, 1, seq_len)\n",
    "            #print(\"mask padding in attention_weights\",mask)\n",
    "            #print(\"mask.shape-------->\",mask.shape)\n",
    "            # Add a large negative value to masked positions\n",
    "            QKscaled = QKscaled + (mask - 1) * 1e9\n",
    "        self.Attention_weights = self.helper.softmax(QKscaled)\n",
    "        #print(\"attention weights\")\n",
    "        #print(self.Attention_weights)\n",
    "         \n",
    "\n",
    "    def forward_attention(self,input_q,input_k,input_v,mask_padding): \n",
    "        self.input_q=input_q\n",
    "        self.input_k=input_k\n",
    "        self.input_v=input_v\n",
    "        #print(\"input_q.shape\",input_q.shape)\n",
    "        self.QKV(input_q,input_k,input_v)\n",
    "        self.attention_weights(mask_padding)\n",
    "        Attention = cp.matmul(self.Attention_weights, self.V) \n",
    "        Attention = cp.array([cp.concatenate(Attention[i], axis=1) for i in range(self.batch_size)]) \n",
    "        Output=self.projection_layer.forward_weights_only(Attention) \n",
    "        return Output\n",
    "    \n",
    "    def forward_masked_attention(self,input_q,input_k,input_v,mask_size,mask_padding):\n",
    "        self.input_q=input_q\n",
    "        self.input_k=input_k\n",
    "        self.input_v=input_v \n",
    "        self.QKV(input_q,input_k,input_v)\n",
    "        self.attention_weights_masked(mask_size,mask_padding)\n",
    "        Attention = cp.matmul(self.Attention_weights, self.V) \n",
    "        Attention = cp.array([cp.concatenate(Attention[i], axis=1) for i in range(self.batch_size)]) \n",
    "        Output=self.projection_layer.forward_weights_only(Attention) \n",
    "        return Output\n",
    "    \n",
    "    def attention_weights_masked(self,mask_size,mask_padding):\n",
    "        #mask_size =  words_per_phrase \n",
    "\n",
    "        QKscaled = cp.matmul(self.Q, cp.transpose(self.K, (0, 1, 3, 2))) / cp.sqrt(self.K.shape[-1])\n",
    "        mask = cp.tril(cp.ones((mask_size, mask_size)))  # (9, 9) lower triangular matrix\n",
    "        mask[mask == 0]=-cp.inf  # Set future tokens to -inf\n",
    "        mask[mask == 1]=0  # Set allowed tokens to 0\n",
    "        self.mask = mask.reshape(1, 1, mask_size, mask_size)\n",
    "        if mask_padding is not None:\n",
    "            \n",
    "            # Ensure mask has shape [batch_size, 1, 1, seq_len] for broadcasting\n",
    "            maskpad = mask_padding[:, cp.newaxis, :]  # Shape: (batch_size, 1, 1, seq_len)\n",
    "            #print(\"mask padding in masked attention\",mask_padding)\n",
    "            #print(\"mask.shape-------->\",maskpad.shape)\n",
    "            # Add a large negative value to masked positions\n",
    "            QKscaled = QKscaled + (maskpad - 1) * 1e9\n",
    "        QKscaled = QKscaled + self.mask\n",
    "       \n",
    "        self.Attention_weights = self.helper.softmax(QKscaled)\n",
    "        \n",
    "    \n",
    "    def diffQi(self,dAttention):\n",
    "        dAttention_weights = self.Attention_weights * (1 - self.Attention_weights) \n",
    "        dLoss_dX=cp.transpose(dAttention, (0, 2, 1)) @ (self.helper.redimension(dAttention_weights @ (self.K * self.V) / cp.sqrt(self.K.shape[-1]))*self.input_q)\n",
    "        self.dLoss_Qi= cp.sum(dLoss_dX,axis=0)\n",
    "    \n",
    "    def diffKi(self,dAttention):\n",
    "        dAttention_weights = self.Attention_weights * (1 - self.Attention_weights) \n",
    "        X = cp.swapaxes(cp.array(cp.array_split(self.input_k, self.num_heads, axis=2)), 0, 1) \n",
    "         \n",
    "        dLoss_dX = cp.transpose(dAttention, (0, 2, 1)) @ self.helper.redimension(\n",
    "            (dAttention_weights * (self.Q @ cp.transpose(self.V, (0, 1, 3, 2))) @ X) / cp.sqrt(self.K.shape[-1])) \n",
    "        self.dLoss_Ki= cp.sum(dLoss_dX,axis=0)\n",
    "    \n",
    "    def diffVi(self,dAttention):\n",
    "        self.dLoss_Vi = cp.sum(cp.sum(cp.transpose(cp.expand_dims(dAttention, axis=1), (0, 1, 3, 2)) @ (\n",
    "                self.Attention_weights @ cp.expand_dims(self.input_v, axis=1)), axis=1), axis=0)\n",
    "       \n",
    "\n",
    "\n",
    "    def diffKInput(self,dAttention):\n",
    "        dAttention_weights = self.Attention_weights * (1 - self.Attention_weights) \n",
    "         \n",
    "        \n",
    "        A = self.helper.redimension(self.V)@self.k.W\n",
    "         \n",
    "        B = self.helper.redimension(dAttention_weights@self.K)\n",
    "         \n",
    "        C=cp.transpose(dAttention,(0,2,1))@B\n",
    "       \n",
    "        dLoss_KI=cp.transpose((C@cp.transpose(A,(0,2,1))),(0,2,1))\n",
    "        #print(\"dLoss_KI.shape\",dLoss_KI.shape)\n",
    "        \n",
    "        return dLoss_KI\n",
    "    \n",
    "    def diffVInput(self,dAttention):\n",
    "        dLoss_V_E = cp.transpose(\n",
    "        cp.mean(cp.transpose(cp.expand_dims(dAttention, axis=1), (0, 1, 3, 2)) @ self.Attention_weights, axis=1), (0, 2, 1))\n",
    "        dLossVI = dLoss_V_E @ self.v.W\n",
    "        #print(\"dLossVI.shape\",dLossVI.shape)\n",
    "        return dLossVI\n",
    "    \n",
    "    def diffQInput(self,dAttention):\n",
    "        dAttention_weights = self.Attention_weights * (1 - self.Attention_weights) \n",
    "        \n",
    "         \n",
    "        A1=self.helper.redimension(dAttention_weights @ (self.K*self.V / cp.sqrt(self.K.shape[-1])))@self.q.W\n",
    "   \n",
    "        dLoss_QI=dAttention*A1\n",
    "        #print(\"dLoss_QI.shape\",dLoss_QI.shape)\n",
    "        return dLoss_QI\n",
    "    \n",
    "    def grad(self,dL_dy): \n",
    "        self.dLoss_dAcr=self.projection_layer.grad(dL_dy)\n",
    "        self.diffQi(self.dLoss_dAcr)\n",
    "        self.diffVi(self.dLoss_dAcr)\n",
    "        self.diffKi(self.dLoss_dAcr)\n",
    "\n",
    "        dLoss_KI=self.diffKInput(self.dLoss_dAcr)\n",
    "        \n",
    "        dLoss_QI=self.diffQInput(self.dLoss_dAcr)\n",
    "        \n",
    "        dLoss_VI=self.diffVInput(self.dLoss_dAcr)\n",
    "       \n",
    "        return dLoss_QI,dLoss_KI,dLoss_VI\n",
    "\n",
    "    def update_weights(self,learning_rate):\n",
    "        \n",
    "\n",
    "        # Increment time step\n",
    "        self.t += 1\n",
    "\n",
    "        # Update first moment (m) and second moment (v) for weights\n",
    "        self.m_Wq = self.beta1 * self.m_Wq + (1 - self.beta1) * self.dLoss_Qi\n",
    "        self.v_Wq = self.beta2 * self.v_Wq + (1 - self.beta2) * cp.square(self.dLoss_Qi)\n",
    "        # Correct bias in first and second moment for weights\n",
    "        m_W_hat = self.m_Wq / (1 - self.beta1 ** self.t)\n",
    "        v_W_hat = self.v_Wq / (1 - self.beta2 ** self.t)\n",
    "        # Update weights using Adam\n",
    "        self.q.W -= learning_rate * m_W_hat / (cp.sqrt(v_W_hat) + self.epsilon)\n",
    "        \n",
    "        \n",
    "        self.m_Wk = self.beta1 * self.m_Wk + (1 - self.beta1) * self.dLoss_Ki\n",
    "        self.v_Wk = self.beta2 * self.v_Wk + (1 - self.beta2) * cp.square(self.dLoss_Ki)\n",
    "        # Correct bias in first and second moment for weights\n",
    "        m_W_hat = self.m_Wk / (1 - self.beta1 ** self.t)\n",
    "        v_W_hat = self.v_Wk / (1 - self.beta2 ** self.t)\n",
    "        # Update weights using Adam\n",
    "        self.k.W -= learning_rate * m_W_hat / (cp.sqrt(v_W_hat) + self.epsilon)\n",
    "\n",
    "        self.m_Wv = self.beta1 * self.m_Wv + (1 - self.beta1) * self.dLoss_Vi\n",
    "        self.v_Wv = self.beta2 * self.v_Wv + (1 - self.beta2) * cp.square(self.dLoss_Vi)\n",
    "        # Correct bias in first and second moment for weights\n",
    "        m_W_hat = self.m_Wv / (1 - self.beta1 ** self.t)\n",
    "        v_W_hat = self.v_Wv / (1 - self.beta2 ** self.t)\n",
    "        # Update weights using Adam\n",
    "        self.v.W -= learning_rate * m_W_hat / (cp.sqrt(v_W_hat) + self.epsilon)\n",
    "\n",
    "\n",
    "        self.projection_layer.update_weights_only(learning_rate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3af4c07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73351bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2333 Edward thinks he is in love with Bella . Rachel wants Edward to open his door . Rachel is outside .\n",
      "vocabulary size 5839\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "min_v = 18\n",
    "max_v = 22\n",
    "helper=Helper()\n",
    "def load_x_y_train_plain():\n",
    "    with open('corpus/train.json', 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            dataset = json.load(f)  # Load the JSON data\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    # #Loop through the list and process each dialogue and summary\n",
    "    for data in dataset:\n",
    "        dialogue = data['dialogue']  # Split dialogue into a list of lines\n",
    "        summary = data['summary']\n",
    "\n",
    "        X_train.append(dialogue)\n",
    "        y_train.append(summary)\n",
    "    return X_train, y_train\n",
    "\n",
    "\n",
    "def split_x_y_train(X_train, y_train):\n",
    "    X_train = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', x) for x in X_train]\n",
    "    y_train = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', y) for y in y_train]\n",
    "    return X_train, y_train\n",
    "\n",
    "\n",
    "# with open('data/vocabolary_full.pkl', 'rb') as f:\n",
    "#     vocabulary=pickle.load(f)\n",
    "def filter_train_data(X_train, y_train, to_eliminate):\n",
    "    filtered_X_train = []\n",
    "    filtered_y_train = []\n",
    "\n",
    "    for x, y in zip(X_train, y_train):\n",
    "        if not any(to_eliminate_str in x for to_eliminate_str in to_eliminate):\n",
    "            filtered_X_train.append(x)\n",
    "            filtered_y_train.append(y)\n",
    "\n",
    "    return filtered_X_train, filtered_y_train\n",
    "\n",
    "\n",
    "def create_complete_vocabulary(X_train, y_train):\n",
    "    nlp_model = spacy.load('en_core_web_lg')\n",
    "    nlp_model.disable_pipes([\"parser\", \"ner\"])\n",
    "    complete_text_target = ' '.join(y_train)\n",
    "    complete_text_origin = ' '.join(X_train)\n",
    "    complete_text = complete_text_target + \" [START] [PAD] [END] \" + complete_text_origin\n",
    "\n",
    "    vocabulary = helper.create_vocabulary(complete_text, \"vocabolary_full\", nlp_model)\n",
    "    print(\"vocabulary size\", len(vocabulary))\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "X_train, y_train = load_x_y_train_plain()\n",
    "# to_eliminate = [\n",
    "#     \"[I hope I'm not coming off as rude - If I am, I'm sorry. I just thought it would be beneficial for the both of us...]\",\n",
    "#     \"[pulls back the curtain and checks out the window]\",\n",
    "#     \"[hopefully, masses of]\"]\n",
    "# X_train, y_train = filter_train_data(X_train, y_train, to_eliminate)\n",
    "\n",
    "\n",
    "sample = [i for i in range(0,len(y_train))]\n",
    "\n",
    "\n",
    "X_train = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', x) for x in X_train]\n",
    "y_train = [re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', y) for y in y_train]\n",
    "\n",
    "\n",
    "X_train = [X_train[i] for i in sample if len(y_train[i]) <= max_v and len(y_train[i]) >= min_v]\n",
    "y_train = [y_train[i] for i in sample if len(y_train[i]) <= max_v and len(y_train[i]) >= min_v]\n",
    "\n",
    "\n",
    "# Calculate lengths of the tokenized phrases\n",
    "\n",
    "\n",
    "def plot_lenghts(X_train,y_train):\n",
    "    X_lengths = [len(x) for x in X_train]\n",
    "    y_lengths = [len(y) for y in y_train]\n",
    "    # Plot histograms\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Histogram for X_train lengths\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(X_lengths, bins=20, kde=False)\n",
    "    plt.title('Histogram of X_train Phrase Lengths')\n",
    "    plt.xlabel('Length of Phrases')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Histogram for y_train lengths\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(y_lengths, bins=20, kde=False)\n",
    "    plt.title('Histogram of y_train Phrase Lengths')\n",
    "    plt.xlabel('Length of Phrases')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Display the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "X_train=[i[::-1] for i in y_train]\n",
    "plot_lenghts(X_train,y_train)\n",
    " \n",
    "\n",
    "X_train=[\" \".join(x) for x in X_train]\n",
    "y_train=[\" \".join(y) for y in y_train]\n",
    "\n",
    "print(len(y_train),y_train[0])\n",
    "\n",
    "vocabulary=create_complete_vocabulary(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c7d792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self,embedding_size,num_heads,linear_layer_size,learning_rate,batch_size,words_per_phrase,clipping_threshold):\n",
    "        self.batch_size=batch_size\n",
    "        self.clipping_threshold=clipping_threshold\n",
    "        self.words_per_phrase=words_per_phrase\n",
    "        self.num_heads=num_heads\n",
    "        self.linear_layer_size=linear_layer_size\n",
    "        self.embedding_size=embedding_size\n",
    "        self.fully_connected_block=fully_connected_block(self.embedding_size,linear_layer_size,clipping_threshold=clipping_threshold)\n",
    "        self.multihead_attention_encoder=multihead_attention(num_heads=2,embedding_size=embedding_size,batch_size=batch_size,threshold=clipping_threshold)\n",
    "        self.residual_layer_1=residual_layer(clipping_threshold)\n",
    "        self.residual_layer_2=residual_layer(clipping_threshold) \n",
    "        self.learning_rate=learning_rate\n",
    "         \n",
    "        self.helper=Helper()\n",
    "     \n",
    "\n",
    "    def forward(self,inputs_e):\n",
    "        self.inputs_e=inputs_e \n",
    "\n",
    "        PrjAe=self.multihead_attention_encoder.forward_attention(inputs_e,inputs_e,inputs_e)\n",
    "        \n",
    "        Ect1=self.residual_layer_1.forward(PrjAe,inputs_e) \n",
    "\n",
    "        FLe2=self.fully_connected_block.forward(Ect1) \n",
    "\n",
    "        Ecout=self.residual_layer_2.forward(FLe2,Ect1)\n",
    "\n",
    "        return Ecout\n",
    "    \n",
    "    def backpropagation(self,dL_Ecout): \n",
    "        \n",
    "        dL_dFLe2,dL_dEct1_residual=self.residual_layer_2.grad(dL_Ecout)\n",
    "        \n",
    "        #print(\"dL_dFLe2\",dL_dFLe2)\n",
    "        #print(\"dL_dEct1_residual\",dL_dFLe2)\n",
    "        \n",
    "\n",
    "        dL_dEct1=self.fully_connected_block.grad(dL_dFLe2)+dL_dEct1_residual\n",
    "        #print(\"dL_dEct1\",dL_dEct1)\n",
    "        \n",
    "        dL_dPrjAe,dL_inputs_e_residual=self.residual_layer_1.grad(dL_dEct1)\n",
    "        #print(\"dL_dPrjAe\",dL_dPrjAe)\n",
    "        #print(\"dL_inputs_e_residual\",dL_inputs_e_residual)\n",
    "        dL_inputs_e_q,dL_inputs_e_k,dL_inputs_e_v=self.multihead_attention_encoder.grad(dL_dPrjAe)\n",
    "        \n",
    "        dL_inputs_e=dL_inputs_e_residual+dL_inputs_e_q+dL_inputs_e_k+dL_inputs_e_v\n",
    "\n",
    "        #dL_inputs_e=clip_gradient(dL_inputs_e,self.clipping_threshold)\n",
    "        #print(\"dL_inputs_e\",dL_inputs_e_residual)\n",
    "        dLoss_dWemb_encoder = dL_inputs_e * self.inputs_e\n",
    "        return dL_inputs_e,dLoss_dWemb_encoder\n",
    "\n",
    "\n",
    "    def update_weights(self,learning_rate,dLoss_dWemb_encoder,X_batch,vocabulary):\n",
    "        self.residual_layer_2.update_weights(learning_rate)\n",
    "        self.fully_connected_block.update_weights(learning_rate)\n",
    "        self.residual_layer_1.update_weights(learning_rate)\n",
    "        self.multihead_attention_encoder.update_weights(learning_rate)\n",
    "        \n",
    "        # input_e=self.inputs_e-learning_rate*dLoss_dWemb_encoder\n",
    "        # vocabulary=self.helper.update_wembedding_encoder(X_batch,input_e,vocabulary,self.words_per_phrase)\n",
    "        return vocabulary\n",
    "        \n",
    "\n",
    "  \n",
    "      \n",
    "     \n",
    "class Decoder:\n",
    "    def __init__(self,embedding_size,num_heads,linear_layer_size,learning_rate,batch_size,words_per_phrase,clipping_threshold):\n",
    "        self.words_per_phrase=words_per_phrase\n",
    "        self.clipping_threshold=clipping_threshold\n",
    "        self.batch_size=batch_size\n",
    "        self.num_heads=num_heads \n",
    "        self.linear_layer_size=linear_layer_size\n",
    "        self.embedding_size=embedding_size  \n",
    "        self.multihead_cross_attention=multihead_attention(num_heads=num_heads,embedding_size=embedding_size,batch_size=batch_size,threshold=clipping_threshold)\n",
    "        self.multihead_attention_decoder=multihead_attention(num_heads=num_heads,embedding_size=embedding_size,batch_size=batch_size,threshold=clipping_threshold) \n",
    "        self.learning_rate=learning_rate\n",
    "        self.helper=Helper() \n",
    "        self.residual_layer_1=residual_layer(threshold=clipping_threshold)\n",
    "        self.residual_layer_2=residual_layer(threshold=clipping_threshold) \n",
    "        self.residual_layer_3=residual_layer(threshold=clipping_threshold) \n",
    "        self.fully_connected_block=fully_connected_block(self.embedding_size,linear_layer_size,clipping_threshold=clipping_threshold)\n",
    "        \n",
    "\n",
    "    def forward(self,inputs_decoder,Ecout):\n",
    "        self.inputs_decoder=inputs_decoder \n",
    "        \n",
    "        PrjA_mask=self.multihead_attention_decoder.forward_masked_attention(inputs_decoder,inputs_decoder,inputs_decoder,mask_size=inputs_decoder.shape[1])\n",
    " \n",
    "        Dt1=self.residual_layer_1.forward(self.inputs_decoder,PrjA_mask)\n",
    " \n",
    "        PrjAcr=self.multihead_cross_attention.forward_attention(Dt1,Ecout,Ecout)\n",
    "   \n",
    "        Dt2=self.residual_layer_2.forward(PrjAcr,Dt1)\n",
    " \n",
    "        FLd2=self.fully_connected_block.forward(Dt2)\n",
    "      \n",
    "        Dout=self.residual_layer_3.forward(FLd2,Dt2)  \n",
    "\n",
    "        return Dout\n",
    "    \n",
    "    # def output(self,Dout):\n",
    "    #     SigmaZout=self.helper.softmax(self.final_projection_layer.forward(Dout))\n",
    "    #     return SigmaZout\n",
    "\n",
    "\n",
    "    def backpropagation(self,dL_dDout): \n",
    "\n",
    "        dL_FLd2,dL_Dt2_residual=self.residual_layer_3.grad(dL_dDout)\n",
    "\n",
    "        dL_Dt2=self.fully_connected_block.grad(dL_FLd2)+dL_Dt2_residual\n",
    "\n",
    "        dL_PrjAcr,dL_Dt1_residual=self.residual_layer_2.grad(dL_Dt2)\n",
    "\n",
    "        dL_Dt1_q,dL_DEcout_k,dL_DEcout_v=self.multihead_cross_attention.grad(dL_PrjAcr)\n",
    "\n",
    "        dL_Dt1=dL_Dt1_residual+dL_Dt1_q\n",
    "\n",
    "        dL_PrjA_mask,dL_inputs_decoder_residual=self.residual_layer_1.grad(dL_Dt1)\n",
    "\n",
    "        dL_inputs_decoder_q,dL_inputs_decoder_k,dL_inputs_decoder_v=self.multihead_attention_decoder.grad(dL_PrjA_mask)\n",
    "\n",
    "        dL_inputs_decoder=dL_inputs_decoder_residual+dL_inputs_decoder_q+dL_inputs_decoder_k+dL_inputs_decoder_v\n",
    "        \n",
    "        dL_Ecout=dL_DEcout_k+dL_DEcout_v\n",
    "        #dL_inputs_decoder=clip_gradient(dL_inputs_decoder,self.clipping_threshold)\n",
    "        dLoss_dWemb_decoder= dL_inputs_decoder * self.inputs_decoder\n",
    "        \n",
    "        return dL_Ecout,dL_inputs_decoder,dLoss_dWemb_decoder\n",
    "\n",
    "\n",
    "    def update_weights(self,learning_rate,dLoss_dWemb_decoder,y_batch,vocabulary):\n",
    "        self.residual_layer_3.update_weights(learning_rate)\n",
    "        self.fully_connected_block.update_weights(learning_rate)\n",
    "        self.residual_layer_2.update_weights(learning_rate)\n",
    "        self.multihead_cross_attention.update_weights(learning_rate)\n",
    "        self.residual_layer_1.update_weights(learning_rate)\n",
    "        self.multihead_attention_decoder.update_weights(learning_rate)\n",
    "        #dLoss_dWemb_decoder=clip_gradient(dLoss_dWemb_decoder,self.clipping_threshold)\n",
    "        # input_d=self.inputs_decoder-learning_rate*dLoss_dWemb_decoder\n",
    "        # vocabulary=self.helper.update_wembedding_decoder(y_batch,input_d,self.words_per_phrase,vocabulary) \n",
    "        return vocabulary\n",
    "        \n",
    "    \n",
    "class Transformer:\n",
    "    def __init__(self,num_layers,embedding_size,num_heads,fl1_size,learning_rate,batch_size,words_per_phrase,clipping_threshold,vocabulary):\n",
    "        self.vocabulary=vocabulary\n",
    "        self.clipping_threshold=clipping_threshold\n",
    "        self.EncoderStack = [Encoder(embedding_size,num_heads,fl1_size,learning_rate,batch_size,words_per_phrase,clipping_threshold) for _ in range(num_layers)]\n",
    "        self.DecoderStack = [Decoder(embedding_size,num_heads,fl1_size,learning_rate,batch_size,words_per_phrase,clipping_threshold) for _ in range(num_layers)]\n",
    "\n",
    "\n",
    "    def forward(self,inputs_e,inputs_decoder,X_batch,y_batch):\n",
    "\n",
    "        self.X_batch=X_batch\n",
    "        self.y_batch=y_batch\n",
    "        Ecout=self.forward_encoder(inputs_e)\n",
    "        Dout=self.forward_decoder(Ecout,inputs_decoder)\n",
    "        return Dout\n",
    "    \n",
    "    def backpropagation(self,dL_dDout):\n",
    "        dL_Ecout,dLoss_dWemb_decoder_tot=self.backpropagation_decoder(dL_dDout)\n",
    "        dL_Ecout,dLoss_dWemb_encoder_tot=self.backpropagation_encoder(dL_Ecout)\n",
    "        return dL_Ecout,dLoss_dWemb_encoder_tot,dLoss_dWemb_decoder_tot\n",
    "\n",
    "    def forward_encoder(self,inputs_e):\n",
    "        for encoder_i in self.EncoderStack:\n",
    "            inputs_e=encoder_i.forward(inputs_e)\n",
    "        return inputs_e\n",
    "    \n",
    "    def forward_decoder(self,Ecout,inputs_decoder):\n",
    "        for decoder_i in self.DecoderStack:\n",
    "            inputs_decoder=decoder_i.forward(inputs_decoder,Ecout)\n",
    "        return inputs_decoder\n",
    "    \n",
    "    def backpropagation_decoder(self,dL_dDout):\n",
    "        tot_dL_dEcout=0\n",
    "        dLoss_dWemb_decoder_tot=0\n",
    "        for decoder_i in reversed(self.DecoderStack):\n",
    "            dL_Ecout_i,dL_dDout,dLoss_dWemb_decoder=decoder_i.backpropagation(dL_dDout)\n",
    "            tot_dL_dEcout+=dL_Ecout_i\n",
    "            dLoss_dWemb_decoder_tot+=dLoss_dWemb_decoder\n",
    "            self.vocabulary=decoder_i.update_weights(decoder_i.learning_rate,dLoss_dWemb_decoder,self.y_batch,self.vocabulary)\n",
    "        return tot_dL_dEcout,dLoss_dWemb_decoder_tot\n",
    "    \n",
    "    def backpropagation_encoder(self,dL_Ecout): \n",
    "        dLoss_dWemb_encoder_tot=0\n",
    "        for encoder_i in reversed(self.EncoderStack):\n",
    "            dL_Ecout,dLoss_dWemb_encoder=encoder_i.backpropagation(dL_Ecout) \n",
    "            dLoss_dWemb_encoder_tot+=dLoss_dWemb_encoder \n",
    "            self.vocabulary=encoder_i.update_weights(encoder_i.learning_rate,dLoss_dWemb_encoder,self.X_batch,self.vocabulary)\n",
    "        return dL_Ecout,dLoss_dWemb_encoder_tot\n",
    "   \n",
    " \n",
    "    #     self.inputs_decoder=self.inputs_decoder-self.learning_rate*dLoss_dWemb_decoder\n",
    "    #     vocabulary=self.helper.update_wembedding_decoder(y_batch,self.inputs_decoder,self.words_per_phrase,vocabulary) \n",
    "    #     return vocabulary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b107b0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "461b1c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import traceback\n",
    "# embedding_size=300\n",
    "# fl1_size=1000 \n",
    "# batch_size=5\n",
    "# num_heads=5\n",
    "# dropout_rate=0.2\n",
    "# words_per_phrase = num_phrases= max_v\n",
    "# num_batches_per_epoch = len(X_train) // batch_size\n",
    "# num_epochs=550\n",
    "# tot_loss_epoch=0\n",
    "# learning_rate=0.001\n",
    "\n",
    "# Output_stack=output_stack(embedding_size,len(vocabulary))\n",
    "# TransformerEncoder=Encoder(embedding_size,num_heads,fl1_size,learning_rate,batch_size=batch_size,words_per_phrase=max_v)\n",
    "\n",
    "\n",
    "\n",
    "# TransformerDecoder=Decoder(embedding_size,num_heads,fl1_size,learning_rate,batch_size=batch_size,words_per_phrase=max_v)\n",
    "# TransformerEncoder1=Encoder(embedding_size,num_heads,fl1_size,learning_rate,batch_size=batch_size,words_per_phrase=max_v)\n",
    "# TransformerEncoder2=Encoder(embedding_size,num_heads,fl1_size,learning_rate,batch_size=batch_size,words_per_phrase=max_v)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "# for epoch in range(1):\n",
    "#     print(\"Loss\",tot_loss_epoch/num_batches_per_epoch)\n",
    "#     tot_loss_epoch=0\n",
    "#     total_accuracy_epoch=0\n",
    "    \n",
    "#     for i in tqdm(range(0,5),desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "#         #try: \n",
    "#         start = i * batch_size\n",
    "#         end = start + batch_size \n",
    "#         X_batch = X_train[start:end]\n",
    "#         y_batch = y_train[start:end] \n",
    "        \n",
    "        \n",
    "#         #helper.print_matrix(y_batch)\n",
    "#         #print(\"X_batch\",X_batch)\n",
    "        \n",
    "        \n",
    "#         inputs_e=helper.create_input_encoder(X_batch,vocabulary,words_per_phrase,embedding_size) \n",
    "#         inputs_decoder=helper.create_decoder_input(y_batch,embedding_size,words_per_phrase,vocabulary) \n",
    "#         target_decoder=helper.create_target(y_batch,vocabulary,words_per_phrase)\n",
    "#         #target_decoder=helper.generate_target_sparse_categorical(y_batch,vocabulary,words_per_phrase)\n",
    "        \n",
    "#         Ecout0 = TransformerEncoder.forward(inputs_e)\n",
    "#         Ecout1=TransformerEncoder1.forward(Ecout0)\n",
    "#         Ecout=TransformerEncoder2.forward(Ecout1)\n",
    "#         Dout = TransformerDecoder.forward(inputs_decoder, Ecout)\n",
    "        \n",
    "        \n",
    "#         SigmaZout = Output_stack.forward(Dout)\n",
    "\n",
    "#         for j in range(SigmaZout.shape[0]):\n",
    "#             for k in range(SigmaZout.shape[1]):\n",
    "#                 #print(np.argmax(SigmaZout[j][k]),np.argmax(target_decoder[j][k]))\n",
    "#                 pass\n",
    "#         #print(target_decoder.shape)\n",
    "\n",
    "#         #print(\"Loss\",Output_stack.sparse_categorical_crossentropy(SigmaZout,target_decoder))\n",
    "#         print(\"Loss\",Output_stack.cross_entropy_loss(SigmaZout,target_decoder))\n",
    "#         dL_dDout = Output_stack.grad_cross_entropy(SigmaZout,target_decoder)\n",
    "\n",
    "#         print(\"dL_dDout\",dL_dDout.shape) \n",
    "         \n",
    "#         dL_Ecout,dLoss_dWemb_decoder = TransformerDecoder.backpropagation(dL_dDout)\n",
    "\n",
    "#         dLoss_dWemb_encoder0=TransformerEncoder.backpropagation(dL_Ecout) \n",
    "#         dLoss_dWemb_encoder1=TransformerEncoder1.backpropagation(dLoss_dWemb_encoder0) \n",
    "#         dLoss_dWemb_encoder2=TransformerEncoder2.backpropagation(dLoss_dWemb_encoder1) \n",
    "\n",
    "        \n",
    "#         TransformerDecoder.update_weights(learning_rate)\n",
    "#         TransformerDecoder.update_weights(learning_rate)\n",
    "#         Output_stack.update_weights(learning_rate)\n",
    "        #inputs_decoder=inputs_decoder-learning_rate*dLoss_dWemb_decoder\n",
    "        #vocabulary=helper.update_wembedding_decoder(y_batch,inputs_decoder,words_per_phrase,vocabulary) \n",
    "        #inputs_e=inputs_e-learning_rate*dLoss_dWemb_encoder\n",
    "        #vocabulary=helper.update_wembedding_encoder(X_batch,inputs_e,vocabulary,words_per_phrase)\n",
    "        #print(\"dL_Ecout\",dL_Ecout.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a903f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47caf68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import traceback\n",
    "# embedding_size=300\n",
    "# fl1_size=2048\n",
    "# batch_size=8\n",
    "# num_heads=10\n",
    "# dropout_rate=0.1\n",
    "# words_per_phrase = num_phrases= max_v\n",
    "# num_batches_per_epoch = len(X_train) // batch_size\n",
    "# num_epochs=550\n",
    "# tot_loss_epoch=0\n",
    "# learning_rate=0.0001\n",
    "\n",
    "# Output_stack=output_stack(embedding_size,len(vocabulary),threshold=1)\n",
    "# TransformerEncoder=Encoder(embedding_size,num_heads,fl1_size,learning_rate,batch_size=batch_size,words_per_phrase=max_v,clipping_threshold=1)\n",
    "# TransformerDecoder=Decoder(embedding_size,num_heads,fl1_size,learning_rate,batch_size=batch_size,words_per_phrase=max_v,clipping_threshold=1)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "# for epoch in range(1):\n",
    "#     print(\"Loss\",tot_loss_epoch/num_batches_per_epoch)\n",
    "#     tot_loss_epoch=0\n",
    "#     total_accuracy_epoch=0\n",
    "    \n",
    "#     for i in tqdm(range(0,6),desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "#         #try: \n",
    "#         start = i * batch_size\n",
    "#         end = start + batch_size \n",
    "#         X_batch = X_train[start:end]\n",
    "#         y_batch = y_train[start:end] \n",
    "        \n",
    "        \n",
    "#         #helper.print_matrix(y_batch)\n",
    "#         #print(\"X_batch\",X_batch)\n",
    "        \n",
    "        \n",
    "#         inputs_e=helper.create_input_encoder(X_batch,vocabulary,words_per_phrase,embedding_size) \n",
    "#         inputs_decoder=helper.create_decoder_input(y_batch,embedding_size,words_per_phrase,vocabulary) \n",
    "#         #target_decoder=helper.create_target(y_batch,vocabulary,words_per_phrase)\n",
    "#         target_decoder=helper.generate_target_sparse_categorical(y_batch,vocabulary,words_per_phrase)\n",
    "        \n",
    "#         Ecout = TransformerEncoder.forward(inputs_e)\n",
    "         \n",
    "#         Dout = TransformerDecoder.forward(inputs_decoder, Ecout)\n",
    "        \n",
    "#         SigmaZout = Output_stack.forward(Dout)\n",
    "        \n",
    "#         print(\"SigmaZout.shape\",SigmaZout.shape)\n",
    "#         print(\"target_decoder.shape\",target_decoder.shape)\n",
    "#         print(\"Loss\",Output_stack.sparse_categorical_crossentropy(SigmaZout,target_decoder))\n",
    "#         dL_dDout = Output_stack.grad_sparse_cross_entropy(SigmaZout,target_decoder)\n",
    "#         #print(\"Loss\",Output_stack.Loss_cross_entropy(SigmaZout,target_decoder))\n",
    "#         #dL_dDout = Output_stack.grad_cross_entropy(SigmaZout,target_decoder)\n",
    "\n",
    "#         print(\"dL_dDout\",dL_dDout.shape) \n",
    "         \n",
    "#         dL_Ecout,dLoss_dWemb_decoder = TransformerDecoder.backpropagation(dL_dDout)\n",
    "\n",
    "#         dLoss_dWemb_encoder=TransformerEncoder.backpropagation(dL_Ecout) \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "#         TransformerDecoder.update_weights(learning_rate)\n",
    "#         TransformerDecoder.update_weights(learning_rate)\n",
    "#         Output_stack.update_weights(learning_rate)\n",
    "#         #inputs_decoder=inputs_decoder-learning_rate*dLoss_dWemb_decoder\n",
    "#         #vocabulary=helper.update_wembedding_decoder(y_batch,inputs_decoder,words_per_phrase,vocabulary) \n",
    "#         #inputs_e=inputs_e-learning_rate*dLoss_dWemb_encoder\n",
    "#         #vocabulary=helper.update_wembedding_encoder(X_batch,inputs_e,vocabulary,words_per_phrase)\n",
    "#         #print(\"dL_Ecout\",dL_Ecout.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dae219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9350b4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2333, 5839)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train),len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a882c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "embedding_size=300\n",
    "fl1_size=2048\n",
    "batch_size=64\n",
    "num_heads=15\n",
    "dropout_rate=0.1\n",
    "words_per_phrase = num_phrases= max_v\n",
    "num_batches_per_epoch = len(X_train) // batch_size\n",
    "num_epochs=550\n",
    "tot_loss_epoch=0\n",
    "learning_rate=0.000005\n",
    "clipping_threshold=9000000000\n",
    "\n",
    "\n",
    "\n",
    "n_layers=7\n",
    "Output_stack=output_stack(embedding_size,len(vocabulary),threshold=clipping_threshold)\n",
    "MyTransformer=Transformer(n_layers,embedding_size,num_heads,fl1_size,learning_rate,batch_size,words_per_phrase,clipping_threshold,vocabulary)\n",
    "output_linear_layer=linear_layer(embedding_size,len(vocabulary),out=True) \n",
    "accuracies=[0,0]\n",
    "mean_acc=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fbf5a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    " \n",
    " \n",
    "# for epoch in range(550):\n",
    "#     print(\"Loss\",tot_loss_epoch/num_batches_per_epoch,\"mean accuracy\",np.mean(np.array(accuracies)))#np.mean(np.array(accuracies))\n",
    "#     tot_loss_epoch=0\n",
    "#     total_accuracy_epoch=0\n",
    "#     mean_acc=0\n",
    "#     accuracies=[]\n",
    "#     for i in tqdm(range(0,num_batches_per_epoch),desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "#         #try: \n",
    "#         start = i * batch_size\n",
    "#         end = start + batch_size \n",
    "#         X_batch = X_train[start:end]\n",
    "#         y_batch = y_train[start:end]  \n",
    "        \n",
    "#         inputs_e=helper.create_input_encoder(X_batch,vocabulary,words_per_phrase,embedding_size) \n",
    "#         inputs_decoder=helper.create_decoder_input(y_batch,embedding_size,words_per_phrase,vocabulary) \n",
    "#         target_decoder=helper.create_target(y_batch,vocabulary,words_per_phrase) \n",
    "#         Dout=MyTransformer.forward(inputs_e,inputs_decoder,X_batch,y_batch)\n",
    "#         counter_beccate=0\n",
    "#         counter_tot=0 \n",
    "#         SigmaZout = Output_stack.forward(Dout)\n",
    "        \n",
    "#         taccuracies=[]\n",
    "#         for n in range(SigmaZout.shape[0]): \n",
    "#             len_phrase=SigmaZout.shape[1]\n",
    "#             counter_beccate=0 \n",
    "#             for l in range(SigmaZout.shape[1]): \n",
    "#                 if np.argmax(SigmaZout[n][l])==np.argmax(target_decoder[n][l]): \n",
    "#                     counter_beccate+=1\n",
    "#                     #print(np.argmax(SigmaZout[n][l]))\n",
    "#             phrase_accuracy=counter_beccate/len_phrase\n",
    "#             taccuracies.append(phrase_accuracy)\n",
    "\n",
    "#         accuracies.append(np.mean(np.array(taccuracies))) \n",
    "\n",
    "\n",
    "#         Loss = Output_stack.cross_entropy_loss(SigmaZout,target_decoder) \n",
    "#         tot_loss_epoch+=Loss\n",
    " \n",
    "#         dL_dDout = Output_stack.grad_cross_entropy(SigmaZout,target_decoder)\n",
    "        \n",
    "         \n",
    "\n",
    "\n",
    "#         dL_Ecout,dLoss_dWemb_encoder_tot,dLoss_dWemb_decoder_tot=MyTransformer.backpropagation(dL_dDout) \n",
    "#         Output_stack.update_weights(learning_rate)\n",
    "#         vocabulary=helper.update_wembedding_decoder(learning_rate,y_batch, dLoss_dWemb_decoder_tot,vocabulary, max_v  )\n",
    "#         vocabulary=helper.update_wembedding_encoder(learning_rate,X_batch, dLoss_dWemb_encoder_tot,vocabulary,max_v)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e29d6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odd 0\n",
      "Addie 1\n",
      "Aria 2\n",
      "Brexit 3\n",
      "disappear 4\n",
      "didn 5\n",
      "Remi 6\n",
      "has 7\n",
      "form 8\n",
      "TI 9\n",
      "agreed 10\n",
      "tiring 11\n",
      "Doris 12\n",
      "opera 13\n",
      "Gralecyn 14\n",
      "tour 15\n",
      "wasted 16\n",
      "fever 17\n",
      "graduation 18\n",
      "Sima 19\n",
      "Brooke 20\n",
      "Nish 21\n",
      "preparing 22\n",
      "fridge 23\n",
      "songs 24\n",
      "Amy 25\n",
      "other 26\n",
      "style 27\n",
      "hist 28\n",
      "aware 29\n",
      "level 30\n",
      "evening 31\n",
      "Maciek 32\n",
      "Pruszkw 33\n",
      "Dharma 34\n",
      "accordingly 35\n",
      "Della 36\n",
      "24th 37\n",
      "contract 38\n",
      "elbow 39\n",
      "Greg 40\n",
      "Mars 41\n",
      "previous 42\n",
      "delivery 43\n",
      "Kaitlyn 44\n",
      "well 45\n",
      "USA 46\n",
      "cleaning 47\n",
      "nights 48\n",
      "Sixty 49\n",
      "Julien 50\n",
      "Saint 51\n",
      "partying 52\n",
      "cooking 53\n",
      "15 54\n",
      "within 55\n",
      "absence 56\n",
      "reports 57\n",
      "nearby 58\n",
      "Journey 59\n",
      "stressful 60\n",
      "setting 61\n",
      "Hank 62\n",
      "accommodation 63\n",
      "day 64\n",
      "Marry 65\n",
      "demotivated 66\n",
      "him 67\n",
      "together 68\n",
      "Gabiel 69\n",
      "handmaidens 70\n",
      "Kendra 71\n",
      "Beth 72\n",
      "joke 73\n",
      "coordinates 74\n",
      "urgently 75\n",
      "plan 76\n",
      "turned 77\n",
      "jars 78\n",
      "palette 79\n",
      "public 80\n",
      "seeing 81\n",
      "yoghurt 82\n",
      "selling 83\n",
      "Marvel 84\n",
      "min 85\n",
      "Cardiff 86\n",
      "ankle 87\n",
      "Robin 88\n",
      "kids 89\n",
      "overwhelmed 90\n",
      "impostor 91\n",
      "Eugene 92\n",
      "Caren 93\n",
      "crocodile 94\n",
      "toothache 95\n",
      "Howard 96\n",
      "ethical 97\n",
      "sledding 98\n",
      "Marge 99\n",
      "Jennifer 100\n",
      "Tim 101\n",
      "throwing 102\n",
      "hygiene 103\n",
      "river 104\n",
      "Richard 105\n",
      "Mae 106\n",
      "Alba 107\n",
      "suggested 108\n",
      "top 109\n",
      "nice 110\n",
      "8PM 111\n",
      "rice 112\n",
      "Focus 113\n",
      "nervous 114\n",
      "engaged 115\n",
      "syrup 116\n",
      "Aron 117\n",
      "cat 118\n",
      "Amari 119\n",
      "abandon 120\n",
      "feelt 121\n",
      "climbing 122\n",
      "Guillermo 123\n",
      "gloves 124\n",
      "digital 125\n",
      "p 126\n",
      "Wild 127\n",
      "120 128\n",
      "vote 129\n",
      "Before 130\n",
      "desperados 131\n",
      "raining 132\n",
      "Nils 133\n",
      "Filip 134\n",
      "Mondays 135\n",
      "lobster 136\n",
      "Ridge 137\n",
      "closet 138\n",
      "lands 139\n",
      "tech 140\n",
      "laryngologist 141\n",
      "IQ 142\n",
      "texts 143\n",
      "Michaela 144\n",
      "Idea 145\n",
      "Eliza 146\n",
      "schedules 147\n",
      "January 148\n",
      "arriving 149\n",
      "[PAD] 150\n",
      "working 151\n",
      "future 152\n",
      "laps 153\n",
      "explosion 154\n",
      "Will 155\n",
      "you 156\n",
      "existence 157\n",
      "negotiated 158\n",
      "Tuber 159\n",
      "memphis 160\n",
      "appreciated 161\n",
      "album 162\n",
      "Jules 163\n",
      "surprised 164\n",
      "Barcelona 165\n",
      "Nipawin 166\n",
      "Celesta 167\n",
      "drama 168\n",
      "Sonia 169\n",
      "insult 170\n",
      "Instagram 171\n",
      "fear 172\n",
      "Redemption 173\n",
      "Elisa 174\n",
      "voodoo 175\n",
      "supposes 176\n",
      "found 177\n",
      "remembers 178\n",
      "sexy 179\n",
      "Filo 180\n",
      "Janet 181\n",
      "needn 182\n",
      "Pacific 183\n",
      "De 184\n",
      "Mel 185\n",
      "visited 186\n",
      "MATH110 187\n",
      "Preston 188\n",
      "Cevapi 189\n",
      "Church 190\n",
      "Saskatchewan 191\n",
      "China 192\n",
      "notify 193\n",
      "Vic 194\n",
      "wallet 195\n",
      "Crystal 196\n",
      "opened 197\n",
      "Smiths 198\n",
      "Ivan 199\n",
      "night 200\n",
      "Trinity 201\n",
      "career 202\n",
      "Scarlett 203\n",
      "launch 204\n",
      "Bolsonaro 205\n",
      "Ford 206\n",
      "rented 207\n",
      "Cabernet 208\n",
      "countries 209\n",
      "Carol 210\n",
      "Jem 211\n",
      "Shelly 212\n",
      "number 213\n",
      "Samuel 214\n",
      "Adrienne 215\n",
      "weight 216\n",
      "channel 217\n",
      "tire 218\n",
      "Susanne 219\n",
      "royal 220\n",
      "dollar 221\n",
      "prepare 222\n",
      "travel 223\n",
      "visit 224\n",
      "concerned 225\n",
      "recipes 226\n",
      "revisit 227\n",
      "Stan 228\n",
      "Seb 229\n",
      "Subway 230\n",
      "sedan 231\n",
      "Cady 232\n",
      "Jim 233\n",
      "Ginnie 234\n",
      "Porter 235\n",
      "Market 236\n",
      "annoyed 237\n",
      "halfway 238\n",
      "flavours 239\n",
      "26 240\n",
      "compared 241\n",
      "1780 242\n",
      "lose 243\n",
      "literature 244\n",
      "crashing 245\n",
      "passes 246\n",
      "lovely 247\n",
      "Cindy 248\n",
      "blacklisted 249\n",
      "There 250\n",
      "keen 251\n",
      "ten 252\n",
      "massage 253\n",
      "greet 254\n",
      "Lampard 255\n",
      "account 256\n",
      "boiler 257\n",
      "Alison 258\n",
      "version 259\n",
      "traffic 260\n",
      "checked 261\n",
      "band 262\n",
      "believes 263\n",
      "gifs 264\n",
      "PE 265\n",
      "Moneverdi 266\n",
      "classroom 267\n",
      "carom 268\n",
      "To 269\n",
      "closing 270\n",
      "minute 271\n",
      "Kayne 272\n",
      "fond 273\n",
      "complimenting 274\n",
      "rip 275\n",
      "assist 276\n",
      "Ted 277\n",
      "Groundhog 278\n",
      "Piyush 279\n",
      "invite 280\n",
      "J 281\n",
      "highschool 282\n",
      "Freddie 283\n",
      "Rhonda 284\n",
      "395 285\n",
      "fish 286\n",
      "Willy 287\n",
      "arrival 288\n",
      "drill 289\n",
      "Claude 290\n",
      "Dorothy 291\n",
      "signs 292\n",
      "Kenneth 293\n",
      "Calgary 294\n",
      "Hedge 295\n",
      "filtered 296\n",
      "currently 297\n",
      "100k 298\n",
      "included 299\n",
      "win 300\n",
      "ignoring 301\n",
      "hadn 302\n",
      "Agatha 303\n",
      "Clare 304\n",
      "present 305\n",
      "invitations 306\n",
      "Latoya 307\n",
      "strawberry 308\n",
      "Grandpa 309\n",
      "Timothy 310\n",
      "washing 311\n",
      "inviting 312\n",
      "Nathan 313\n",
      "fracture 314\n",
      "dry 315\n",
      "apologizes 316\n",
      "Mark 317\n",
      "favour 318\n",
      "Cinema 319\n",
      "grab 320\n",
      "Bose 321\n",
      "prefer 322\n",
      "housewife 323\n",
      "adorable 324\n",
      "travelling 325\n",
      "solo 326\n",
      "Rapper 327\n",
      "Braden 328\n",
      "If 329\n",
      "entrance 330\n",
      "Heart 331\n",
      "Freda 332\n",
      "Rob 333\n",
      "using 334\n",
      "reception 335\n",
      "do 336\n",
      "smoking 337\n",
      "Melany 338\n",
      "95 339\n",
      "correct 340\n",
      "safety 341\n",
      "Quarter 342\n",
      "killers 343\n",
      "Orhan 344\n",
      "teacher 345\n",
      "would 346\n",
      "fight 347\n",
      "cabinet 348\n",
      "compliment 349\n",
      "followed 350\n",
      "Cane 351\n",
      "30ish 352\n",
      "Juliet 353\n",
      "Brangelina 354\n",
      "Historical 355\n",
      "manage 356\n",
      "India 357\n",
      "Yate 358\n",
      "Joanne 359\n",
      "sign 360\n",
      "o 361\n",
      "messaged 362\n",
      "apartment 363\n",
      "Tara 364\n",
      "decides 365\n",
      "Garcia 366\n",
      "Arena 367\n",
      "stewart 368\n",
      "Sadie 369\n",
      "several 370\n",
      "perfume 371\n",
      "they 372\n",
      "Bale 373\n",
      "Smooshy 374\n",
      "master 375\n",
      "repair 376\n",
      "Elizabeth 377\n",
      "downstairs 378\n",
      "Sue 379\n",
      "manicure 380\n",
      "scan 381\n",
      "island 382\n",
      "information 383\n",
      "married 384\n",
      "without 385\n",
      "cm 386\n",
      "drain 387\n",
      "mistake 388\n",
      "books 389\n",
      "ll 390\n",
      "heater 391\n",
      "against 392\n",
      "Sara 393\n",
      "success 394\n",
      "Russ 395\n",
      "Wisconsin 396\n",
      "museum 397\n",
      "group 398\n",
      "sell 399\n",
      "player 400\n",
      "note 401\n",
      "Renee 402\n",
      "Carlo 403\n",
      "cats 404\n",
      "Game 405\n",
      "Martha 406\n",
      "cappuccino 407\n",
      "Masuria 408\n",
      "keep 409\n",
      "Arianna 410\n",
      "escape 411\n",
      "carbonara 412\n",
      "tasks 413\n",
      "High 414\n",
      "Mexican 415\n",
      "Vienna 416\n",
      "Grasso 417\n",
      "Raymond 418\n",
      "expects 419\n",
      "Beaujolais 420\n",
      "provide 421\n",
      "dieting 422\n",
      "Cameron 423\n",
      "hospitalized 424\n",
      "Globe 425\n",
      "Jeffery 426\n",
      "scores 427\n",
      "calling 428\n",
      "stairs 429\n",
      "70 430\n",
      "Regent 431\n",
      "hamster 432\n",
      "Ronson 433\n",
      "trailer 434\n",
      "replace 435\n",
      "Bernie 436\n",
      "talk 437\n",
      "May 438\n",
      "Kev 439\n",
      "substance 440\n",
      "reply 441\n",
      "posted 442\n",
      "differences 443\n",
      "Karolina 444\n",
      "still 445\n",
      "for 446\n",
      "gone 447\n",
      "Blizzard 448\n",
      "+ 449\n",
      "McGregor 450\n",
      "Claudia 451\n",
      "booking 452\n",
      "any 453\n",
      "Nikos 454\n",
      "fundraiser 455\n",
      "Charlotte 456\n",
      "Nikolas 457\n",
      "Jaron 458\n",
      "many 459\n",
      "La 460\n",
      "relax 461\n",
      "technical 462\n",
      "Bodyguard 463\n",
      "nominate 464\n",
      "Brad 465\n",
      "His 466\n",
      "Harry 467\n",
      "things 468\n",
      "PS4 469\n",
      "bake 470\n",
      "Coffee 471\n",
      "wrong 472\n",
      "panicking 473\n",
      "Phoebe 474\n",
      "Arcadia 475\n",
      "attempt 476\n",
      "Ryan 477\n",
      "anniversary 478\n",
      "explanations 479\n",
      "psychologist 480\n",
      "possibility 481\n",
      "Pepper 482\n",
      "Hunter 483\n",
      "ambassador 484\n",
      "adult 485\n",
      "Dalia 486\n",
      "collapsed 487\n",
      "approve 488\n",
      "official 489\n",
      "Herman 490\n",
      "Elaine 491\n",
      "Wendani 492\n",
      "North 493\n",
      "lived 494\n",
      "neighbor 495\n",
      "caused 496\n",
      "brought 497\n",
      "Lucas 498\n",
      "whose 499\n",
      "as 500\n",
      "earphones 501\n",
      "consideration 502\n",
      "behaviour 503\n",
      "awaiting 504\n",
      "pump 505\n",
      "terms 506\n",
      "sacrificed 507\n",
      "integration 508\n",
      "ecstatic 509\n",
      "finds 510\n",
      "spends 511\n",
      "lotion 512\n",
      "omelette 513\n",
      "centre 514\n",
      "September 515\n",
      "Abu 516\n",
      "B 517\n",
      "greek 518\n",
      "childish 519\n",
      "Lebron 520\n",
      "log 521\n",
      "Aztec 522\n",
      "Voice 523\n",
      "frame 524\n",
      "term 525\n",
      "variety 526\n",
      "Mill 527\n",
      "random 528\n",
      "Ruby 529\n",
      "flying 530\n",
      "candle 531\n",
      "designed 532\n",
      "Eaton 533\n",
      "bottled 534\n",
      "' 535\n",
      "Emma 536\n",
      "lovers 537\n",
      "pasta 538\n",
      "pharmacy 539\n",
      "globalization 540\n",
      "such 541\n",
      "ready 542\n",
      "Mercy 543\n",
      "studied 544\n",
      "accommodating 545\n",
      "Ewa 546\n",
      "professor 547\n",
      "aidy 548\n",
      "wasn 549\n",
      "gig 550\n",
      "fact 551\n",
      "Bree 552\n",
      "Brandi 553\n",
      "ill 554\n",
      "bag 555\n",
      "Friday 556\n",
      "Melvin 557\n",
      "handbag 558\n",
      "prize 559\n",
      "thousand 560\n",
      "Raphael 561\n",
      "advices 562\n",
      "Kenya 563\n",
      "tax 564\n",
      "Dan 565\n",
      "needed 566\n",
      "church 567\n",
      "rents 568\n",
      "retirement 569\n",
      "Cambridge 570\n",
      "Vegas 571\n",
      "guilty 572\n",
      "Even 573\n",
      "Maryann 574\n",
      "26th 575\n",
      "full 576\n",
      "shared 577\n",
      "Make 578\n",
      "Nationalists 579\n",
      "Wizz 580\n",
      "Dixit 581\n",
      "hypocrisy 582\n",
      "Youssouf 583\n",
      "Leanne 584\n",
      "Lila 585\n",
      "celeb 586\n",
      "personal 587\n",
      "Finn 588\n",
      "asking 589\n",
      "aerobic 590\n",
      "prom 591\n",
      "twice 592\n",
      "drive 593\n",
      "Tabby 594\n",
      "pound 595\n",
      "Meir 596\n",
      "lower 597\n",
      "Louise 598\n",
      "advert 599\n",
      "stapler 600\n",
      "Someone 601\n",
      "Steven 602\n",
      "Anny 603\n",
      "Owen 604\n",
      "Alfred 605\n",
      "groceries 606\n",
      "cafetaria 607\n",
      "hold 608\n",
      "sponsorship 609\n",
      "Pete 610\n",
      "ibuprofen 611\n",
      "Sylwia 612\n",
      "TUI 613\n",
      "buffet 614\n",
      "Nettie 615\n",
      "reading 616\n",
      "Wendy 617\n",
      "Todd 618\n",
      "K 619\n",
      "avoid 620\n",
      "lactose 621\n",
      "casa 622\n",
      "Kamila 623\n",
      " 624\n",
      "TA 625\n",
      "caught 626\n",
      "twins 627\n",
      "Brook 628\n",
      "heading 629\n",
      "stopped 630\n",
      "Ned 631\n",
      "superhero 632\n",
      "shouldn 633\n",
      "packs 634\n",
      "CD 635\n",
      "toolbox 636\n",
      "Philippe 637\n",
      "Micheal 638\n",
      "hand 639\n",
      "Jaslene 640\n",
      "handle 641\n",
      "prank 642\n",
      "Magdalene 643\n",
      "involved 644\n",
      "starts 645\n",
      "hours 646\n",
      "beaten 647\n",
      "Jessica 648\n",
      "scoring 649\n",
      "carne 650\n",
      "S 651\n",
      "Roger 652\n",
      "becoming 653\n",
      "Wells 654\n",
      "BL 655\n",
      "sociology 656\n",
      "impression 657\n",
      "Julies 658\n",
      "plane 659\n",
      "swimming 660\n",
      "away 661\n",
      "Lavender 662\n",
      "mikey 663\n",
      "denies 664\n",
      "Apetite 665\n",
      "pup 666\n",
      "Yesterday 667\n",
      "Amazon 668\n",
      "fired 669\n",
      "waste 670\n",
      "bringing 671\n",
      "Paddy 672\n",
      "food 673\n",
      "brainstorming 674\n",
      "Neo 675\n",
      "Amore 676\n",
      "no 677\n",
      "recycling 678\n",
      "Xander 679\n",
      "property 680\n",
      "giant 681\n",
      "Kenny 682\n",
      "recommending 683\n",
      "doing 684\n",
      "winner 685\n",
      "herself 686\n",
      "brothers 687\n",
      "art 688\n",
      "does 689\n",
      "Marisa 690\n",
      "Sweden 691\n",
      "Darius 692\n",
      "seriously 693\n",
      "Cristine 694\n",
      "gate 695\n",
      "Rowley 696\n",
      "Barrett 697\n",
      "Alan 698\n",
      "Lina 699\n",
      "Irwin 700\n",
      "Jeff 701\n",
      "Simone 702\n",
      "African 703\n",
      "pirate 704\n",
      "Talia 705\n",
      "lawyer 706\n",
      "However 707\n",
      "Diggle 708\n",
      "Radiohead 709\n",
      "endorse 710\n",
      "healthy 711\n",
      "head 712\n",
      "Mercury 713\n",
      "died 714\n",
      "advise 715\n",
      "occasion 716\n",
      "overslept 717\n",
      "Fridhemsplan 718\n",
      "errors 719\n",
      "framing 720\n",
      "article 721\n",
      "charges 722\n",
      "Kelly 723\n",
      "now 724\n",
      "freedoms 725\n",
      "Actually 726\n",
      "revealed 727\n",
      "slip 728\n",
      "bins 729\n",
      "Killing 730\n",
      "set 731\n",
      "Long 732\n",
      "info 733\n",
      "Zuza 734\n",
      "Marin 735\n",
      "Moira 736\n",
      "project 737\n",
      "Hudgens 738\n",
      "Their 739\n",
      "vaping 740\n",
      "Toronto 741\n",
      "online 742\n",
      "ideas 743\n",
      "Gigi 744\n",
      "Antifa 745\n",
      "chip 746\n",
      "Gallery 747\n",
      "town 748\n",
      "Parents 749\n",
      "festive 750\n",
      "stained 751\n",
      "comic 752\n",
      "Following 753\n",
      "play 754\n",
      "supper 755\n",
      "Jasmine 756\n",
      "perform 757\n",
      "hopes 758\n",
      "Clarissa 759\n",
      "accepts 760\n",
      "Alana 761\n",
      "initial 762\n",
      "wallpapering 763\n",
      "Judy 764\n",
      "Podsiadlo 765\n",
      "Timmy 766\n",
      "30 767\n",
      "youth 768\n",
      "Jonas 769\n",
      "who 770\n",
      "mad 771\n",
      "heavily 772\n",
      "emailed 773\n",
      "Jimena 774\n",
      "relations 775\n",
      "Camilla 776\n",
      "prevent 777\n",
      "Best 778\n",
      "study 779\n",
      "herb 780\n",
      "20 781\n",
      "wrote 782\n",
      "Queen 783\n",
      "Half 784\n",
      "say 785\n",
      "Mirror 786\n",
      "Ronaldo 787\n",
      "job 788\n",
      "beauty 789\n",
      "saw 790\n",
      "usual 791\n",
      "snobby 792\n",
      "seems 793\n",
      "organising 794\n",
      "disgracing 795\n",
      "Orwell 796\n",
      "forrest 797\n",
      "race 798\n",
      "travels 799\n",
      "Billy 800\n",
      "Chelsea 801\n",
      "aggresive 802\n",
      "nails 803\n",
      "blocked 804\n",
      "upon 805\n",
      "unbearable 806\n",
      "authorities 807\n",
      "windowpane 808\n",
      "year 809\n",
      "Ginny 810\n",
      "appeared 811\n",
      "HP 812\n",
      "40 813\n",
      "elder 814\n",
      "Cate 815\n",
      "appreciates 816\n",
      "Emanuel 817\n",
      "Exercises 818\n",
      "coffee 819\n",
      "stunned 820\n",
      "drank 821\n",
      "landlord 822\n",
      "sounded 823\n",
      "Odin 824\n",
      "Paco 825\n",
      "reschedule 826\n",
      "Kaja 827\n",
      "Rose 828\n",
      "Moss 829\n",
      "William 830\n",
      "chick 831\n",
      "Horacy 832\n",
      "Cynthia 833\n",
      "miss 834\n",
      "Tegan 835\n",
      "owes 836\n",
      "dreading 837\n",
      "gaming 838\n",
      "UN 839\n",
      "tommorow 840\n",
      "Hector 841\n",
      "Paulina 842\n",
      "Alanis 843\n",
      "Stop 844\n",
      "details 845\n",
      "scratching 846\n",
      "Elias 847\n",
      "walk 848\n",
      "Chapel 849\n",
      "Lindsay 850\n",
      "felt 851\n",
      "writing 852\n",
      "Orthodox 853\n",
      "procedure 854\n",
      "Adella 855\n",
      "biked 856\n",
      "Kento 857\n",
      "Dad 858\n",
      "tipping 859\n",
      "bad 860\n",
      "reminds 861\n",
      "posts 862\n",
      "waiver 863\n",
      "few 864\n",
      "Lois 865\n",
      "improves 866\n",
      "pho 867\n",
      "Papa 868\n",
      "Queens 869\n",
      "eat 870\n",
      "picked 871\n",
      "sad 872\n",
      "of 873\n",
      "Ezra 874\n",
      "wonder 875\n",
      "Holly 876\n",
      "preference 877\n",
      "sunflowers 878\n",
      "Guggenheim 879\n",
      "overtime 880\n",
      "Player 881\n",
      "Snape 882\n",
      "Grant 883\n",
      "coats 884\n",
      "Emily 885\n",
      "Tobias 886\n",
      "everything 887\n",
      "game 888\n",
      "teach 889\n",
      "Dima 890\n",
      "bluetooth 891\n",
      "entered 892\n",
      "texted 893\n",
      "caves 894\n",
      "Maciej 895\n",
      "Hortons 896\n",
      "Meg 897\n",
      "Birgit 898\n",
      "white 899\n",
      "once 900\n",
      "Justin 901\n",
      "journey 902\n",
      "grades 903\n",
      "Fay 904\n",
      "Morty 905\n",
      "Riri 906\n",
      "material 907\n",
      "arguments 908\n",
      "Rio 909\n",
      "comforts 910\n",
      "Kenzie 911\n",
      "Friends 912\n",
      "whatsoever 913\n",
      "Laurel 914\n",
      "print 915\n",
      "Keeley 916\n",
      "potato 917\n",
      "somewhere 918\n",
      "generally 919\n",
      "Narcos 920\n",
      "helpdesk 921\n",
      "documents 922\n",
      "Javon 923\n",
      "Jorim 924\n",
      "Odette 925\n",
      "ham 926\n",
      "Eve 927\n",
      "crusty 928\n",
      "apart 929\n",
      "pants 930\n",
      "Celeb 931\n",
      "Hurley 932\n",
      "Monopoly 933\n",
      "properly 934\n",
      "sneak 935\n",
      "Lucia 936\n",
      "Hulk 937\n",
      "Zakharovs 938\n",
      "playing 939\n",
      "elevator 940\n",
      "receive 941\n",
      "essay 942\n",
      "Bookstore 943\n",
      "carrot 944\n",
      "Zeppelin 945\n",
      "pointed 946\n",
      "involving 947\n",
      "cheesecake 948\n",
      "PM 949\n",
      "behind 950\n",
      "ambulance 951\n",
      "legal 952\n",
      "grandma 953\n",
      "optimistic 954\n",
      "Jason 955\n",
      "exhibition 956\n",
      "concept 957\n",
      "conversation 958\n",
      "parents 959\n",
      "CPS 960\n",
      "formulas 961\n",
      "Lena 962\n",
      "forward 963\n",
      "goat 964\n",
      "scrambled 965\n",
      "meme 966\n",
      "melancholic 967\n",
      "cell 968\n",
      "clan 969\n",
      "slides 970\n",
      "Guy 971\n",
      "Pastor 972\n",
      "political 973\n",
      "Store 974\n",
      "lunar 975\n",
      "Mick 976\n",
      "office 977\n",
      "poked 978\n",
      "pickle 979\n",
      "Lil 980\n",
      "takes 981\n",
      "Bojana 982\n",
      "pumpkin 983\n",
      "last 984\n",
      "pregnancy 985\n",
      "Serena 986\n",
      "tables 987\n",
      "shirt 988\n",
      "Roy 989\n",
      "Valentine 990\n",
      "temperature 991\n",
      "Mateusz 992\n",
      "Joy 993\n",
      "Bunny 994\n",
      "poll 995\n",
      "month 996\n",
      "Cathy 997\n",
      "are 998\n",
      "Trier 999\n",
      "Italian 1000\n",
      "element 1001\n",
      "nutcracker 1002\n",
      "wth 1003\n",
      "suddenly 1004\n",
      "Varadero 1005\n",
      "screenshot 1006\n",
      "Vanessa 1007\n",
      "Stephania 1008\n",
      "stuffed 1009\n",
      "machine 1010\n",
      "Italy 1011\n",
      "Poly 1012\n",
      "boarding 1013\n",
      "Fernando 1014\n",
      "land 1015\n",
      "falling 1016\n",
      "facts 1017\n",
      "phone 1018\n",
      "miserable 1019\n",
      "find 1020\n",
      "red 1021\n",
      "Janine 1022\n",
      "current 1023\n",
      "mail 1024\n",
      "Gwen 1025\n",
      "Milton 1026\n",
      "Getz 1027\n",
      "math 1028\n",
      "Alexi 1029\n",
      "S9 1030\n",
      "controlled 1031\n",
      "Hans 1032\n",
      "Gotham 1033\n",
      "terrifying 1034\n",
      "phones 1035\n",
      "verify 1036\n",
      "Mirco 1037\n",
      "hunting 1038\n",
      "useless 1039\n",
      "Brenden 1040\n",
      "3sat 1041\n",
      "Promo 1042\n",
      "German 1043\n",
      "hammer 1044\n",
      "eBay 1045\n",
      "Patty 1046\n",
      "rash 1047\n",
      "Yani 1048\n",
      "authenticate 1049\n",
      "scandinavian 1050\n",
      "Donna 1051\n",
      "sweet 1052\n",
      "Jefferson 1053\n",
      "Toby 1054\n",
      "hesitating 1055\n",
      "freaking 1056\n",
      "Damian 1057\n",
      "Marlie 1058\n",
      "Avery 1059\n",
      "Corralejo 1060\n",
      "cake 1061\n",
      "bucks 1062\n",
      "iPhoneXR 1063\n",
      "mini 1064\n",
      "mentioned 1065\n",
      "dance 1066\n",
      "elements 1067\n",
      "pressure 1068\n",
      "allows 1069\n",
      "avalanche 1070\n",
      "Cross 1071\n",
      "first 1072\n",
      "afterwards 1073\n",
      "Sims 1074\n",
      "Helene 1075\n",
      "parking 1076\n",
      "Demi 1077\n",
      "Drury 1078\n",
      "isn 1079\n",
      "second 1080\n",
      "protests 1081\n",
      "pubs 1082\n",
      "Dreamcatcher 1083\n",
      "Armstrong 1084\n",
      "Kora 1085\n",
      "social 1086\n",
      "jets 1087\n",
      "Neither 1088\n",
      "Hakuna 1089\n",
      "Messi 1090\n",
      "expressed 1091\n",
      "Yo 1092\n",
      "Bath 1093\n",
      "recommend 1094\n",
      "Matthew 1095\n",
      "Jodie 1096\n",
      "Peyton 1097\n",
      "royale 1098\n",
      "apply 1099\n",
      "interesting 1100\n",
      "locks 1101\n",
      "Opium 1102\n",
      "Brigitte 1103\n",
      "straight 1104\n",
      "tyres 1105\n",
      "papers 1106\n",
      "Wera 1107\n",
      "guest 1108\n",
      "Yoseph 1109\n",
      "st 1110\n",
      "by 1111\n",
      "Milan 1112\n",
      "law 1113\n",
      "Isla 1114\n",
      "recognised 1115\n",
      "basketball 1116\n",
      "Carey 1117\n",
      "wingman 1118\n",
      "guy 1119\n",
      "D1 1120\n",
      "Kowalsky 1121\n",
      "28th 1122\n",
      "O 1123\n",
      "smelling 1124\n",
      "Fiasco 1125\n",
      "friends 1126\n",
      "pet 1127\n",
      "Harley 1128\n",
      "jogging 1129\n",
      "named 1130\n",
      "listened 1131\n",
      "Sash 1132\n",
      "promises 1133\n",
      "limiting 1134\n",
      "cancel 1135\n",
      "Cristina 1136\n",
      "holds 1137\n",
      "gala 1138\n",
      "request 1139\n",
      "suspicion 1140\n",
      "Peter 1141\n",
      "Johnathan 1142\n",
      "Tree 1143\n",
      "Adelaide 1144\n",
      "tickets 1145\n",
      "till 1146\n",
      "soon 1147\n",
      "reduce 1148\n",
      "fondue 1149\n",
      "Gael 1150\n",
      "Lily 1151\n",
      "Harriet 1152\n",
      "cabin 1153\n",
      "Viktoria 1154\n",
      "hyped 1155\n",
      "Thrones 1156\n",
      "relieved 1157\n",
      "membership 1158\n",
      "Beckham 1159\n",
      "Treaty 1160\n",
      "ridiculous 1161\n",
      "citizen 1162\n",
      "Dolores 1163\n",
      "dishwasher 1164\n",
      "homework 1165\n",
      "niece 1166\n",
      "Gus 1167\n",
      "Hight 1168\n",
      "8 1169\n",
      "confirm 1170\n",
      "supposed 1171\n",
      "INFOLEG 1172\n",
      "trip 1173\n",
      "Wishful 1174\n",
      "courses 1175\n",
      "First 1176\n",
      "talked 1177\n",
      "puzzle 1178\n",
      "March 1179\n",
      "overreacted 1180\n",
      "couple 1181\n",
      "hour 1182\n",
      "gong 1183\n",
      "trying 1184\n",
      "fill 1185\n",
      "change 1186\n",
      "Chuck 1187\n",
      "via 1188\n",
      "Isabelle 1189\n",
      "Iron 1190\n",
      "impossible 1191\n",
      "doodle 1192\n",
      "pepperoni 1193\n",
      "Prison 1194\n",
      "Elisabeth 1195\n",
      "waited 1196\n",
      "potential 1197\n",
      "music 1198\n",
      "Dianne 1199\n",
      "Michelle 1200\n",
      "winnipeg 1201\n",
      "Ari 1202\n",
      "4AB 1203\n",
      "Nate 1204\n",
      "accidentally 1205\n",
      "minuted 1206\n",
      "Ricardo 1207\n",
      "calms 1208\n",
      "nuts 1209\n",
      "landed 1210\n",
      "providing 1211\n",
      "goes 1212\n",
      "Barb 1213\n",
      "connection 1214\n",
      "departure 1215\n",
      "bicycles 1216\n",
      "recons 1217\n",
      "skiing 1218\n",
      "Latin 1219\n",
      "Solana 1220\n",
      "awful 1221\n",
      "water 1222\n",
      "gived 1223\n",
      "Pub 1224\n",
      "Russia 1225\n",
      "Common 1226\n",
      "Died 1227\n",
      "dissapointed 1228\n",
      "Frankie 1229\n",
      "spider 1230\n",
      "hello 1231\n",
      "Jill 1232\n",
      "Copper 1233\n",
      "kindergarten 1234\n",
      "cries 1235\n",
      "additional 1236\n",
      "Suzan 1237\n",
      "exam 1238\n",
      "Lesser 1239\n",
      "Dog 1240\n",
      "preferably 1241\n",
      "Mia 1242\n",
      "painted 1243\n",
      "wind 1244\n",
      "bailed 1245\n",
      "CV 1246\n",
      "female 1247\n",
      "basketfuls 1248\n",
      "recorded 1249\n",
      "LLC 1250\n",
      "Nazi 1251\n",
      "coffe 1252\n",
      "overweight 1253\n",
      "Beatrix 1254\n",
      "pumkin 1255\n",
      "Jack 1256\n",
      "Alexander 1257\n",
      "listen 1258\n",
      "new 1259\n",
      "Something 1260\n",
      "instructor 1261\n",
      "gluten 1262\n",
      "Reese 1263\n",
      "attended 1264\n",
      "Zuri 1265\n",
      "New 1266\n",
      "location 1267\n",
      "Rebeca 1268\n",
      "serious 1269\n",
      "smoke 1270\n",
      "Malm 1271\n",
      "Allan 1272\n",
      "Erin 1273\n",
      "vouchers 1274\n",
      "RS 1275\n",
      "SWJ 1276\n",
      "want 1277\n",
      "Marco 1278\n",
      "Sandrine 1279\n",
      "some 1280\n",
      "Argentinian 1281\n",
      "tomorrow 1282\n",
      "lasted 1283\n",
      "siks 1284\n",
      "Joan 1285\n",
      "Imagine 1286\n",
      "luggage 1287\n",
      "Tilly 1288\n",
      "Fashion 1289\n",
      "sausage 1290\n",
      "Spain 1291\n",
      "Rafal 1292\n",
      "injuries 1293\n",
      "Nice 1294\n",
      "volcano 1295\n",
      "Lexie 1296\n",
      "Marrisa 1297\n",
      "Viola 1298\n",
      "1st 1299\n",
      "Matt 1300\n",
      "Meghan 1301\n",
      "end 1302\n",
      "where 1303\n",
      "looks 1304\n",
      "world 1305\n",
      "falls 1306\n",
      "trousers 1307\n",
      "quite 1308\n",
      "Macedonia 1309\n",
      "correspondence 1310\n",
      "Shania 1311\n",
      "Three 1312\n",
      "driver 1313\n",
      "Mountaineering 1314\n",
      "Tuesday 1315\n",
      "Abbie 1316\n",
      "responsible 1317\n",
      "thanks 1318\n",
      "helps 1319\n",
      "Brittany 1320\n",
      "torn 1321\n",
      "mark 1322\n",
      "Cody 1323\n",
      "Favourite 1324\n",
      "like 1325\n",
      "plays 1326\n",
      "Ophelia 1327\n",
      "installation 1328\n",
      "Melanie 1329\n",
      "them 1330\n",
      "Tomasz 1331\n",
      "Mat 1332\n",
      "extreme 1333\n",
      "donate 1334\n",
      "Ali 1335\n",
      "Muhammad 1336\n",
      "absent 1337\n",
      "reunion 1338\n",
      "Viki 1339\n",
      "Strand 1340\n",
      "Benedict 1341\n",
      "Zora 1342\n",
      "split 1343\n",
      "become 1344\n",
      "wake 1345\n",
      "situation 1346\n",
      "Mathew 1347\n",
      "square 1348\n",
      "passed 1349\n",
      "Tanvi 1350\n",
      "retake 1351\n",
      "cafeteria 1352\n",
      "warm 1353\n",
      "Snow 1354\n",
      "BS6 1355\n",
      "Louie 1356\n",
      "Fernanda 1357\n",
      "Resident 1358\n",
      "Nally 1359\n",
      "Jane 1360\n",
      "distrubed 1361\n",
      "expensive 1362\n",
      "with 1363\n",
      "Isabel 1364\n",
      "Travis 1365\n",
      "flakes 1366\n",
      "interested 1367\n",
      "hard 1368\n",
      "row 1369\n",
      "theme 1370\n",
      "Tancredi 1371\n",
      "fix 1372\n",
      "Kaila 1373\n",
      "sports 1374\n",
      "Katie 1375\n",
      "junk 1376\n",
      "smoothie 1377\n",
      "wonders 1378\n",
      "Son 1379\n",
      "finishing 1380\n",
      "baked 1381\n",
      "Alonzo 1382\n",
      "Bieszczady 1383\n",
      "watched 1384\n",
      "Warren 1385\n",
      "documenting 1386\n",
      "Camila 1387\n",
      "months 1388\n",
      "twisted 1389\n",
      "Maddie 1390\n",
      "Whitby 1391\n",
      "123 1392\n",
      "Prince 1393\n",
      "cannot 1394\n",
      "cuddle 1395\n",
      "mock 1396\n",
      "Shum 1397\n",
      "York 1398\n",
      "Essie 1399\n",
      "Most 1400\n",
      "Samantha 1401\n",
      "Venezuela 1402\n",
      "stalker 1403\n",
      "spotted 1404\n",
      "volume 1405\n",
      "Intel 1406\n",
      "excuse 1407\n",
      "come 1408\n",
      "Georgina 1409\n",
      "Boston 1410\n",
      "Deborah 1411\n",
      "nouveau 1412\n",
      "messages 1413\n",
      "Henry 1414\n",
      "Reyna 1415\n",
      "Master 1416\n",
      "Morton 1417\n",
      "Sprite 1418\n",
      "Aucar 1419\n",
      "badly 1420\n",
      "Boredom 1421\n",
      "upset 1422\n",
      "chose 1423\n",
      "Ghost 1424\n",
      "Washington 1425\n",
      "wouldn 1426\n",
      "Przemek 1427\n",
      "America 1428\n",
      "Kent 1429\n",
      "professional 1430\n",
      "chance 1431\n",
      "Charlie 1432\n",
      "35 1433\n",
      "leaving 1434\n",
      "Led 1435\n",
      "router 1436\n",
      "Lydia 1437\n",
      "charged 1438\n",
      "Wifi 1439\n",
      "condition 1440\n",
      "Stefan 1441\n",
      "tail 1442\n",
      "electricity 1443\n",
      "Megan 1444\n",
      "matters 1445\n",
      "main 1446\n",
      "wearing 1447\n",
      "videos 1448\n",
      "something 1449\n",
      "On 1450\n",
      "surprise 1451\n",
      "Terence 1452\n",
      "editing 1453\n",
      "Clash 1454\n",
      "engine 1455\n",
      "Neomi 1456\n",
      "Einar 1457\n",
      "June 1458\n",
      "behaving 1459\n",
      "finish 1460\n",
      "Cillian 1461\n",
      "watch 1462\n",
      "trainer 1463\n",
      "Benjamin 1464\n",
      "mum 1465\n",
      "Cracow 1466\n",
      "called 1467\n",
      "bikes 1468\n",
      "bit 1469\n",
      "Tinley 1470\n",
      "didgeridoos 1471\n",
      "spaghetti 1472\n",
      "Roma 1473\n",
      "Lane 1474\n",
      "Carrie 1475\n",
      "responsibilities 1476\n",
      "pdf 1477\n",
      "may 1478\n",
      "beer 1479\n",
      "hoops 1480\n",
      "Sodastream 1481\n",
      "alone 1482\n",
      "Marta 1483\n",
      "crying 1484\n",
      "door 1485\n",
      "compares 1486\n",
      "crisis 1487\n",
      "Sophia 1488\n",
      "engines 1489\n",
      "hotel 1490\n",
      "Welden 1491\n",
      "Purity 1492\n",
      "road 1493\n",
      "packed 1494\n",
      "surfing 1495\n",
      "fulfil 1496\n",
      "Belgium 1497\n",
      "crapped 1498\n",
      "frequently 1499\n",
      "twenty 1500\n",
      "shift 1501\n",
      "ashamed 1502\n",
      "break 1503\n",
      "Benny 1504\n",
      "Hermosa 1505\n",
      "allergic 1506\n",
      "referendum 1507\n",
      "thinking 1508\n",
      "reactivate 1509\n",
      "Giovanna 1510\n",
      "good 1511\n",
      "wears 1512\n",
      "debating 1513\n",
      "sauna 1514\n",
      "bartending 1515\n",
      "minus 1516\n",
      "robe 1517\n",
      "Drive 1518\n",
      "studying 1519\n",
      "Lauren 1520\n",
      "prostitute 1521\n",
      "Carols 1522\n",
      "course 1523\n",
      "Mats 1524\n",
      "invested 1525\n",
      "urged 1526\n",
      "received 1527\n",
      "Uncle 1528\n",
      "further 1529\n",
      "train 1530\n",
      "Konnor 1531\n",
      "obligations 1532\n",
      "Tonight 1533\n",
      "miniature 1534\n",
      "grateful 1535\n",
      "Feel 1536\n",
      "science 1537\n",
      "indulge 1538\n",
      "Rapsody 1539\n",
      "intermeddle 1540\n",
      "alleged 1541\n",
      "applied 1542\n",
      "Sinclair 1543\n",
      "Matata 1544\n",
      "files 1545\n",
      "cosmetics 1546\n",
      "Flash 1547\n",
      "maths 1548\n",
      "skipped 1549\n",
      "fire 1550\n",
      "midterms 1551\n",
      "Natty 1552\n",
      "Chez 1553\n",
      "really 1554\n",
      "Instead 1555\n",
      "NGO 1556\n",
      "Bruce 1557\n",
      "workout 1558\n",
      "colleague 1559\n",
      "antibiotics 1560\n",
      "its 1561\n",
      "suggestion 1562\n",
      "changes 1563\n",
      "Jakub 1564\n",
      "pizza 1565\n",
      "skype 1566\n",
      "wedding 1567\n",
      "Louisa 1568\n",
      "Robert 1569\n",
      "illumination 1570\n",
      "employees 1571\n",
      "Marya 1572\n",
      "mess 1573\n",
      "rushes 1574\n",
      "Orchestra 1575\n",
      "Cole 1576\n",
      "Gladys 1577\n",
      "Ra 1578\n",
      "whisk 1579\n",
      "Mom 1580\n",
      "bonus 1581\n",
      "strange 1582\n",
      "won 1583\n",
      "Georgia 1584\n",
      "serum 1585\n",
      "Yann 1586\n",
      "uric 1587\n",
      "Diana 1588\n",
      "noises 1589\n",
      "Austria 1590\n",
      "cups 1591\n",
      "consequences 1592\n",
      "became 1593\n",
      "scientific 1594\n",
      "Diane 1595\n",
      "brother 1596\n",
      "Abigail 1597\n",
      "Damari 1598\n",
      "Mountain 1599\n",
      "largely 1600\n",
      "hired 1601\n",
      "woken 1602\n",
      "Karl 1603\n",
      "stocked 1604\n",
      "Barry 1605\n",
      "about 1606\n",
      "municipal 1607\n",
      "singer 1608\n",
      "brand 1609\n",
      "all 1610\n",
      "cable 1611\n",
      "Donald 1612\n",
      "8th 1613\n",
      "An 1614\n",
      "Nines 1615\n",
      "noticed 1616\n",
      "coding 1617\n",
      "mom 1618\n",
      "Leona 1619\n",
      "arguing 1620\n",
      "also 1621\n",
      "Rich 1622\n",
      "and 1623\n",
      "presentations 1624\n",
      "bottle 1625\n",
      "Frances 1626\n",
      "Influence 1627\n",
      "Ute 1628\n",
      "looked 1629\n",
      "For 1630\n",
      "Clinic 1631\n",
      "accepted 1632\n",
      "accept 1633\n",
      "grinder 1634\n",
      "Brina 1635\n",
      "marketing 1636\n",
      "lent 1637\n",
      "card 1638\n",
      "Darcy 1639\n",
      "Dermi 1640\n",
      "Charles 1641\n",
      "Las 1642\n",
      "Vesuvius 1643\n",
      "Nicholas 1644\n",
      "living 1645\n",
      "she 1646\n",
      "admit 1647\n",
      "Pam 1648\n",
      "tasty 1649\n",
      "GoForIt 1650\n",
      "add 1651\n",
      "missed 1652\n",
      "Zilda 1653\n",
      "interface 1654\n",
      "Aretha 1655\n",
      "1 1656\n",
      "Akira 1657\n",
      "booths 1658\n",
      "learned 1659\n",
      "girlish 1660\n",
      "corporate 1661\n",
      "chores 1662\n",
      "Telma 1663\n",
      "emigration 1664\n",
      "Thames 1665\n",
      "05 1666\n",
      "picks 1667\n",
      "American 1668\n",
      "disappointed 1669\n",
      "experience 1670\n",
      "although 1671\n",
      "Chris 1672\n",
      "Roxy 1673\n",
      "Hyperion 1674\n",
      "performed 1675\n",
      "knows 1676\n",
      "Sylvia 1677\n",
      "menstrual 1678\n",
      "regarding 1679\n",
      "confirmation 1680\n",
      "Carter 1681\n",
      "Trevor 1682\n",
      "Alexa 1683\n",
      "Bawarian 1684\n",
      "Steve 1685\n",
      "Pro 1686\n",
      "Tina 1687\n",
      "45 1688\n",
      "Zuccarini 1689\n",
      "shampoo 1690\n",
      "Kings 1691\n",
      "bonds 1692\n",
      "wakes 1693\n",
      "doorbell 1694\n",
      "since 1695\n",
      "Jade 1696\n",
      "Vicky 1697\n",
      "shelf 1698\n",
      "passing 1699\n",
      "reliable 1700\n",
      "graves 1701\n",
      "reasonable 1702\n",
      "Sandy 1703\n",
      "halloween 1704\n",
      "acid 1705\n",
      "performances 1706\n",
      "Stanley 1707\n",
      "rehabilitation 1708\n",
      "congratulate 1709\n",
      "WhatsApp 1710\n",
      "Niall 1711\n",
      "Queensland 1712\n",
      "Mercedes 1713\n",
      "sees 1714\n",
      "potted 1715\n",
      "Michael 1716\n",
      "spooky 1717\n",
      "brewery 1718\n",
      "Brock 1719\n",
      "higher 1720\n",
      "Adventures 1721\n",
      "bowl 1722\n",
      "speed 1723\n",
      "Olivier 1724\n",
      "refuse 1725\n",
      "pissed 1726\n",
      "Youtube 1727\n",
      "Corina 1728\n",
      "physics 1729\n",
      "suggests 1730\n",
      "Gilmore 1731\n",
      "Sunflower 1732\n",
      "Gdynia 1733\n",
      "transfer 1734\n",
      "slice 1735\n",
      "1pm 1736\n",
      "Anabella 1737\n",
      "Steakhouse 1738\n",
      "mirror 1739\n",
      "girlfriend 1740\n",
      "Ms 1741\n",
      "Stu 1742\n",
      "Gaga 1743\n",
      "human 1744\n",
      "St 1745\n",
      "pancakes 1746\n",
      "Natalia 1747\n",
      "Melbourne 1748\n",
      "South 1749\n",
      "Gilda 1750\n",
      "agency 1751\n",
      "pounds 1752\n",
      "papel 1753\n",
      "studies 1754\n",
      "costs 1755\n",
      "facebook 1756\n",
      "ads 1757\n",
      "Gainsbourg 1758\n",
      "comeback 1759\n",
      "Hailey 1760\n",
      "Hernandez 1761\n",
      "parcel 1762\n",
      "percent 1763\n",
      "Tess 1764\n",
      "sceptical 1765\n",
      "flirt 1766\n",
      "goint 1767\n",
      "80 1768\n",
      "noise 1769\n",
      "Ally 1770\n",
      "Hayley 1771\n",
      "Mari 1772\n",
      "cook 1773\n",
      "Hamza 1774\n",
      "Thelma 1775\n",
      "wear 1776\n",
      "Jayce 1777\n",
      "Reynold 1778\n",
      "Kerri 1779\n",
      "perfect 1780\n",
      "judges 1781\n",
      "Foxi 1782\n",
      "Escape 1783\n",
      "donuts 1784\n",
      "cocktail 1785\n",
      "Tallinn 1786\n",
      "Irma 1787\n",
      "fine 1788\n",
      "8pm 1789\n",
      "carbon 1790\n",
      "roles 1791\n",
      "test 1792\n",
      "mins 1793\n",
      "these 1794\n",
      "painting 1795\n",
      "friendly 1796\n",
      "Edric 1797\n",
      "kissing 1798\n",
      "shake 1799\n",
      "uber 1800\n",
      "Mella 1801\n",
      "Lonnie 1802\n",
      "clarify 1803\n",
      "warmer 1804\n",
      "autographs 1805\n",
      "enjoys 1806\n",
      "Decathlon 1807\n",
      "leftovers 1808\n",
      "Pat 1809\n",
      "pharmacies 1810\n",
      "similar 1811\n",
      "Wales 1812\n",
      "Lorry 1813\n",
      "Jude 1814\n",
      "executive 1815\n",
      "positions 1816\n",
      "heel 1817\n",
      "6pm 1818\n",
      "amaretti 1819\n",
      "gel 1820\n",
      "prices 1821\n",
      "xyz 1822\n",
      "disconnected 1823\n",
      "Egbert 1824\n",
      "tips 1825\n",
      "screaming 1826\n",
      "Phil 1827\n",
      "questions 1828\n",
      "pomodori 1829\n",
      "use 1830\n",
      "Co 1831\n",
      "Page 1832\n",
      "removal 1833\n",
      "across 1834\n",
      "actors 1835\n",
      "pavement 1836\n",
      "passport 1837\n",
      "organization 1838\n",
      "coat 1839\n",
      "team 1840\n",
      "Jemma 1841\n",
      "Kasper 1842\n",
      "software 1843\n",
      "times 1844\n",
      "Ocean 1845\n",
      "During 1846\n",
      "shelter 1847\n",
      "Jordan 1848\n",
      "Petra 1849\n",
      "upstairs 1850\n",
      "explains 1851\n",
      "Heidi 1852\n",
      "Watson 1853\n",
      "bracelets 1854\n",
      "Royals 1855\n",
      "backyard 1856\n",
      "Barney 1857\n",
      "Sicily 1858\n",
      "hockey 1859\n",
      "shall 1860\n",
      "looses 1861\n",
      ". 1862\n",
      "boyfriend 1863\n",
      "buying 1864\n",
      "Baltimore 1865\n",
      "degrees 1866\n",
      "psychotherapy 1867\n",
      "neurologist 1868\n",
      "ago 1869\n",
      "Paulson 1870\n",
      "atmosphere 1871\n",
      "Hartley 1872\n",
      "Poppy 1873\n",
      "Jonny 1874\n",
      "Briar 1875\n",
      "unsure 1876\n",
      "Black 1877\n",
      "competition 1878\n",
      "cauliflower 1879\n",
      "Riverdale 1880\n",
      "Warsaw 1881\n",
      "McKlaren 1882\n",
      "fan 1883\n",
      "Scolt 1884\n",
      "Vincent 1885\n",
      "ferry 1886\n",
      "ok 1887\n",
      "Jacek 1888\n",
      "Canadian 1889\n",
      "offer 1890\n",
      "Edgar 1891\n",
      "sometimes 1892\n",
      "Sigismund 1893\n",
      "tartare 1894\n",
      "Kai 1895\n",
      "Erica 1896\n",
      "desk 1897\n",
      "Sephora 1898\n",
      "country 1899\n",
      "Layla 1900\n",
      "prepared 1901\n",
      "Marianna 1902\n",
      "intolerant 1903\n",
      "deer 1904\n",
      "noisy 1905\n",
      "Ellis 1906\n",
      "prefers 1907\n",
      "Mall 1908\n",
      "Veronica 1909\n",
      "dirty 1910\n",
      "beers 1911\n",
      "attention 1912\n",
      "waves 1913\n",
      "Basque 1914\n",
      "themed 1915\n",
      "Ariel 1916\n",
      "employers 1917\n",
      "Mon 1918\n",
      "sleep 1919\n",
      "propose 1920\n",
      "convenient 1921\n",
      "Jared 1922\n",
      "Zach 1923\n",
      "recovers 1924\n",
      "Val 1925\n",
      "store 1926\n",
      "mean 1927\n",
      "December 1928\n",
      "station 1929\n",
      "Faith 1930\n",
      "products 1931\n",
      "department 1932\n",
      "FIFA 1933\n",
      "Nicole 1934\n",
      "regions 1935\n",
      "Kaka 1936\n",
      "whereas 1937\n",
      "assistant 1938\n",
      "kid 1939\n",
      "November 1940\n",
      "redecorate 1941\n",
      "Robyn 1942\n",
      "shopping 1943\n",
      "Due 1944\n",
      "East 1945\n",
      "best 1946\n",
      "laundry 1947\n",
      "Lottie 1948\n",
      "Starurday 1949\n",
      "around 1950\n",
      "Norfolk 1951\n",
      "Ines 1952\n",
      "mike 1953\n",
      "off 1954\n",
      "make 1955\n",
      "Doll 1956\n",
      "fashion 1957\n",
      "Trump 1958\n",
      "Adam 1959\n",
      "sends 1960\n",
      "President 1961\n",
      "pointer 1962\n",
      "Sabine 1963\n",
      "leaking 1964\n",
      "Jonna 1965\n",
      "stew 1966\n",
      "drawer 1967\n",
      "Christian 1968\n",
      "sugar 1969\n",
      "scared 1970\n",
      "flowers 1971\n",
      "103 1972\n",
      "emergency 1973\n",
      "Predator 1974\n",
      "Spanish 1975\n",
      "Lantern 1976\n",
      "Han 1977\n",
      "piles 1978\n",
      "round 1979\n",
      "Lukas 1980\n",
      "Street 1981\n",
      "begs 1982\n",
      "thriller 1983\n",
      "pattern 1984\n",
      "Theater 1985\n",
      "Ant 1986\n",
      "players 1987\n",
      "Dhabi 1988\n",
      "gift 1989\n",
      "Sainsbury 1990\n",
      "window 1991\n",
      "Jake 1992\n",
      "curious 1993\n",
      "stuck 1994\n",
      "craps 1995\n",
      "whether 1996\n",
      "choir 1997\n",
      "teenager 1998\n",
      "universities 1999\n",
      "Crane 2000\n",
      "Leticia 2001\n",
      "Mines 2002\n",
      "Thailand 2003\n",
      "include 2004\n",
      "declares 2005\n",
      "Broderick 2006\n",
      "six 2007\n",
      "MRI 2008\n",
      "DJ 2009\n",
      "Lyx 2010\n",
      "deliver 2011\n",
      "olivier 2012\n",
      "necks 2013\n",
      "hi 2014\n",
      "Xavier 2015\n",
      "fires 2016\n",
      "Nino 2017\n",
      "journal 2018\n",
      "Lia 2019\n",
      "almond 2020\n",
      "Brandy 2021\n",
      "Marton 2022\n",
      "mother 2023\n",
      "fighting 2024\n",
      "goals 2025\n",
      "withdrawing 2026\n",
      "drawings 2027\n",
      "Tom 2028\n",
      "Vivien 2029\n",
      "arrangements 2030\n",
      "Pep 2031\n",
      "suits 2032\n",
      "Easter 2033\n",
      "messy 2034\n",
      "spreadsheet 2035\n",
      "Kat 2036\n",
      "what 2037\n",
      "Sydney 2038\n",
      "brown 2039\n",
      "karaoke 2040\n",
      "partner 2041\n",
      "yelled 2042\n",
      "Nadia 2043\n",
      "Lapa 2044\n",
      "Koshy 2045\n",
      "accent 2046\n",
      "Ipanema 2047\n",
      "Thai 2048\n",
      "points 2049\n",
      "then 2050\n",
      "Cooler 2051\n",
      "substitutes 2052\n",
      "sleeping 2053\n",
      "claims 2054\n",
      "Barba 2055\n",
      "Macy 2056\n",
      "Mackenzie 2057\n",
      "added 2058\n",
      "closer 2059\n",
      "Otherwise 2060\n",
      "Two 2061\n",
      "tatoo 2062\n",
      "code 2063\n",
      "same 2064\n",
      "07 2065\n",
      "Scandinavian 2066\n",
      "July 2067\n",
      "ebooks 2068\n",
      "Spiderman 2069\n",
      "Mason 2070\n",
      "crushed 2071\n",
      "dinner 2072\n",
      "send 2073\n",
      "displeased 2074\n",
      "farm 2075\n",
      "workshop 2076\n",
      "Jimmy 2077\n",
      "regaining 2078\n",
      "Penelope 2079\n",
      "announced 2080\n",
      "App 2081\n",
      "dogs 2082\n",
      "dated 2083\n",
      "October 2084\n",
      "said 2085\n",
      "Nala 2086\n",
      "fallen 2087\n",
      "hesitated 2088\n",
      "stroke 2089\n",
      "Audrey 2090\n",
      "Kragmortha 2091\n",
      "restaurant 2092\n",
      "popping 2093\n",
      "relief 2094\n",
      "Walmart 2095\n",
      "treat 2096\n",
      "film 2097\n",
      "Rumer 2098\n",
      "Rd 2099\n",
      "give 2100\n",
      "Jackson 2101\n",
      "tradition 2102\n",
      "liberal 2103\n",
      "Gwyneth 2104\n",
      "Andy 2105\n",
      "2018 2106\n",
      "translator 2107\n",
      "Golden 2108\n",
      "Gina 2109\n",
      "7th 2110\n",
      "theatre 2111\n",
      "; 2112\n",
      "googling 2113\n",
      "Kitty 2114\n",
      "Victoria 2115\n",
      "grade 2116\n",
      "Mell 2117\n",
      "compose 2118\n",
      "Channel 2119\n",
      "Ziggy 2120\n",
      "Norway 2121\n",
      "english 2122\n",
      "raving 2123\n",
      "Zack 2124\n",
      "gather 2125\n",
      "Spencer 2126\n",
      "iceskating 2127\n",
      "essays 2128\n",
      "Ani 2129\n",
      "Oscar 2130\n",
      "organisational 2131\n",
      "proposes 2132\n",
      "possessions 2133\n",
      "navigation 2134\n",
      "IB 2135\n",
      "crush 2136\n",
      "judged 2137\n",
      "Sapo 2138\n",
      "Susannah 2139\n",
      "Tibi 2140\n",
      "Columbia 2141\n",
      "Stephanie 2142\n",
      "warehouse 2143\n",
      "Olly 2144\n",
      "Luka 2145\n",
      "Darline 2146\n",
      "low 2147\n",
      "Tesla 2148\n",
      "excited 2149\n",
      "Ray 2150\n",
      "storage 2151\n",
      "stories 2152\n",
      "Cara 2153\n",
      "Reece 2154\n",
      "Stewart 2155\n",
      "typing 2156\n",
      "183 2157\n",
      "uniforms 2158\n",
      "Tyson 2159\n",
      "Shabab 2160\n",
      "graduated 2161\n",
      "Angeline 2162\n",
      "van 2163\n",
      "fortieth 2164\n",
      "Beasts 2165\n",
      "Joaquin 2166\n",
      "pharyngitis 2167\n",
      "hungry 2168\n",
      "Tamara 2169\n",
      "Darcey 2170\n",
      "1990s 2171\n",
      "titles 2172\n",
      "book 2173\n",
      "Axel 2174\n",
      "piano 2175\n",
      "coke 2176\n",
      "size 2177\n",
      "Liza 2178\n",
      "rocky 2179\n",
      "ASAP 2180\n",
      "hasn 2181\n",
      "Messenger 2182\n",
      "paparazzis 2183\n",
      "Jonathan 2184\n",
      "gmail 2185\n",
      "police 2186\n",
      "Thursdays 2187\n",
      "village 2188\n",
      "yoga 2189\n",
      "Isobel 2190\n",
      "airpords 2191\n",
      "fed 2192\n",
      "Riders 2193\n",
      "Today 2194\n",
      "Eventually 2195\n",
      "reservations 2196\n",
      "5th 2197\n",
      "great 2198\n",
      "Mirko 2199\n",
      "tacky 2200\n",
      "convince 2201\n",
      "6am 2202\n",
      "long 2203\n",
      "throws 2204\n",
      "Melody 2205\n",
      "Jenny 2206\n",
      "Bonnie 2207\n",
      "let 2208\n",
      "dumping 2209\n",
      "often 2210\n",
      "Laura 2211\n",
      "sold 2212\n",
      "horror 2213\n",
      "Ashlee 2214\n",
      "life 2215\n",
      "fries 2216\n",
      "proper 2217\n",
      "view 2218\n",
      "Pump 2219\n",
      "exotic 2220\n",
      "walls 2221\n",
      "forgive 2222\n",
      "host 2223\n",
      "Restaurant 2224\n",
      "Aaron 2225\n",
      "Mario 2226\n",
      "Coldplay 2227\n",
      "Manchester 2228\n",
      "Gera 2229\n",
      "Dominika 2230\n",
      "Stacey 2231\n",
      "hurts 2232\n",
      "In 2233\n",
      "August 2234\n",
      "calluses 2235\n",
      "successful 2236\n",
      "3rd 2237\n",
      "considers 2238\n",
      "funds 2239\n",
      "restaurants 2240\n",
      "abroad 2241\n",
      "Sharon 2242\n",
      "Ives 2243\n",
      "cloth 2244\n",
      "Hotel 2245\n",
      "101 2246\n",
      "75 2247\n",
      "updates 2248\n",
      "street 2249\n",
      "baking 2250\n",
      "pretty 2251\n",
      "list 2252\n",
      "Percy 2253\n",
      "Prudential 2254\n",
      "stole 2255\n",
      "schedule 2256\n",
      "Ashley 2257\n",
      "ironing 2258\n",
      "Iris 2259\n",
      "cost 2260\n",
      "crazy 2261\n",
      "alright 2262\n",
      "Caleta 2263\n",
      "complete 2264\n",
      "Misha 2265\n",
      "Sabrina 2266\n",
      "Celeste 2267\n",
      "Laurent 2268\n",
      "shy 2269\n",
      "Olia 2270\n",
      "just 2271\n",
      "comfortable 2272\n",
      "shows 2273\n",
      "Ben 2274\n",
      "Bart 2275\n",
      "Edna 2276\n",
      "Plaza 2277\n",
      "pencil 2278\n",
      "two 2279\n",
      "bulbs 2280\n",
      "politicians 2281\n",
      "turn 2282\n",
      "Jum 2283\n",
      "Bronn 2284\n",
      "data 2285\n",
      "Hyde 2286\n",
      "watches 2287\n",
      "raisins 2288\n",
      "browsing 2289\n",
      "messenger 2290\n",
      "aks 2291\n",
      "Bob 2292\n",
      "Luis 2293\n",
      "Filipino 2294\n",
      "21st 2295\n",
      "del 2296\n",
      "Lexi 2297\n",
      "Felicia 2298\n",
      "Bristol 2299\n",
      "arrivals 2300\n",
      "staying 2301\n",
      "Alisson 2302\n",
      "cancellation 2303\n",
      "animal 2304\n",
      "Evelyn 2305\n",
      "laughing 2306\n",
      "salsa 2307\n",
      "mushrooms 2308\n",
      "Merrill 2309\n",
      "Tay 2310\n",
      "kissed 2311\n",
      "went 2312\n",
      "Gale 2313\n",
      "forwarded 2314\n",
      "Christine 2315\n",
      "Gerda 2316\n",
      "vinyls 2317\n",
      "opposition 2318\n",
      "pick 2319\n",
      "along 2320\n",
      "porch 2321\n",
      "satisfying 2322\n",
      "Amir 2323\n",
      "Julianna 2324\n",
      "returned 2325\n",
      "Josh 2326\n",
      "Maggie 2327\n",
      "cup 2328\n",
      "vector 2329\n",
      "silver 2330\n",
      "Naomi 2331\n",
      "Joey 2332\n",
      "always 2333\n",
      "critisizes 2334\n",
      "ceiling 2335\n",
      "de 2336\n",
      "Joziah 2337\n",
      "decluttering 2338\n",
      "Glasgow 2339\n",
      "gravity 2340\n",
      "sample 2341\n",
      "review 2342\n",
      "Ukraine 2343\n",
      "smoothies 2344\n",
      "Anderson 2345\n",
      "Monday 2346\n",
      "Hilda 2347\n",
      "Hilda\n",
      "Felix 2348\n",
      "community 2349\n",
      "overqualified 2350\n",
      "bay 2351\n",
      "Suzie 2352\n",
      "funny 2353\n",
      "Yvonne 2354\n",
      "hook 2355\n",
      "Suzanne 2356\n",
      "runs 2357\n",
      "rearrange 2358\n",
      "Berlin 2359\n",
      "stationery 2360\n",
      "sells 2361\n",
      "impressed 2362\n",
      "Stuart 2363\n",
      "burger 2364\n",
      "AI 2365\n",
      "Brandon 2366\n",
      "Using 2367\n",
      "Shoppers 2368\n",
      "faulty 2369\n",
      "cancels 2370\n",
      "Cuba 2371\n",
      "guard 2372\n",
      "abilities 2373\n",
      "lasagne 2374\n",
      "inefficient 2375\n",
      "Direct 2376\n",
      "PR 2377\n",
      "Lidsay 2378\n",
      "snacks 2379\n",
      "Criminal 2380\n",
      "requests 2381\n",
      "favorite 2382\n",
      "announces 2383\n",
      "special 2384\n",
      "stop 2385\n",
      "Lorrie 2386\n",
      "fair 2387\n",
      "have 2388\n",
      "Panamanian 2389\n",
      "sick 2390\n",
      "east 2391\n",
      "final 2392\n",
      "step 2393\n",
      "seemed 2394\n",
      "2 2395\n",
      "Katarina 2396\n",
      "used 2397\n",
      "Tomorrow 2398\n",
      "schools 2399\n",
      "message 2400\n",
      "Gizzi 2401\n",
      "stationary 2402\n",
      "smog 2403\n",
      "Jo 2404\n",
      "McDonald 2405\n",
      "cheating 2406\n",
      "understand 2407\n",
      "Sally 2408\n",
      "colours 2409\n",
      "Karan 2410\n",
      "Nina 2411\n",
      "Missy 2412\n",
      "Polly 2413\n",
      "flights 2414\n",
      "appointment 2415\n",
      "between 2416\n",
      "green 2417\n",
      "It 2418\n",
      "wish 2419\n",
      "women 2420\n",
      "upload 2421\n",
      "Eden 2422\n",
      "Arnold 2423\n",
      "Alaina 2424\n",
      "Francine 2425\n",
      "in 2426\n",
      "comics 2427\n",
      "San 2428\n",
      "congratulating 2429\n",
      "Thanks 2430\n",
      "Sage 2431\n",
      "Chandler 2432\n",
      "his 2433\n",
      "meet 2434\n",
      "Keanu 2435\n",
      "Marty 2436\n",
      "Wacky 2437\n",
      "seats 2438\n",
      "Alfie 2439\n",
      "features 2440\n",
      "frustration 2441\n",
      "before 2442\n",
      "Madison 2443\n",
      "Sayeed 2444\n",
      "Cecil 2445\n",
      "meetings 2446\n",
      "bus 2447\n",
      "checking 2448\n",
      "quiet 2449\n",
      "covered 2450\n",
      "Allison 2451\n",
      "Bill 2452\n",
      "Bridget 2453\n",
      "chosen 2454\n",
      "despite 2455\n",
      "managed 2456\n",
      "zero 2457\n",
      "vacant 2458\n",
      "sitting 2459\n",
      "BBC 2460\n",
      "doh 2461\n",
      "every 2462\n",
      "Jessie 2463\n",
      "Pauline 2464\n",
      "soup 2465\n",
      "key 2466\n",
      "peanut 2467\n",
      "comfort 2468\n",
      "seven 2469\n",
      "Granada 2470\n",
      "ice 2471\n",
      "Samsung 2472\n",
      "hybrid 2473\n",
      "12 2474\n",
      "croutons 2475\n",
      "speaker 2476\n",
      "another 2477\n",
      "colour 2478\n",
      "Angie 2479\n",
      "Clara 2480\n",
      "bandage 2481\n",
      "Sheeran 2482\n",
      "joking 2483\n",
      "diagnosis 2484\n",
      "were 2485\n",
      "Ulrich 2486\n",
      "6ix9ine 2487\n",
      "Stella 2488\n",
      "Management 2489\n",
      "handout 2490\n",
      "shown 2491\n",
      "locations 2492\n",
      "Geraldine 2493\n",
      "4pm 2494\n",
      "Oz 2495\n",
      "boat 2496\n",
      "yt 2497\n",
      "speak 2498\n",
      "Basinger 2499\n",
      "Evergreen 2500\n",
      "feed 2501\n",
      "1980s 2502\n",
      "Samba 2503\n",
      "Elvis 2504\n",
      "Lori 2505\n",
      "Fluff 2506\n",
      "face 2507\n",
      "watching 2508\n",
      "weeks 2509\n",
      "piece 2510\n",
      "Steph 2511\n",
      "cousins 2512\n",
      "Blainville 2513\n",
      "Frederic 2514\n",
      "Mike 2515\n",
      "Sixx 2516\n",
      "director 2517\n",
      "pregnant 2518\n",
      "30th 2519\n",
      "never 2520\n",
      "burgers 2521\n",
      "Tasty 2522\n",
      "Franco 2523\n",
      "baby 2524\n",
      "shat 2525\n",
      "links 2526\n",
      "arranging 2527\n",
      "Still 2528\n",
      "feeling 2529\n",
      "cafe 2530\n",
      "Branden 2531\n",
      "Nata 2532\n",
      "garage 2533\n",
      "Aiden 2534\n",
      "Bladerunner 2535\n",
      "petition 2536\n",
      "Temple 2537\n",
      "addresses 2538\n",
      "meteor 2539\n",
      "sweets 2540\n",
      "hobbies 2541\n",
      "solve 2542\n",
      "Blackwell 2543\n",
      "hesitates 2544\n",
      "Mohamed 2545\n",
      "people 2546\n",
      "convention 2547\n",
      "renovated 2548\n",
      "sensible 2549\n",
      "gifts 2550\n",
      "loved 2551\n",
      "notes 2552\n",
      "Lufthansa 2553\n",
      "Rudy 2554\n",
      "tastes 2555\n",
      "payment 2556\n",
      "delay 2557\n",
      "tricks 2558\n",
      "Aubrey 2559\n",
      "built 2560\n",
      "Becky 2561\n",
      "task 2562\n",
      "parties 2563\n",
      "Greenbook 2564\n",
      "Food 2565\n",
      "played 2566\n",
      "Evil 2567\n",
      "stretching 2568\n",
      "anyway 2569\n",
      "insurance 2570\n",
      "Scofield 2571\n",
      "involvement 2572\n",
      "Natalie 2573\n",
      "plans 2574\n",
      "boy 2575\n",
      "commercial 2576\n",
      "Mine 2577\n",
      "medication 2578\n",
      "Johnny 2579\n",
      "Tale 2580\n",
      "session 2581\n",
      "killed 2582\n",
      "Kas 2583\n",
      "released 2584\n",
      "offers 2585\n",
      "attacking 2586\n",
      "bathroom 2587\n",
      "British 2588\n",
      "Fionna 2589\n",
      "cards 2590\n",
      "Theresa 2591\n",
      "Russian 2592\n",
      "Cage 2593\n",
      "unused 2594\n",
      "Wednesday 2595\n",
      "this 2596\n",
      "Spotify 2597\n",
      "Samoa 2598\n",
      "dislikes 2599\n",
      "Edith 2600\n",
      "access 2601\n",
      "Poppins 2602\n",
      "wil 2603\n",
      "blue 2604\n",
      "call 2605\n",
      "Paul 2606\n",
      "turtle 2607\n",
      "on 2608\n",
      "snow 2609\n",
      "20k 2610\n",
      "enough 2611\n",
      "house 2612\n",
      "Francisco 2613\n",
      "Joyce 2614\n",
      "ex 2615\n",
      "Ross 2616\n",
      "scapegoat 2617\n",
      "Opel 2618\n",
      "dessert 2619\n",
      "Marly 2620\n",
      "Manuel 2621\n",
      "overcrowded 2622\n",
      "flat 2623\n",
      "clothing 2624\n",
      "Caf 2625\n",
      "rescued 2626\n",
      "impatient 2627\n",
      "d 2628\n",
      "Portable 2629\n",
      "Dana 2630\n",
      "Petter 2631\n",
      "London 2632\n",
      "Darlene 2633\n",
      "thank 2634\n",
      "geometry 2635\n",
      "Jordans 2636\n",
      "bravio 2637\n",
      "Sandra 2638\n",
      "IKEA 2639\n",
      "replacing 2640\n",
      "shouting 2641\n",
      "pax 2642\n",
      "Miles 2643\n",
      "credit 2644\n",
      "tried 2645\n",
      "Ultra 2646\n",
      "Amor 2647\n",
      "Escobar 2648\n",
      "create 2649\n",
      "Daria 2650\n",
      "Tinder 2651\n",
      "cart 2652\n",
      "eats 2653\n",
      "threw 2654\n",
      "untill 2655\n",
      "Baldarelli 2656\n",
      "embassy 2657\n",
      "jeff 2658\n",
      "com 2659\n",
      "Edwin 2660\n",
      "Erick 2661\n",
      "understanding 2662\n",
      "behavior 2663\n",
      "eaten 2664\n",
      "uni 2665\n",
      "Colombia 2666\n",
      "damage 2667\n",
      "Aisha 2668\n",
      "Shanon 2669\n",
      "1200 2670\n",
      "Ava 2671\n",
      "Isa 2672\n",
      "recognition 2673\n",
      "Patrick 2674\n",
      "re 2675\n",
      "Pierre 2676\n",
      "WOK 2677\n",
      "Giovanni 2678\n",
      "Tammie 2679\n",
      "been 2680\n",
      "Hill 2681\n",
      "National 2682\n",
      "Adar 2683\n",
      "Nellie 2684\n",
      "fell 2685\n",
      "Tommy 2686\n",
      "shop 2687\n",
      "Frederick 2688\n",
      "stand 2689\n",
      "pillow 2690\n",
      "waits 2691\n",
      "Adrianna 2692\n",
      "calendars 2693\n",
      "Branson 2694\n",
      "desktop 2695\n",
      "surgery 2696\n",
      "dressing 2697\n",
      "Sisi 2698\n",
      "Afhaam 2699\n",
      "uses 2700\n",
      "Blaire 2701\n",
      "Mila 2702\n",
      "height 2703\n",
      "contractor 2704\n",
      "fake 2705\n",
      "Danielle 2706\n",
      "pen 2707\n",
      "dog 2708\n",
      "gussied 2709\n",
      "Beebe 2710\n",
      "Harvey 2711\n",
      "Gap 2712\n",
      "neighbors 2713\n",
      "After 2714\n",
      "tea 2715\n",
      "Miley 2716\n",
      "Erasmus 2717\n",
      "wife 2718\n",
      "Infinity 2719\n",
      "grammar 2720\n",
      "Gill 2721\n",
      "moments 2722\n",
      "high 2723\n",
      "Dominic 2724\n",
      "word 2725\n",
      "Gabriel 2726\n",
      "Jamie 2727\n",
      "unpolite 2728\n",
      "mat 2729\n",
      "Harissa 2730\n",
      "Gabi 2731\n",
      "worry 2732\n",
      "binge 2733\n",
      "c 2734\n",
      "W 2735\n",
      "Jurek 2736\n",
      "afraid 2737\n",
      "ministries 2738\n",
      "embarrassing 2739\n",
      "enjoy 2740\n",
      "offered 2741\n",
      "festival 2742\n",
      "probably 2743\n",
      "hub 2744\n",
      "finale 2745\n",
      "laugh 2746\n",
      "improve 2747\n",
      "handsome 2748\n",
      "taxify 2749\n",
      "Gemini 2750\n",
      "soap 2751\n",
      "Fraser 2752\n",
      "doctor 2753\n",
      "Wetransfer 2754\n",
      "viral 2755\n",
      "Girls 2756\n",
      "chest 2757\n",
      "delicious 2758\n",
      "choices 2759\n",
      "plumber 2760\n",
      "breaks 2761\n",
      "pyjamas 2762\n",
      "Luxembourg 2763\n",
      "Casa 2764\n",
      "handcrafts 2765\n",
      "Pass 2766\n",
      "asleep 2767\n",
      "interactive 2768\n",
      "company 2769\n",
      "Derick 2770\n",
      "movies 2771\n",
      "babysitter 2772\n",
      "presented 2773\n",
      "Verity 2774\n",
      "relaxed 2775\n",
      "Piotr 2776\n",
      "Kire 2777\n",
      "Ghul 2778\n",
      "Hudson 2779\n",
      "rubella 2780\n",
      "Vera 2781\n",
      "Lanzarote 2782\n",
      "Keegan 2783\n",
      "is 2784\n",
      "wi 2785\n",
      "look 2786\n",
      "Amal 2787\n",
      "Linn 2788\n",
      "% 2789\n",
      "Dakota 2790\n",
      "Deal 2791\n",
      "held 2792\n",
      "Castle 2793\n",
      "tje 2794\n",
      "Gene 2795\n",
      "try 2796\n",
      "collect 2797\n",
      "katie 2798\n",
      "trees 2799\n",
      "suit 2800\n",
      "rugby 2801\n",
      "classmates 2802\n",
      "appreciate 2803\n",
      "Mateo 2804\n",
      "date 2805\n",
      "Jaeden 2806\n",
      "dye 2807\n",
      "21 2808\n",
      "stressed 2809\n",
      "232 2810\n",
      "Maurice 2811\n",
      "influence 2812\n",
      "Indians 2813\n",
      "Scottish 2814\n",
      "Elmer 2815\n",
      "promotion 2816\n",
      "should 2817\n",
      "background 2818\n",
      "library 2819\n",
      "dressed 2820\n",
      "husband 2821\n",
      "Troy 2822\n",
      "Malcolm 2823\n",
      "No 2824\n",
      "basics 2825\n",
      "nanny 2826\n",
      "levels 2827\n",
      "barbecue 2828\n",
      "Chesterfield 2829\n",
      "intermediate 2830\n",
      "HR 2831\n",
      "morning 2832\n",
      "order 2833\n",
      "supermarket 2834\n",
      "youtube 2835\n",
      "under 2836\n",
      "had 2837\n",
      "arrived 2838\n",
      "rude 2839\n",
      "McKayla 2840\n",
      "formal 2841\n",
      "practical 2842\n",
      "go 2843\n",
      "Fatima 2844\n",
      "Vasily 2845\n",
      "march 2846\n",
      "drink 2847\n",
      "argument 2848\n",
      "incorrect 2849\n",
      "Conrad 2850\n",
      "Shari 2851\n",
      "unemployed 2852\n",
      "more 2853\n",
      "database 2854\n",
      "Yorke 2855\n",
      "Armani 2856\n",
      "Trudy 2857\n",
      "Haiti 2858\n",
      "comfy 2859\n",
      "Kathryn 2860\n",
      "market 2861\n",
      "Westin 2862\n",
      "able 2863\n",
      "arrange 2864\n",
      "whom 2865\n",
      "nobody 2866\n",
      "Hummer 2867\n",
      "introduce 2868\n",
      "issues 2869\n",
      "April 2870\n",
      "Carolyn 2871\n",
      "Sid 2872\n",
      "Rowing 2873\n",
      "aired 2874\n",
      "Sule 2875\n",
      "airlines 2876\n",
      "Vicki 2877\n",
      "R 2878\n",
      "Candy 2879\n",
      "state 2880\n",
      "neither 2881\n",
      "spa 2882\n",
      "gossiping 2883\n",
      "Logan 2884\n",
      "here 2885\n",
      "Olaf 2886\n",
      "clean 2887\n",
      "dizzy 2888\n",
      "28 2889\n",
      "dreams 2890\n",
      "Other 2891\n",
      "Pierrot 2892\n",
      "operating 2893\n",
      "Haesh 2894\n",
      "robotics 2895\n",
      "Jell 2896\n",
      "19th 2897\n",
      "liquid 2898\n",
      "appeal 2899\n",
      "because 2900\n",
      "beef 2901\n",
      "This 2902\n",
      "Doctor 2903\n",
      "Faby 2904\n",
      "Zachary 2905\n",
      "Misunderstood 2906\n",
      "physical 2907\n",
      "Tito 2908\n",
      "organises 2909\n",
      "Wikipedia 2910\n",
      "Aliana 2911\n",
      "extraordinary 2912\n",
      "collecting 2913\n",
      "Khadija 2914\n",
      "exaggerating 2915\n",
      "Minds 2916\n",
      "Viloet 2917\n",
      "taxi 2918\n",
      "Valerie 2919\n",
      "remark 2920\n",
      "immediately 2921\n",
      "delays 2922\n",
      "Rosie 2923\n",
      "write 2924\n",
      "lend 2925\n",
      "Javion 2926\n",
      "landing 2927\n",
      "announcing 2928\n",
      "birthday 2929\n",
      "resign 2930\n",
      "Persia 2931\n",
      "persuades 2932\n",
      "fashioned 2933\n",
      "degree 2934\n",
      "dates 2935\n",
      "someone 2936\n",
      "Bartek 2937\n",
      "dark 2938\n",
      "big 2939\n",
      "Liechtenstein 2940\n",
      "wet 2941\n",
      "Senza 2942\n",
      "puppy 2943\n",
      "Ashlyn 2944\n",
      "Bertil 2945\n",
      "fast 2946\n",
      "Dr 2947\n",
      "Marianne 2948\n",
      "DHL 2949\n",
      "college 2950\n",
      "not 2951\n",
      "imagine 2952\n",
      "Lance 2953\n",
      "there 2954\n",
      "translation 2955\n",
      "exercise 2956\n",
      "thrown 2957\n",
      "third 2958\n",
      "kayaking 2959\n",
      "week 2960\n",
      "podcast 2961\n",
      "cloister 2962\n",
      "unambiguous 2963\n",
      "Fred 2964\n",
      "learning 2965\n",
      "tonight 2966\n",
      "Space 2967\n",
      "Isaiah 2968\n",
      "Fiona 2969\n",
      "Envy 2970\n",
      "enjoyable 2971\n",
      "kind 2972\n",
      "soft 2973\n",
      "next 2974\n",
      "Sammy 2975\n",
      "Island 2976\n",
      "offence 2977\n",
      "rain 2978\n",
      "mud 2979\n",
      "Jasper 2980\n",
      "sex 2981\n",
      "stalling 2982\n",
      "Calvin 2983\n",
      "Vince 2984\n",
      "asap 2985\n",
      "orders 2986\n",
      "service 2987\n",
      "highway 2988\n",
      "RDR2 2989\n",
      "Jackie 2990\n",
      "neighbours 2991\n",
      "cream 2992\n",
      "camping 2993\n",
      "rest 2994\n",
      "block 2995\n",
      "Ashton 2996\n",
      "Jose 2997\n",
      "option 2998\n",
      "Tbilisi 2999\n",
      "Jawie 3000\n",
      "assessment 3001\n",
      "laundromat 3002\n",
      "drop 3003\n",
      "Gabrial 3004\n",
      "Eileen 3005\n",
      "inserted 3006\n",
      "pay 3007\n",
      "places 3008\n",
      "nan 3009\n",
      "owe 3010\n",
      "Reich 3011\n",
      "from 3012\n",
      "Aaliyah 3013\n",
      "Leandra 3014\n",
      "Gala 3015\n",
      "wannabe 3016\n",
      "however 3017\n",
      "Crush 3018\n",
      "Deedee 3019\n",
      "resistant 3020\n",
      "asked 3021\n",
      "Justine 3022\n",
      "gave 3023\n",
      "guys 3024\n",
      "salary 3025\n",
      "forgot 3026\n",
      "Greta 3027\n",
      "violated 3028\n",
      "Laos 3029\n",
      "Alfonso 3030\n",
      "Beyonce 3031\n",
      "trick 3032\n",
      "Sheldon 3033\n",
      "missing 3034\n",
      "following 3035\n",
      "Marcus 3036\n",
      "towel 3037\n",
      "funeral 3038\n",
      "Gosling 3039\n",
      "suffers 3040\n",
      "displayed 3041\n",
      "5 3042\n",
      "One 3043\n",
      "Mattia 3044\n",
      "needs 3045\n",
      "health 3046\n",
      "textbook 3047\n",
      "ring 3048\n",
      "Kit 3049\n",
      "amount 3050\n",
      "Agnes 3051\n",
      "e 3052\n",
      "having 3053\n",
      "signed 3054\n",
      "driving 3055\n",
      "almost 3056\n",
      "Marcin 3057\n",
      "worked 3058\n",
      "Dec 3059\n",
      "Natasha 3060\n",
      "cancer 3061\n",
      "Caroline 3062\n",
      "photography 3063\n",
      "wardrobe 3064\n",
      "chips 3065\n",
      "Warrick 3066\n",
      "goalkeeper 3067\n",
      "Blanca 3068\n",
      "mountains 3069\n",
      "shrimps 3070\n",
      "$ 3071\n",
      "Kamil 3072\n",
      "separately 3073\n",
      "Great 3074\n",
      "overtired 3075\n",
      "Amalia 3076\n",
      "old 3077\n",
      "borrowed 3078\n",
      "visitor 3079\n",
      "place 3080\n",
      "license 3081\n",
      "Cristofer 3082\n",
      "programme 3083\n",
      "left 3084\n",
      "Anita 3085\n",
      "Dexter 3086\n",
      "deal 3087\n",
      "Christopher 3088\n",
      "Glenn 3089\n",
      "38 3090\n",
      "Caron 3091\n",
      "table 3092\n",
      "friend 3093\n",
      "towards 3094\n",
      "Simmons 3095\n",
      "Mea 3096\n",
      "move 3097\n",
      "Brown 3098\n",
      "clash 3099\n",
      "loose 3100\n",
      "password 3101\n",
      "funding 3102\n",
      "networking 3103\n",
      "Extra 3104\n",
      "bottom 3105\n",
      "Annette 3106\n",
      "wished 3107\n",
      "fancy 3108\n",
      "Randall 3109\n",
      "origin 3110\n",
      "10 3111\n",
      "blocking 3112\n",
      "trips 3113\n",
      "choose 3114\n",
      "mind 3115\n",
      "Leah 3116\n",
      "fireman 3117\n",
      "eclipse 3118\n",
      "congratulates 3119\n",
      "leave 3120\n",
      "canteen 3121\n",
      "struggling 3122\n",
      "dripping 3123\n",
      "cooked 3124\n",
      "installs 3125\n",
      "bath 3126\n",
      "Switzerland 3127\n",
      "Winston 3128\n",
      "notice 3129\n",
      "fi 3130\n",
      "discusses 3131\n",
      "classes 3132\n",
      "dental 3133\n",
      "England 3134\n",
      "Tempur 3135\n",
      "Knight 3136\n",
      "32 3137\n",
      "Austin 3138\n",
      "stay 3139\n",
      "miracle 3140\n",
      "King 3141\n",
      "chapter 3142\n",
      "took 3143\n",
      "Sienna 3144\n",
      "Google 3145\n",
      "happy 3146\n",
      "girlrfriend 3147\n",
      "Tesco 3148\n",
      "tattoo 3149\n",
      "booked 3150\n",
      "coupon 3151\n",
      "Daredevil 3152\n",
      "Jacob 3153\n",
      "Katy 3154\n",
      "Adrianne 3155\n",
      "zloty 3156\n",
      "activities 3157\n",
      "fenugreek 3158\n",
      "outside 3159\n",
      "Boscaiola 3160\n",
      "Love 3161\n",
      "Trish 3162\n",
      "lunchtime 3163\n",
      "California 3164\n",
      "Arturo 3165\n",
      "Kevin 3166\n",
      "woman 3167\n",
      "quality 3168\n",
      "Odo 3169\n",
      "Germany 3170\n",
      "stakes 3171\n",
      "Jeanie 3172\n",
      "promoting 3173\n",
      "sweater 3174\n",
      "postpone 3175\n",
      "cashback 3176\n",
      "Janice 3177\n",
      "dislike 3178\n",
      "right 3179\n",
      "sages 3180\n",
      "Carlos 3181\n",
      "16 3182\n",
      "Switch 3183\n",
      "Camille 3184\n",
      "Rosemary 3185\n",
      "Hillary 3186\n",
      "Abrielle 3187\n",
      "b 3188\n",
      "republican 3189\n",
      "daughter 3190\n",
      "Susan 3191\n",
      "grocery 3192\n",
      "hamburgers 3193\n",
      "exams 3194\n",
      "Presidential 3195\n",
      "soccer 3196\n",
      "exchange 3197\n",
      "campus 3198\n",
      "Micah 3199\n",
      "Jesse 3200\n",
      "girl 3201\n",
      "Ronnie 3202\n",
      "Malini 3203\n",
      "media 3204\n",
      "Superman 3205\n",
      "Leon 3206\n",
      "dollars 3207\n",
      "monkey 3208\n",
      "Terry 3209\n",
      "supportive 3210\n",
      "Gracelyn 3211\n",
      "complexion 3212\n",
      "Korean 3213\n",
      "Head 3214\n",
      "maps 3215\n",
      "diet 3216\n",
      "advises 3217\n",
      "cricket 3218\n",
      "bullied 3219\n",
      "male 3220\n",
      "cute 3221\n",
      "Nothing 3222\n",
      "asks 3223\n",
      "episode 3224\n",
      "CA20192735641 3225\n",
      "euros 3226\n",
      "ordered 3227\n",
      "email 3228\n",
      "XXXL 3229\n",
      "Club 3230\n",
      "man 3231\n",
      "compensated 3232\n",
      "Jeniffer 3233\n",
      "phase 3234\n",
      "event 3235\n",
      "address 3236\n",
      "productively 3237\n",
      "apartments 3238\n",
      "mushroom 3239\n",
      "checkout 3240\n",
      "visiting 3241\n",
      "beaver 3242\n",
      "Mommy 3243\n",
      "Nancy 3244\n",
      "By 3245\n",
      "Isaac 3246\n",
      "7000 3247\n",
      "gamble 3248\n",
      "trouble 3249\n",
      "Broadway 3250\n",
      "enjoying 3251\n",
      "Jeremy 3252\n",
      "headphones 3253\n",
      "keeps 3254\n",
      "Sergey 3255\n",
      "Yves 3256\n",
      "Jada 3257\n",
      "Mindy 3258\n",
      "estate 3259\n",
      "querying 3260\n",
      "Margot 3261\n",
      "leg 3262\n",
      "Karlo 3263\n",
      "volunteering 3264\n",
      "Ronald 3265\n",
      "granola 3266\n",
      "recipe 3267\n",
      "Lilly 3268\n",
      "haircut 3269\n",
      "Casey 3270\n",
      "prescription 3271\n",
      "tracking 3272\n",
      "bronchitis 3273\n",
      "Gino 3274\n",
      "tests 3275\n",
      "Mother 3276\n",
      "hectic 3277\n",
      "Bay 3278\n",
      "rolls 3279\n",
      "mute 3280\n",
      "lectures 3281\n",
      "34 3282\n",
      "catch 3283\n",
      "885th 3284\n",
      "von 3285\n",
      "Sports 3286\n",
      "Jenniffer 3287\n",
      "US 3288\n",
      "invitation 3289\n",
      "Off 3290\n",
      "settings 3291\n",
      "Ronny 3292\n",
      "\" 3293\n",
      "Pamuk 3294\n",
      "favourite 3295\n",
      "clock 3296\n",
      "into 3297\n",
      "dryer 3298\n",
      "Kaya 3299\n",
      "photographed 3300\n",
      "torchlight 3301\n",
      "crate 3302\n",
      "fit 3303\n",
      "sound 3304\n",
      "Ruth 3305\n",
      "Tessa 3306\n",
      "Jez 3307\n",
      "taller 3308\n",
      "Chinese 3309\n",
      "Astra 3310\n",
      "Jones 3311\n",
      "recovering 3312\n",
      "given 3313\n",
      "17 3314\n",
      "lead 3315\n",
      "angry 3316\n",
      "Rhys 3317\n",
      "timings 3318\n",
      "bleeding 3319\n",
      "Sender 3320\n",
      "Monica 3321\n",
      "center 3322\n",
      "Leo 3323\n",
      "Swedish 3324\n",
      "homemade 3325\n",
      "mustard 3326\n",
      "6th 3327\n",
      "Walt 3328\n",
      "short 3329\n",
      "finished 3330\n",
      "Catherine 3331\n",
      "floor 3332\n",
      "headlamps 3333\n",
      "destroy 3334\n",
      "tree 3335\n",
      "Diego 3336\n",
      "share 3337\n",
      "Bitcoin 3338\n",
      "Mona 3339\n",
      "CAMP 3340\n",
      "jacket 3341\n",
      "Abbey 3342\n",
      "Ivy 3343\n",
      "Connie 3344\n",
      "nap 3345\n",
      "skip 3346\n",
      "Florence 3347\n",
      "annoying 3348\n",
      "strongly 3349\n",
      "fully 3350\n",
      "Presentations 3351\n",
      "regard 3352\n",
      "Liz 3353\n",
      "Japan 3354\n",
      "Victor 3355\n",
      "children 3356\n",
      "versus 3357\n",
      "dentist 3358\n",
      "lenses 3359\n",
      "instruction 3360\n",
      "Marshall 3361\n",
      "Sebastian 3362\n",
      "challenging 3363\n",
      "bathe 3364\n",
      "apologize 3365\n",
      "Courtney 3366\n",
      "Nichole 3367\n",
      "lake 3368\n",
      "Barbie 3369\n",
      "Damien 3370\n",
      "will 3371\n",
      "Costa 3372\n",
      "football 3373\n",
      "broken 3374\n",
      "Taylor 3375\n",
      "Scott 3376\n",
      "Restarting 3377\n",
      "especially 3378\n",
      "Svetlana 3379\n",
      "seek 3380\n",
      "Polynesia 3381\n",
      "Square 3382\n",
      "meets 3383\n",
      "hope 3384\n",
      "buliding 3385\n",
      "recommends 3386\n",
      "Clair 3387\n",
      "Beverly 3388\n",
      "Pembroke 3389\n",
      "Raul 3390\n",
      "welcome 3391\n",
      "accordance 3392\n",
      "Eva 3393\n",
      "easy 3394\n",
      "Shuhui 3395\n",
      "Nav 3396\n",
      "mill 3397\n",
      "Libby 3398\n",
      "1984 3399\n",
      "Milo 3400\n",
      "Bert 3401\n",
      "Daniela 3402\n",
      "Flora 3403\n",
      "glass 3404\n",
      "concerts 3405\n",
      "Ludo 3406\n",
      "Bahn 3407\n",
      "beautiful 3408\n",
      "lift 3409\n",
      "loves 3410\n",
      "A5 3411\n",
      "Norah 3412\n",
      "Joshua 3413\n",
      "sorry 3414\n",
      "improved 3415\n",
      "sleepiness 3416\n",
      "So 3417\n",
      "Allie 3418\n",
      "Eastern 3419\n",
      "bunny 3420\n",
      "Batman 3421\n",
      "titled 3422\n",
      "package 3423\n",
      "Ethan 3424\n",
      "Dixie 3425\n",
      "Doug 3426\n",
      "Elm 3427\n",
      "sore 3428\n",
      "Mounir 3429\n",
      "recognise 3430\n",
      "free 3431\n",
      "playground 3432\n",
      "House 3433\n",
      "geography 3434\n",
      "poor 3435\n",
      "getting 3436\n",
      "complied 3437\n",
      "Korea 3438\n",
      "shocked 3439\n",
      "ancestors 3440\n",
      "Dance 3441\n",
      "Election 3442\n",
      "breathing 3443\n",
      "positive 3444\n",
      "Little 3445\n",
      "estimate 3446\n",
      "js 3447\n",
      "holiday 3448\n",
      "feelings 3449\n",
      "Agnieszka 3450\n",
      "pass 3451\n",
      "notifications 3452\n",
      "Erik 3453\n",
      "2019 3454\n",
      "biography 3455\n",
      "chase 3456\n",
      "pint 3457\n",
      "Venom 3458\n",
      "observe 3459\n",
      "Selma 3460\n",
      "Brayden 3461\n",
      "@ 3462\n",
      "history 3463\n",
      "flatmate 3464\n",
      "Wroclaw 3465\n",
      "drawing 3466\n",
      "Giles 3467\n",
      "comes 3468\n",
      "encourage 3469\n",
      "Caribbean 3470\n",
      "plot 3471\n",
      "foundations 3472\n",
      "lighting 3473\n",
      "pub 3474\n",
      "University 3475\n",
      "Ada 3476\n",
      "dedicated 3477\n",
      "Fedora 3478\n",
      "Sarah 3479\n",
      "smokes 3480\n",
      "Garden 3481\n",
      "presidential 3482\n",
      "parts 3483\n",
      "haven 3484\n",
      "Darcie 3485\n",
      "Venice 3486\n",
      "slightly 3487\n",
      "Dale 3488\n",
      "polo 3489\n",
      "link 3490\n",
      "lamp 3491\n",
      "DVD 3492\n",
      "Europe 3493\n",
      "Zoe 3494\n",
      "invited 3495\n",
      "accurate 3496\n",
      "130 3497\n",
      "reminded 3498\n",
      "Hazel 3499\n",
      "Aunt 3500\n",
      "deadline 3501\n",
      "Excel 3502\n",
      "Hilary 3503\n",
      "Passion 3504\n",
      "tired 3505\n",
      "zoo 3506\n",
      "shops 3507\n",
      "but 3508\n",
      "provider 3509\n",
      "2k 3510\n",
      "sent 3511\n",
      "poking 3512\n",
      "Jarod 3513\n",
      "stream 3514\n",
      "Nag 3515\n",
      "stick 3516\n",
      "Julio 3517\n",
      "get 3518\n",
      "Essex 3519\n",
      "Apollo 3520\n",
      "Cheryl 3521\n",
      "Ella 3522\n",
      "scored 3523\n",
      "Aneta 3524\n",
      "Royal 3525\n",
      "starving 3526\n",
      "juggling 3527\n",
      "Ikea 3528\n",
      "twist 3529\n",
      "Christa 3530\n",
      "wine 3531\n",
      "heard 3532\n",
      "Seth 3533\n",
      "helpful 3534\n",
      "Hana 3535\n",
      "Meryl 3536\n",
      "David 3537\n",
      "fooled 3538\n",
      "paid 3539\n",
      "pill 3540\n",
      "be 3541\n",
      "Palace 3542\n",
      "gold 3543\n",
      "Metaphor 3544\n",
      "fly 3545\n",
      "Alice 3546\n",
      "names 3547\n",
      "Frank 3548\n",
      "Jen 3549\n",
      "Melissa 3550\n",
      "bread 3551\n",
      "switched 3552\n",
      "Gabriella 3553\n",
      "60 3554\n",
      "wall 3555\n",
      "summer 3556\n",
      "live 3557\n",
      "tutoring 3558\n",
      "splashes 3559\n",
      "Adrian 3560\n",
      "Republic 3561\n",
      "Teresa 3562\n",
      "satisfied 3563\n",
      "Barbra 3564\n",
      "Declan 3565\n",
      "think 3566\n",
      "[END] 3567\n",
      "I 3568\n",
      "Sina 3569\n",
      "fuse 3570\n",
      "happening 3571\n",
      "repertoire 3572\n",
      "multiple 3573\n",
      "beginning 3574\n",
      "believe 3575\n",
      "don 3576\n",
      "Riley 3577\n",
      "yet 3578\n",
      "Fanny 3579\n",
      "appendix 3580\n",
      "compilation 3581\n",
      "50 3582\n",
      "Lenny 3583\n",
      "hurry 3584\n",
      "regrets 3585\n",
      "wineries 3586\n",
      "drunk 3587\n",
      "Jan 3588\n",
      "confirmed 3589\n",
      "Early 3590\n",
      "Now 3591\n",
      "likes 3592\n",
      "World 3593\n",
      "fault 3594\n",
      "little 3595\n",
      "Mariam 3596\n",
      "betting 3597\n",
      "Fleur 3598\n",
      "explaining 3599\n",
      "glad 3600\n",
      "Valentina 3601\n",
      "Wojtek 3602\n",
      "Selvik 3603\n",
      "treadmill 3604\n",
      "anymore 3605\n",
      "Nestor 3606\n",
      "slow 3607\n",
      "Matilda 3608\n",
      "weekend 3609\n",
      "Shades 3610\n",
      "Slovakia 3611\n",
      "Antilles 3612\n",
      "Millie 3613\n",
      "00 3614\n",
      "distance 3615\n",
      "Lacey 3616\n",
      "closed 3617\n",
      "Tori 3618\n",
      "raising 3619\n",
      "riding 3620\n",
      "9 3621\n",
      "Pipa 3622\n",
      "All 3623\n",
      "dropped 3624\n",
      "Debbie 3625\n",
      "meal 3626\n",
      "tart 3627\n",
      "Priya 3628\n",
      "chat 3629\n",
      "Eui 3630\n",
      "Lilian 3631\n",
      "that 3632\n",
      "bike 3633\n",
      "Ottawa 3634\n",
      "vacancies 3635\n",
      "Lidia 3636\n",
      "earlier 3637\n",
      "Szymon 3638\n",
      "Gisele 3639\n",
      "embezzlement 3640\n",
      "detail 3641\n",
      "mechanic 3642\n",
      "Nat 3643\n",
      "Noah 3644\n",
      "CJay 3645\n",
      "mailbox 3646\n",
      "up 3647\n",
      "Morgan 3648\n",
      "Gavin 3649\n",
      "Dick 3650\n",
      "Baltoro 3651\n",
      "Tasha 3652\n",
      "Polish 3653\n",
      "praised 3654\n",
      "seaside 3655\n",
      "playlist 3656\n",
      "walking 3657\n",
      "Ashleen 3658\n",
      "Juan 3659\n",
      "Sylas 3660\n",
      "convey 3661\n",
      "Dom 3662\n",
      "Brandie 3663\n",
      "fruits 3664\n",
      "500 3665\n",
      "Gunner 3666\n",
      "sharing 3667\n",
      "Zac 3668\n",
      "geysers 3669\n",
      "got 3670\n",
      "beginners 3671\n",
      "150 3672\n",
      "Fonda 3673\n",
      "remain 3674\n",
      "drinks 3675\n",
      "Pandora 3676\n",
      "problematic 3677\n",
      "tap 3678\n",
      "Lawrence 3679\n",
      "Fai 3680\n",
      "Maryam 3681\n",
      "Rossi 3682\n",
      "boutique 3683\n",
      "Colin 3684\n",
      "42 3685\n",
      "club 3686\n",
      "Joel 3687\n",
      "Hee 3688\n",
      "reyurn 3689\n",
      "out 3690\n",
      "hype 3691\n",
      "Tuscany 3692\n",
      "Baba 3693\n",
      "boost 3694\n",
      "chemical 3695\n",
      "plant 3696\n",
      "sunbathe 3697\n",
      ": 3698\n",
      "Booba 3699\n",
      "Ana 3700\n",
      "neck 3701\n",
      "tutorial 3702\n",
      "discuss 3703\n",
      "contact 3704\n",
      "versatile 3705\n",
      "tie 3706\n",
      "pondering 3707\n",
      "speaking 3708\n",
      "Alexis 3709\n",
      "wishing 3710\n",
      "convinced 3711\n",
      "video 3712\n",
      "listening 3713\n",
      "fiancee 3714\n",
      "matter 3715\n",
      "accuses 3716\n",
      "advised 3717\n",
      "families 3718\n",
      "sofa 3719\n",
      "Italia 3720\n",
      "poster 3721\n",
      "bakery 3722\n",
      "Ersin 3723\n",
      "lives 3724\n",
      "Drew 3725\n",
      "Brian 3726\n",
      "freezing 3727\n",
      "story 3728\n",
      "terminal 3729\n",
      "mixed 3730\n",
      "selfie 3731\n",
      "th 3732\n",
      "[START] 3733\n",
      "Emilia 3734\n",
      "Dora 3735\n",
      "damaged 3736\n",
      "Kellie 3737\n",
      "Spirit 3738\n",
      "clip 3739\n",
      "Oslo 3740\n",
      "smile 3741\n",
      "hide 3742\n",
      "severe 3743\n",
      "Cold 3744\n",
      "Christina 3745\n",
      "management 3746\n",
      "Twiggy 3747\n",
      "pursue 3748\n",
      "Katherine 3749\n",
      "jewelry 3750\n",
      "100m 3751\n",
      "girls 3752\n",
      "pear 3753\n",
      "pages 3754\n",
      "it 3755\n",
      "Tabitha 3756\n",
      "Steffen 3757\n",
      "arrive 3758\n",
      "rehearsals 3759\n",
      "act 3760\n",
      "wishes 3761\n",
      "Battle 3762\n",
      "hear 3763\n",
      "uptown 3764\n",
      "Alec 3765\n",
      "fears 3766\n",
      "cinema 3767\n",
      "anything 3768\n",
      "years 3769\n",
      "comment 3770\n",
      "exchanged 3771\n",
      "Dawid 3772\n",
      "maching 3773\n",
      "lockers 3774\n",
      "protesting 3775\n",
      "61 3776\n",
      "a 3777\n",
      "000 3778\n",
      "Dean 3779\n",
      "Coca 3780\n",
      "told 3781\n",
      "The 3782\n",
      "War 3783\n",
      "Mazy 3784\n",
      "Khaled 3785\n",
      "Neil 3786\n",
      "John 3787\n",
      "season 3788\n",
      "late 3789\n",
      "socially 3790\n",
      "ability 3791\n",
      "Aya 3792\n",
      "postcard 3793\n",
      "presentation 3794\n",
      "Ollie 3795\n",
      "joining 3796\n",
      "Jarvis 3797\n",
      "apples 3798\n",
      "seat 3799\n",
      "Raise 3800\n",
      "Cass 3801\n",
      "lot 3802\n",
      "Miriam 3803\n",
      "Pearl 3804\n",
      "Arkia 3805\n",
      "hangover 3806\n",
      "glittery 3807\n",
      "fitness 3808\n",
      "board 3809\n",
      "Hallie 3810\n",
      "Gareth 3811\n",
      "read 3812\n",
      "Nicky 3813\n",
      "Brazilian 3814\n",
      "support 3815\n",
      "brownies 3816\n",
      "Teddy 3817\n",
      "way 3818\n",
      "Cola 3819\n",
      "- 3820\n",
      "talking 3821\n",
      "route 3822\n",
      "very 3823\n",
      "Anne 3824\n",
      "adhesive 3825\n",
      "front 3826\n",
      "Day 3827\n",
      "Tonny 3828\n",
      "V 3829\n",
      "poorly 3830\n",
      "gay 3831\n",
      "tries 3832\n",
      "woke 3833\n",
      "Handmaid 3834\n",
      "boots 3835\n",
      "Stroad 3836\n",
      "Franklin 3837\n",
      "later 3838\n",
      "Paloma 3839\n",
      "shirts 3840\n",
      "Women 3841\n",
      "Paige 3842\n",
      "extra 3843\n",
      "pm 3844\n",
      "Christmas 3845\n",
      "Cooper 3846\n",
      "reasons 3847\n",
      "Ion 3848\n",
      "Carson 3849\n",
      "Central 3850\n",
      "Caleb 3851\n",
      "endorses 3852\n",
      "scolded 3853\n",
      "Martina 3854\n",
      "Arabella 3855\n",
      "revenge 3856\n",
      "Waytt 3857\n",
      "bough 3858\n",
      "Brasil 3859\n",
      "disappearing 3860\n",
      "Rod 3861\n",
      "adivce 3862\n",
      "embroider 3863\n",
      "Marston 3864\n",
      "Johnson 3865\n",
      "Alvin 3866\n",
      "Angela 3867\n",
      "lately 3868\n",
      "onto 3869\n",
      "scammed 3870\n",
      "focus 3871\n",
      "Danuta 3872\n",
      "bowling 3873\n",
      "attend 3874\n",
      "Faro 3875\n",
      "coach 3876\n",
      "until 3877\n",
      "redecorating 3878\n",
      "Tracy 3879\n",
      "Jazmine 3880\n",
      "taken 3881\n",
      "real 3882\n",
      "ejoying 3883\n",
      "Abby 3884\n",
      "Gerry 3885\n",
      "Sebastien 3886\n",
      "report 3887\n",
      "Chai 3888\n",
      "participants 3889\n",
      "Freddy 3890\n",
      "apple 3891\n",
      "Dona 3892\n",
      "content 3893\n",
      "Gil 3894\n",
      "Hugo 3895\n",
      "Phillip 3896\n",
      "Carlie 3897\n",
      "Benton 3898\n",
      "Janette 3899\n",
      "maybe 3900\n",
      "young 3901\n",
      "Dex 3902\n",
      "cigarettes 3903\n",
      "Virginia 3904\n",
      "assignment 3905\n",
      "Bud 3906\n",
      "learn 3907\n",
      "Dostoevsky 3908\n",
      "Australia 3909\n",
      "believing 3910\n",
      "agree 3911\n",
      "Field 3912\n",
      "autograph 3913\n",
      "arranged 3914\n",
      "Dyer 3915\n",
      "Byblos 3916\n",
      "Karla 3917\n",
      "Judith 3918\n",
      "Internet 3919\n",
      "shades 3920\n",
      "cough 3921\n",
      "claim 3922\n",
      "Ulia 3923\n",
      "Marina 3924\n",
      "housewarming 3925\n",
      "telling 3926\n",
      "Society 3927\n",
      "accident 3928\n",
      "smaller 3929\n",
      "paycheck 3930\n",
      "7pm 3931\n",
      "grandfather 3932\n",
      "taking 3933\n",
      "focusing 3934\n",
      "Don 3935\n",
      "Luke 3936\n",
      "pears 3937\n",
      "bookstore 3938\n",
      "banana 3939\n",
      "junior 3940\n",
      "tissues 3941\n",
      "Josephine 3942\n",
      "warming 3943\n",
      "29th 3944\n",
      "paper 3945\n",
      "couch 3946\n",
      "ABC 3947\n",
      "itch 3948\n",
      "Both 3949\n",
      "each 3950\n",
      "invoice 3951\n",
      "Kylie 3952\n",
      "euro 3953\n",
      "Vikings 3954\n",
      "Art 3955\n",
      "Rufus 3956\n",
      "sightseeing 3957\n",
      "bibles 3958\n",
      "reckon 3959\n",
      "performance 3960\n",
      "reindeer 3961\n",
      "Jeffrey 3962\n",
      "Wonders 3963\n",
      "theories 3964\n",
      "Lorenzo 3965\n",
      "Julie 3966\n",
      "gym 3967\n",
      "Hampson 3968\n",
      "tenant 3969\n",
      "searched 3970\n",
      "Terrance 3971\n",
      "did 3972\n",
      "Oh 3973\n",
      "Arlo 3974\n",
      "Lizzy 3975\n",
      "Year 3976\n",
      "ECON 3977\n",
      "through 3978\n",
      "Geoffrey 3979\n",
      "reschedules 3980\n",
      "swim 3981\n",
      "Southern 3982\n",
      "championship 3983\n",
      "Elle 3984\n",
      "frequency 3985\n",
      "vocabulary 3986\n",
      "deposit 3987\n",
      "might 3988\n",
      "supervisor 3989\n",
      "Cast 3990\n",
      "postponed 3991\n",
      "LinkedIn 3992\n",
      "services 3993\n",
      "suffered 3994\n",
      "dish 3995\n",
      "celebrate 3996\n",
      "Marie 3997\n",
      "inspiration 3998\n",
      "concert 3999\n",
      "civic 4000\n",
      "policy 4001\n",
      "plastic 4002\n",
      "Bernard 4003\n",
      "Airport 4004\n",
      "Shirley 4005\n",
      "per 4006\n",
      "name 4007\n",
      "prudential 4008\n",
      "Butterball 4009\n",
      "works 4010\n",
      "butter 4011\n",
      "admire 4012\n",
      "Iceland 4013\n",
      "difference 4014\n",
      "Kelsie 4015\n",
      "Brice 4016\n",
      "eve 4017\n",
      "drinking 4018\n",
      "Andrea 4019\n",
      "gathering 4020\n",
      "flu 4021\n",
      "PLN 4022\n",
      "language 4023\n",
      "Mrs 4024\n",
      "Croydon 4025\n",
      "grandpa 4026\n",
      "lentils 4027\n",
      "settles 4028\n",
      "Things 4029\n",
      "gives 4030\n",
      "contacts 4031\n",
      "Jasmin 4032\n",
      "time 4033\n",
      "fascists 4034\n",
      "Shayla 4035\n",
      "Alex 4036\n",
      "Venus 4037\n",
      "Larry 4038\n",
      "formatting 4039\n",
      "corrections 4040\n",
      "Ivonne 4041\n",
      "interviews 4042\n",
      "near 4043\n",
      "seller 4044\n",
      "Tiffany 4045\n",
      "Kuwait 4046\n",
      "Fin 4047\n",
      "jenga 4048\n",
      "Ed 4049\n",
      "idea 4050\n",
      "Japanese 4051\n",
      "Mariah 4052\n",
      "es 4053\n",
      "Bojack 4054\n",
      "Hanks 4055\n",
      "Ida 4056\n",
      "matches 4057\n",
      "Olga 4058\n",
      "Christy 4059\n",
      "Sasha 4060\n",
      "soda 4061\n",
      "students 4062\n",
      "Dickins 4063\n",
      "helped 4064\n",
      "47th 4065\n",
      "caf 4066\n",
      "Breaks 4067\n",
      "start 4068\n",
      "robots 4069\n",
      "hospital 4070\n",
      "shoes 4071\n",
      "training 4072\n",
      "Hut 4073\n",
      "frustrated 4074\n",
      "attack 4075\n",
      "legend 4076\n",
      "lies 4077\n",
      "Geri 4078\n",
      "Darren 4079\n",
      "Kubica 4080\n",
      "Oleg 4081\n",
      "Yanis 4082\n",
      "Armenian 4083\n",
      "roommate 4084\n",
      "uncle 4085\n",
      "Nutcracker 4086\n",
      "bar 4087\n",
      "foot 4088\n",
      "stadium 4089\n",
      "TaoTao 4090\n",
      "reduced 4091\n",
      "declines 4092\n",
      "Old 4093\n",
      "organize 4094\n",
      "crossed 4095\n",
      "Francis 4096\n",
      "shares 4097\n",
      "angrier 4098\n",
      "triple 4099\n",
      "destroyed 4100\n",
      "Joe 4101\n",
      "description 4102\n",
      "saying 4103\n",
      "Rory 4104\n",
      "Bohemian 4105\n",
      "Ursula 4106\n",
      "Drake 4107\n",
      "Jonah 4108\n",
      "Red 4109\n",
      "rock 4110\n",
      "Xenia 4111\n",
      "steak 4112\n",
      "BigBuy 4113\n",
      "back 4114\n",
      "election 4115\n",
      "southeast 4116\n",
      "rise 4117\n",
      "Octavia 4118\n",
      "clubbing 4119\n",
      "2016 4120\n",
      "Munro 4121\n",
      "Brenda 4122\n",
      "Main 4123\n",
      "dissatisfied 4124\n",
      "sleepy 4125\n",
      "chocolates 4126\n",
      "Liv 4127\n",
      "Cristian 4128\n",
      "secret 4129\n",
      "Tony 4130\n",
      "presents 4131\n",
      "older 4132\n",
      "Roisin 4133\n",
      "hate 4134\n",
      "Reykjavik 4135\n",
      "reservation 4136\n",
      "planted 4137\n",
      "Daniel 4138\n",
      "struggles 4139\n",
      "Adrew 4140\n",
      "forever 4141\n",
      "calls 4142\n",
      "lottery 4143\n",
      "forehead 4144\n",
      "pillows 4145\n",
      "pens 4146\n",
      "Hagrid 4147\n",
      "Melania 4148\n",
      "Lora 4149\n",
      "awkward 4150\n",
      "SUV 4151\n",
      "primary 4152\n",
      "weird 4153\n",
      "drowned 4154\n",
      "devastated 4155\n",
      "spend 4156\n",
      "Avengers 4157\n",
      "Conor 4158\n",
      "conference 4159\n",
      "Tomas 4160\n",
      "beard 4161\n",
      "lots 4162\n",
      "teenth 4163\n",
      "toilet 4164\n",
      "pole 4165\n",
      "singing 4166\n",
      "stinks 4167\n",
      "Affairs 4168\n",
      "Spud 4169\n",
      "ajvar 4170\n",
      "boys 4171\n",
      "wanted 4172\n",
      "hill 4173\n",
      "Hashimoto 4174\n",
      "submit 4175\n",
      "skills 4176\n",
      "though 4177\n",
      "know 4178\n",
      "69 4179\n",
      "Amber 4180\n",
      "luck 4181\n",
      "repeat 4182\n",
      "Eric 4183\n",
      "frends 4184\n",
      "parliament 4185\n",
      "cleaner 4186\n",
      "James 4187\n",
      "General 4188\n",
      "diamond 4189\n",
      "Grace 4190\n",
      "half 4191\n",
      "Claire 4192\n",
      "MoMA 4193\n",
      "sparkly 4194\n",
      "Cheyanne 4195\n",
      "Lebanese 4196\n",
      "elections 4197\n",
      "ladies 4198\n",
      "bets 4199\n",
      "practice 4200\n",
      "theater 4201\n",
      "Ripple 4202\n",
      "Kaleigh 4203\n",
      "far 4204\n",
      "They 4205\n",
      "disagrees 4206\n",
      "anyone 4207\n",
      "km 4208\n",
      "most 4209\n",
      "search 4210\n",
      "unpleasant 4211\n",
      "Nora 4212\n",
      "cookies 4213\n",
      "Violet 4214\n",
      "serving 4215\n",
      "cheer 4216\n",
      "cold 4217\n",
      "Vesna 4218\n",
      "spot 4219\n",
      "Marion 4220\n",
      "6 4221\n",
      "central 4222\n",
      "Julian 4223\n",
      "born 4224\n",
      "Shaz 4225\n",
      "Hp 4226\n",
      "priorities 4227\n",
      "finishes 4228\n",
      "Menno 4229\n",
      "coypu 4230\n",
      "cars 4231\n",
      "T 4232\n",
      "proof 4233\n",
      "Leslie 4234\n",
      "wait 4235\n",
      "Muriel 4236\n",
      "Next 4237\n",
      "dangerous 4238\n",
      "Stones 4239\n",
      "crashed 4240\n",
      "Rebecca 4241\n",
      "foundation 4242\n",
      "nevertheless 4243\n",
      "Luna 4244\n",
      "Julia 4245\n",
      "dad 4246\n",
      "Afterwards 4247\n",
      "Daryl 4248\n",
      "pedicure 4249\n",
      "rooms 4250\n",
      "Niles 4251\n",
      "comedian 4252\n",
      "congested 4253\n",
      "countersign 4254\n",
      "after 4255\n",
      "Ivona 4256\n",
      "principal 4257\n",
      "jail 4258\n",
      "Cards 4259\n",
      "Payne 4260\n",
      " 4261\n",
      "citizens 4262\n",
      "forgotten 4263\n",
      "disappeared 4264\n",
      "Kathy 4265\n",
      "sure 4266\n",
      "collects 4267\n",
      "Tuesdays 4268\n",
      "accents 4269\n",
      "kg 4270\n",
      "Calleigh 4271\n",
      "promised 4272\n",
      "photo 4273\n",
      "bring 4274\n",
      "Stephen 4275\n",
      "thinks 4276\n",
      "Freya 4277\n",
      "tapas 4278\n",
      "legs 4279\n",
      "Mitch 4280\n",
      "hills 4281\n",
      "page 4282\n",
      "unavailable 4283\n",
      "Bam 4284\n",
      "games 4285\n",
      "ate 4286\n",
      "soy 4287\n",
      "11pm 4288\n",
      "Smith 4289\n",
      "bill 4290\n",
      "Matthias 4291\n",
      "Rachel 4292\n",
      "Indian 4293\n",
      "airpods 4294\n",
      "Illness 4295\n",
      "joined 4296\n",
      "google 4297\n",
      "Pilar 4298\n",
      "assistants 4299\n",
      "Oliver 4300\n",
      "showing 4301\n",
      "answering 4302\n",
      "Dayna 4303\n",
      "length 4304\n",
      "planned 4305\n",
      "ticket 4306\n",
      "father 4307\n",
      "secretary 4308\n",
      "Marriott 4309\n",
      "Dylan 4310\n",
      "Nick 4311\n",
      "manifestations 4312\n",
      "City 4313\n",
      "giving 4314\n",
      "hissing 4315\n",
      "pain 4316\n",
      "sit 4317\n",
      "Derrick 4318\n",
      "latino 4319\n",
      "dub 4320\n",
      "promoted 4321\n",
      "comments 4322\n",
      "Antwerp 4323\n",
      "Dilly 4324\n",
      "bed 4325\n",
      "rid 4326\n",
      "reads 4327\n",
      "printer 4328\n",
      "Xiaomi 4329\n",
      "excellent 4330\n",
      "flower 4331\n",
      "Hugh 4332\n",
      "Stranger 4333\n",
      "usually 4334\n",
      "Mina 4335\n",
      "consults 4336\n",
      "painkillers 4337\n",
      "breaking 4338\n",
      "Hannah 4339\n",
      "statistics 4340\n",
      "Truck 4341\n",
      "dictionary 4342\n",
      "Flint 4343\n",
      "Isabella 4344\n",
      "maid 4345\n",
      "Olivia 4346\n",
      "outdoor 4347\n",
      "suggestions 4348\n",
      "m 4349\n",
      "Eleanor 4350\n",
      "Self 4351\n",
      "picking 4352\n",
      "democracy 4353\n",
      "park 4354\n",
      "Ahad 4355\n",
      "French 4356\n",
      "bugs 4357\n",
      "hungover 4358\n",
      "helping 4359\n",
      "rare 4360\n",
      "Yoko 4361\n",
      "CRM 4362\n",
      "enhancing 4363\n",
      "thirty 4364\n",
      "themselves 4365\n",
      "France 4366\n",
      "necessary 4367\n",
      "warlike 4368\n",
      "experiment 4369\n",
      "8ish 4370\n",
      "Peggy 4371\n",
      "decisions 4372\n",
      "End 4373\n",
      "south 4374\n",
      "Lower 4375\n",
      "deteriorated 4376\n",
      "dumped 4377\n",
      "keys 4378\n",
      "came 4379\n",
      "premiere 4380\n",
      "related 4381\n",
      "installed 4382\n",
      "intentions 4383\n",
      "thought 4384\n",
      "going 4385\n",
      "scholarship 4386\n",
      "samples 4387\n",
      "Jennie 4388\n",
      "Pippo 4389\n",
      "design 4390\n",
      "McDonalds 4391\n",
      "Starfish 4392\n",
      "Marilyn 4393\n",
      "Cyrus 4394\n",
      "Last 4395\n",
      "application 4396\n",
      "Kerry 4397\n",
      ", 4398\n",
      "unexpected 4399\n",
      "chained 4400\n",
      "decision 4401\n",
      "Station 4402\n",
      "burnt 4403\n",
      "Marlene 4404\n",
      "Void 4405\n",
      "Gabe 4406\n",
      "turns 4407\n",
      "Dimitri 4408\n",
      "need 4409\n",
      "work 4410\n",
      "undergo 4411\n",
      "dumplings 4412\n",
      "noodle 4413\n",
      "person 4414\n",
      "volunteers 4415\n",
      "stress 4416\n",
      "teriyaki 4417\n",
      "Yasmina 4418\n",
      "Adrien 4419\n",
      "factory 4420\n",
      "slippers 4421\n",
      "15x360 4422\n",
      "prospect 4423\n",
      "cousin 4424\n",
      "Omer 4425\n",
      "wantsto 4426\n",
      "lost 4427\n",
      "created 4428\n",
      "Cathie 4429\n",
      "Mandy 4430\n",
      "protect 4431\n",
      "100 4432\n",
      "Angelica 4433\n",
      "Kristen 4434\n",
      "Monty 4435\n",
      "worker 4436\n",
      "departing 4437\n",
      "Pisa 4438\n",
      "lyrics 4439\n",
      "intorelant 4440\n",
      "Carmen 4441\n",
      "monogram 4442\n",
      "Brussels 4443\n",
      "Dwayne 4444\n",
      "substitute 4445\n",
      "death 4446\n",
      "song 4447\n",
      "Arthut 4448\n",
      "borrow 4449\n",
      "thing 4450\n",
      "update 4451\n",
      "Clayton 4452\n",
      "Coimbra 4453\n",
      "Dennic 4454\n",
      "Fisher 4455\n",
      "Gloria 4456\n",
      "communicate 4457\n",
      "busy 4458\n",
      "bottles 4459\n",
      "60z 4460\n",
      "Amelia 4461\n",
      "profiles 4462\n",
      "looking 4463\n",
      "mall 4464\n",
      "M 4465\n",
      "cheapest 4466\n",
      "university 4467\n",
      "Craig 4468\n",
      "Whitney 4469\n",
      "Nel 4470\n",
      "proposed 4471\n",
      "envy 4472\n",
      "Gabrielle 4473\n",
      "Jesus 4474\n",
      "badge 4475\n",
      "meat 4476\n",
      "Maries 4477\n",
      "Jax 4478\n",
      "Muse 4479\n",
      "Charity 4480\n",
      "Phyllis 4481\n",
      "Kinga 4482\n",
      "Jon 4483\n",
      "Russel 4484\n",
      "brilliantly 4485\n",
      "Regina 4486\n",
      "M6 4487\n",
      "family 4488\n",
      "moved 4489\n",
      "Ala 4490\n",
      "bagpack 4491\n",
      "leaked 4492\n",
      "security 4493\n",
      "Thanksgiving 4494\n",
      "visa 4495\n",
      "Noyce 4496\n",
      "skatepark 4497\n",
      "Rita 4498\n",
      "purchase 4499\n",
      "writes 4500\n",
      "Stacy 4501\n",
      "expected 4502\n",
      "vermouth 4503\n",
      "pc 4504\n",
      "lunch 4505\n",
      "Cora 4506\n",
      "care 4507\n",
      "when 4508\n",
      "sushi 4509\n",
      "bet 4510\n",
      "Ring 4511\n",
      "Rejmonta 4512\n",
      "men 4513\n",
      "Franck 4514\n",
      "difficulty 4515\n",
      "Arthur 4516\n",
      "intermission 4517\n",
      "Doylan 4518\n",
      "Concentrix 4519\n",
      "Bruno 4520\n",
      "yesterday 4521\n",
      "OK 4522\n",
      "informing 4523\n",
      "started 4524\n",
      "A 4525\n",
      "worth 4526\n",
      "uncomfortable 4527\n",
      "Ellie 4528\n",
      "according 4529\n",
      "expectations 4530\n",
      "kilometer 4531\n",
      "Mogens 4532\n",
      "complained 4533\n",
      "Mr 4534\n",
      "idiot 4535\n",
      "argued 4536\n",
      "wigs 4537\n",
      "extremely 4538\n",
      "airport 4539\n",
      "Asperger 4540\n",
      "Joanna 4541\n",
      "doesn 4542\n",
      "Scotland 4543\n",
      "role 4544\n",
      "platform 4545\n",
      "Herv 4546\n",
      "Bonny 4547\n",
      "heart 4548\n",
      "Albert 4549\n",
      "served 4550\n",
      "decided 4551\n",
      "Enid 4552\n",
      "Rowan 4553\n",
      "grey 4554\n",
      "Chamber 4555\n",
      "days 4556\n",
      "recently 4557\n",
      "salon 4558\n",
      "manual 4559\n",
      "Her 4560\n",
      "Road 4561\n",
      "Incredible 4562\n",
      "UK 4563\n",
      "Willow 4564\n",
      "Yuri 4565\n",
      "sandwiches 4566\n",
      "Theatre 4567\n",
      "happened 4568\n",
      "Diablo 4569\n",
      "persuaded 4570\n",
      "vlogmas 4571\n",
      "even 4572\n",
      "Edi 4573\n",
      "joins 4574\n",
      "dress 4575\n",
      "Nobody 4576\n",
      "others 4577\n",
      "rather 4578\n",
      "dead 4579\n",
      "cheaper 4580\n",
      "answer 4581\n",
      "Algarve 4582\n",
      "Enrique 4583\n",
      "home 4584\n",
      "voted 4585\n",
      "neighborhood 4586\n",
      "minutes 4587\n",
      "charity 4588\n",
      "Prompted 4589\n",
      "an 4590\n",
      "actress 4591\n",
      "whole 4592\n",
      "Daisy 4593\n",
      "Elena 4594\n",
      "regards 4595\n",
      "urgent 4596\n",
      "criticizing 4597\n",
      "join 4598\n",
      "wash 4599\n",
      "Alger 4600\n",
      "body 4601\n",
      "Dennis 4602\n",
      "Mendes 4603\n",
      "lounge 4604\n",
      "departures 4605\n",
      "Jojo 4606\n",
      "spices 4607\n",
      "financial 4608\n",
      "Siddhi 4609\n",
      "reassure 4610\n",
      "Lisabeth 4611\n",
      "Tatiana 4612\n",
      "Colby 4613\n",
      "SPA 4614\n",
      "mentions 4615\n",
      "bored 4616\n",
      "unhappy 4617\n",
      "Starbucks 4618\n",
      "quits 4619\n",
      "hit 4620\n",
      "see 4621\n",
      "discounted 4622\n",
      "Monika 4623\n",
      "Samara 4624\n",
      "post 4625\n",
      "grown 4626\n",
      "match 4627\n",
      "hotels 4628\n",
      "Zoeh 4629\n",
      "could 4630\n",
      "Somebody 4631\n",
      "running 4632\n",
      "bibliography 4633\n",
      "episodes 4634\n",
      "Peppa 4635\n",
      "Suchecka 4636\n",
      "4 4637\n",
      "cheap 4638\n",
      "Flaming 4639\n",
      "Theo 4640\n",
      "casino 4641\n",
      "react 4642\n",
      "due 4643\n",
      "hiking 4644\n",
      "heavy 4645\n",
      "baptise 4646\n",
      "ride 4647\n",
      "honor 4648\n",
      "convinces 4649\n",
      "Helmut 4650\n",
      "letter 4651\n",
      "pockets 4652\n",
      "hair 4653\n",
      "Elian 4654\n",
      "sale 4655\n",
      "corner 4656\n",
      "array 4657\n",
      "boring 4658\n",
      "Elsie 4659\n",
      "opinion 4660\n",
      "Rachael 4661\n",
      "Potter 4662\n",
      "weather 4663\n",
      "Kenna 4664\n",
      "agreement 4665\n",
      "Man 4666\n",
      "sneaky 4667\n",
      "mold 4668\n",
      "mistakenly 4669\n",
      "Bianca 4670\n",
      "Berenice 4671\n",
      "Leyla 4672\n",
      "while 4673\n",
      "Studio 4674\n",
      "Miranda 4675\n",
      "made 4676\n",
      "receives 4677\n",
      "leaves 4678\n",
      "Roul 4679\n",
      "Quietcomfort 4680\n",
      "Woof 4681\n",
      "computer 4682\n",
      "notary 4683\n",
      "box 4684\n",
      "Cassie 4685\n",
      "Myra 4686\n",
      "overnight 4687\n",
      "Greenwood 4688\n",
      "Crew 4689\n",
      "Emmanuel 4690\n",
      "text 4691\n",
      "Audi 4692\n",
      "Sami 4693\n",
      "anywas 4694\n",
      "Lennart 4695\n",
      "Roberta 4696\n",
      "48 4697\n",
      "Sophie 4698\n",
      "feel 4699\n",
      "making 4700\n",
      "Festival 4701\n",
      "According 4702\n",
      "mobile 4703\n",
      "brawl 4704\n",
      "Some 4705\n",
      "Karim 4706\n",
      "district 4707\n",
      "s 4708\n",
      "Maybelline 4709\n",
      "exercises 4710\n",
      "Shining 4711\n",
      "Po 4712\n",
      "love 4713\n",
      "influencers 4714\n",
      "grainy 4715\n",
      "gets 4716\n",
      "Eveline 4717\n",
      "fun 4718\n",
      "cut 4719\n",
      "behalf 4720\n",
      "stamps 4721\n",
      "mon 4722\n",
      "picture 4723\n",
      "Becca 4724\n",
      "everyone 4725\n",
      "14 4726\n",
      "likely 4727\n",
      "Madrid 4728\n",
      "conditioner 4729\n",
      "Karen 4730\n",
      "Cathleen 4731\n",
      "five 4732\n",
      "Gaby 4733\n",
      "laptop 4734\n",
      "participate 4735\n",
      "discussion 4736\n",
      "consider 4737\n",
      "wondering 4738\n",
      "city 4739\n",
      "Joseph 4740\n",
      "Normandy 4741\n",
      "pop 4742\n",
      "desperate 4743\n",
      "naan 4744\n",
      "tutor 4745\n",
      "sort 4746\n",
      "fat 4747\n",
      "outstanding 4748\n",
      "three 4749\n",
      "cage 4750\n",
      "Helen 4751\n",
      "silly 4752\n",
      "Cait 4753\n",
      "Hawkins 4754\n",
      "ahead 4755\n",
      "Rosario 4756\n",
      "Because 4757\n",
      "husky 4758\n",
      "least 4759\n",
      "Galaxy 4760\n",
      "recommended 4761\n",
      "shout 4762\n",
      "Susie 4763\n",
      "Break 4764\n",
      "Clarisse 4765\n",
      "recommendations 4766\n",
      "show 4767\n",
      "White 4768\n",
      "Pietro 4769\n",
      "Cory 4770\n",
      "moody 4771\n",
      "syndrome 4772\n",
      "interviewing 4773\n",
      "sings 4774\n",
      "Jayden 4775\n",
      "Acres 4776\n",
      "Dragons 4777\n",
      "lesson 4778\n",
      "Rebekah 4779\n",
      "wolf 4780\n",
      "loses 4781\n",
      "he 4782\n",
      "Nicolas 4783\n",
      "Scarlet 4784\n",
      "alarm 4785\n",
      "unwell 4786\n",
      "Oli 4787\n",
      "scarf 4788\n",
      "noon 4789\n",
      "Jean 4790\n",
      "Rubens 4791\n",
      "211 4792\n",
      "tells 4793\n",
      "nothing 4794\n",
      "lets 4795\n",
      "party 4796\n",
      "PS 4797\n",
      "different 4798\n",
      "fare 4799\n",
      "value 4800\n",
      "Liam 4801\n",
      "bills 4802\n",
      "Good 4803\n",
      "Lilah 4804\n",
      "Mum 4805\n",
      "Finland 4806\n",
      "nickname 4807\n",
      "somebody 4808\n",
      "independent 4809\n",
      "visits 4810\n",
      "versions 4811\n",
      "hypocrites 4812\n",
      "Tyler 4813\n",
      "save 4814\n",
      "rental 4815\n",
      "Martins 4816\n",
      "Bev 4817\n",
      "Antoine 4818\n",
      "Enter 4819\n",
      "already 4820\n",
      "am 4821\n",
      "stuff 4822\n",
      "Bon 4823\n",
      "Pizza 4824\n",
      "documentary 4825\n",
      "dr 4826\n",
      "Philadelphia 4827\n",
      "cheated 4828\n",
      "plants 4829\n",
      "loaf 4830\n",
      "Johny 4831\n",
      "Karina 4832\n",
      "case 4833\n",
      "shower 4834\n",
      "Manesh 4835\n",
      "Barca 4836\n",
      "discount 4837\n",
      "finger 4838\n",
      "one 4839\n",
      "Everything 4840\n",
      "deleting 4841\n",
      "Sean 4842\n",
      "Hope 4843\n",
      "today 4844\n",
      "connected 4845\n",
      "DQ 4846\n",
      "run 4847\n",
      "Kin 4848\n",
      "title 4849\n",
      "companions 4850\n",
      "cramps 4851\n",
      "stood 4852\n",
      "school 4853\n",
      "vet 4854\n",
      "large 4855\n",
      "inexpensive 4856\n",
      "copy 4857\n",
      "autumn 4858\n",
      "seen 4859\n",
      "explain 4860\n",
      "Oti 4861\n",
      "reach 4862\n",
      "2000 4863\n",
      "Carl 4864\n",
      "Zara 4865\n",
      "Klay 4866\n",
      "Mystery 4867\n",
      "close 4868\n",
      "Quinn 4869\n",
      "tall 4870\n",
      "Patel 4871\n",
      "Mathilde 4872\n",
      "cache 4873\n",
      "internet 4874\n",
      "common 4875\n",
      "movie 4876\n",
      "therapy 4877\n",
      "When 4878\n",
      "Klara 4879\n",
      "salad 4880\n",
      "critical 4881\n",
      "preparation 4882\n",
      "bars 4883\n",
      "agrees 4884\n",
      "Aldona 4885\n",
      "popcorn 4886\n",
      "Nelly 4887\n",
      "discussing 4888\n",
      "Flynn 4889\n",
      "toasts 4890\n",
      "3pm 4891\n",
      "resembled 4892\n",
      "Penny 4893\n",
      "biggest 4894\n",
      "Keith 4895\n",
      "Loretta 4896\n",
      "knew 4897\n",
      "delayed 4898\n",
      "Princess 4899\n",
      "lamps 4900\n",
      "completely 4901\n",
      "audience 4902\n",
      "system 4903\n",
      "cooker 4904\n",
      "Skype 4905\n",
      "Joselyn 4906\n",
      "al 4907\n",
      "Beatrice 4908\n",
      "money 4909\n",
      "Wade 4910\n",
      "threatens 4911\n",
      "Lola 4912\n",
      "Lennie 4913\n",
      "Asia 4914\n",
      "remake 4915\n",
      "Heather 4916\n",
      "Mai 4917\n",
      "Sheila 4918\n",
      "Saturday 4919\n",
      "me 4920\n",
      "Wish 4921\n",
      "Linda 4922\n",
      "Avi 4923\n",
      "registered 4924\n",
      "RPG 4925\n",
      "babysit 4926\n",
      "irritated 4927\n",
      "cryptocurrencies 4928\n",
      "Bieber 4929\n",
      "Hood 4930\n",
      "construction 4931\n",
      "hurt 4932\n",
      "4th 4933\n",
      "describes 4934\n",
      "cancelled 4935\n",
      "plenty 4936\n",
      "exorcist 4937\n",
      "glasses 4938\n",
      "seeded 4939\n",
      "so 4940\n",
      "sister 4941\n",
      "economics 4942\n",
      "Simon 4943\n",
      "gossip 4944\n",
      "nocciolato 4945\n",
      "Holy 4946\n",
      "sport 4947\n",
      "Thomas 4948\n",
      "age 4949\n",
      "informed 4950\n",
      "Canada 4951\n",
      "makes 4952\n",
      "Lou 4953\n",
      "Skylar 4954\n",
      "Macca 4955\n",
      "threats 4956\n",
      "Like 4957\n",
      "prior 4958\n",
      "Zane 4959\n",
      "else 4960\n",
      "disc 4961\n",
      "Reginald 4962\n",
      "fits 4963\n",
      "hates 4964\n",
      "Facebook 4965\n",
      "bought 4966\n",
      "Poland 4967\n",
      "chicken 4968\n",
      "Kyle 4969\n",
      "slows 4970\n",
      "figure 4971\n",
      "website 4972\n",
      "Dustin 4973\n",
      "Trace 4974\n",
      "He 4975\n",
      "iPhone 4976\n",
      "star 4977\n",
      "Payton 4978\n",
      "Eternity 4979\n",
      "Sherry 4980\n",
      "singles 4981\n",
      "advice 4982\n",
      "return 4983\n",
      "seduce 4984\n",
      "destination 4985\n",
      "infant 4986\n",
      "yourself 4987\n",
      "Ananya 4988\n",
      "Priti 4989\n",
      "series 4990\n",
      "Michal 4991\n",
      "manager 4992\n",
      "longer 4993\n",
      "kitten 4994\n",
      "Kayah 4995\n",
      "consent 4996\n",
      "gathers 4997\n",
      "afternoon 4998\n",
      "parrot 4999\n",
      "39 5000\n",
      "Philip 5001\n",
      "pie 5002\n",
      "Town 5003\n",
      "Sam 5004\n",
      "employer 5005\n",
      "Lupe 5006\n",
      "finally 5007\n",
      "timing 5008\n",
      "Cork 5009\n",
      "aren 5010\n",
      "Neal 5011\n",
      "Mavis 5012\n",
      "wants 5013\n",
      "Elliott 5014\n",
      "allowance 5015\n",
      "private 5016\n",
      "hall 5017\n",
      "Ritz 5018\n",
      "11C 5019\n",
      "Wes 5020\n",
      "voucher 5021\n",
      "Fran 5022\n",
      "misspelling 5023\n",
      "History 5024\n",
      "Evie 5025\n",
      "1256 5026\n",
      "Sunny 5027\n",
      "xmas 5028\n",
      "salmon 5029\n",
      "Jess 5030\n",
      "owner 5031\n",
      "negotiations 5032\n",
      "Felicity 5033\n",
      "Sven 5034\n",
      "Elijah 5035\n",
      "Clint 5036\n",
      "cause 5037\n",
      "Bialowieza 5038\n",
      "Lee 5039\n",
      "disease 5040\n",
      "Martin 5041\n",
      "marks 5042\n",
      "railway 5043\n",
      "their 5044\n",
      "Maisy 5045\n",
      "sleeve 5046\n",
      "merge 5047\n",
      "Brooklyn 5048\n",
      "PC 5049\n",
      "must 5050\n",
      "student 5051\n",
      "if 5052\n",
      "Roxanna 5053\n",
      "considering 5054\n",
      "arm 5055\n",
      "TV 5056\n",
      "much 5057\n",
      "Rosa 5058\n",
      "Cafe 5059\n",
      "Donte 5060\n",
      "Ingrid 5061\n",
      "lessons 5062\n",
      "guesses 5063\n",
      "bow 5064\n",
      "Vail 5065\n",
      "class 5066\n",
      "98765432 5067\n",
      "keyed 5068\n",
      "As 5069\n",
      "put 5070\n",
      "Terrence 5071\n",
      "suitcase 5072\n",
      "11 5073\n",
      "Jay 5074\n",
      "Woodson 5075\n",
      "hedgehog 5076\n",
      "Angola 5077\n",
      "demand 5078\n",
      "dismissed 5079\n",
      "both 5080\n",
      "Anthony 5081\n",
      "ve 5082\n",
      "results 5083\n",
      "pack 5084\n",
      "vodka 5085\n",
      "consult 5086\n",
      "question 5087\n",
      "Janelle 5088\n",
      "promote 5089\n",
      "goodbye 5090\n",
      "Karenina 5091\n",
      "Chloe 5092\n",
      "Gary 5093\n",
      "Getting 5094\n",
      "Apple 5095\n",
      "nearly 5096\n",
      "Mary 5097\n",
      "research 5098\n",
      "help 5099\n",
      "skating 5100\n",
      "ones 5101\n",
      "volunteer 5102\n",
      "Dave 5103\n",
      "Omarosa 5104\n",
      "Ellen 5105\n",
      "Gery 5106\n",
      "Annie 5107\n",
      "available 5108\n",
      "directions 5109\n",
      "Billie 5110\n",
      "conflict 5111\n",
      "Hamilton 5112\n",
      "invites 5113\n",
      "Monrovia 5114\n",
      "Valentia 5115\n",
      "small 5116\n",
      "undergone 5117\n",
      "vegan 5118\n",
      "understood 5119\n",
      "Connell 5120\n",
      "Omar 5121\n",
      "sausages 5122\n",
      "homeless 5123\n",
      "gathered 5124\n",
      "starting 5125\n",
      "Ralph 5126\n",
      "Daddy 5127\n",
      "bicycle 5128\n",
      "Valery 5129\n",
      "normal 5130\n",
      "Bella 5131\n",
      "rum 5132\n",
      "YouTube 5133\n",
      "Susy 5134\n",
      "Nando 5135\n",
      "Suzy 5136\n",
      "classical 5137\n",
      "reckons 5138\n",
      "being 5139\n",
      "Pho 5140\n",
      "Donations 5141\n",
      "Gotti 5142\n",
      "Hattie 5143\n",
      "depression 5144\n",
      "14C 5145\n",
      "Pamela 5146\n",
      "tell 5147\n",
      "rumours 5148\n",
      "& 5149\n",
      "fixing 5150\n",
      "wireless 5151\n",
      "winning 5152\n",
      "broke 5153\n",
      "garden 5154\n",
      "pranked 5155\n",
      "vegetables 5156\n",
      "Colton 5157\n",
      "entire 5158\n",
      "paint 5159\n",
      "Naima 5160\n",
      "Youtuber 5161\n",
      "Francesca 5162\n",
      "trucks 5163\n",
      "3 5164\n",
      "reason 5165\n",
      "bananas 5166\n",
      "backpack 5167\n",
      "Artur 5168\n",
      "Alexandra 5169\n",
      "fp 5170\n",
      "forget 5171\n",
      "Blake 5172\n",
      "Wayne 5173\n",
      "relationship 5174\n",
      "12th 5175\n",
      "marathon 5176\n",
      "accompany 5177\n",
      "Ital 5178\n",
      "Nathaniel 5179\n",
      "deli 5180\n",
      "Gemma 5181\n",
      "Skinner 5182\n",
      "lipstick 5183\n",
      "Rene 5184\n",
      "Syah 5185\n",
      "Rick 5186\n",
      "opposed 5187\n",
      "Jody 5188\n",
      "Javier 5189\n",
      "Norma 5190\n",
      "nor 5191\n",
      "pool 5192\n",
      "appalling 5193\n",
      "dried 5194\n",
      "Magda 5195\n",
      "Betty 5196\n",
      "matching 5197\n",
      "Glen 5198\n",
      "Lisbon 5199\n",
      "18 5200\n",
      "again 5201\n",
      "meeting 5202\n",
      "committee 5203\n",
      "cash 5204\n",
      "Lisa 5205\n",
      "Cauliflower 5206\n",
      "rent 5207\n",
      "traditional 5208\n",
      "client 5209\n",
      "snowing 5210\n",
      "Ola 5211\n",
      "Android 5212\n",
      "Markus 5213\n",
      "7 5214\n",
      "medical 5215\n",
      "beacuse 5216\n",
      "pierogi 5217\n",
      "Frequency 5218\n",
      "demanding 5219\n",
      "writer 5220\n",
      "politically 5221\n",
      "suggest 5222\n",
      "Baldwin 5223\n",
      "Bryan 5224\n",
      "sporting 5225\n",
      "rehab 5226\n",
      "mash 5227\n",
      "Bea 5228\n",
      "Williams 5229\n",
      "harsh 5230\n",
      "point 5231\n",
      "Marcel 5232\n",
      "Chicago 5233\n",
      "crowded 5234\n",
      "subway 5235\n",
      "Callie 5236\n",
      "apologized 5237\n",
      "mandarins 5238\n",
      "Makayla 5239\n",
      "Aquaman 5240\n",
      "Saturdays 5241\n",
      "Marzena 5242\n",
      "Derek 5243\n",
      "eating 5244\n",
      "wool 5245\n",
      "CEMS 5246\n",
      "track 5247\n",
      "problem 5248\n",
      "arrives 5249\n",
      "Bus 5250\n",
      "Bessie 5251\n",
      "carefully 5252\n",
      "period 5253\n",
      "headache 5254\n",
      "Willard 5255\n",
      "Maya 5256\n",
      "ignored 5257\n",
      "arrested 5258\n",
      "\n",
      " 5259\n",
      "Ian 5260\n",
      "Bee 5261\n",
      "Corsica 5262\n",
      "hers 5263\n",
      "lemon 5264\n",
      "radio 5265\n",
      "early 5266\n",
      "how 5267\n",
      "her 5268\n",
      "doubts 5269\n",
      "Yannick 5270\n",
      "Sontag 5271\n",
      "Current 5272\n",
      "Irene 5273\n",
      "gianduja 5274\n",
      "duck 5275\n",
      "stolen 5276\n",
      "Silvia 5277\n",
      "buys 5278\n",
      "chess 5279\n",
      "down 5280\n",
      "Grinch 5281\n",
      "Earnest 5282\n",
      "leak 5283\n",
      "amazing 5284\n",
      "grow 5285\n",
      "lover 5286\n",
      "grounded 5287\n",
      "Khabib 5288\n",
      "experienced 5289\n",
      "Katya 5290\n",
      "studio 5291\n",
      "rolling 5292\n",
      "Arnie 5293\n",
      "treated 5294\n",
      "Museum 5295\n",
      "Asus 5296\n",
      "cd 5297\n",
      "workplace 5298\n",
      "gas 5299\n",
      "locked 5300\n",
      "transmission 5301\n",
      "defence 5302\n",
      "Ike 5303\n",
      "jobs 5304\n",
      "Verde 5305\n",
      "Uber 5306\n",
      "Simpsons 5307\n",
      "Paula 5308\n",
      "clothes 5309\n",
      "proud 5310\n",
      "Bobby 5311\n",
      "grandmas 5312\n",
      "confirms 5313\n",
      "attending 5314\n",
      "during 5315\n",
      "English 5316\n",
      "parmezan 5317\n",
      "Rossman 5318\n",
      "Luca 5319\n",
      "Walter 5320\n",
      "habits 5321\n",
      "feels 5322\n",
      "the 5323\n",
      "double 5324\n",
      "cry 5325\n",
      "She 5326\n",
      "consulting 5327\n",
      "startups 5328\n",
      "guests 5329\n",
      "Philips 5330\n",
      "Katrina 5331\n",
      "open 5332\n",
      "alcohol 5333\n",
      "Seamus 5334\n",
      "enjoyed 5335\n",
      "ingredients 5336\n",
      "stays 5337\n",
      "document 5338\n",
      "Randy 5339\n",
      "too 5340\n",
      "hometown 5341\n",
      "denial 5342\n",
      "discovered 5343\n",
      "Professor 5344\n",
      "H 5345\n",
      "photos 5346\n",
      "than 5347\n",
      "Paisley 5348\n",
      "download 5349\n",
      "Elie 5350\n",
      "Ost 5351\n",
      "stains 5352\n",
      "important 5353\n",
      "con 5354\n",
      "JAckie 5355\n",
      "Debra 5356\n",
      "afford 5357\n",
      "Stoke 5358\n",
      "Al 5359\n",
      "grandmother 5360\n",
      "release 5361\n",
      "Treasure 5362\n",
      "lawn 5363\n",
      "suspects 5364\n",
      "or 5365\n",
      "which 5366\n",
      "Sunday 5367\n",
      "Entertainment 5368\n",
      "spending 5369\n",
      "inform 5370\n",
      "workmates 5371\n",
      "Angelina 5372\n",
      "Edlyna 5373\n",
      "modern 5374\n",
      "chemistry 5375\n",
      "over 5376\n",
      "login 5377\n",
      "intense 5378\n",
      "remember 5379\n",
      "kennels 5380\n",
      "Boris 5381\n",
      "chocolate 5382\n",
      "changed 5383\n",
      "Connor 5384\n",
      "Lindsey 5385\n",
      "outage 5386\n",
      "Clark 5387\n",
      "Chili 5388\n",
      "Danie 5389\n",
      "Thom 5390\n",
      "effective 5391\n",
      "bullet 5392\n",
      "SH 5393\n",
      "Park 5394\n",
      "tennis 5395\n",
      "interview 5396\n",
      "heating 5397\n",
      "February 5398\n",
      "met 5399\n",
      "child 5400\n",
      "Amanda 5401\n",
      "informs 5402\n",
      "Lynn 5403\n",
      "guitar 5404\n",
      "Tawny 5405\n",
      "permission 5406\n",
      "Roberts 5407\n",
      "dishes 5408\n",
      "Anton 5409\n",
      "horrible 5410\n",
      "SWAT 5411\n",
      "Saul 5412\n",
      "candles 5413\n",
      "Dominique 5414\n",
      "furious 5415\n",
      "Delgado 5416\n",
      "At 5417\n",
      "Hedwig 5418\n",
      "muscles 5419\n",
      "Eminem 5420\n",
      "Raiola 5421\n",
      "resume 5422\n",
      "Lin 5423\n",
      "cheat 5424\n",
      "Izayah 5425\n",
      "la 5426\n",
      "quit 5427\n",
      "Max 5428\n",
      "camera 5429\n",
      "Evan 5430\n",
      "Sulawesi 5431\n",
      "Formula 5432\n",
      "Eddie 5433\n",
      "switch 5434\n",
      "Blackett 5435\n",
      "moment 5436\n",
      "decorations 5437\n",
      "business 5438\n",
      "font 5439\n",
      "latest 5440\n",
      "scary 5441\n",
      "Roxanne 5442\n",
      "buy 5443\n",
      "Kimi 5444\n",
      "Ken 5445\n",
      "9pm 5446\n",
      "cleaned 5447\n",
      "classico 5448\n",
      "liked 5449\n",
      "classy 5450\n",
      "Salty 5451\n",
      "pictures 5452\n",
      "worried 5453\n",
      "Spice 5454\n",
      "497 5455\n",
      "why 5456\n",
      "shooting 5457\n",
      "Eurovision 5458\n",
      "Rella 5459\n",
      "Keiran 5460\n",
      "charge 5461\n",
      "Rome 5462\n",
      "Vinnie 5463\n",
      "Saoirse 5464\n",
      "sending 5465\n",
      "zodiac 5466\n",
      "Pola 5467\n",
      "at 5468\n",
      "strangely 5469\n",
      "price 5470\n",
      "Hall 5471\n",
      "record 5472\n",
      "shortly 5473\n",
      "resulted 5474\n",
      "Granda 5475\n",
      "Juventus 5476\n",
      "aunt 5477\n",
      "position 5478\n",
      "organized 5479\n",
      "Margo 5480\n",
      "22 5481\n",
      "planning 5482\n",
      "spicy 5483\n",
      "kitchen 5484\n",
      "Debora 5485\n",
      "nominated 5486\n",
      "coming 5487\n",
      "brutal 5488\n",
      "wedidng 5489\n",
      "automobiles 5490\n",
      "BSB 5491\n",
      "Netflix 5492\n",
      "space 5493\n",
      "breath 5494\n",
      "Asha 5495\n",
      "item 5496\n",
      "blog 5497\n",
      "Weapon 5498\n",
      "Portia 5499\n",
      "PlayStation 5500\n",
      "George 5501\n",
      "Ceaser 5502\n",
      "reintroducing 5503\n",
      "boss 5504\n",
      "MicroSip 5505\n",
      "Klaudia 5506\n",
      "instead 5507\n",
      "Tolstoy 5508\n",
      "SW 5509\n",
      "Nail 5510\n",
      "calculus 5511\n",
      "fans 5512\n",
      "building 5513\n",
      "Bridger 5514\n",
      " 5515\n",
      "amazed 5516\n",
      "Kimberly 5517\n",
      "Margaret 5518\n",
      "Kim 5519\n",
      "midnight 5520\n",
      "buildings 5521\n",
      "only 5522\n",
      "Thursday 5523\n",
      "Edward 5524\n",
      "inside 5525\n",
      "Feyi 5526\n",
      "Puerto 5527\n",
      "couldn 5528\n",
      "profile 5529\n",
      "beans 5530\n",
      "waiting 5531\n",
      "Shaun 5532\n",
      "local 5533\n",
      "Royale 5534\n",
      "Damir 5535\n",
      "says 5536\n",
      "Marcela 5537\n",
      "take 5538\n",
      "fox 5539\n",
      "excluded 5540\n",
      "fingers 5541\n",
      "mood 5542\n",
      "Maja 5543\n",
      "visible 5544\n",
      "25 5545\n",
      "fixed 5546\n",
      "Gethesmankirche 5547\n",
      "notices 5548\n",
      "applying 5549\n",
      "Brett 5550\n",
      "behaviourist 5551\n",
      "Ricky 5552\n",
      "lady 5553\n",
      "Away 5554\n",
      "realms 5555\n",
      "Jorge 5556\n",
      "Mickey 5557\n",
      "Garrett 5558\n",
      "stall 5559\n",
      "site 5560\n",
      "bank 5561\n",
      "Barbara 5562\n",
      "t 5563\n",
      "Ron 5564\n",
      "smarter 5565\n",
      "done 5566\n",
      "prvate 5567\n",
      "Rosanne 5568\n",
      "G 5569\n",
      "app 5570\n",
      "guessing 5571\n",
      "disciplinary 5572\n",
      "Patricia 5573\n",
      "Andrew 5574\n",
      "Dungeons 5575\n",
      "Bangkok 5576\n",
      "deals 5577\n",
      "Elsa 5578\n",
      "victory 5579\n",
      "contacted 5580\n",
      "Denver 5581\n",
      "moving 5582\n",
      "winter 5583\n",
      "Ayden 5584\n",
      "Cece 5585\n",
      "Maroon 5586\n",
      "Jacky 5587\n",
      "Ula 5588\n",
      "perceive 5589\n",
      "cookbook 5590\n",
      "Kris 5591\n",
      "Kate 5592\n",
      "15th 5593\n",
      "earring 5594\n",
      "165 5595\n",
      "eggs 5596\n",
      "kisser 5597\n",
      "Manpreet 5598\n",
      "Death 5599\n",
      "IT 5600\n",
      "organisation 5601\n",
      "D 5602\n",
      "dating 5603\n",
      "lends 5604\n",
      "black 5605\n",
      "auction 5606\n",
      "Elly 5607\n",
      "Lucy 5608\n",
      "tommorrow 5609\n",
      "blood 5610\n",
      "enter 5611\n",
      "worksheet 5612\n",
      "belong 5613\n",
      "sometime 5614\n",
      "Brave 5615\n",
      "Alain 5616\n",
      "check 5617\n",
      "Gregory 5618\n",
      "Tereza 5619\n",
      "transport 5620\n",
      "Fuerteventura 5621\n",
      "refuses 5622\n",
      "stupid 5623\n",
      "90 5624\n",
      "Jenna 5625\n",
      "Georg 5626\n",
      "transferred 5627\n",
      "either 5628\n",
      "celebrating 5629\n",
      "Alie 5630\n",
      "Dead 5631\n",
      "injured 5632\n",
      "uploaded 5633\n",
      "ball 5634\n",
      "updating 5635\n",
      "3am 5636\n",
      "FB 5637\n",
      "Vistula 5638\n",
      "image 5639\n",
      "Outlaw 5640\n",
      "toy 5641\n",
      "Paris 5642\n",
      "Cash 5643\n",
      "Ann 5644\n",
      "European 5645\n",
      "Gone 5646\n",
      "outfit 5647\n",
      "heels 5648\n",
      "causing 5649\n",
      "failed 5650\n",
      "P 5651\n",
      "Marika 5652\n",
      "assignments 5653\n",
      "file 5654\n",
      "lecture 5655\n",
      "faver 5656\n",
      "juice 5657\n",
      "part 5658\n",
      "shopped 5659\n",
      "Grey 5660\n",
      "mattress 5661\n",
      "abortion 5662\n",
      "Flores 5663\n",
      "witch 5664\n",
      "Anna 5665\n",
      "mould 5666\n",
      "Lea 5667\n",
      "power 5668\n",
      "descent 5669\n",
      "choc 5670\n",
      "Lewis 5671\n",
      "Dee 5672\n",
      "dresses 5673\n",
      "Parker 5674\n",
      "saving 5675\n",
      "Esme 5676\n",
      "Jerry 5677\n",
      "spreading 5678\n",
      "room 5679\n",
      "sales 5680\n",
      "Denis 5681\n",
      "Britney 5682\n",
      "Kian 5683\n",
      "Josiah 5684\n",
      "own 5685\n",
      "milk 5686\n",
      "ZA 5687\n",
      "less 5688\n",
      "AM 5689\n",
      "Nuclear 5690\n",
      "spath 5691\n",
      "Cassidy 5692\n",
      "Czech 5693\n",
      "radiators 5694\n",
      "gadgets 5695\n",
      "selecting 5696\n",
      "telescope 5697\n",
      "referral 5698\n",
      "misses 5699\n",
      "Eli 5700\n",
      "Scots 5701\n",
      "oven 5702\n",
      "Portugal 5703\n",
      "patrols 5704\n",
      "Danny 5705\n",
      "confessed 5706\n",
      "repaired 5707\n",
      "son 5708\n",
      "stayed 5709\n",
      "Tress 5710\n",
      "pizzeria 5711\n",
      "Tokken 5712\n",
      "clients 5713\n",
      "potatoes 5714\n",
      "Rodger 5715\n",
      "suspended 5716\n",
      "quarter 5717\n",
      "famous 5718\n",
      "Kurt 5719\n",
      "cheese 5720\n",
      "monsters 5721\n",
      "cover 5722\n",
      "queueing 5723\n",
      "But 5724\n",
      "Alicia 5725\n",
      "defrost 5726\n",
      "hat 5727\n",
      "Mckayla 5728\n",
      "wrapped 5729\n",
      "Summer 5730\n",
      "Marc 5731\n",
      "warned 5732\n",
      "jam 5733\n",
      "laxatives 5734\n",
      "Mathews 5735\n",
      "violence 5736\n",
      "01 5737\n",
      "Macarena 5738\n",
      "Nana 5739\n",
      "flight 5740\n",
      "four 5741\n",
      "Lester 5742\n",
      "crowds 5743\n",
      "O2 5744\n",
      "temporarily 5745\n",
      "Otto 5746\n",
      "surprising 5747\n",
      "can 5748\n",
      "rubbish 5749\n",
      "fruit 5750\n",
      "fall 5751\n",
      "Pret 5752\n",
      "Anette 5753\n",
      "himself 5754\n",
      "Disneyland 5755\n",
      "figured 5756\n",
      "breakfast 5757\n",
      "outdoors 5758\n",
      "umbrella 5759\n",
      "awfully 5760\n",
      "was 5761\n",
      "Anastasia 5762\n",
      "extension 5763\n",
      "Audley 5764\n",
      "sounds 5765\n",
      "car 5766\n",
      "decide 5767\n",
      "cellar 5768\n",
      "distraction 5769\n",
      "enroll 5770\n",
      "disagree 5771\n",
      "ProtonMail 5772\n",
      "Alisha 5773\n",
      "Doreen 5774\n",
      "Maria 5775\n",
      "spent 5776\n",
      "tight 5777\n",
      "Jelmer 5778\n",
      "terrible 5779\n",
      "Harper 5780\n",
      "jokes 5781\n",
      "finding 5782\n",
      "laptops 5783\n",
      "Beatles 5784\n",
      "to 5785\n",
      "reaction 5786\n",
      "sandwich 5787\n",
      "keeping 5788\n",
      "divorce 5789\n",
      "attractive 5790\n",
      "issue 5791\n",
      "minimalist 5792\n",
      "Practitioner 5793\n",
      "Harmonie 5794\n",
      "pink 5795\n",
      "battery 5796\n",
      "rows 5797\n",
      "ask 5798\n",
      "slipped 5799\n",
      "problems 5800\n",
      "Moms 5801\n",
      "model 5802\n",
      "respond 5803\n",
      "Dumbo 5804\n",
      "hang 5805\n",
      "layout 5806\n",
      "coctails 5807\n",
      "orange 5808\n",
      "emigrate 5809\n",
      "Keira 5810\n",
      "Brain 5811\n",
      "expedient 5812\n",
      "holidays 5813\n",
      "beach 5814\n",
      "Marsha 5815\n",
      "24 5816\n",
      "Robbie 5817\n",
      "204 5818\n",
      "better 5819\n",
      "Molly 5820\n",
      "Kristie 5821\n",
      "whenever 5822\n",
      "sauce 5823\n",
      "kawaii 5824\n",
      "revieved 5825\n",
      "Janek 5826\n",
      "organizes 5827\n",
      "earned 5828\n",
      "Roman 5829\n",
      "past 5830\n",
      "disco 5831\n",
      "Cape 5832\n",
      "remove 5833\n",
      "treatment 5834\n",
      "incidents 5835\n",
      "pair 5836\n",
      "news 5837\n",
      "Gustav 5838\n"
     ]
    }
   ],
   "source": [
    "for i,(j,k) in vocabulary.items():\n",
    "    print(i,k)\n",
    "    if k==2347:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b644a6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\dials\\Desktop\\LLMs from scratch\\spacy_env\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.0 mean accuracy nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/550:   0%|          | 0/36 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "multihead_attention.forward_attention() missing 1 required positional argument: 'mask_padding'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 29\u001b[0m\n\u001b[0;32m     23\u001b[0m target_decoder\u001b[38;5;241m=\u001b[39mhelper\u001b[38;5;241m.\u001b[39mcreate_target(y_batch,vocabulary,words_per_phrase)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#target_decoder=helper.generate_target_sparse_categorical(y_batch,vocabulary,words_per_phrase)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#print(\"target_decoder\",target_decoder.shape)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#print(\"inputs_decoder\",inputs_decoder.shape)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#target_decoder=inputs_decoder\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m Dout\u001b[38;5;241m=\u001b[39m\u001b[43mMyTransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43minputs_decoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m counter_beccate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     31\u001b[0m counter_tot\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[14], line 159\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, inputs_e, inputs_decoder, X_batch, y_batch)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_batch\u001b[38;5;241m=\u001b[39mX_batch\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_batch\u001b[38;5;241m=\u001b[39my_batch\n\u001b[1;32m--> 159\u001b[0m Ecout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_e\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m Dout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_decoder(Ecout,inputs_decoder)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Dout\n",
      "Cell \u001b[1;32mIn[14], line 170\u001b[0m, in \u001b[0;36mTransformer.forward_encoder\u001b[1;34m(self, inputs_e)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_encoder\u001b[39m(\u001b[38;5;28mself\u001b[39m,inputs_e):\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoder_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEncoderStack:\n\u001b[1;32m--> 170\u001b[0m         inputs_e\u001b[38;5;241m=\u001b[39m\u001b[43mencoder_i\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_e\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inputs_e\n",
      "Cell \u001b[1;32mIn[14], line 21\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, inputs_e)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,inputs_e):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs_e\u001b[38;5;241m=\u001b[39minputs_e \n\u001b[1;32m---> 21\u001b[0m     PrjAe\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultihead_attention_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43minputs_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43minputs_e\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     Ect1\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_layer_1\u001b[38;5;241m.\u001b[39mforward(PrjAe,inputs_e) \n\u001b[0;32m     25\u001b[0m     FLe2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfully_connected_block\u001b[38;5;241m.\u001b[39mforward(Ect1) \n",
      "\u001b[1;31mTypeError\u001b[0m: multihead_attention.forward_attention() missing 1 required positional argument: 'mask_padding'"
     ]
    }
   ],
   "source": [
    " \n",
    "# learning_rate=0.00001\n",
    " \n",
    "# for epoch in range(550):\n",
    "#     print(\"Loss\",tot_loss_epoch/num_batches_per_epoch,\"mean accuracy\",np.mean(np.array(accuracies)))#np.mean(np.array(accuracies))\n",
    "#     tot_loss_epoch=0\n",
    "#     total_accuracy_epoch=0\n",
    "#     mean_acc=0\n",
    "#     accuracies=[]\n",
    "#     for i in tqdm(range(0,num_batches_per_epoch),desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "#         #try: \n",
    "#         start = i * batch_size\n",
    "#         end = start + batch_size \n",
    "#         X_batch = X_train[start:end]\n",
    "#         y_batch = y_train[start:end] \n",
    "        \n",
    "        \n",
    "#         #helper.print_matrix(y_batch)\n",
    "#         #print(\"X_batch\",X_batch)\n",
    "        \n",
    "        \n",
    "#         inputs_e=helper.create_input_encoder(X_batch,vocabulary,words_per_phrase,embedding_size) \n",
    "#         inputs_decoder=helper.create_decoder_input(y_batch,embedding_size,words_per_phrase,vocabulary) \n",
    "#         target_decoder=helper.create_target(y_batch,vocabulary,words_per_phrase)\n",
    "#         #target_decoder=helper.generate_target_sparse_categorical(y_batch,vocabulary,words_per_phrase)\n",
    "#         #print(\"target_decoder\",target_decoder.shape)\n",
    "#         #print(\"inputs_decoder\",inputs_decoder.shape)\n",
    "#         #target_decoder=inputs_decoder\n",
    "\n",
    "#         Dout=MyTransformer.forward(inputs_e,inputs_decoder,X_batch,y_batch)\n",
    "#         counter_beccate=0\n",
    "#         counter_tot=0\n",
    "#         #Zout = output_linear_layer.forward(Dout)\n",
    "#         #SigmaZout = helper.softmax(Zout)\n",
    "#         SigmaZout = Output_stack.forward(Dout)\n",
    "        \n",
    "#         taccuracies=[]\n",
    "#         for n in range(SigmaZout.shape[0]): \n",
    "#             len_phrase=SigmaZout.shape[1]\n",
    "#             counter_beccate=0 \n",
    "#             for l in range(SigmaZout.shape[1]): \n",
    "#                 if np.argmax(SigmaZout[n][l])==np.argmax(target_decoder[n][l]): \n",
    "#                     counter_beccate+=1\n",
    "#                     #print(np.argmax(SigmaZout[n][l]))\n",
    "#             phrase_accuracy=counter_beccate/len_phrase\n",
    "#             taccuracies.append(phrase_accuracy)\n",
    "\n",
    "#         accuracies.append(np.mean(np.array(taccuracies)))\n",
    "#         # if np.mean(np.array(accuracies))>0:\n",
    "#         #         print(np.mean(np.array(accuracies)))\n",
    "#                 #print(np.argmax(SigmaZout[n][l]),np.argmax(target_decoder[n][l]))\n",
    "#         #print(target_decoder)\n",
    "\n",
    "#         # for n in range(SigmaZout.shape[0]):\n",
    "#         #     phrase_len=SigmaZout.shape[1]\n",
    "#         #     phrase_accuracy=0\n",
    "#         #     counter_beccate=0\n",
    "#         #     for l in range(SigmaZout.shape[1]):\n",
    "#         #         if np.argmax(SigmaZout[n][l])==target_decoder[n][l]:\n",
    "#         #             counter_beccate+=1\n",
    "#         #             #print(target_decoder[n][l],np.argmax(SigmaZout[n][l]))\n",
    "#         #     phrase_accuracy=counter_beccate/phrase_len\n",
    "#         #     if(phrase_accuracy>0):\n",
    "#         #         pass\n",
    "#                 #print(\"phrase_accuracy\",phrase_accuracy)\n",
    "                \n",
    "#         #  #print(\"Dout\",Dout.shape)\n",
    "#         #print(\"SigmaZout\",SigmaZout.shape)\n",
    "#         #mean_acc+=(phrase_accuracy/counter_tot)\n",
    "#         #Loss=Output_stack.cross_entropy_loss(SigmaZout,target_decoder)\n",
    "#         #Loss=Output_stack.sparse_categorical_crossentropy(SigmaZout,target_decoder)\n",
    "#         #print(\"Loss\",Loss)\n",
    "\n",
    "\n",
    "\n",
    "#         Loss = Output_stack.cross_entropy_loss(SigmaZout,target_decoder)\n",
    "#         #dLoss_dZout = SigmaZout - target_decoder \n",
    "        \n",
    "#         #dLoss_dDout= output_linear_layer.grad(dLoss_dZout)\n",
    "#         #dL_dDout = dLoss_dZout @ output_linear_layer.W.T\n",
    "#         tot_loss_epoch+=Loss\n",
    "\n",
    "\n",
    "\n",
    "#         #dL_dDout = Output_stack.grad_sparse_cross_entropy(SigmaZout,target_decoder)\n",
    "#         dL_dDout = Output_stack.grad_cross_entropy(SigmaZout,target_decoder)\n",
    "        \n",
    "         \n",
    "\n",
    "\n",
    "#         dL_Ecout,dLoss_dWemb_encoder_tot,dLoss_dWemb_decoder_tot=MyTransformer.backpropagation(dL_dDout)\n",
    "#         #inputs_decoder=inputs_decoder-learning_rate*dLoss_dWemb_decoder\n",
    "#         #vocabulary=helper.update_wembedding_decoder(y_batch,inputs_decoder,words_per_phrase,vocabulary) \n",
    "#         #inputs_e=inputs_e-learning_rate*dLoss_dWemb_encoder\n",
    "#         #vocabulary=helper.update_wembedding_encoder(X_batch,inputs_e,vocabulary,words_per_phrase)\n",
    "#         #print(\"dL_Ecout\",dL_Ecout.shape)\n",
    "#         Output_stack.update_weights(learning_rate)\n",
    "#         vocabulary=helper.update_wembedding_decoder(learning_rate,y_batch, dLoss_dWemb_decoder_tot,vocabulary, max_v  )\n",
    "#         vocabulary=helper.update_wembedding_encoder(learning_rate,X_batch, dLoss_dWemb_encoder_tot,vocabulary,max_v)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024879bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/550:   0%|          | 0/36 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "multihead_attention.forward_attention() missing 1 required positional argument: 'mask_padding'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m inputs_e\u001b[38;5;241m=\u001b[39mhelper\u001b[38;5;241m.\u001b[39mcreate_input_encoder(X_batch,vocabulary,words_per_phrase,embedding_size) \n\u001b[0;32m     28\u001b[0m inputs_decoder\u001b[38;5;241m=\u001b[39mhelper\u001b[38;5;241m.\u001b[39mcreate_decoder_input(y_batch,embedding_size,words_per_phrase,vocabulary) \n\u001b[1;32m---> 29\u001b[0m Ecout\u001b[38;5;241m=\u001b[39m\u001b[43mTransformerEncoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_e\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#print(\"inputs_d.shape\",inputs_d.shape,\"inputs_e.shape\",inputs_e.shape) \u001b[39;00m\n\u001b[0;32m     33\u001b[0m target_d\u001b[38;5;241m=\u001b[39mhelper\u001b[38;5;241m.\u001b[39mcreate_target(y_batch,vocabulary,words_per_phrase,embedding_size)\n",
      "Cell \u001b[1;32mIn[14], line 21\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, inputs_e)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,inputs_e):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs_e\u001b[38;5;241m=\u001b[39minputs_e \n\u001b[1;32m---> 21\u001b[0m     PrjAe\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultihead_attention_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43minputs_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43minputs_e\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     Ect1\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_layer_1\u001b[38;5;241m.\u001b[39mforward(PrjAe,inputs_e) \n\u001b[0;32m     25\u001b[0m     FLe2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfully_connected_block\u001b[38;5;241m.\u001b[39mforward(Ect1) \n",
      "\u001b[1;31mTypeError\u001b[0m: multihead_attention.forward_attention() missing 1 required positional argument: 'mask_padding'"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "inputs_e=cp.random.rand(4,11,300)\n",
    "inputs_d=cp.random.rand(11,4,11,300)\n",
    "target_i=cp.array([1, 2,3,4])\n",
    "vocabulary_size=len(vocabulary)\n",
    "TransformerEncoder=Encoder(embedding_size,num_heads,fl1_size,learning_rate,batch_size=batch_size,words_per_phrase=max_v,clipping_threshold=0)\n",
    "TransformerDecoder=Decoder(embedding_size,num_heads,fl1_size,learning_rate,batch_size=batch_size,words_per_phrase=max_v,clipping_threshold=0)\n",
    "words_per_phrase = num_phrases= max_v\n",
    "output_linear_layer=linear_layer(embedding_size,len(vocabulary),out=True)  \n",
    "num_batches_per_epoch = len(X_train) // batch_size\n",
    "num_epochs=550\n",
    "tot_loss_epoch=0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Loss\",tot_loss_epoch/num_batches_per_epoch)\n",
    "    tot_loss_epoch=0\n",
    "    total_accuracy_epoch=0\n",
    "    \n",
    "    for i in tqdm(range(0,num_batches_per_epoch),desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        #try: \n",
    "        start = i * batch_size\n",
    "        end = start + batch_size \n",
    "        X_batch = X_train[start:end]\n",
    "        y_batch = y_train[start:end] \n",
    "        #print(\"y_batch\",y_batch)\n",
    "        #print(X_batch)\n",
    "        inputs_e=helper.create_input_encoder(X_batch,vocabulary,words_per_phrase,embedding_size) \n",
    "        inputs_decoder=helper.create_decoder_input(y_batch,embedding_size,words_per_phrase,vocabulary) \n",
    "        Ecout=TransformerEncoder.forward(inputs_e,)\n",
    "        \n",
    "        \n",
    "        #print(\"inputs_d.shape\",inputs_d.shape,\"inputs_e.shape\",inputs_e.shape) \n",
    "        target_d=helper.create_target(y_batch,vocabulary,words_per_phrase,embedding_size)\n",
    "       \n",
    "\n",
    "        Dout = TransformerDecoder.forward(inputs_decoder, Ecout,mask_e=None,mask_d=None)\n",
    "        target_d=helper.pad_sequences(y_batch,lenght=words_per_phrase,target_type=\"target\") \n",
    "        target_d=[re.findall(r'\\[.*?\\]|\\w+|[^\\w\\s]|\\n', xx) for xx in target_d] \n",
    "        tot_loss=0\n",
    "        totdLoss_dAcr=0\n",
    "        counter_correct=0\n",
    "        total_accuracy_batch=0\n",
    "        for step in range(0, inputs_d.shape[0]):\n",
    "            inputs_decoder = inputs_d[step]\n",
    "            target_i = cp.array([helper.get_one_hot(x[step], vocabulary) for x in target_d])\n",
    "            #target_i=cp.array([vocabulary[x[step]][1] for x in target_d])\n",
    "            Dout = TransformerDecoder.forward(inputs_decoder, Ecout,mask_d=None,mask_e=None)\n",
    "            Zout = output_linear_layer.forward(Dout)\n",
    "            SigmaZout = helper.softmax(Zout)\n",
    "            print(SigmaZout.shape)\n",
    "            # Calculate loss\n",
    "            Loss = helper.cross_entropy_loss(SigmaZout, target_i)\n",
    "            dLoss_dZout = SigmaZout - target_i \n",
    "            # Loss = helper.sparse_categorical_crossentropy(SigmaZout, target_i)  \n",
    "            # dLoss_dZout = SigmaZout.copy()  # Create a copy of the softmax output \n",
    "            # dLoss_dZout[np.arange(target_i.shape[0]), target_i] -= 1 \n",
    "\n",
    "            #helper.log_sparse_entropy(SigmaZout, target_i,y_batch,step)\n",
    "            print(\"Loss\",Loss)\n",
    "            print(\"dLoss_dZout\",dLoss_dZout.shape)\n",
    "            #Backpropagation\n",
    "            \n",
    "            clip_value = 1.0\n",
    "            dLoss_dZout = cp.clip(dLoss_dZout, -clip_value, clip_value)\n",
    "            dLoss_dDout= output_linear_layer.grad(dLoss_dZout)\n",
    "            dLoss_dDout = dLoss_dZout @ output_linear_layer.W.T\n",
    "            \n",
    "            #Backpropagation through decoder\n",
    "            dLoss_dAcr = TransformerDecoder.backpropagation(dLoss_dDout)\n",
    "            tot_loss+=Loss \n",
    "            batch_acc=helper.accruacy_sparse_entropy(SigmaZout,target_i)\n",
    "            total_accuracy_batch+=batch_acc\n",
    "            if batch_acc>0:\n",
    "                helper.print_target_vs_prediction_sparce_loss(SigmaZout,target_i)\n",
    "        print(\"total_accuracy_batch\",total_accuracy_batch)\n",
    "        \n",
    "        dLoss_dAcr = cp.clip(dLoss_dAcr, -clip_value, clip_value)\n",
    "        dLoss_Ecout=cp.clip(dLoss_dAcr, -clip_value, clip_value)\n",
    "        #After decoder, calculate gradients for encoder\n",
    "        dLoss_Ecout_k = TransformerDecoder.multihead_cross_attention.diffKInput(dLoss_dAcr, TransformerDecoder.Kc.W)\n",
    "        dLoss_Ecout_v = TransformerDecoder.multihead_cross_attention.diffVInput(dLoss_dAcr, TransformerDecoder.Vc.W)\n",
    "        dLoss_Ecout = dLoss_Ecout_k + dLoss_Ecout_v\n",
    "        \n",
    "            # Backpropagation through encoder\n",
    "        vocabulary = TransformerEncoder.backpropagation(dLoss_Ecout, vocabulary, X_batch)\n",
    "        \n",
    "        tot_loss_epoch+=tot_loss/inputs_d.shape[0] \n",
    "        total_accuracy_epoch+=total_accuracy_batch\n",
    " \n",
    "        # except Exception as e:\n",
    "        #     print(e)\n",
    "        #     traceback.print_exc()  \n",
    "    print(\"total_accuracy_epoch\",total_accuracy_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb80648f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
