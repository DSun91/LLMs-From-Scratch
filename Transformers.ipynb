{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "430cf7fc-f02d-414f-b79a-28fe47b7fbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  29457\n",
      "Function 'create_vocabulary' executed in 3.2156 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 2/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'generate_input' executed in 0.0000 seconds\n",
      "['i love soy sauce', 'my dog is cute']\n",
      "[[[0.46195328 0.13579082 0.6548934  ... 0.16695213 0.07322681 0.21672976]\n",
      "  [0.6734519  0.4317391  0.8660369  ... 0.21976137 0.05613589 0.0024066 ]\n",
      "  [0.6274011  0.67449486 0.631884   ... 0.4727906  0.3639574  0.31044948]\n",
      "  [0.42594814 0.50421154 0.39506936 ... 0.5835451  0.7005005  0.9929346 ]]\n",
      "\n",
      " [[0.57852244 0.3697554  0.23043132 ... 0.9848201  0.38312972 0.8222574 ]\n",
      "  [0.30123627 0.5854081  0.2083242  ... 0.79928184 0.8889991  0.37048984]\n",
      "  [0.5242338  0.14272118 0.10307431 ... 0.4563253  0.7924125  0.87941873]\n",
      "  [0.96102893 0.40895152 0.5783787  ... 0.05882668 0.53607345 0.41859365]]]\n",
      "Function 'generate_input' executed in 0.0000 seconds\n",
      "['you are crazy strong', 'the friend is good']\n",
      "[[[0.68645215 0.6083869  0.36374485 ... 0.9921442  0.42366004 0.5363716 ]\n",
      "  [0.8448074  0.74833643 0.297593   ... 0.7307147  0.47371364 0.82315874]\n",
      "  [0.8186954  0.10007882 0.42771804 ... 0.6315285  0.27904475 0.8962424 ]\n",
      "  [0.7429799  0.80841935 0.811288   ... 0.88177454 0.18374455 0.9042758 ]]\n",
      "\n",
      " [[0.66104245 0.46179426 0.5644541  ... 0.8342985  0.18415058 0.09656143]\n",
      "  [0.1376698  0.34866285 0.26823556 ... 0.6369879  0.5731766  0.03096592]\n",
      "  [0.5242338  0.14272118 0.10307431 ... 0.4563253  0.7924125  0.87941873]\n",
      "  [0.06039107 0.6136633  0.23277593 ... 0.91798484 0.96175253 0.8511845 ]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    " \n",
    "import jax.numpy as jnp \n",
    "import re\n",
    "import cupy as cp\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np \n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jax\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "def log_time(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()  # Record start time\n",
    "        result = func(*args, **kwargs)  # Execute the wrapped function\n",
    "        end_time = time.time()  # Record end time\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Function '{func.__name__}' executed in {elapsed_time:.4f} seconds\")\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "class Transformer:\n",
    "\n",
    "    def __init__(self,embedding_size,complete_text,epochs,batch_size,flush_vocab=True):\n",
    "        self.embedding_size=embedding_size\n",
    "        self.defaultkey=jax.random.key(55)\n",
    "        self.epochs=epochs\n",
    "        self.batch_size= batch_size\n",
    "        self.flush_vocab=flush_vocab\n",
    "        self.vocabulary=self.create_vocabulary(complete_text)\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def cross_entropy_loss(self, predictions, target):\n",
    "        # Cross-entropy loss for a batch of predictions and targets\n",
    "        batch_loss = -jnp.sum(target * jnp.log(predictions + 1e-9), axis=1)\n",
    "        return jnp.mean(batch_loss)\n",
    "\n",
    "    def softmax(self, x, axis=-1):\n",
    "        x = jnp.clip(x, -1e4, 1e4)  # Clip for numerical stability\n",
    "        e_x = jnp.exp(x - jnp.max(x, axis=axis, keepdims=True))\n",
    "        return e_x / jnp.sum(e_x, axis=axis, keepdims=True)\n",
    "    \n",
    "    @log_time\n",
    "    def create_vocabulary(self,complete_text):\n",
    "\n",
    "        existing_vocab = Path(\"data/vocabulary.pkl\")\n",
    "        if existing_vocab.is_file() and self.flush_vocab==False:\n",
    "            with open('data/vocabulary.pkl', 'rb') as f:\n",
    "                vocabulary=pickle.load(f)\n",
    "\n",
    "        else: \n",
    "            text = re.sub(r'[^\\w\\s]', ' ', complete_text).split()\n",
    "            words_list = list(set(text))\n",
    "            vocabulary=dict()\n",
    "            for i in words_list:\n",
    "                vocabulary[i]=jax.random.uniform(jax.random.key(np.random.randint(10000)),self.embedding_size)\n",
    "            \n",
    "            print(\"Vocabulary size: \", len(vocabulary))\n",
    "            with open('data/vocabulary.pkl', 'wb') as handle:\n",
    "                pickle.dump(vocabulary, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "        return vocabulary\n",
    "    \n",
    "  \n",
    " \n",
    "    @log_time\n",
    "    def generate_input(self, batch_phrases):\n",
    "\n",
    "        print(batch_phrases)\n",
    "        phrase_vectors=[]   \n",
    "        xi=[]\n",
    "        \n",
    "        phrase_vectors=[x.split() for x in batch_phrases]\n",
    "        \n",
    "        xi=jnp.array([[self.vocabulary[word] for word in phrase_vector] for phrase_vector in phrase_vectors])\n",
    "        \n",
    "        yield xi\n",
    " \n",
    "\n",
    "    \n",
    "    \n",
    "    def train(self,X_train,yi):\n",
    "\n",
    "         for epoch in range(self.epochs):\n",
    "            self.iterations=0\n",
    "            total_loss = 0 \n",
    "            num_batches_per_epoch = len(X_train) // self.batch_size\n",
    "\n",
    "            for i in tqdm(range(num_batches_per_epoch), desc=f\"Epoch {epoch + 1}/{self.epochs}\"):\n",
    "                \n",
    "                start = i * self.batch_size\n",
    "                end = start + self.batch_size\n",
    "                X_batch_phrases = X_train[start:end]\n",
    "\n",
    "                for xi in self.generate_input(X_batch_phrases): \n",
    "                    print(xi) \n",
    "\n",
    "                \n",
    "                # start = i * self.batch_size\n",
    "                # end = start + self.batch_size\n",
    "                \n",
    "                # X_batch = X_train[start:end]\n",
    "                # y_batch = y_train[start:end] \n",
    "            # if self.validation_split > 0:\n",
    "            #     self.training_validation(X_train_validation, y_train_validation, Loss)\n",
    "            # print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {(total_loss / num_batches_per_epoch):.4f}, learning rate: {self.learning_rate}\")\n",
    "\n",
    "\n",
    "        \n",
    " \n",
    "df = pd.read_csv(\"data/bbc-text.csv\")\n",
    "# complete_text = ' '.join(df['text'].str.split()) \n",
    "complete_text = ' '.join(df[\"text\"].tolist())\n",
    "embedding_size = 512 \n",
    "\n",
    "model = Transformer(embedding_size,complete_text,epochs=1,batch_size=2,flush_vocab=True)\n",
    " \n",
    "X_train=[\"i love soy sauce\", \n",
    "         \"my dog is cute\", \n",
    "         \"you are crazy strong\",\n",
    "         \"the friend is good\"]\n",
    "yi=[]\n",
    "model.train(X_train,yi) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a2c903-f003-42ef-9d3a-a216b1a3185f",
   "metadata": {},
   "source": [
    "# Prepare input lookup table\n",
    "\n",
    "Step-by-Step Process:\n",
    "Create a Vocabulary:\n",
    "Map each word to an index (token) and initialize a random vector for each word.\n",
    "Initialize Embedding Vectors:\n",
    "For each word in the vocabulary, initialize a random embedding vector (say of dimension 3 or 512).\n",
    "For Each Input Sequence:\n",
    "Convert the words to their corresponding vectors using the vocabulary.\n",
    "Stack the vectors to form an input matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9e19b31-64ea-4dd2-bc04-9bf120a00f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.3952378 , 0.45534575, 0.91380334, 0.29122305, 0.82850766],\n",
       "       [0.58672273, 0.6880075 , 0.23995149, 0.81122804, 0.08536363],\n",
       "       [0.34470356, 0.04894471, 0.00256085, 0.6435065 , 0.50082767],\n",
       "       [0.22316742, 0.05539286, 0.23274505, 0.45073962, 0.51079834],\n",
       "       [0.01448357, 0.23985529, 0.0051235 , 0.70521474, 0.73882663]],      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.random   \n",
    "seed=55\n",
    "\n",
    "@log_time\n",
    "def create_vocabulary(complete_text):\n",
    "\n",
    "    existing_vocab = Path(\"data/vocabulary.pkl\")\n",
    "    if existing_vocab.is_file() and self.flush_vocab==False:\n",
    "        with open('data/vocabulary.pkl', 'rb') as f:\n",
    "            vocabulary=pickle.load(f)\n",
    "\n",
    "    else: \n",
    "        text = re.sub(r'[^\\w\\s]', ' ', complete_text).split()\n",
    "        words_list = list(set(text))\n",
    "        vocabulary=dict()\n",
    "        for i in words_list:\n",
    "            vocabulary[i]=jax.random.uniform(jax.random.key(np.random.randint(10000)),self.embedding_size)\n",
    "        \n",
    "        print(\"Vocabulary size: \", len(vocabulary))\n",
    "        with open('data/vocabulary.pkl', 'wb') as handle:\n",
    "            pickle.dump(vocabulary, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return vocabulary\n",
    "\n",
    "@log_time\n",
    "def generate_input(batch_phrases):\n",
    "\n",
    "    phrase_vectors=[]   \n",
    "    xi=[]\n",
    "    \n",
    "    phrase_vectors=[x.split() for x in batch_phrases]\n",
    "    \n",
    "    xi=jnp.array([[self.vocabulary[word] for word in phrase_vector] for phrase_vector in phrase_vectors])\n",
    "    \n",
    "    return xi\n",
    "\n",
    "\n",
    "embedding_size=512\n",
    "key=jax.random.key(seed)\n",
    "x = jax.random.uniform(key,(5,5))\n",
    "y = jnp.arange(5)\n",
    "x\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba893391-ff82-47a9-8a60-baf6c6b3fe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "x+=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3c96b25-a83f-4dcf-b4e3-6944a2fb33f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x.at[1:3].set([50,40,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c912210c-6de6-438b-bc56-3aff61dfac77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
