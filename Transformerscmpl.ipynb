{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9c14a08",
   "metadata": {},
   "source": [
    "# Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a2ca59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import jax.numpy as jnp\n",
    "def get_positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Returns a non-learnable (sinusoidal) positional encoding.\n",
    "    \n",
    "    seq_len: Length of the input sequence.\n",
    "    d_model: Dimension of the embeddings.\n",
    "    \"\"\"\n",
    "    pos = np.arange(seq_len)[:, np.newaxis]  # Shape: [seq_len, 1]\n",
    "    i = np.arange(d_model)[np.newaxis, :]    # Shape: [1, d_model]\n",
    "\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "\n",
    "    # Apply sine to even indices, cosine to odd indices\n",
    "    pos_encoding = np.zeros((seq_len, d_model))\n",
    "    pos_encoding[:, 0::2] = np.sin(pos * angle_rates[:, 0::2])  # sine on even indices\n",
    "    pos_encoding[:, 1::2] = np.cos(pos * angle_rates[:, 1::2])  # cosine on odd indices\n",
    "\n",
    "    return pos_encoding\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    # Subtract the max value for numerical stability\n",
    "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "def layer_norm(x, epsilon=1e-6):\n",
    "    # Calculate the mean and variance\n",
    "        mean = jnp.mean(x, axis=-1, keepdims=True)\n",
    "        var = jnp.var(x, axis=-1, keepdims=True) \n",
    "        \n",
    "        # Normalize the output\n",
    "        x_norm = (x - mean) / jnp.sqrt(var + epsilon) \n",
    "        #print(x)\n",
    "        #print(mean)\n",
    "        #print(\"mean\",mean.shape)\n",
    "        #print(\"x_norm.shape\",x_norm.shape)\n",
    "        return x_norm,mean,var,x.shape[-1]\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def pad_sequence(seq, max_len, pad_value=0):\n",
    "    \"\"\"Pad a sequence with a given value up to max_len.\"\"\"\n",
    "    current_len = seq.shape[0]\n",
    "    pad_width = max_len - current_len\n",
    "    if pad_width > 0:\n",
    "        # Pad sequence with zeros (or any pad_value you provide)\n",
    "        seq = jnp.pad(seq, ((0, pad_width), (0, 0)), mode='constant', constant_values=pad_value)\n",
    "    return seq\n",
    "\n",
    "def create_timestaped_input(input_d,words_per_phrase):\n",
    "    input_translation=[]\n",
    "    for j in range(input_d.shape[0]):\n",
    "    # Create padded sequences\n",
    "        padded_sequences = [pad_sequence(input_d[j][0:i], words_per_phrase) for i in range(1, input_d.shape[1] + 1)]\n",
    "        input_translation.append(padded_sequences)\n",
    "    return jnp.array(input_translation)\n",
    "\n",
    "def cross_entropy_loss(predictions, target):\n",
    "    # Cross-entropy loss for a batch of predictions and targets\n",
    "    batch_loss = -jnp.sum(target * jnp.log(predictions + 1e-9), axis=1)\n",
    "    return jnp.mean(batch_loss)\n",
    "\n",
    "def diff_norm(X,var,mu,N):\n",
    "    epsilon=1e-6\n",
    "    AA=((1-(1/N))*(1/(jnp.sqrt(var+epsilon))))\n",
    "    BB=(1/N)*((X-mu)**2)\n",
    "    CC=((var+epsilon)**(3/2))\n",
    "    result=(AA-(BB/CC)) \n",
    "    return result\n",
    "\n",
    "def redimension(X):\n",
    "    return jnp.concatenate(jnp.swapaxes(X,0,1),axis=-1) \n",
    "\n",
    "def diffQKV(dAttention,Attention_weights,X1,X2,X3,dk):\n",
    "    dAttention_weights=Attention_weights*(1-Attention_weights)\n",
    "    V1=redimension(dAttention_weights@X1/jnp.sqrt(dk)) \n",
    "    \n",
    "    V2=redimension(X2)\n",
    "    \n",
    "    V3=V1*V2*X3\n",
    "    dLoss_dX=jnp.sum(jnp.transpose(dAttention,(0,2,1))@V3,axis=0)\n",
    "    return dLoss_dX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563e49ab",
   "metadata": {},
   "source": [
    "# Summary Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "07246b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape:  (5, 15, 4)\n",
      "Qval.shape:  (5, 2, 15, 2)\n",
      "Kval.shape:  (5, 2, 15, 2)\n",
      "Vval.shape:  (5, 2, 15, 2)\n",
      "Attention_weights shape: (5, 2, 15, 15)\n",
      "Attention shape: (5, 2, 15, 2)\n",
      "Attention shape concat: (5, 15, 4)\n",
      "Ect1.shape (5, 15, 4) 4\n",
      "Xe1.shape (5, 15, 100)\n",
      "FLe1.shape (5, 15, 100)\n",
      "FLe2.shape (5, 15, 4)\n",
      "Ecout.shape (5, 15, 4) 4\n",
      "K_C.shape:  (5, 2, 15, 2)\n",
      "V_C.shape:  (5, 2, 15, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    " \n",
    "vocab_size=7 \n",
    "num_phrases = 5\n",
    "words_per_phrase = 15 \n",
    "dk = dv = embedding_size = 4 # constrain of transformer all embedding size both input embedding and attention embedding are the same encoder\n",
    "num_heads=2\n",
    " \n",
    "pos_encoding=get_positional_encoding(words_per_phrase,embedding_size)\n",
    " \n",
    "inputs_e = np.random.rand(num_phrases,words_per_phrase, embedding_size)\n",
    "inputs_e=pos_encoding+inputs_e\n",
    "print(\"inputs.shape: \",inputs_e.shape)\n",
    "\n",
    "Qe = np.random.rand(embedding_size, dk) / jnp.sqrt(embedding_size)\n",
    "Ke = np.random.rand(embedding_size, dk) / jnp.sqrt(embedding_size)\n",
    "Ve = np.random.rand(embedding_size, dv) / jnp.sqrt(embedding_size)\n",
    "\n",
    "Q_E= jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs_e, Qe),num_heads,axis=2)), 0, 1)\n",
    "print(\"Qval.shape: \",Q_E.shape)\n",
    "\n",
    "K_E = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs_e, Ke),num_heads,axis=2)), 0, 1)\n",
    "print(\"Kval.shape: \",K_E.shape)\n",
    "\n",
    "\n",
    "V_E = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs_e,Ve),num_heads,axis=2)), 0, 1)\n",
    "print(\"Vval.shape: \",V_E.shape)\n",
    "\n",
    "\n",
    "QKscaled = jnp.matmul(Q_E, jnp.transpose(K_E, (0, 1, 3, 2))) / jnp.sqrt(dk)\n",
    "\n",
    "Attention_weights_e = softmax(QKscaled)\n",
    "print(\"Attention_weights shape:\",Attention_weights_e.shape)\n",
    "\n",
    "\n",
    "Ae = jnp.matmul(Attention_weights_e, V_E)\n",
    "print(\"Attention shape:\",Ae.shape)\n",
    "\n",
    "\n",
    "Ae=jnp.array([jnp.concatenate(Ae[i], axis=1) for i in range(num_phrases)])\n",
    "print(\"Attention shape concat:\",Ae.shape)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "Xe=Ae+inputs_e\n",
    "Ect1,mu_e,var_e,Ne=layer_norm(Xe)\n",
    "print(\"Ect1.shape\",Ect1.shape,Ne)\n",
    "\n",
    "fl1_size=100\n",
    "Wfl1e=np.random.rand(num_phrases,dv, fl1_size)   \n",
    "bfl1e=np.random.rand(num_phrases,1,fl1_size)\n",
    "Xe1=jnp.matmul(Ect1,Wfl1e)+bfl1e\n",
    "print(\"Xe1.shape\",Xe1.shape)\n",
    "\n",
    "FLe1=relu(Xe1)\n",
    "print(\"FLe1.shape\",FLe1.shape)\n",
    "\n",
    "\n",
    "fl2_size=50\n",
    "Wfl2e=np.random.rand(num_phrases,FLe1.shape[2], dv)   \n",
    "bfl2e=np.random.rand(num_phrases,1,dv)\n",
    "FLe2=jnp.matmul(FLe1,Wfl2e)+bfl2e\n",
    "print(\"FLe2.shape\",FLe2.shape)\n",
    "\n",
    "Xe2=FLe2+Ect1\n",
    "Ecout,mu_e2,var_e2,N_e2=layer_norm(Xe2)\n",
    "print(\"Ecout.shape\",Ecout.shape,N_e2)\n",
    "\n",
    "Kc = np.random.rand(Ecout.shape[-1], dk) / jnp.sqrt(Ecout.shape[-1])\n",
    "Vc = np.random.rand(Ecout.shape[-1], dv) / jnp.sqrt(Ecout.shape[-1])\n",
    "\n",
    "\n",
    "\n",
    "K_C  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(Ecout, Kc),num_heads,axis=2)), 0, 1)\n",
    "print(\"K_C.shape: \",K_C.shape)# shape is: num_phrases, numbheads, words_per_phrase, dv/num_heads\n",
    "\n",
    "\n",
    "V_C  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(Ecout,Vc),num_heads,axis=2)), 0, 1)\n",
    "print(\"V_C.shape: \",V_C.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e801290b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_d complete shape (15, 5, 15, 4)\n",
      "Loss: 9.036615\n",
      "Loss: 10.489328\n",
      "Loss: 13.923627\n",
      "Loss: 11.954871\n",
      "Loss: 7.488725\n",
      "Loss: 13.54043\n",
      "Loss: 9.03979\n",
      "Loss: 14.729125\n",
      "Loss: 8.649913\n",
      "Loss: 8.228803\n",
      "Loss: 11.801648\n",
      "Loss: 17.237494\n",
      "Loss: 18.08123\n",
      "Loss: 13.9557085\n",
      "Loss: 11.357043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dials\\AppData\\Local\\Temp\\ipykernel_95140\\3221661961.py:305: RuntimeWarning: invalid value encountered in matmul\n",
      "  dLoss_dVe=f=np.sum(np.sum(np.transpose(np.expand_dims(dLoss_Ae, axis=1),(0,1,3,2))@(Attention_weights_e@np.expand_dims(inputs_e, axis=1)),axis=1),axis=0)\n"
     ]
    }
   ],
   "source": [
    "pos_encoding=get_positional_encoding(words_per_phrase,dv)\n",
    " \n",
    "input_d = np.random.rand(num_phrases,words_per_phrase, embedding_size)\n",
    "inputs_d=input_d+pos_encoding\n",
    " \n",
    "# Convert to an array for batching\n",
    "inputs_d = jnp.swapaxes(create_timestaped_input(input_d,words_per_phrase),0,1)\n",
    "target_d=jnp.swapaxes(create_timestaped_input(input_d,words_per_phrase),0,1)\n",
    "print(\"inputs_d complete shape\",inputs_d.shape)# shape is: words_per_phrase,num_phrases,words_per_phrase,embedding_size at each\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "Qd = np.random.rand(embedding_size, dk) / jnp.sqrt(embedding_size)\n",
    "Kd = np.random.rand(embedding_size, dk) / jnp.sqrt(embedding_size)\n",
    "Vd = np.random.rand(embedding_size, dv) / jnp.sqrt(embedding_size)\n",
    "\n",
    "\n",
    "learning_rate=0.01\n",
    "step=0\n",
    "\n",
    "for step in range(inputs_d.shape[0]):\n",
    "    #Q_D  = jnp.swapaxes(jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs_d[step], Qd),num_heads,axis=3)), 0, 1),1,2)\n",
    "    Q_D  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs_d[step], Qd),num_heads,axis=2)), 0, 1)\n",
    "    #print(\"Qval.shape: \",Q_D.shape)# numwords, num_phrases, numheads, num_words, dv/num_heads\n",
    "\n",
    "    #K_D  = jnp.swapaxes(jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs_d[step], Kd),num_heads,axis=3)), 0, 1),1,2)\n",
    "    K_D  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs_d[step], Kd),num_heads,axis=2)), 0, 1)\n",
    "    #print(\"Kval.shape: \",K_D.shape)\n",
    "\n",
    "\n",
    "    #V_D  = jnp.swapaxes(jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs_d[step], Vd),num_heads,axis=3)), 0, 1),1,2)\n",
    "    V_D  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(inputs_d[step], Vd),num_heads,axis=2)), 0, 1)\n",
    "    #print(\"Vval.shape: \",V_D.shape)\n",
    "\n",
    "\n",
    "    #QKscaled_decoder  = jnp.matmul(Q_D, jnp.transpose(K_D, (0, 1, 2, 4,3))) / jnp.sqrt(dv) #+ jnp.triu(jnp.ones((words_per_phrase, words_per_phrase)))* -1e9 \n",
    "    QKscaled_decoder  = jnp.matmul(Q_D, jnp.transpose(K_D, (0, 1, 3, 2))) / jnp.sqrt(dv)\n",
    "    # Step 1: Create a causal mask of shape (1, 1, 9, 9) to broadcast across heads and batch\n",
    "    mask = jnp.tril(jnp.ones((words_per_phrase, words_per_phrase)))  # (9, 9) lower triangular matrix\n",
    "    mask = mask.at[mask == 0].set(-jnp.inf)  # Set future tokens to -inf\n",
    "    mask = mask.at[mask == 1].set(0)  # Set allowed tokens to 0\n",
    "    mask = mask.reshape(1, 1, words_per_phrase, words_per_phrase)   \n",
    "\n",
    "    # Step 2: Apply mask to QKscaled_decoder (it will broadcast across batch and heads)\n",
    "    QKscaled_decoder = QKscaled_decoder + mask \n",
    "\n",
    "    Attention_weights_masked = softmax(QKscaled_decoder)\n",
    "\n",
    "\n",
    "    A_mask = jnp.matmul(Attention_weights_masked, V_D)\n",
    "    #print(\"A_mask.shape non concat: \",A_mask.shape)\n",
    "    \n",
    "    #A_mask=jnp.swapaxes(jnp.concatenate(jnp.swapaxes(A_mask,0,2),axis=-1),0,1)\n",
    "    A_mask= jnp.concatenate(jnp.swapaxes(A_mask,0,1),axis=-1) \n",
    "    #print(\"A_mask.shape concat: \",A_mask.shape)\n",
    "    #print(\"inputs_d.shape: \",inputs_d[step].shape)\n",
    "\n",
    "\n",
    "\n",
    "    Xd = inputs_d[step] + A_mask\n",
    "    Dt1,mu_d,var_d,N_d = layer_norm(Xd)\n",
    "    #print(\"Dt1.shape\",Dt1.shape)\n",
    "\n",
    "    Qc = np.random.rand(Dt1.shape[-1], dv) / jnp.sqrt(Dt1.shape[-1])\n",
    "    #print(\"Qc.shape\",Qc.shape)\n",
    "\n",
    "    #Q_C  = jnp.swapaxes(jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(Dt1, Qc),num_heads,axis=3)), 0, 1),1,2)\n",
    "    Q_C  = jnp.swapaxes(jnp.array(jnp.array_split(jnp.matmul(Dt1, Qc),num_heads,axis=2)), 0, 1)\n",
    "    #print(\"Q_C.shape: \",Q_C.shape)# shape words_per_phrase,num_heads,words_per_phrase,dv/num_heads\n",
    "    #print(\"K_C.shape: \",K_C.shape)# shape num_phrase,num_heads,words_per_phrase,dv/num_heads\n",
    "    #print(\"V_C.shape: \",V_C.shape)# shape num_phrase,num_heads,words_per_phrase,dv/num_heads\n",
    "\n",
    "    QKscaled_cross_attention  = jnp.matmul(Q_C, jnp.transpose(K_C , (0, 1, 3, 2)))/ jnp.sqrt(dv)\n",
    "    Attention_weights_cross = softmax(QKscaled_cross_attention)\n",
    "    Acr = jnp.matmul(Attention_weights_cross, V_C)\n",
    "    #print(\"Acr.shape non concat\",Acr.shape)\n",
    "    Acr=jnp.concatenate(jnp.swapaxes(Acr,0,1),axis=-1)\n",
    "    #print(\"Acr.shape concat\",Acr.shape)\n",
    "    Res=Acr + Dt1\n",
    "    Dt2, mu_res,var_res,N_res = layer_norm(Res)  # residual_output is (9, 9, 10)\n",
    "    #print(\"Dt2 shape:\", Dt2.shape)\n",
    "\n",
    "\n",
    "    fl1d_size=100\n",
    "    Wfl1d=np.random.rand(Dt2.shape[-1], fl1d_size)   \n",
    "    #print(\"Wfl1d.shape\",Wfl1d.shape)\n",
    "    bfl1d=np.random.rand(fl1d_size)\n",
    "    #print(\"bfl1d.shape\",bfl1d.shape)\n",
    "    Xd1=jnp.matmul(Dt2,Wfl1d)+bfl1d\n",
    "    #print(\"Xd1.shape\",Xd1.shape)\n",
    "\n",
    "    FLd1=relu(Xd1)\n",
    "    #print(\"FLe1.shape\",FLd1.shape)\n",
    "\n",
    "\n",
    "    Wfl2d=np.random.rand(FLd1.shape[-1], dv)   \n",
    "    #print(\"Wfl2d.shape\",Wfl2d.shape)\n",
    "    bfl2d=np.random.rand(dv)\n",
    "    #print(\"bfl2d.shape\",bfl2d.shape)\n",
    "    FLd2=jnp.matmul(FLd1,Wfl2d)+bfl2d\n",
    "    #print(\"FLd2.shape\",FLd2.shape)\n",
    "\n",
    "\n",
    "\n",
    "    Xd2=FLd2+Dt2\n",
    "    Dout,mu_d2,var_d2,N_d2=layer_norm(Xd2)\n",
    "    Dout.shape\n",
    "    #print(\"Dout.shape\",Dout.shape)\n",
    "    Dout=Dout.reshape(num_phrases,Dout.shape[1]*Dout.shape[2])\n",
    "    #print(\"Dout.shape concat\",Dout.shape)\n",
    "\n",
    "\n",
    "\n",
    "    W0=np.random.rand(Dout.shape[-1],vocab_size)   \n",
    "    b0=np.random.rand(vocab_size)\n",
    "    Zout=jnp.matmul(Dout,W0)+b0\n",
    "    #print(\"Zout.shape\",Zout.shape)\n",
    "    SigmaZout = softmax(Zout) \n",
    "    #print(\"SigmaZout.shape\",SigmaZout.shape)\n",
    "    SigmaZout \n",
    "\n",
    "    target_d=np.random.rand(words_per_phrase,num_phrases,vocab_size)\n",
    "    target_d.shape,target_d[step].shape\n",
    "    print(\"Loss:\",cross_entropy_loss(SigmaZout, target_d[step]))\n",
    "\n",
    " \n",
    "\n",
    "    ##################################################################backpropagatation\n",
    "    dLoss_dZout=SigmaZout-target_d[step]\n",
    "    #print(\"dLoss_dZout.shape\",dLoss_dZout.shape)\n",
    "    dLoss_W0=jnp.transpose(dLoss_dZout,(1,0))@Dout\n",
    "    #print(\"dLoss_W0.shape\",dLoss_W0.shape,\"W0.shape\",W0.shape)\n",
    "    dLoss_b0=jnp.sum(dLoss_dZout, axis=0)\n",
    "    #print(\"dLoss_b0.shape\",dLoss_b0.shape,\"b0.shape\",b0.shape)\n",
    "    dLoss_Dout=dLoss_dZout@W0.T\n",
    "    dLoss_Dout=dLoss_Dout.reshape(num_phrases,words_per_phrase,embedding_size)\n",
    "    #print(\"dLoss_Dout.shape\",dLoss_Dout.shape)\n",
    "    #print(\"Dout.shape\",Dout.shape)\n",
    "    dLoss_FLd2=dLoss_Dout*diff_norm(Xd2,var_d2,mu_d2,N_d2)\n",
    "    #print(\"dLoss_FLd2.shape\",dLoss_FLd2.shape) \n",
    "    dLoss_Dt2_a=dLoss_FLd2\n",
    "    #print(\"dLoss_Dt2_a.shape\",dLoss_Dt2_a.shape) \n",
    "    #print(\"Dt2.shape\",Dt2.shape) \n",
    "    dLoss_FLd1=dLoss_FLd2@jnp.transpose(Wfl2d,(1,0))\n",
    "    #print(\"dLoss_FLd1.shape\",dLoss_FLd1.shape) \n",
    "    #print(\"FLd1.shape\",FLd1.shape) \n",
    "    dLoss_Wfl2d=jnp.sum(jnp.transpose(dLoss_FLd2,(0,2,1))@FLd1,axis=0)\n",
    "    #print(\"dLoss_Wfl2d.shape\",dLoss_Wfl2d.shape) # do the mean here over each phrase\n",
    "    #print(\"Wfl2d.shape\",Wfl2d.shape) \n",
    "    dLoss_bfl2d=jnp.sum(jnp.sum(dLoss_FLd2, axis=0),axis=0)\n",
    "    #print(\"dLoss_bfl2d.shape\",dLoss_bfl2d.shape) # do the mean here over each phrase\n",
    "    #print(\"bfl2d.shape\",bfl2d.shape) \n",
    "    if Xd1.all()>0:\n",
    "        DLoss_Dt2_b=dLoss_FLd1@jnp.transpose(Wfl1d,(1,0))\n",
    "    else:\n",
    "        DLoss_Dt2_b=0\n",
    "    DLoss_Dt2=dLoss_Dt2_a+DLoss_Dt2_b\n",
    "    #print(\"DLoss_Dt2.shape\",DLoss_Dt2.shape) # do the mean here over each phrase\n",
    "    #print(\"Dt2.shape\",Dt2.shape) \n",
    "    if Xd1.all()>0:\n",
    "        dLoss_Wfl1d=jnp.sum(jnp.transpose(dLoss_FLd1,(0,2,1))@Dt2,axis=0)\n",
    "    else:\n",
    "        dLoss_Wfl1d=0\n",
    "    #print(\"dLoss_Wfl1d.shape\",dLoss_Wfl1d.shape) # do the mean here over each phrase\n",
    "    #print(\"Wfl1d.shape\",Wfl1d.shape) \n",
    "    if Xd1.all()>0:\n",
    "        dLoss_bfl1d=jnp.sum(jnp.sum(dLoss_FLd1,axis=0),axis=0)\n",
    "    else:\n",
    "        dLoss_bfl1d=0\n",
    "    #print(\"dLoss_bfl1d.shape\",dLoss_bfl1d.shape) # do the mean here over each phrase\n",
    "    #print(\"bfl1d.shape\",bfl1d.shape) \n",
    "    dLoss_Dt2=dLoss_Dt2_a+DLoss_Dt2_b\n",
    "    dLoss_Dt2.shape,diff_norm(Res,var_res,mu_res,N_res).shape \n",
    "    dLoss_Acr=dLoss_Dt2*diff_norm(Res,var_res,mu_res,N_res)\n",
    "    #print(\"dLoss_Acr.shape\",dLoss_Acr.shape) # do the mean here over each phrase\n",
    "    #print(\"Acr.shape\",Acr.shape) \n",
    "    dLoss_Dt1_a=dLoss_Dt2*diff_norm(Res,var_res,mu_res,N_res)\n",
    "    #print(\"dLoss_Dt1.shape\",dLoss_Dt1_a.shape) # do the mean here over each phrase\n",
    "    #print(\"Dt1.shape\",Dt1.shape) \n",
    "    dLoss_Qc=diffQKV(dLoss_Acr,Attention_weights_cross,K_C,V_C,Dt1,dk)\n",
    "    #print(\"dLoss_dQc.shape\",dLoss_Qc.shape) # do the mean here over each phrase\n",
    "    #print(\"Qc.shape\",Qc.shape) \n",
    "    dLoss_Kc=diffQKV(dLoss_Acr,Attention_weights_cross,Q_C,V_C,Ecout,dk)\n",
    "    #print(\"dLoss_dKc.shape\",dLoss_Kc.shape) # do the mean here over each phrase\n",
    "    #print(\"Kc.shape\",Kc.shape) \n",
    "    dLoss_Vc=f=np.sum(np.mean(np.transpose(np.expand_dims(dLoss_Acr, axis=1),(0,1,3,2))@(Attention_weights_cross@np.expand_dims(Ecout, axis=1)),axis=1),axis=0)\n",
    "    #print(\"dLoss_dVc.shape\",dLoss_Vc.shape) # do the mean here over each phrase\n",
    "    #print(\"Vc.shape\",Vc.shape) \n",
    "\n",
    "    dAttention_weights_cross=Attention_weights_cross*(1-Attention_weights_cross)\n",
    "    V1=redimension(dAttention_weights_cross@K_C/jnp.sqrt(dk)) \n",
    "\n",
    "    V2=redimension(V_C)\n",
    "\n",
    "    V3=V1*V2@Qc\n",
    "    dLoss_Dt1_b=dLoss_Acr*V3\n",
    "    #print(\"dLoss_Dt1_b.shape\",dLoss_Dt1_b.shape) # do the mean here over each phrase\n",
    "    #print(\"dLoss_Dt1_a.shape\",dLoss_Dt1_a.shape) \n",
    "    dLoss_Dt1=dLoss_Dt1_a+dLoss_Dt1_b\n",
    "    dLoss_Amask=dLoss_Dt1*diff_norm(Xd,var_d,mu_d,N_d)\n",
    "    #print(\"dLoss_DAmask.shape\",dLoss_Amask.shape)  \n",
    "    dLoss_inputd_a=dLoss_Amask\n",
    "    #print(\"dLoss_Dinputd_a.shape\",dLoss_inputd_a.shape) \n",
    "    dLoss_Kd=diffQKV(dLoss_Amask,Attention_weights_masked,Q_D,V_D,input_d,dk) \n",
    "    #print(\"dLoss_Kd.shape\",dLoss_Kd.shape) \n",
    "    dLoss_Qd=diffQKV(dLoss_Amask,Attention_weights_masked,K_D,V_D,input_d,dk) \n",
    "    #print(\"dLoss_Qd.shape\",dLoss_Qd.shape) \n",
    "    dLoss_Vd=f=np.sum(np.mean(np.transpose(np.expand_dims(dLoss_Amask, axis=1),(0,1,3,2))@(Attention_weights_masked@np.expand_dims(input_d, axis=1)),axis=1),axis=0)\n",
    "    #print(\"dLoss_Vd.shape\",dLoss_Vd.shape) # do the mean here over each phrase\n",
    "    #print(\"Vd.shape\",Vd.shape) \n",
    "    dLoss_V_D=np.transpose(np.mean(np.transpose(np.expand_dims(dLoss_Amask, axis=1),(0,1,3,2))@Attention_weights_masked,axis=1),(0,2,1))\n",
    "    dLoss_V_D.shape\n",
    "    dLoss_inputd_v=dLoss_V_D@Vd\n",
    "\n",
    "    # print(\"dLoss_inputd_v.shape\",dLoss_inputd_v.shape) # do the mean here over each phrase\n",
    "    # print(\"input_d.shape\",input_d.shape) \n",
    "\n",
    "    dAttention_weights_masked=Attention_weights_masked*(1-Attention_weights_masked)\n",
    "    V1=redimension(dAttention_weights_masked@K_D/jnp.sqrt(dk)) \n",
    "    V2=redimension(V_D)\n",
    "    V3=V1*V2\n",
    "    dLoss_Q_D=dLoss_Amask*V3\n",
    "    dLoss_Q_D.shape\n",
    "    dLoss_inputd_q=dLoss_Q_D@Qd\n",
    "    #print(\"dLoss_inputd_q.shape\",dLoss_inputd_q.shape)\n",
    "\n",
    "    dAttention_weights_masked=Attention_weights_masked*(1-Attention_weights_masked)\n",
    "    V1=redimension(dAttention_weights_masked@Q_D/jnp.sqrt(dk)) \n",
    "    V2=redimension(V_D)\n",
    "    V3=V1*V2\n",
    "    dLoss_K_D=dLoss_Amask*V3\n",
    "    dLoss_K_D.shape\n",
    "    dLoss_inputd_k=dLoss_K_D@Kd\n",
    "    #print(\"dLoss_inputd_k.shape\",dLoss_inputd_k.shape)\n",
    "    dLoss_inputd=dLoss_inputd_a+dLoss_inputd_k+dLoss_inputd_q+dLoss_inputd_v\n",
    "\n",
    "    dLoss_dWemb_decoder=dLoss_inputd*input_d\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    dAttention_weights_cross=Attention_weights_cross*(1-Attention_weights_cross)\n",
    "    V1=redimension(dAttention_weights_cross@Q_C/jnp.sqrt(dk)) \n",
    "\n",
    "    V2=redimension(V_C)\n",
    "\n",
    "    V3=V1*V2\n",
    "\n",
    "    \n",
    "    dLoss_K_C=dLoss_Acr*V3\n",
    "    dLoss_K_C.shape\n",
    "\n",
    "    dLoss_Ecout_k=dLoss_K_C@Kc\n",
    "    #print(\"dLoss_Ecout_k.shape\",dLoss_Ecout_k.shape) \n",
    "\n",
    "    dLoss_V_C=np.transpose(np.mean(np.transpose(np.expand_dims(dLoss_Acr, axis=1),(0,1,3,2))@Attention_weights_cross,axis=1),(0,2,1))\n",
    "    dLoss_V_C.shape\n",
    "    dLoss_Ecout_v=dLoss_V_C@Vc\n",
    "\n",
    "    #print(\"dLoss_Ecout_v.shape\",dLoss_Ecout_v.shape) # do the mean here over each phrase\n",
    "    dLoss_Ecout=dLoss_Ecout_k+dLoss_Ecout_v\n",
    "\n",
    "    dLoss_Ecout_k=dLoss_Kc*Kc \n",
    "    dLoss_dFLe2=dLoss_Ecout*diff_norm(Xe2,var_e2,mu_e2,N_e2)\n",
    "    dLoss_Ect1_a=dLoss_dFLe2\n",
    "    dLoss_dFLe1=dLoss_dFLe2@jnp.transpose(Wfl2e,(0,2,1))\n",
    "    dLoss_dWfl2e=jnp.transpose(dLoss_dFLe2,(0,2,1))@FLe1\n",
    "    dLoss_dbfl2e=jnp.sum(dLoss_dFLe2,axis=1)\n",
    "    dLoss_dbfl2e.shape,bfl2e.reshape(bfl2e.shape[0],bfl2e.shape[-1]).shape\n",
    "    if Xe1.all()>0:\n",
    "        dLoss_Ect1_b=dLoss_dFLe1@jnp.transpose(Wfl1e,(0,2,1))\n",
    "    else:\n",
    "        dLoss_Ect1_b=0\n",
    "\n",
    "    dLoss_Ect1=dLoss_Ect1_b+dLoss_Ect1_a\n",
    "    if Xe1.all()>0:\n",
    "        dLoss_Wfl1e=jnp.transpose(dLoss_dFLe1,(0,2,1))@Ect1\n",
    "    else:\n",
    "        dLoss_Wfl1e=0\n",
    "\n",
    "    dLoss_Wfl1e.shape,Wfl1e.shape\n",
    "    if Xe1.all()>0:\n",
    "        dLoss_bfl1e=jnp.transpose(dLoss_dFLe1,(0,2,1)) \n",
    "    else:\n",
    "        dLoss_bfl1e=0\n",
    "\n",
    "    dLoss_bfl1e.shape,bfl1e.shape\n",
    "\n",
    "\n",
    "    #################### Encoder BP\n",
    "    dLoss_Ae=dLoss_Ect1*diff_norm(Xe,var_e,mu_e,Ne)\n",
    "    dLoss_Ae.shape\n",
    "    dLoss_inpute_a=dLoss_Ae\n",
    "    dLoss_inpute_a.shape\n",
    "    dLoss_dQe=diffQKV(dLoss_Ae,Attention_weights_e,K_E,V_E,inputs_e,dk)\n",
    "    #print(\"dLoss_dQe.shape\",dLoss_dQe.shape) # do the mean here over each phrase\n",
    "    #print(\"Qe.shape\",Qe.shape) \n",
    "    dLoss_dKe=diffQKV(dLoss_Ae,Attention_weights_e,Q_E,V_E,inputs_e,dk)\n",
    "    #print(\"dLoss_dKe.shape\",dLoss_dKe.shape) # do the mean here over each phrase\n",
    "    #print(\"Ke.shape\",Ke.shape) \n",
    "    dLoss_dVe=f=np.sum(np.sum(np.transpose(np.expand_dims(dLoss_Ae, axis=1),(0,1,3,2))@(Attention_weights_e@np.expand_dims(inputs_e, axis=1)),axis=1),axis=0)\n",
    "    #print(\"dLoss_dVe.shape\",dLoss_dVe.shape) # do the mean here over each phrase\n",
    "    #print(\"Ve.shape\",Ke.shape) \n",
    "    dLoss_V_E=np.transpose(np.sum(np.transpose(np.expand_dims(dLoss_Ae, axis=1),(0,1,3,2))@Attention_weights_e,axis=1),(0,2,1))\n",
    "    dLoss_V_E.shape\n",
    "    dLoss_inpute_v=dLoss_V_E@Ve\n",
    "\n",
    "    #print(\"dLoss_inpute_v.shape\",dLoss_inpute_v.shape)  \n",
    "    dAttention_weights_e=Attention_weights_e*(1-Attention_weights_e)\n",
    "    V1=redimension(dAttention_weights_e@K_E/jnp.sqrt(dk)) \n",
    "\n",
    "    V2=redimension(V_E)\n",
    "\n",
    "    V3=V1*V2\n",
    "\n",
    "    \n",
    "    dLoss_Q_E=dLoss_Ae*V3\n",
    "    dLoss_Q_E.shape\n",
    "\n",
    "    dLoss_inpute_q=dLoss_Q_E@Qe\n",
    "    #print(\"dLoss_inpute_q.shape\",dLoss_inpute_q.shape)\n",
    "    dAttention_weights_e=Attention_weights_e*(1-Attention_weights_e)\n",
    "    V1=redimension(dAttention_weights_e@Q_E/jnp.sqrt(dk)) \n",
    "\n",
    "    V2=redimension(V_E)\n",
    "\n",
    "    V3=V1*V2\n",
    "\n",
    "    \n",
    "    dLoss_K_E=dLoss_Ae*V3\n",
    "    dLoss_K_E.shape\n",
    "\n",
    "    dLoss_inpute_k=dLoss_K_E@Ke\n",
    "    #print(\"dLoss_inpute_k.shape\",dLoss_inpute_k.shape)\n",
    "    dLoss_inpute=dLoss_inpute_a+dLoss_inpute_k+dLoss_inpute_q+dLoss_inpute_v\n",
    "    dLoss_dWemb_encoder=dLoss_inpute*inputs_e\n",
    "\n",
    "    W0=W0-learning_rate*dLoss_W0.T\n",
    "    b0=b0-learning_rate*dLoss_b0\n",
    "    Wfl2d=Wfl2d-learning_rate*dLoss_Wfl2d.T\n",
    "    bfl2d=bfl2d-learning_rate*dLoss_bfl2d\n",
    "    Wfl1d=Wfl1d-learning_rate*dLoss_Wfl1d.T\n",
    "    bfl1d=bfl1d-learning_rate*dLoss_bfl1d\n",
    "    Qc=Qc-learning_rate*dLoss_Qc\n",
    "    Kc=Kc-learning_rate*dLoss_Kc\n",
    "    Vc=Vc-learning_rate*dLoss_Vc\n",
    "    Qd=Qd-learning_rate*dLoss_Qd\n",
    "    Kd=Kd-learning_rate*dLoss_Kd\n",
    "    Vd=Vd-learning_rate*dLoss_Vd\n",
    "    input_d=input_d-learning_rate*dLoss_dWemb_decoder\n",
    "\n",
    "    Wfl2e=Wfl2e-learning_rate*jnp.transpose(dLoss_dWfl2e ,(0,2,1))\n",
    "    bfl2e=bfl2e-learning_rate*bfl2e\n",
    "    Wfl1e=Wfl1e-learning_rate*jnp.transpose(dLoss_Wfl1e ,(0,2,1))\n",
    "    bfl1e=bfl1e-learning_rate*bfl1e\n",
    "    Qe=Qe-learning_rate*dLoss_dQe\n",
    "    Ke=Ke-learning_rate*dLoss_dKe\n",
    "    Ve=Ve-learning_rate*dLoss_dVe\n",
    "    inputs_e=inputs_e-learning_rate*dLoss_dWemb_encoder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6e8f9c",
   "metadata": {},
   "source": [
    "## Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd47bb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fba14fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "W0=W0-learning_rate*dLoss_W0.T\n",
    "b0=b0-learning_rate*dLoss_b0\n",
    "Wfl2d=Wfl2d-learning_rate*dLoss_Wfl2d.T\n",
    "bfl2d=bfl2d-learning_rate*dLoss_bfl2d\n",
    "Wfl1d=Wfl1d-learning_rate*dLoss_Wfl1d.T\n",
    "bfl1d=bfl1d-learning_rate*dLoss_bfl1d\n",
    "Qc=Qc-learning_rate*dLoss_Qc\n",
    "Kc=Kc-learning_rate*dLoss_Kc\n",
    "Vc=Vc-learning_rate*dLoss_Vc\n",
    "Qd=Qd-learning_rate*dLoss_Qd\n",
    "Kd=Kd-learning_rate*dLoss_Kd\n",
    "Vd=Vd-learning_rate*dLoss_Vd\n",
    "input_d=input_d-learning_rate*dLoss_dWemb_decoder\n",
    "\n",
    "Wfl2e=Wfl2e-learning_rate*jnp.transpose(dLoss_dWfl2e ,(0,2,1))\n",
    "bfl2e=bfl2e-learning_rate*bfl2e\n",
    "Wfl1e=Wfl1e-learning_rate*jnp.transpose(dLoss_Wfl1e ,(0,2,1))\n",
    "bfl1e=bfl1e-learning_rate*bfl1e\n",
    "Qe=Qe-learning_rate*dLoss_dQe\n",
    "Ke=Ke-learning_rate*dLoss_dKe\n",
    "Ve=Ve-learning_rate*dLoss_dVe\n",
    "inputs_e=inputs_e-learning_rate*dLoss_dWemb_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c067efa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "71f9fafa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "72e65056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cddc15c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300d7467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1271c89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37a2627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
