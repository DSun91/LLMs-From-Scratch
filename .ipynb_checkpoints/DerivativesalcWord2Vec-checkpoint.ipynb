{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a62b3d20-53d1-4dc4-b016-ee5307a328e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('c:\\\\python312\\\\lib\\\\site-packages')\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "from numba import jit\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import re\n",
    "from spacy.tokens import Doc\n",
    "from functools import partial\n",
    "from numba import njit\n",
    "import time\n",
    "import numpy as np\n",
    "import numba\n",
    "from numba import njit\n",
    "from numba.typed import Dict\n",
    "from numba.types import int64, unicode_type\n",
    "import cupy as cp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "568da604-e309-4207-b31a-98df212dba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(\"data/bbc-text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72551055-5dbb-448c-85f1-392f51f8497e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  29457\n",
      "29457\n"
     ]
    }
   ],
   "source": [
    "def create_vocabulary(complete_text):\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', complete_text).split()\n",
    "        vocabulary = list(set(text))\n",
    "        print(\"Vocabulary size: \", len(vocabulary))\n",
    "        return vocabulary\n",
    "complete_text = ' '.join(df[\"text\"].tolist())\n",
    "vocabulary=create_vocabulary(complete_text)\n",
    "vocabulary = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba4ab941-bd43-4766-96f8-44187d619588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@njit\n",
    "def numpy_matrix_test():\n",
    "    A = np.zeros((12, 12))\n",
    "    return A\n",
    "numpy_matrix_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a880ff0-3c52-45e5-b09a-7e9ebe516fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def log_time(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()  # Record start time\n",
    "        result = func(*args, **kwargs)  # Execute the wrapped function\n",
    "        end_time = time.time()  # Record end time\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Function '{func.__name__}' executed in {elapsed_time:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90e39123-6ed9-47ce-aa8c-ff1336631ec8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'inputs_window_words' executed in 0.3648 seconds\n",
      "744\n"
     ]
    }
   ],
   "source": [
    "side_window_size=3\n",
    "def input_one_hot_vectors(words,vocabulary):\n",
    "     \n",
    "    inputs=np.zeros((len(words),len(vocabulary)),int)\n",
    "     \n",
    "    for i in range(len(words)): \n",
    "        inputs[i][vocabulary[words[i]]]=1\n",
    "        \n",
    "    return inputs \n",
    "\n",
    " \n",
    "\n",
    "@log_time \n",
    "def inputs_window_words(sequence,vocabulary,side_window_size):\n",
    "    training_samples=[]\n",
    "    for i in range(side_window_size,len(sequence)-side_window_size):\n",
    "        words_before=sequence[i-side_window_size:i]\n",
    "        words_after=sequence[i+1:i+1+side_window_size]\n",
    "        #print(words_before)\n",
    "        #print(words_after)\n",
    "        words_input=words_before+words_after\n",
    "        X_i=input_one_hot_vectors(words_input,vocabulary)\n",
    "        y_i=input_one_hot_vectors([sequence[i]],vocabulary)\n",
    "        #print(X_i,y_i)\n",
    "        training_sample=[X_i,y_i]\n",
    "        training_samples.append(training_sample)\n",
    "    return training_samples\n",
    "\n",
    "\n",
    "\n",
    "vocab_dict = {word: idx for idx, word in enumerate(vocabulary)}    \n",
    "for i in range(1):\n",
    "    \n",
    "    training_samples=inputs_window_words(re.sub(r'[^\\w\\s]',' ',df[\"text\"].iloc[i]).split(),vocab_dict,side_window_size)\n",
    "    \n",
    "    print(len(training_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957218e8-0f23-4a21-9ab8-5e091bcb19b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa25f578-980e-41ed-a570-52fb0e09182b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 29457)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xi=np.array(training_samples[0][0])\n",
    "Xi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da4e1f79-0e0b-48a4-8a33-e4e3d76bc9b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 29457)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yi=np.array(training_samples[0][1])\n",
    "yi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "331b73eb-a1ad-4ae2-9373-bf35ed1d5270",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size=300\n",
    "semi_context_window=3\n",
    "\n",
    "words_len_embedding_layer= np.random.rand(len(vocabulary),embedding_size)   \n",
    "words_len_embedding_bias = np.random.rand(embedding_size)\n",
    " \n",
    "outlayer_maps_vocab_average = np.random.rand(embedding_size, len(vocabulary))\n",
    "out_bias_maps_vocab_average = np.random.rand(len(vocabulary))\n",
    "outlayer_maps_vocab_concat = np.random.rand(semi_context_window * 2 * embedding_size, len(vocabulary))\n",
    "out_bias_maps_vocab_concat = np.random.rand(len(vocabulary))\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a8e1d5-9659-464d-b96d-25d4f2f60af5",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c91f6df-207f-4671-906f-459de6ba483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_time\n",
    "def softmax(x, axis=-1):\n",
    "        x = np.clip(x, -1e4, 1e4)  # Clip for numerical stability\n",
    "        e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "        return e_x / np.sum(e_x, axis=axis, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3954f0cf-2f13-4634-8253-271bf81c2fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "415da4d6-0406-4e39-beef-3fc8e53e7288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'softmax' executed in 0.0000 seconds\n",
      "Function 'softmax' executed in 0.0000 seconds\n",
      "Function 'forward' executed in 0.0490 seconds\n"
     ]
    }
   ],
   "source": [
    "@log_time\n",
    "def forward(Input):\n",
    "    global words_len_embedding_layer,words_len_embedding_bias\n",
    "    global outlayer_maps_vocab_average,out_bias_maps_vocab_average\n",
    "    global outlayer_maps_vocab_concat,out_bias_maps_vocab_concat\n",
    "    global embedding_size,semi_context_window,learning_rate\n",
    "    sigma_zout_one = softmax(np.matmul(np.array(Input), words_len_embedding_layer) +  words_len_embedding_bias)\n",
    "\n",
    "\n",
    "    if flattening_strategy == \"concat\":\n",
    "        sigma_zout_one = sigma_zout_one.reshape(1, sigma_zout_one.shape[0] * embedding_size)\n",
    "        sigma_zout_output = softmax(\n",
    "            np.matmul(sigma_zout_one, outlayer_maps_vocab_concat) + out_bias_maps_vocab_concat)\n",
    "\n",
    "\n",
    "    elif flattening_strategy == \"average\":\n",
    "        sigma_zout_one = np.mean(sigma_zout_one, axis=0).reshape(1, sigma_zout_one.shape[1])\n",
    "        sigma_zout_output = softmax(\n",
    "            np.matmul(sigma_zout_one, outlayer_maps_vocab_average) + out_bias_maps_vocab_average)\n",
    "\n",
    "    return [sigma_zout_one, sigma_zout_output]\n",
    "\n",
    "flattening_strategy= \"concat\"\n",
    "sigma_zout_one, sigma_zout_output=forward(Xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f10fcc17-b7ce-499f-abc2-48cda55ab7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'cross_entropy_loss' executed in 0.0010 seconds\n",
      "loss: 10.2160702645441\n"
     ]
    }
   ],
   "source": [
    "@log_time\n",
    "def cross_entropy_loss(predictions, target):\n",
    "        # Cross-entropy loss for a batch of predictions and targets\n",
    "        batch_loss = -np.sum(target * np.log(predictions + 1e-9), axis=1)\n",
    "        return np.mean(batch_loss)\n",
    "\n",
    "Loss=cross_entropy_loss(sigma_zout_output, yi)\n",
    " \n",
    "print(\"loss:\", Loss)\n",
    "\n",
    "dLoss_dZ2=sigma_zout_output-yi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56da858e-134b-4b66-b8f9-21942eddb67a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing cublas: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 58\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (cp\u001b[38;5;241m.\u001b[39masnumpy(words_len_embedding_layer), cp\u001b[38;5;241m.\u001b[39masnumpy(words_len_embedding_bias),\n\u001b[0;32m     52\u001b[0m             cp\u001b[38;5;241m.\u001b[39masnumpy(outlayer_maps_vocab_concat), cp\u001b[38;5;241m.\u001b[39masnumpy(out_bias_maps_vocab_concat))\n\u001b[0;32m     57\u001b[0m learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m   \n\u001b[1;32m---> 58\u001b[0m \u001b[43mfast_backpropagation_gpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdLoss_dZ2\u001b[49m\u001b[43m,\u001b[49m\u001b[43msigma_zout_one\u001b[49m\u001b[43m,\u001b[49m\u001b[43msigma_zout_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43mXi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m               \u001b[49m\u001b[43mwords_len_embedding_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                \u001b[49m\u001b[43mwords_len_embedding_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m                \u001b[49m\u001b[43moutlayer_maps_vocab_concat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m                \u001b[49m\u001b[43mout_bias_maps_vocab_concat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m                \u001b[49m\u001b[43membedding_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m                \u001b[49m\u001b[43msemi_context_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m    \n",
      "Cell \u001b[1;32mIn[18], line 6\u001b[0m, in \u001b[0;36mlog_time.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m      5\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Record start time\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Execute the wrapped function\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Record end time\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     elapsed_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[1;32mIn[28], line 26\u001b[0m, in \u001b[0;36mfast_backpropagation_gpu\u001b[1;34m(dLoss_dZ2, sigma_Z_1, sigma_Z_2, Xi, words_len_embedding_layer, words_len_embedding_bias, outlayer_maps_vocab_concat, out_bias_maps_vocab_concat, embedding_size, semi_context_window, learning_rate)\u001b[0m\n\u001b[0;32m     22\u001b[0m out_bias_maps_vocab_concat \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39masarray(out_bias_maps_vocab_concat)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flattening_strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcat\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Matrix multiplication on GPU\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m     dLoss_dW2 \u001b[38;5;241m=\u001b[39m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43msigma_Z_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdLoss_dZ2\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (1800, 29457)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Gradient of loss with respect to b2\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     dLoss_db2 \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39msum(dLoss_dZ2, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# (29457,)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\python312\\lib\\site-packages\\cupy\\_core\\_gufuncs.py:683\u001b[0m, in \u001b[0;36m_GUFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    677\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid shape for out \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mouts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    678\u001b[0m                          \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m needs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    680\u001b[0m     _raise_if_invalid_cast(\n\u001b[0;32m    681\u001b[0m         ret_dtype, outs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype, casting, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 683\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_func_to_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimsizess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop_output_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mouts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;66;03m# This code credit goes to Dask\u001b[39;00m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;66;03m# https://github.com/dask/dask/blob/61b578f5a3ad88cbc6a8b9a73ce08c551bd969fa/dask/array/gufunc.py#L462-L503\u001b[39;00m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;66;03m# Treat direct output\u001b[39;00m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\python312\\lib\\site-packages\\cupy\\_core\\_gufuncs.py:397\u001b[0m, in \u001b[0;36m_GUFunc._apply_func_to_inputs\u001b[1;34m(self, func, dim, sizes, dims, args, outs)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_supports_out \u001b[38;5;129;01mand\u001b[39;00m outs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    396\u001b[0m     outs \u001b[38;5;241m=\u001b[39m outs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(outs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m outs\n\u001b[1;32m--> 397\u001b[0m     \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mouts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    399\u001b[0m     fouts \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32mcupy\\_core\\_routines_linalg.pyx:809\u001b[0m, in \u001b[0;36mcupy._core._routines_linalg.matmul\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\_core\\_routines_linalg.pyx:830\u001b[0m, in \u001b[0;36mcupy._core._routines_linalg.matmul\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing cublas: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "\n",
    "\n",
    "@log_time\n",
    "def fast_backpropagation_gpu(dLoss_dZ2, sigma_Z_1, sigma_Z_2, Xi,\n",
    "                             words_len_embedding_layer,\n",
    "                             words_len_embedding_bias,\n",
    "                             outlayer_maps_vocab_concat,\n",
    "                             out_bias_maps_vocab_concat,\n",
    "                             embedding_size,\n",
    "                             semi_context_window,\n",
    "                             learning_rate):\n",
    "    \n",
    "    # Convert everything to CuPy arrays to leverage GPU acceleration\n",
    "    dLoss_dZ2 = cp.asarray(dLoss_dZ2)\n",
    "    sigma_Z_1 = cp.asarray(sigma_Z_1)\n",
    "    sigma_Z_2 = cp.asarray(sigma_Z_2)\n",
    "    Xi = cp.asarray(Xi)\n",
    "    words_len_embedding_layer = cp.asarray(words_len_embedding_layer)\n",
    "    words_len_embedding_bias = cp.asarray(words_len_embedding_bias)\n",
    "    outlayer_maps_vocab_concat = cp.asarray(outlayer_maps_vocab_concat)\n",
    "    out_bias_maps_vocab_concat = cp.asarray(out_bias_maps_vocab_concat)\n",
    "\n",
    "    if flattening_strategy == \"concat\":\n",
    "        # Matrix multiplication on GPU\n",
    "        dLoss_dW2 = cp.matmul(sigma_Z_1.T, dLoss_dZ2)  # (1800, 29457)\n",
    "\n",
    "        # Gradient of loss with respect to b2\n",
    "        dLoss_db2 = cp.sum(dLoss_dZ2, axis=0)  # (29457,)\n",
    "\n",
    "        # Gradient of loss with respect to Z1\n",
    "        dLoss_dZ1 = cp.matmul(dLoss_dZ2, outlayer_maps_vocab_concat.T)  # (1, 1800)\n",
    "\n",
    "        # Softmax gradient with element-wise operations on GPU\n",
    "        dZ1_dPreSoftmax = sigma_Z_1 * (1 - sigma_Z_1)\n",
    "        dLoss_dPreSoftmax = dLoss_dZ1 * dZ1_dPreSoftmax\n",
    "\n",
    "        # Gradient of loss with respect to W1\n",
    "        dLoss_dW1 = cp.matmul(Xi.T, dLoss_dPreSoftmax.reshape(Xi.shape[0], -1))  # (29457, 1800)\n",
    "\n",
    "        # Gradient of loss with respect to b1\n",
    "        dLoss_db1 = cp.sum(dLoss_dPreSoftmax, axis=(0, 1))  # (300,)\n",
    "\n",
    "        # Update weights and biases (in-place operations on GPU)\n",
    "        outlayer_maps_vocab_concat -= learning_rate * dLoss_dW2\n",
    "        out_bias_maps_vocab_concat -= learning_rate * dLoss_db2\n",
    "        words_len_embedding_layer -= learning_rate * dLoss_dW1\n",
    "        words_len_embedding_bias -= learning_rate * dLoss_db1\n",
    "\n",
    "    # Convert everything back to NumPy arrays\n",
    "    return (cp.asnumpy(words_len_embedding_layer), cp.asnumpy(words_len_embedding_bias),\n",
    "            cp.asnumpy(outlayer_maps_vocab_concat), cp.asnumpy(out_bias_maps_vocab_concat))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "learning_rate=0.01   \n",
    "fast_backpropagation_gpu(dLoss_dZ2,sigma_zout_one,sigma_zout_output,Xi,\n",
    "               words_len_embedding_layer,\n",
    "                words_len_embedding_bias,\n",
    "                outlayer_maps_vocab_concat,\n",
    "                out_bias_maps_vocab_concat,\n",
    "                embedding_size,\n",
    "                semi_context_window,\n",
    "                learning_rate)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb0687bb-b2aa-4d01-9a97-17004bb9ca04",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cupy_backends.cuda.api.runtime' has no attribute 'getVersion'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mruntime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetVersion\u001b[49m())\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cupy_backends.cuda.api.runtime' has no attribute 'getVersion'"
     ]
    }
   ],
   "source": [
    "print(cp.cuda.runtime.getVersion())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7496f388-a696-4c94-81b9-1c03129f37c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3c0343-6fdc-4c89-9b75-ceddd4df00e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ada8503-3aa1-43c6-8a7e-468b898abc4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee5b7e2-b0db-4f02-a1cd-578337e9a28a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0187890-20f2-46ba-a3a2-250e6e7803dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdfc2b4-8d27-4c85-ae4b-4098e57d21c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3751aa-38cf-44f3-8730-cc09500309b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "665d9aa8-7430-4d50-b5ad-5f5aa0aa1fc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs_window_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df)):\n\u001b[1;32m----> 2\u001b[0m     training_samples\u001b[38;5;241m=\u001b[39m\u001b[43minputs_window_words\u001b[49m(re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m,df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[i])\u001b[38;5;241m.\u001b[39msplit(),vocabulary,side_window_size)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(training_samples))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'inputs_window_words' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(len(df)):\n",
    "    training_samples=inputs_window_words(re.sub(r'[^\\w\\s]',' ',df[\"text\"].iloc[i]).split(),vocabulary,side_window_size)\n",
    "    print(len(training_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca966219-8214-4440-b38b-13f99665cadc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d524d6d9-651a-4982-957d-a9b041159445",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    start = time.time()\n",
    "    training_samples=inputs_window_words(re.sub(r'[^\\w\\s]',' ',df[\"text\"].iloc[i]).split(),vocabulary,side_window_size)\n",
    "    end = time.time()\n",
    "    print(len(training_samples),\" delta t\",end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18e94638-3929-4343-beee-78cf2a026c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "0.0\n",
      "744  delta t 3.73740816116333\n",
      "294  delta t 1.3751134872436523\n",
      "242  delta t 1.2456028461456299\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df)):\n\u001b[0;32m     36\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 37\u001b[0m     training_samples\u001b[38;5;241m=\u001b[39m\u001b[43minputs_window_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m[^\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m,\u001b[49m\u001b[43mside_window_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(training_samples),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m delta t\u001b[39m\u001b[38;5;124m\"\u001b[39m,end \u001b[38;5;241m-\u001b[39m start)\n",
      "Cell \u001b[1;32mIn[4], line 28\u001b[0m, in \u001b[0;36minputs_window_words\u001b[1;34m(sequence, vocabulary, side_window_size)\u001b[0m\n\u001b[0;32m     26\u001b[0m words_input\u001b[38;5;241m=\u001b[39mwords_before\u001b[38;5;241m+\u001b[39mwords_after\n\u001b[0;32m     27\u001b[0m X_i\u001b[38;5;241m=\u001b[39minput_one_hot_vectors(words_input,vocabulary)\n\u001b[1;32m---> 28\u001b[0m y_i\u001b[38;5;241m=\u001b[39m\u001b[43minput_one_hot_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#print(X_i,y_i)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m training_sample\u001b[38;5;241m=\u001b[39m[X_i,y_i]\n",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m, in \u001b[0;36minput_one_hot_vectors\u001b[1;34m(words, vocabulary)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minput_one_hot_vectors\u001b[39m(words,vocabulary):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m#print(len(words))\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m#print(inputs.shape)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(words)): \n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "print(\"hello\")\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "def input_one_hot_vectors(words,vocabulary):\n",
    "    #print(len(words))\n",
    "    inputs=np.zeros((len(words),len(vocabulary)),int)\n",
    "    #print(inputs.shape)\n",
    "    for i in range(len(words)): \n",
    "        inputs[i][vocabulary.index(words[i])]=1\n",
    "    return inputs\n",
    "side_window_size=3\n",
    "\n",
    " \n",
    "def inputs_window_words(sequence,vocabulary,side_window_size):\n",
    "   \n",
    "    training_samples=[]\n",
    "    for i in range(side_window_size,len(sequence)-side_window_size):\n",
    "        words_before=sequence[i-side_window_size:i]\n",
    "        words_after=sequence[i+1:i+1+side_window_size]\n",
    "        #print(words_before)\n",
    "        #print(words_after)\n",
    "        words_input=words_before+words_after\n",
    "        X_i=input_one_hot_vectors(words_input,vocabulary)\n",
    "        y_i=input_one_hot_vectors([sequence[i]],vocabulary)\n",
    "        #print(X_i,y_i)\n",
    "        training_sample=[X_i,y_i]\n",
    "        training_samples.append(training_sample)\n",
    "    return training_samples\n",
    "    \n",
    " \n",
    "for i in range(len(df)):\n",
    "    start = time.time()\n",
    "    training_samples=inputs_window_words(re.sub(r'[^\\w\\s]',' ',df[\"text\"].iloc[i]).split(),vocabulary,side_window_size)\n",
    "    end = time.time()\n",
    "    print(len(training_samples),\" delta t\",end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4ae3b8-cfb4-4c51-9caa-cff741938f51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87958af8-a2c1-4960-a92d-5f65a7491732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numba import njit\n",
    "import numpy as np\n",
    "@njit\n",
    "def foo(x):\n",
    "   lst = []\n",
    "   for i in range(x):\n",
    "       lst.append(np.zeros(5))\n",
    "   return lst\n",
    "\n",
    "foo(4)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65d66ec4-3714-4ce7-99dd-71e9c01e97a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((4,3),int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8142abce-d32e-46ef-b1a7-ee66dfc7f780",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
