{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00344db5-aab6-4409-84d9-ef8cdf2e2db9",
   "metadata": {},
   "source": [
    " # NLP Model Implementation Using the QVC Attention Mechanism\n",
    "\n",
    "This guide focuses on developing a Natural Language Processing (NLP) model using the QVC (Query, Value, Context) attention mechanism from scratch using Python and Numpy. The attention mechanism is a critical component in modern NLP models, enhancing their ability to focus on different parts of the input sequence to make more accurate predictions.\n",
    "\n",
    "## Key Components:\n",
    "\n",
    "- **QVC Attention Mechanism**: Understanding and implementing the Query, Value, and Context (QVC) attention mechanism from scratch.\n",
    "- **Model Architecture**: Building the architecture of the NLP model utilizing QVC attention.\n",
    "- **Training and Evaluation**: Training the model with appropriate datasets and evaluating its performance.\n",
    "\n",
    "This project aims to provide a comprehensive guide to implementing and experimenting with attention mechanisms in NLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7146946e-0ba5-4973-84d5-b9e5b5086ab8",
   "metadata": {},
   "source": [
    "# Preparing Input Data for NLP Model\n",
    "\n",
    "In this section, we are preparing the input data for our NLP model by defining arrays representing word embeddings and combining them into a structured format.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa334299-b0e7-483b-ae85-95b418fd030e",
   "metadata": {},
   "source": [
    "Let's consider as starting point for example 3 phrases made of 4 words each where each word have embedding size 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d9826cbc-8702-4db0-8063-c3425427ed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('c:\\\\python312\\\\lib\\\\site-packages')\n",
    "import numpy as np\n",
    " \n",
    "\n",
    "# Phrase 1\n",
    "word1 = np.array([0.1, 0.2, 0.3, 0.4, 0.5,0.3])\n",
    "word2 = np.array([0.5, 0.4, 0.7,0.3, 0.2,0.3])\n",
    "word3 = np.array([0.2,0.7, 0.3, 0.5, 0.4,0.4])\n",
    "word4 = np.array([0.4, 0.1,0.7, 0.2, 0.5,0.7])\n",
    "\n",
    "# Phrase 2\n",
    "word5 = np.array([0.1, 0.9, 0.3, 0.4, 0.5,0.2])\n",
    "word6 = np.array([0.4, 0.4, 0.7,0.3, 0.4,0.6])\n",
    "word7 = np.array([0.2,0.7, 0.4, 0.5, 0.4,0.2])\n",
    "word8 = np.array([0.4, 0.5,0.7, 0.7, 0.5,0.1])\n",
    "\n",
    "# Phrase 3\n",
    "word9 = np.array([0.1, 0.2, 0.3, 0.8, 0.5,0.2])\n",
    "word10 = np.array([0.4, 0.5, 0.7,0.3, 0.8,0.4])\n",
    "word11 = np.array([0.9,0.7, 0.3, 0.5, 0.4,0.6])\n",
    "word12 = np.array([0.4, 0.5,0.1, 0.7, 0.4,0.4])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa490b91-9b93-48fe-a5f4-c68e4323be09",
   "metadata": {},
   "source": [
    "Finally, we combine all these word embeddings into a single matrix. This matrix, `inputs`, has the shape `(3, 4, 6)`, where:\n",
    "- `3` represents the number of phrases (batch size),\n",
    "- `4` is the number of words in each phrase (sequence length),\n",
    "- `6` is the dimensionality of each word embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "010108fc-3efd-4649-84d4-1e405ea6af37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0.1, 0.2, 0.3, 0.4, 0.5, 0.3],\n",
       "         [0.5, 0.4, 0.7, 0.3, 0.2, 0.3],\n",
       "         [0.2, 0.7, 0.3, 0.5, 0.4, 0.4],\n",
       "         [0.4, 0.1, 0.7, 0.2, 0.5, 0.7]],\n",
       " \n",
       "        [[0.1, 0.9, 0.3, 0.4, 0.5, 0.2],\n",
       "         [0.4, 0.4, 0.7, 0.3, 0.4, 0.6],\n",
       "         [0.2, 0.7, 0.4, 0.5, 0.4, 0.2],\n",
       "         [0.4, 0.5, 0.7, 0.7, 0.5, 0.1]],\n",
       " \n",
       "        [[0.1, 0.2, 0.3, 0.8, 0.5, 0.2],\n",
       "         [0.4, 0.5, 0.7, 0.3, 0.8, 0.4],\n",
       "         [0.9, 0.7, 0.3, 0.5, 0.4, 0.6],\n",
       "         [0.4, 0.5, 0.1, 0.7, 0.4, 0.4]]]),\n",
       " (3, 4, 6))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = np.stack([[word1, word2, word3, word4],[word5, word6, word7, word8],[word9, word10, word11, word12]])\n",
    "inputs, inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01448d4b-217c-4fd2-99ce-e432932482f4",
   "metadata": {},
   "source": [
    "Implementing the classifier model class we can start by adding these parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a32f97bf-07f9-4931-834f-98b0a2c86862",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QKVAttentionClassifier:\n",
    "    def __init__(self,word_len,batch_size):\n",
    "        self.word_len=word_len\n",
    "        self.batch_size = batch_size "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04c80dc-cdad-492c-b715-0dbdb1e01c13",
   "metadata": {},
   "source": [
    "# Attention Head\n",
    "\n",
    "An attention head in the attention mechanism is a crucial component of the model that computes the weighted sum of the values based on the similarity between the queries and keys.\n",
    "The primary goal of the attention mechanism is to derive better and richer representations of word embeddings. By focusing on different parts of the input sequence, the attention mechanism helps the model capture intricate relationships and dependencies between words. This enhanced representation improves the model’s ability to understand context and perform various NLP tasks more effectively.\n",
    "\n",
    "The output of an attention head can be represented mathematically as follows:\n",
    "\n",
    "1. **Calculate the Scores**: The attention scores are computed as the dot product of the query matrix \\\\( Q \\\\) with the transpose of the key matrix \\\\( K \\\\). To ensure that the gradients are well-behaved and to prevent excessively large values in the softmax step, the dot product is scaled by \\\\( \\sqrt{d_k} \\\\):\n",
    "  $$\n",
    "  \\text{Scores} = \\frac{QK^T}{\\sqrt{d_k}}\n",
    "  $$\n",
    "   where \\\\( d_k \\\\) refers to the dimensionality of the key vectors. Specifically, \\\\( d_k \\\\) is equal to the number of neurons in the key matrix \\\\( K \\\\). This dimension is crucial for scaling the attention scores, which helps in stabilizing the gradients during training.\n",
    "\n",
    "2. **Apply Softmax**: Apply the softmax function to the scores to get the attention weights:\n",
    "   $$\n",
    "   \\text{Attention Weights} = \\text{softmax}(\\text{Scores})\n",
    "   $$\n",
    "\n",
    "3. **Compute the Weighted Sum**: Multiply the attention weights by the value matrix \\\\( V \\\\) to get the weighted sum:\n",
    "   $$\n",
    "   \\text{Output} = \\text{Attention Weights} \\times V\n",
    "   $$\n",
    "\n",
    "Here’s a more detailed breakdown:\n",
    "\n",
    "- **Query Matrix \\( Q \\)**: Represents the queries for which we are computing attention.\n",
    "- **Key Matrix \\( K \\)**: Represents the keys that are used to compute the similarity with the queries.\n",
    "- **Value Matrix \\( V \\)**: Contains the values that will be weighted by the attention weights to produce the final output.\n",
    "\n",
    "While the query and key matrices have dimensions \\\\(\\text{word\\_length} \\times d_k\\\\), the value matrix \\\\( V \\\\) has a second dimension \\\\( d_v \\\\) that specifies how the words will be represented after attention. This dimension \\\\( d_v \\\\) influences the output representation of the words, allowing the model to create a more meaningful and rich representation based on the attention mechanism.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682f61a6-2c2d-45a1-83c0-621fc877d3e8",
   "metadata": {},
   "source": [
    "![Alt text](imgs/NotebookAttention1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5cc195bf-ea34-41a7-8682-3f902a54a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_len=6\n",
    "dk=3\n",
    "dv=3\n",
    "Q = np.random.rand(word2vec_len, dk)/ np.sqrt(word2vec_len)\n",
    "K = np.random.rand(word2vec_len, dk)/ np.sqrt(word2vec_len)\n",
    "V = np.random.rand(word2vec_len, dv)/ np.sqrt(word2vec_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918bd284-1b11-46af-9381-c5f93d7f81b1",
   "metadata": {},
   "source": [
    "Adding Q,K,V to the classifier we have:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bbf587ef-f7dc-4c44-960c-ba87237d8597",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QKVAttentionClassifier:\n",
    "    def __init__(self,word2vec_len,batch_size,dk,dv):\n",
    "        self.word2vec_len=word2vec_len\n",
    "        self.batch_size = batch_size\n",
    "        self.dk=dk\n",
    "        self.dv = dv\n",
    "        self.Q = np.random.rand(self.word2vec_len, self.dk) / np.sqrt(self.word_len)\n",
    "        self.K = np.random.rand(self.word2vec_len, self.dk) / np.sqrt(self.word_len)\n",
    "        self.V = np.random.rand(self.word2vec_len, self.dv) / np.sqrt(self.word_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a393f1d5-2200-4f81-bf46-fc39f2841dee",
   "metadata": {},
   "source": [
    "## Forward Pass in Attention Mechanism\n",
    "\n",
    "The `forward` method in the attention mechanism is responsible for computing the query, key, and value vectors from the input embeddings. Let’s break down the code provided:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4ca4a1e1-9ccc-4c10-91e4-13ee411b1b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0.22577369, 0.42706725, 0.37118764],\n",
       "         [0.31867248, 0.54100725, 0.43120605],\n",
       "         [0.27692632, 0.6088111 , 0.53412471],\n",
       "         [0.37436676, 0.53774899, 0.4664951 ]],\n",
       " \n",
       "        [[0.24606572, 0.6172034 , 0.55233752],\n",
       "         [0.36968967, 0.61518327, 0.51698646],\n",
       "         [0.26502203, 0.60293486, 0.50820893],\n",
       "         [0.35880584, 0.71648738, 0.567437  ]],\n",
       " \n",
       "        [[0.24913562, 0.52973741, 0.41847591],\n",
       "         [0.40705565, 0.71741545, 0.65233669],\n",
       "         [0.43832427, 0.76354827, 0.75228171],\n",
       "         [0.29396457, 0.60104685, 0.55585522]]]),\n",
       " (3, 4, 3))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qval=np.matmul(inputs, Q)\n",
    "Qval,Qval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9d3002d6-50aa-4319-9ff8-51b6fe4e0e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.34591897, 0.33559158, 0.29740082],\n",
       "        [0.41271255, 0.56074489, 0.3693872 ],\n",
       "        [0.4537308 , 0.42839923, 0.35616238],\n",
       "        [0.41098946, 0.69268228, 0.37994884]],\n",
       "\n",
       "       [[0.41327595, 0.34992201, 0.33653826],\n",
       "        [0.45735313, 0.66351386, 0.40210514],\n",
       "        [0.44672889, 0.38650795, 0.36747856],\n",
       "        [0.58499327, 0.51811483, 0.51326718]],\n",
       "\n",
       "       [[0.49013491, 0.30751583, 0.39914827],\n",
       "        [0.50754346, 0.66766376, 0.47570383],\n",
       "        [0.59920288, 0.75884143, 0.47974607],\n",
       "        [0.51549273, 0.43448965, 0.39160742]]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Kval=np.dot(inputs, K)\n",
    "Kval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "53b206d8-fd56-4a0e-994b-9c04c755a067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.40320791, 0.55761701, 0.44606745],\n",
       "        [0.45176174, 0.66719408, 0.60134744],\n",
       "        [0.53578071, 0.74814978, 0.64087468],\n",
       "        [0.53726383, 0.75161334, 0.67461275]],\n",
       "\n",
       "       [[0.58191354, 0.72381895, 0.63153992],\n",
       "        [0.57536543, 0.80941657, 0.73089011],\n",
       "        [0.52512727, 0.71742403, 0.60850101],\n",
       "        [0.58477764, 0.85514081, 0.69017719]],\n",
       "\n",
       "       [[0.41875796, 0.66885641, 0.48216923],\n",
       "        [0.71470181, 0.89995424, 0.7843814 ],\n",
       "        [0.59159801, 0.9076409 , 0.78549829],\n",
       "        [0.45254626, 0.72723259, 0.57066216]]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vval=np.dot(inputs, V)\n",
    "Vval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe060e4-c99f-4dab-acc6-ab43500fd004",
   "metadata": {},
   "source": [
    "Having the values of the \\\\(Q\\\\), \\\\(K\\\\), \\\\(V\\\\) matrices and the \\\\(dk\\\\) values we can calculate the scores:\n",
    "$$\n",
    "  \\text{QKscaled} = \\frac{QK^T}{\\sqrt{d_k}}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fc4e23-d919-4a86-9a2f-28a2ae4f1cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3af7a0da-fbeb-4ae6-a83f-242f65e5410a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.19157122, 0.27122032, 0.2411008 , 0.30579059],\n",
       "        [0.24250638, 0.34304374, 0.30595984, 0.38656684],\n",
       "        [0.26497784, 0.37699674, 0.33295751, 0.42635384],\n",
       "        [0.25905743, 0.36278565, 0.32700022, 0.40622034]],\n",
       "\n",
       "       [[0.29072404, 0.4296408 , 0.31838032, 0.43141098],\n",
       "        [0.31294448, 0.45330325, 0.34231428, 0.46208444],\n",
       "        [0.28379027, 0.41893558, 0.31072299, 0.42046836],\n",
       "        [0.34061633, 0.50094929, 0.37281697, 0.5036622 ]],\n",
       "\n",
       "       [[0.26098925, 0.39213874, 0.43418568, 0.30164894],\n",
       "        [0.3928914 , 0.57498879, 0.63581821, 0.4486033 ],\n",
       "        [0.43296256, 0.6293842 , 0.6945301 , 0.49207904],\n",
       "        [0.31799435, 0.47049396, 0.51898768, 0.36393982]]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QKscaled=np.matmul(Qval, np.transpose(Kval, (0, 2, 1)))/np.sqrt(K.shape[1])\n",
    "QKscaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2874ca4b-689b-42e5-b5fb-e19733e3a3b1",
   "metadata": {},
   "source": [
    "and finally the \\\\(attention\\\\) \\\\(weights\\\\) as:\n",
    "$$\n",
    "  \\text{Attention weights} =\\sigma( \\frac{QK^T}{\\sqrt{d_k}})\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "57134a6b-fb58-4943-9a33-89df9fc70197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.23503521, 0.25452128, 0.24696951, 0.263474  ],\n",
       "        [0.2311482 , 0.25559558, 0.2462907 , 0.26696552],\n",
       "        [0.22914734, 0.2563091 , 0.24526638, 0.26927717],\n",
       "        [0.23051324, 0.25570809, 0.24671926, 0.26705942]],\n",
       "\n",
       "       [[0.2310464 , 0.26547889, 0.23752546, 0.26594925],\n",
       "        [0.2303443 , 0.26505409, 0.23720979, 0.26739183],\n",
       "        [0.23156399, 0.26507202, 0.23788538, 0.26547862],\n",
       "        [0.22811691, 0.26778682, 0.23558197, 0.26851429]],\n",
       "\n",
       "       [[0.22879445, 0.26085729, 0.2720594 , 0.23828887],\n",
       "        [0.22065316, 0.26472445, 0.28132734, 0.23329505],\n",
       "        [0.21848833, 0.26590903, 0.28380863, 0.23179401],\n",
       "        [0.22551088, 0.26266199, 0.27571334, 0.23611379]]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    # Subtract the max value for numerical stability\n",
    "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "Attention_weights=softmax(QKscaled)\n",
    "Attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b132f5-af6a-44a1-af9e-49de32810dd2",
   "metadata": {},
   "source": [
    "and finally the \\\\(Attention\\\\) value as:\n",
    "$$\n",
    "  \\text{Attention} =\\sigma( \\frac{QK^T}{\\sqrt{d_k}})V\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "4f9a3f8b-7be8-4f5b-996c-0ad41fef0bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.48362758, 0.68367548, 0.5939167 ],\n",
       "        [0.48405781, 0.6843412 , 0.59474925],\n",
       "        [0.48426655, 0.68467268, 0.59518882],\n",
       "        [0.48413268, 0.6844534 , 0.59487167]],\n",
       "\n",
       "       [[0.56744868, 0.7799493 , 0.66803751],\n",
       "        [0.56747352, 0.7801044 , 0.66808717],\n",
       "        [0.56742955, 0.77985037, 0.6679612 ],\n",
       "        [0.56755127, 0.78049613, 0.66846198]],\n",
       "\n",
       "       [[0.55103121, 0.80801392, 0.66461388],\n",
       "        [0.55360881, 0.81082913, 0.6681519 ],\n",
       "        [0.55433752, 0.81160776, 0.66912971],\n",
       "        [0.55212335, 0.80917652, 0.66607515]]])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention=np.matmul(Attention_weights, Vval)\n",
    "Attention "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308da6ec-5fef-4fec-aabd-c632f8d16a15",
   "metadata": {},
   "source": [
    "Consider now the change in the dimensionality of each word:\n",
    "\n",
    "- **Initial Embedding Size**: The word embeddings begin with a dimensionality of 6.\n",
    "- **Attention Output Size**: After applying the attention mechanism, the output of the attention layer typically has a reduced dimensionality, in our case it end up with a size of 3.\n",
    "\n",
    "This reduction occurs because:\n",
    "  - The attention mechanism often projects the embeddings into a lower-dimensional space to capture the most relevant information while reducing computational complexity.\n",
    "  - The output dimension of the attention mechanism is determined by the number of neurons in the linear layer of V matrix of the attention mechanism.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff92ed80-c07e-4513-9263-ac9742f88310",
   "metadata": {},
   "source": [
    "# Computing Phrase Representation from Attention Scores\n",
    "\n",
    "In the attention mechanism, once the attention scores are computed and applied, we often need to summarize or aggregate these scores to obtain a representation of the entire phrase. Here’s how we compute the `phrase_representation`:\n",
    "\n",
    "### Context\n",
    "\n",
    "Given an `Attention` matrix that represents the attention weights applied to each word in a phrase, the goal is to aggregate these weights to obtain a single representation for the phrase.\n",
    "\n",
    "### Computing Phrase Representation\n",
    "\n",
    "1. **Attention Matrix**:\n",
    "   - **Shape**: \\\\((\\text{batch\\_size}, \\text{sequence\\_length}, \\text{embedding\\_dim})\\\\) \n",
    "   - **Purpose**: Contains the attention weights for each word in each phrase. Each entry in this matrix represents the weighted influence of words in the phrase.\n",
    "\n",
    "2. **Phrase Representation Calculation**:\n",
    "   - To obtain a single representation for each phrase, we compute the average of the attention weights along the sequence length dimension.\n",
    "\n",
    "   ```python\n",
    "   phrase_representation = np.mean(Attention, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e00244b-1414-4f05-aaee-9d5f84715048",
   "metadata": {},
   "source": [
    "![Alt text](imgs/NotebookAttention2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "da1fc1bc-fc23-4e16-933c-3d9af598890f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase Representation:\n",
      "[[0.48402116 0.68428569 0.59468161]\n",
      " [0.56747575 0.78010005 0.66813696]\n",
      " [0.55277522 0.80990683 0.66699266]]\n"
     ]
    }
   ],
   "source": [
    "phrase_representation = np.mean(Attention, axis=1)\n",
    "print(\"Phrase Representation:\")\n",
    "print(phrase_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d0cd05-aa0f-445b-9dd2-e29c173864c3",
   "metadata": {},
   "source": [
    "At this point, we have computed the following:\n",
    "\n",
    "- **Attention Mechanism**:\n",
    "  - `Qval`: The result of multiplying the input with the query weight matrix \\( Q \\).\n",
    "  - `Kval`: The result of multiplying the input with the key weight matrix \\( K \\).\n",
    "  - `Vval`: The result of multiplying the input with the value weight matrix \\( V \\).\n",
    "  - `QKscaled`: The scaled dot product of `Qval` and `Kval`, normalized by the square root of the dimensionality of the key vectors \\( d_k \\).\n",
    "  - `attention_weights`: The result of applying the softmax function to `QKscaled` to get attention weights.\n",
    "  - `attention`: The result of multiplying `attention_weights` with `Vval`.\n",
    "We can now add these parts to the classifier model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6d3846a3-848f-476a-bda5-448164a63f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QKVAttentionClassifier:\n",
    "    def __init__(self,word2vec_len,batch_size,dk,dv):\n",
    "        self.word2vec_len=word2vec_len\n",
    "        self.batch_size = batch_size\n",
    "        self.dk=dk\n",
    "        self.dv = dv\n",
    "        # Initialize weights with Xavier/Glorot initialization\n",
    "        self.Q = np.random.rand(self.word2vec_len, self.dk) / np.sqrt(self.word_len)\n",
    "        self.K = np.random.rand(self.word2vec_len, self.dk) / np.sqrt(self.word_len)\n",
    "        self.V = np.random.rand(self.word2vec_len, self.dv) / np.sqrt(self.word_len)\n",
    "\n",
    "    \n",
    "    def AttentionHead(self, Inputs):\n",
    "        self.Qval = np.dot(Inputs, self.Q)\n",
    "        self.Kval = np.dot(Inputs, self.K)\n",
    "        self.Vval = np.dot(Inputs, self.V) \n",
    "        QKscaled = np.matmul(self.Qval, np.transpose(self.Kval, (0, 2, 1))) / np.sqrt(self.K.shape[1]) \n",
    "        self.Attention_weights = self.softmax(QKscaled) \n",
    "        return np.matmul(self.Attention_weights, self.Vval)\n",
    "    \n",
    "    def forward(self, Inputs):\n",
    "        Attention = self.AttentionHead(Inputs)\n",
    "        self.phrase_representation = np.mean(Attention, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeb7df4-17d1-4cad-ae0d-e337352a00c9",
   "metadata": {},
   "source": [
    "With these computations complete, we are now ready to feed the `phrase_rep` into the linear layer for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960dfe9b-1d3a-4ee5-9fc0-f517be2ee94d",
   "metadata": {},
   "source": [
    "![Alt text](imgs/Notebooktxtclass.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f760b274-9ded-4466-8b1e-2a8404f2fb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2  # Example number of classes (binary classification)\n",
    "linearlayer= np.random.rand(dv, num_classes)   \n",
    "linear_bias = np.random.rand(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5e84053a-063e-4a51-925c-dc2e6a91caf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.41860061, 0.58139939],\n",
       "       [0.43331587, 0.56668413],\n",
       "       [0.43393571, 0.56606429]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sigma_Zout=softmax(np.matmul(phrase_representation, linearlayer) + linear_bias)\n",
    "Sigma_Zout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62c9059-abe9-47a8-b709-27cfb505317f",
   "metadata": {},
   "source": [
    "We add the linearlayer and Sigma_Zout calculation to the classifier:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3727326d-f64c-44d9-b23e-49df570ea4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QKVAttentionClassifier:\n",
    "    def __init__(self, word_len, words_per_phrase, batch_size, dk, dv, num_classes):\n",
    "\n",
    "        self.word_len = word_len\n",
    "        self.batch_size = batch_size\n",
    "        self.dk = dk\n",
    "        self.dv = dv\n",
    "        self.num_classes = num_classes\n",
    "        self.words_per_phrase = words_per_phrase\n",
    "\n",
    "        # Initialize weights with Xavier/Glorot initialization\n",
    "        self.Q = np.random.randn(self.word_len, self.dk) / np.sqrt(self.word_len)  \n",
    "        self.K = np.random.randn(self.word_len, self.dk) / np.sqrt(self.word_len)  \n",
    "        self.V = np.random.randn(self.word_len, self.dk) / np.sqrt(self.word_len)  \n",
    "\n",
    "        # Initialize linear layer weights\n",
    "        self.linearlayer = np.random.randn(self.dk, self.num_classes) / np.sqrt(self.dk)\n",
    "        self.linear_bias = np.zeros(self.num_classes) \n",
    "        \n",
    "    def LinearLayer(self):\n",
    "        output = np.matmul(self.phrase_representation, self.linearlayer) + self.linear_bias\n",
    "        return output\n",
    "\n",
    "    def forward(self, Inputs):\n",
    "        Attention = self.AttentionHead(Inputs)\n",
    "        self.phrase_representation = np.mean(Attention, axis=1)\n",
    "\n",
    "        Zout = self.LinearLayer()\n",
    "        Sigma_Zout = self.softmax(Zout)\n",
    "\n",
    "        return Sigma_Zout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df671f0-1e4e-47b4-9af2-9ca0bbc910ee",
   "metadata": {},
   "source": [
    "### Cross-Entropy Loss Calculation\n",
    "\n",
    "At this stage, we have calculated the cross-entropy loss between our predictions and the true target values.\n",
    "\n",
    " \n",
    "\n",
    "1. **True Target**:\n",
    "   - The `target` variable represents the true class labels for each example in the batch. It is a list of one-hot encoded vectors. For instance:\n",
    "     - `np.array([0, 1])` represents the true class for the first example.\n",
    "     - `np.array([1, 0])` represents the true class for the second example.\n",
    "     - `np.array([1, 0])` represents the true class for the third example.\n",
    "\n",
    "2. **Cross-Entropy Loss Calculation**:\n",
    "   - The cross-entropy loss is computed using the formula:\n",
    "     \\\\[\n",
    "     \\text{batch\\_loss} = -\\sum (\\text{target} \\cdot \\log(\\text{predictions} + 1e-8))\n",
    "     \\\\]\n",
    "   - This loss function measures the difference between the predicted probabilities and the true class labels. The `1e-8` term is added to avoid taking the logarithm of zero, which could result in undefined values.\n",
    "\n",
    "3. **Result**:\n",
    "   - The computed loss, `loss`, is an average of the individual losses across the batch. It quantifies how well the predicted probabilities match the true labels. A lower loss indicates better performance.\n",
    "\n",
    "In summary, this step provides a measure of how well our model's predictions align with the actual class labels in our batch. The output of this calculation will be used to guide the training process through backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2c487b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy Loss:  0.737821499876489\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    " \n",
    "target = [np.array([0, 1]),np.array([1, 0]),np.array([1, 0])]\n",
    "\n",
    "\n",
    "def cross_entropy_loss(predictions, target): \n",
    "    batch_loss = -np.sum(target * np.log(predictions + 1e-8), axis=1)\n",
    "    return np.mean(batch_loss) \n",
    " \n",
    "\n",
    "loss = cross_entropy_loss(Sigma_Zout, target)\n",
    "print(\"Cross-Entropy Loss: \",loss)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a431718c-20aa-432b-9947-3a8e289e337d",
   "metadata": {},
   "source": [
    "We can now add both softmax and cross entropy loss functions to the classifier model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "31a1505c-b77c-4921-bdcc-27ed7f1e3160",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QKVAttentionClassifier:\n",
    "     \n",
    "    def softmax(self, x, axis=-1):\n",
    "        x = np.clip(x, -1e4, 1e4)  # Clip for numerical stability\n",
    "        e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "        return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "    def cross_entropy_loss(self, predictions, target):\n",
    "        # Cross-entropy loss for a batch of predictions and targets\n",
    "        batch_loss = -np.sum(target * np.log(predictions + 1e-9), axis=1)\n",
    "        return np.mean(batch_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5138a1a-2c16-4d06-82f3-8a889dc0106c",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "To update the model parameters, we need to compute the gradient of the loss function with respect to the output probabilities. This gradient indicates how much the loss function would change if the output probabilities were adjusted.\n",
    "\n",
    "### Gradient of loss with respect to output probabilities:\n",
    "\n",
    "\n",
    "The gradient of the loss with respect to the logits \\( Z^{out} \\) can be expressed as:\n",
    "\n",
    "\\\\[\n",
    "\\frac{\\partial \\text{Loss}}{\\partial Z^{out}} = \\frac{\\partial \\text{Loss}}{\\partial \\sigma(Z^{out})} \\frac{\\partial \\sigma(Z^{out})}{\\partial Z^{out}} = \\sigma(Z^{out}_i) - y_i\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a46ee5c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.41860061, -0.41860061],\n",
       "       [-0.56668413,  0.56668413],\n",
       "       [-0.56606429,  0.56606429]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient of loss with respect to output probabilities\n",
    "dLoss_dSigma_Zout =Sigma_Zout - np.stack(target)\n",
    "dLoss_dSigma_Zout\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86d5926-b7ef-4b54-9d69-c259363d4723",
   "metadata": {},
   "source": [
    "### Gradient of the loss with respect to linear layer and bias:\n",
    "\n",
    "The gradient of the loss with respect to the linear layer weights and bias can be expressed as:\n",
    "\\\\[\n",
    "\\frac{\\partial Loss}{\\partial W}=\\begin{cases} \\frac{\\partial Loss}{\\partial Z^{out}}=\\frac{\\partial Loss}{\\partial \\sigma(Z^{out})}\\frac{\\partial \\sigma(Z^{out})}{\\partial  Z^{out}}= \\sigma(Z^{out})-y_{true} \\\\  \\frac{\\partial Loss}{\\partial w_{11}}=\\frac{\\partial Loss}{\\partial \\sigma(Z^{out}_1)}\\frac{\\partial \\sigma(Z^{out}_1)}{\\partial Z^{out}_1}\\frac{\\partial Z^{out}_1}{\\partial w_{11}}= [\\sigma(Z^{out}_1)-y_{1}]\\cdot Y_1\\\\  \\frac{\\partial Loss}{\\partial w_{21}}=\\frac{\\partial Loss}{\\partial \\sigma(Z^{out}_1)}\\frac{\\partial \\sigma(Z^{out}_1)}{\\partial Z^{out}_1}\\frac{\\partial Z^{out}_1}{\\partial w_{21}}= [\\sigma(Z^{out}_1)-y_{1}]\\cdot Y_2\\\\  \\frac{\\partial Loss}{\\partial w_{31}}=\\frac{\\partial Loss}{\\partial \\sigma(Z^{out}_1)}\\frac{\\partial \\sigma(Z^{out}_1)}{\\partial Z^{out}_1}\\frac{\\partial Z^{out}_1}{\\partial w_{31}}= [\\sigma(Z^{out}_1)-y_{1}]\\cdot Y_3\\\\  \\frac{\\partial Loss}{\\partial w_{12}}=\\frac{\\partial Loss}{\\partial \\sigma(Z^{out}_2)}\\frac{\\partial \\sigma(Z^{out}_2)}{\\partial Z^{out}_2}\\frac{\\partial Z^{out}_2}{\\partial w_{12}}= [\\sigma(Z^{out}_2)-y_{2}]\\cdot Y_1\\\\  \\frac{\\partial Loss}{\\partial w_{22}}=\\frac{\\partial Loss}{\\partial \\sigma(Z^{out}_2)}\\frac{\\partial \\sigma(Z^{out}_2)}{\\partial Z^{out}_2}\\frac{\\partial Z^{out}_2}{\\partial w_{22}}= [\\sigma(Z^{out}_2)-y_{2}]\\cdot Y_2\\\\  \\frac{\\partial Loss}{\\partial w_{32}}=\\frac{\\partial Loss}{\\partial \\sigma(Z^{out}_2)}\\frac{\\partial \\sigma(Z^{out}_2)}{\\partial Z^{out}_2}\\frac{\\partial Z^{out}_2}{\\partial w_{32}}= [\\sigma(Z^{out}_2)-y_{2}]\\cdot Y_3\\\\  \\frac{\\partial Loss}{\\partial B_{1}}=\\frac{\\partial Loss}{\\partial \\sigma(Z^{out}_1)}\\frac{\\partial \\sigma(Z^{out}_1)}{\\partial Z^{out}_1}\\frac{\\partial Z^{out}_2}{\\partial B_{1}}= [\\sigma(Z^{out}_1)-y_{1}]\\cdot 1\\\\   \\frac{\\partial Loss}{\\partial B_{2}}=\\frac{\\partial Loss}{\\partial \\sigma(Z^{out}_2)}\\frac{\\partial \\sigma(Z^{out}_2)}{\\partial Z^{out}_2}\\frac{\\partial Z^{out}_2}{\\partial B_{2}}= [\\sigma(Z^{out}_2)-y_{2}]\\cdot 1\\\\ \\end{cases}\n",
    "\\\\]\n",
    "\n",
    "These values will be used to update the weights of the linear layer trought the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2f3098d2-304b-4007-84fd-a07e3652b289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.43187427,  0.43187427],\n",
       "        [-0.61408725,  0.61408725],\n",
       "        [-0.50724925,  0.50724925]]),\n",
       " array([-0.71414781,  0.71414781]))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient for linear layer and bias\n",
    "d_linear = np.dot(phrase_representation.T, dLoss_dSigma_Zout) \n",
    "d_bias =  np.sum(dLoss_dSigma_Zout, axis=0)\n",
    "d_linear,d_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2666bc3-a1e0-461a-903c-b8bdc3886468",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QKVAttentionClassifier: \n",
    "    def BackPropagation(self, dLoss_dSigma_Zout, inputs):\n",
    "        \n",
    "        # Gradient for linear layer\n",
    "        dlinear_dW = np.dot(self.phrase_representation.T, dLoss_dSigma_Zout)\n",
    "\n",
    "        # Gradient for bias\n",
    "        d_bias = np.sum(dLoss_dSigma_Zout, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8550983-d28e-4555-b782-11872d8723f4",
   "metadata": {},
   "source": [
    "### Gradient of the loss with respect to phrase representation:\n",
    " \n",
    "\n",
    "The gradient of the loss with respect to phrase representation can be expressed as:\n",
    "\\\\[\n",
    "\\frac{\\partial Loss}{\\partial Y}=\\begin{cases} \\frac{\\partial Loss}{\\partial Y^{z_1}_1}=\\frac{\\partial Loss}{\\partial \\sigma(Z^{out}_1)}\\frac{\\partial \\sigma(Z^{out}_1)}{\\partial Z^{out}_1}\\frac{\\partial Z^{out}_1}{\\partial Y_1}=[\\sigma(Z_1^{out})-y_1]\\cdot w_{11}\\\\  \\frac{\\partial Loss}{\\partial Y^{z_2}_1}=\\frac{\\partial Loss}{\\partial \\sigma(Z^{out}_2)}\\frac{\\partial \\sigma(Z^{out}_2)}{\\partial Z^{out}_2}\\frac{\\partial Z^{out}_2}{\\partial Y_1}=[\\sigma(Z_2^{out})-y_2]\\cdot w_{12}\\\\    \\frac{\\partial Loss}{\\partial Y^{z_1}_2}=\\frac{\\partial Loss}{\\partial \\sigma(Z^{out}_1)}\\frac{\\partial \\sigma(Z^{out}_1)}{\\partial Z^{out}_1}\\frac{\\partial Z^{out}_1}{\\partial Y_2}=[\\sigma(Z_1^{out})-y_1]\\cdot w_{21}\\\\  \\frac{\\partial Loss}{\\partial Y^{z_2}_2}=\\frac{\\partial Loss}{\\partial \\sigma(Z^{out}_2)}\\frac{\\partial \\sigma(Z^{out}_2)}{\\partial Z^{out}_2}\\frac{\\partial Z^{out}_2}{\\partial Y_2}=[\\sigma(Z_2^{out})-y_2]\\cdot w_{22}\\\\     \\frac{\\partial Loss}{\\partial Y^{z_1}_3}=\\frac{\\partial Loss}{\\partial \\sigma(Z^{out}_1)}\\frac{\\partial \\sigma(Z^{out}_1)}{\\partial Z^{out}_1}\\frac{\\partial Z^{out}_1}{\\partial Y_3}=[\\sigma(Z_1^{out})-y_1]\\cdot w_{31}\\\\  \\frac{\\partial Loss}{\\partial Y^{z_2}_3}=\\frac{\\partial Loss}{\\partial \\sigma(Z^{out}_2)}\\frac{\\partial \\sigma(Z^{out}_2)}{\\partial Z^{out}_2}\\frac{\\partial Z^{out}_2}{\\partial Y_2}=[\\sigma(Z_2^{out})-y_3]\\cdot w_{32}\\end{cases}=\\begin{cases}\\frac{\\partial Loss}{\\partial Y_1}=\\frac{\\partial Loss}{\\partial Y^{z_1}_1}+\\frac{\\partial Loss}{\\partial Y^{z_2}_1}\\\\  \\frac{\\partial Loss}{\\partial Y_2}=\\frac{\\partial Loss}{\\partial Y^{z_1}_2}+\\frac{\\partial Loss}{\\partial Y^{z_2}_2}\\\\  \\frac{\\partial Loss}{\\partial Y_3}=\\frac{\\partial Loss}{\\partial Y^{z_1}_3}+\\frac{\\partial Loss}{\\partial Y^{z_2}_3}\\end{cases}\n",
    "\\\\]\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "57f28c04-c59d-40ef-bd6c-03591fb0fedb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.26741035,  0.16078187, -0.17055435],\n",
       "       [-0.36200903, -0.21765982,  0.23088939],\n",
       "       [-0.36161307, -0.21742174,  0.23063685]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient for phrase representation\n",
    "d_phrase_rep = np.dot(dLoss_dSigma_Zout, linearlayer.T)\n",
    "d_phrase_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ede1786d-8c88-429e-8007-34c4d3029288",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QKVAttentionClassifier: \n",
    "    def BackPropagation(self, dLoss_dSigma_Zout, inputs):\n",
    "        \n",
    "        # Gradient for linear layer\n",
    "        dlinear_dW = np.dot(self.phrase_representation.T, dLoss_dSigma_Zout)\n",
    "\n",
    "        # Gradient for bias\n",
    "        d_bias = np.sum(dLoss_dSigma_Zout, axis=0)\n",
    "        \n",
    "         # Gradient for phrase representation\n",
    "        d_phrase_rep = np.dot(dLoss_dSigma_Zout, self.linearlayer.T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4653d85a-7ba3-4c00-8621-31eb998fca4a",
   "metadata": {},
   "source": [
    "### Gradient of the Loss with Respect to Attention Output\n",
    "\n",
    "Given the attention output matrix:\n",
    "\n",
    "\\\\[\n",
    "\\text{Attention} = \n",
    "\\begin{bmatrix}\n",
    "Y_a^1 & Y_a^2 & Y_a^3 \\\\\n",
    "Y_b^1 & Y_b^2 & Y_b^3 \\\\\n",
    "Y_c^1 & Y_c^2 & Y_c^3 \\\\\n",
    "Y_d^1 & Y_d^2 & Y_d^3 \\\\\n",
    "\\end{bmatrix}\n",
    "\\\\]\n",
    "\n",
    "Each row \\\\(Y_a, Y_b, Y_c, Y_d\\\\) represents the attention for each input token.\n",
    "\n",
    "#### Phrase Representation\n",
    "\n",
    "We compute the phrase representation by averaging over the rows for each component:\n",
    "\n",
    "\\\\[\n",
    "\\text{Phrase Representation} = Y =\n",
    "\\begin{bmatrix}\n",
    "\\frac{Y^1_a + Y^1_b + Y^1_c + Y^1_d}{4} \\\\\n",
    "\\frac{Y^2_a + Y^2_b + Y^2_c + Y^2_d}{4} \\\\\n",
    "\\frac{Y^3_a + Y^3_b + Y^3_c + Y^3_d}{4} \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "{Y_1}\\\\\n",
    "{Y_2}\\\\\n",
    "{Y_3}\\\\\n",
    "\\end{bmatrix}\n",
    "\\\\]\n",
    "\n",
    "#### Loss Function and Gradient\n",
    "\n",
    "The loss is computed based on the phrase representation. To compute the gradient of the loss with respect to each attention component \\\\(Y_x^i\\\\), we apply the chain rule:\n",
    "\n",
    "\\\\[\n",
    "\\frac{\\partial Loss}{\\partial Y_x^i} = \\frac{\\partial Loss}{\\partial \\text{Y}_i} \\cdot \\frac{\\partial \\text{Y}_i}{\\partial Y_x^i}\n",
    "\\\\]\n",
    "\n",
    "So we end up with the following set of equations:\n",
    "\\\\[\n",
    "\\begin{align*}\n",
    "\\frac{\\partial Loss}{\\partial Y_x^1} = \\begin{cases}\n",
    "\\frac{\\partial Loss}{\\partial Y^1_a} = \\frac{\\partial Loss}{\\partial \\text{Y}_1} \\cdot \\frac{\\partial \\text{Y}_1}{\\partial Y_1^a}\\\\\n",
    "\\frac{\\partial Loss}{\\partial Y^1_b} = \\frac{\\partial Loss}{\\partial \\text{Y}_1} \\cdot \\frac{\\partial \\text{Y}_1}{\\partial Y_1^b}\\\\\n",
    "\\frac{\\partial Loss}{\\partial Y^1_c} = \\frac{\\partial Loss}{\\partial \\text{Y}_1} \\cdot \\frac{\\partial \\text{Y}_1}{\\partial Y_1^c}\\\\\n",
    "\\frac{\\partial Loss}{\\partial Y^1_d} = \\frac{\\partial Loss}{\\partial \\text{Y}_1} \\cdot \\frac{\\partial \\text{Y}_1}{\\partial Y_1^d}\\\\\n",
    "\\end{cases}  & \n",
    "\\frac{\\partial Loss}{\\partial Y_x^2} = \\begin{cases}\n",
    "\\frac{\\partial Loss}{\\partial Y^2_a} = \\frac{\\partial Loss}{\\partial \\text{Y}_2} \\cdot \\frac{\\partial \\text{Y}_2}{\\partial Y_2^a}\\\\\n",
    "\\frac{\\partial Loss}{\\partial Y^2_b} = \\frac{\\partial Loss}{\\partial \\text{Y}_2} \\cdot \\frac{\\partial \\text{Y}_2}{\\partial Y_2^b}\\\\\n",
    "\\frac{\\partial Loss}{\\partial Y^2_c} = \\frac{\\partial Loss}{\\partial \\text{Y}_2} \\cdot \\frac{\\partial \\text{Y}_2}{\\partial Y_2^c}\\\\\n",
    "\\frac{\\partial Loss}{\\partial Y^2_d} = \\frac{\\partial Loss}{\\partial \\text{Y}_2} \\cdot \\frac{\\partial \\text{Y}_2}{\\partial Y_2^d}\\\\\n",
    "\\end{cases}  & \n",
    "\\frac{\\partial Loss}{\\partial Y_x^3} = \\begin{cases}\n",
    "\\frac{\\partial Loss}{\\partial Y^3_a} = \\frac{\\partial Loss}{\\partial \\text{Y}_3} \\cdot \\frac{\\partial \\text{Y}_3}{\\partial Y_3^a}\\\\\n",
    "\\frac{\\partial Loss}{\\partial Y^3_b} = \\frac{\\partial Loss}{\\partial \\text{Y}_3} \\cdot \\frac{\\partial \\text{Y}_3}{\\partial Y_3^b}\\\\\n",
    "\\frac{\\partial Loss}{\\partial Y^3_c} = \\frac{\\partial Loss}{\\partial \\text{Y}_3} \\cdot \\frac{\\partial \\text{Y}_3}{\\partial Y_3^c}\\\\\n",
    "\\frac{\\partial Loss}{\\partial Y^3_d} = \\frac{\\partial Loss}{\\partial \\text{Y}_3} \\cdot \\frac{\\partial \\text{Y}_3}{\\partial Y_3^d}\\\\\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\\\\]\n",
    "\n",
    "\n",
    "\n",
    "Since the phrase representation we used is the mean of the attention outputs, we can simplify the derivative with respect to each attention component as:\n",
    "\n",
    "\\\\[\n",
    "\\frac{\\partial \\text{Y}_i}{\\partial Y_x^i} = \\frac{1}{4}\n",
    "\\\\]\n",
    "\n",
    "Therefore, the gradient of the loss with respect to each attention component is:\n",
    "\n",
    "\\\\[ \n",
    "\\frac{\\partial Loss}{\\partial A} = \\begin{cases}\n",
    "\\frac{\\partial Loss}{\\partial Y_a^i} = \\frac{1}{4} \\frac{\\partial Loss}{\\partial \\text{Y}_i}; \\\\\n",
    "\\frac{\\partial Loss}{\\partial Y_b^i} = \\frac{1}{4} \\frac{\\partial Loss}{\\partial \\text{Y}_i}; \\\\\n",
    "\\frac{\\partial Loss}{\\partial Y_c^i} = \\frac{1}{4} \\frac{\\partial Loss}{\\partial \\text{Y}_i}; \\\\\n",
    "\\frac{\\partial Loss}{\\partial Y_d^i} = \\frac{1}{4} \\frac{\\partial Loss}{\\partial \\text{Y}_i};  \n",
    "\\end{cases} \n",
    "\\\\]\n",
    "This shows how the gradient propagates through the mean operation during backpropagation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "2734b391-4b7c-49a9-9151-3828b844b17d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.06685259,  0.04019547, -0.04263859],\n",
       "        [ 0.06685259,  0.04019547, -0.04263859],\n",
       "        [ 0.06685259,  0.04019547, -0.04263859],\n",
       "        [ 0.06685259,  0.04019547, -0.04263859]],\n",
       "\n",
       "       [[-0.09050226, -0.05441495,  0.05772235],\n",
       "        [-0.09050226, -0.05441495,  0.05772235],\n",
       "        [-0.09050226, -0.05441495,  0.05772235],\n",
       "        [-0.09050226, -0.05441495,  0.05772235]],\n",
       "\n",
       "       [[-0.09040327, -0.05435543,  0.05765921],\n",
       "        [-0.09040327, -0.05435543,  0.05765921],\n",
       "        [-0.09040327, -0.05435543,  0.05765921],\n",
       "        [-0.09040327, -0.05435543,  0.05765921]]])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient for attention\n",
    "dL_dA = np.array([np.outer(np.ones(inputs.shape[1]), d_phrase_rep[i, :]) for i in range(d_phrase_rep.shape[0])])  / inputs.shape[1]\n",
    "dL_dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "eaeb3407-984b-48f3-a746-49b233f04db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QKVAttentionClassifier: \n",
    "    def BackPropagation(self, dLoss_dSigma_Zout, inputs):\n",
    "        \n",
    "        # Gradient for linear layer\n",
    "        dlinear_dW = np.dot(self.phrase_representation.T, dLoss_dSigma_Zout)\n",
    "\n",
    "        # Gradient for bias\n",
    "        d_bias = np.sum(dLoss_dSigma_Zout, axis=0)\n",
    "        \n",
    "         # Gradient for phrase representation\n",
    "        d_phrase_rep = np.dot(dLoss_dSigma_Zout, self.linearlayer.T)\n",
    "\n",
    "        # Gradient for attention\n",
    "        dL_dA = np.array([np.outer(np.ones(self.words_per_phrase), d_phrase_rep[i, :]) for i in range(d_phrase_rep.shape[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b60002-82cd-4ec8-bdf9-23e91566d6d6",
   "metadata": {},
   "source": [
    "### Gradient of the loss with respect to values matrix V:\n",
    " \n",
    "\n",
    "The gradient of the loss with respect to values matrix can be expressed as:\n",
    "\\\\[\n",
    "\\text{A} =\\sigma( \\frac{Q_{val}K_{val}^T}{\\sqrt{d_k}})V_{val} = \\sigma( \\frac{Q_{val}K_{val}^T}{\\sqrt{d_k}})\\cdot Inputs \\cdot V\n",
    "\\\\]\n",
    "\\\\[ \n",
    "\\frac{\\partial Loss}{\\partial \\text{V}} =\\frac{\\partial Loss}{\\partial A} \\cdot \\frac{\\partial A}{\\partial V}=\\frac{\\partial Loss}{\\partial A} \\cdot \\sigma( \\frac{Q_{val}K_{val}^T}{\\sqrt{d_k}})\\cdot Inputs\n",
    "\\\\] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "8e6ad508-c70e-4964-8057-87fada6de5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02864241, -0.03035565, -0.01701608],\n",
       "       [-0.0496356 , -0.05260454, -0.02948786],\n",
       "       [-0.02213913, -0.02346337, -0.01315257],\n",
       "       [-0.04540561, -0.04812154, -0.02697488],\n",
       "       [-0.03685711, -0.03906171, -0.02189633],\n",
       "       [-0.01405762, -0.01489847, -0.00835145]])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient for V\n",
    "d_Vval=np.matmul(np.transpose(d_attention,(0,2,1)), Attention_weights) \n",
    "dLoss_dV = np.mean(np.matmul(d_Vval,inputs),axis=0).T\n",
    "dLoss_dV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbb8a0f-28a1-4190-9bf2-0f41f2ce9139",
   "metadata": {},
   "source": [
    "### Gradient of the loss with respect to values matrix Q:\n",
    " \n",
    " \n",
    "The gradient of the loss with respect to queries matrix can be expressed as:\n",
    "\\\\[\n",
    "\\text{A} =\\sigma( \\frac{Q_{val}K_{val}^T}{\\sqrt{d_k}})V_{val}  \n",
    "\\\\]\n",
    "\\\\[ \n",
    "\\frac{\\partial Loss}{\\partial \\text{Q}} =\\frac{\\partial Loss}{\\partial A} \\cdot \\frac{\\partial A}{\\partial Q} \n",
    "\\\\] \n",
    "\\\\[ \n",
    "\\frac{\\partial A}{\\partial Q}  = \\frac{\\partial \\sigma( \\frac{Q_{val}K_{val}^T}{\\sqrt{d_k}})}{\\partial Q} \\cdot V_{val} \n",
    "\\\\] \n",
    "\\\\[ \n",
    "\\frac{\\partial \\sigma( \\frac{Q_{val}K_{val}^T}{\\sqrt{d_k}})}{\\partial Q}   = \\sigma( \\frac{Q_{val}K_{val}^T}{\\sqrt{d_k}}) \\cdot [1-\\sigma( \\frac{Q_{val}K_{val}^T}{\\sqrt{d_k}})] \\cdot \n",
    "\\frac{\\partial (\\frac{Q_{val}K_{val}^T}{\\sqrt{d_k}})}{\\partial Q}\n",
    "\\\\] \n",
    "\\\\[ \n",
    "\\frac{\\partial (\\frac{Q_{val}K_{val}^T}{\\sqrt{d_k}})}{\\partial Q} = \\frac{\\partial (\\frac{Inputs \\cdot Q \\cdot K_{val}^T}{\\sqrt{d_k}})}{\\partial Q}=\\frac{Inputs \\cdot K_{val}^T}{\\sqrt{d_k}}\n",
    "\\\\]  \n",
    "\\\\[ \n",
    "\\frac{\\partial Loss}{\\partial \\text{Q}} =\\frac{\\partial Loss}{\\partial A} \\cdot [\\sigma( \\frac{Q_{val}K_{val}^T}{\\sqrt{d_k}}) \\cdot [1-\\sigma( \\frac{Q_{val}K_{val}^T}{\\sqrt{d_k}})]] \\cdot \\frac{Inputs \\cdot K_{val}^T}{\\sqrt{d_k}} \\cdot V_{val}\n",
    "\\\\] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "ee3113bd-d3ac-48ab-9eba-93de732e2fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04795894, -0.06923879, -0.05675471],\n",
       "       [-0.06897801, -0.09796685, -0.08134587],\n",
       "       [-0.04595965, -0.06485524, -0.05357087],\n",
       "       [-0.06370296, -0.09108245, -0.07527546],\n",
       "       [-0.05713204, -0.08177329, -0.06737821],\n",
       "       [-0.03578613, -0.05167345, -0.04199959]])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Gradient of Q\n",
    "dQKscaled_dQ=np.matmul(np.transpose(inputs,(0,2,1)),Kval)/np.sqrt(dk)\n",
    "dAttention_dSoftmax=np.matmul(Attention_weights,(1-Attention_weights))\n",
    "dLoss_dQ=np.mean(np.transpose(np.transpose(np.transpose(dL_dA,(0,2,1))@dAttention_dSoftmax,(0,2,1))@np.transpose( dQKscaled_dQ,(0,2,1)),(0,2,1))@Vval,axis=0)\n",
    "dLoss_dQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4271928d-103a-44e4-800d-ce8cc38c4bec",
   "metadata": {},
   "source": [
    "### Gradient of the loss with respect to values matrix K:\n",
    " \n",
    " \n",
    "The gradient of the loss with respect to queries matrix can be expressed as:\n",
    "\\\\[\n",
    "\\text{A} =\\sigma( \\frac{Q_{val}K_{val}^T}{\\sqrt{d_k}})V_{val}  \n",
    "\\\\]\n",
    "\\\\[ \n",
    "\\frac{\\partial Loss}{\\partial \\text{K}} =\\frac{\\partial Loss}{\\partial A} \\cdot \\frac{\\partial A}{\\partial K} \n",
    "\\\\] \n",
    "\\\\[ \n",
    "\\frac{\\partial A}{\\partial K}  = \\frac{\\partial \\sigma( \\frac{Q_{val}K_{val}^T}{\\sqrt{d_k}})}{\\partial K} \\cdot V_{val} \n",
    "\\\\] \n",
    "\\\\[ \n",
    "\\frac{\\partial \\sigma( \\frac{Q_{val}K_{val}^T}{\\sqrt{d_k}})}{\\partial K}   = \\sigma( \\frac{Q_{val}K_{val}^T}{\\sqrt{d_k}}) \\cdot [1-\\sigma( \\frac{Q_{val}K_{val}^T}{\\sqrt{d_k}})] \\cdot \n",
    "\\frac{\\partial (\\frac{Q_{val}K_{val}^T}{\\sqrt{d_k}})}{\\partial K}\n",
    "\\\\] \n",
    "\\\\[ \n",
    "\\frac{\\partial (\\frac{Q_{val}K_{val}^T}{\\sqrt{d_k}})}{\\partial K} = \\frac{\\partial (\\frac{Q_{val} \\cdot Inputs \\cdot K }{\\sqrt{d_k}})}{\\partial K}=\\frac{Q_{val} \\cdot Inputs}{\\sqrt{d_k}}\n",
    "\\\\]  \n",
    "\\\\[ \n",
    "\\frac{\\partial Loss}{\\partial \\text{K}} =\\frac{\\partial Loss}{\\partial A} \\cdot [\\sigma( \\frac{Q_{val}K_{val}^T}{\\sqrt{d_k}}) \\cdot [1-\\sigma( \\frac{Q_{val}K_{val}^T}{\\sqrt{d_k}})]] \\cdot \\frac{Q_{val} \\cdot Inputs}{\\sqrt{d_k}} \\cdot V_{val}\n",
    "\\\\] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "6104f84f-0b8e-4fde-b0f8-1c183e99cd67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02960017, -0.04259179, -0.03494746],\n",
       "       [-0.04455991, -0.06311051, -0.05248634],\n",
       "       [-0.03128414, -0.04405873, -0.03643295],\n",
       "       [-0.04187331, -0.05970672, -0.04942408],\n",
       "       [-0.03797199, -0.05424964, -0.04474604],\n",
       "       [-0.02203745, -0.03173937, -0.02579298]])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dQKscaled_dK=np.transpose(Qval,(0,2,1))@inputs/np.sqrt(dk)\n",
    "dAttention_dSoftmax=np.matmul(Attention_weights,(1-Attention_weights))\n",
    "dLoss_dK=np.mean(np.transpose(np.transpose(np.transpose(dL_dA,(0,2,1))@dAttention_dSoftmax,(0,2,1))@dQKscaled_dK,(0,2,1))@Vval,axis=0)\n",
    "dLoss_dK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74766bc9-de8b-48d3-b7c4-fd377c1508bf",
   "metadata": {},
   "source": [
    "Adding all the gradients in the backpropagation of our classifier we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "a76b2355-c353-4cfc-8167-e842ff53b484",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QKVAttentionClassifier: \n",
    "    def BackPropagation(self, dLoss_dSigma_Zout, inputs):\n",
    "        \n",
    "        # Gradient for linear layer\n",
    "        dlinear_dW = np.dot(self.phrase_representation.T, dLoss_dSigma_Zout)\n",
    "\n",
    "        # Gradient for bias\n",
    "        d_bias = np.sum(dLoss_dSigma_Zout, axis=0)\n",
    "        \n",
    "         # Gradient for phrase representation\n",
    "        d_phrase_rep = np.dot(dLoss_dSigma_Zout, self.linearlayer.T)\n",
    "\n",
    "        # Gradient for attention\n",
    "        dL_dA = np.array([np.outer(np.ones(self.words_per_phrase), d_phrase_rep[i, :]) for i in range(d_phrase_rep.shape[0])])\n",
    "\n",
    "        # Gradient for V\n",
    "        d_Vval=np.matmul(np.transpose(d_attention,(0,2,1)), self.Attention_weights) \n",
    "        dLoss_dV = np.mean(np.matmul(d_Vval,inputs),axis=0).T\n",
    "        \n",
    "        #Gradient of softmax\n",
    "        dAttention_dSoftmax=np.matmul(self.Attention_weights,(1-self.Attention_weights))\n",
    "        \n",
    "        #Gradient of Q\n",
    "        dQKscaled_dQ=np.matmul(np.transpose(inputs,(0,2,1)),self.Kval)/np.sqrt(self.dk)\n",
    "        dLoss_dQ=np.mean(np.transpose(np.transpose(np.transpose(dL_dA,(0,2,1))@dAttention_dSoftmax,(0,2,1))@np.transpose( dQKscaled_dQ,(0,2,1)),(0,2,1))@self.Vval,axis=0)\n",
    "        \n",
    "        #Gradient of K\n",
    "        dQKscaled_dK=np.transpose(self.Qval,(0,2,1))@inputs/np.sqrt(self.dk) \n",
    "        dLoss_dK=np.mean(np.transpose(np.transpose(np.transpose(dL_dA,(0,2,1))@dAttention_dSoftmax,(0,2,1))@dQKscaled_dK,(0,2,1))@Vself.val,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b98c1f-4438-4183-af8a-3855d863b94d",
   "metadata": {},
   "source": [
    "### Parameters update:\n",
    " \n",
    " \n",
    "Fixing a value for the learning rate \\eta we can now proceed to update the model parameters:\n",
    "\\\\[\n",
    "\\text{Q} = \\text{Q} - \\eta \\cdot \\frac{\\partial Loss}{\\partial Q}\\\\   \n",
    "\\\\]\n",
    "\\\\[ \n",
    "\\text{V} = \\text{V} - \\eta \\cdot \\frac{\\partial Loss}{\\partial V}\\\\ \n",
    "\\\\] \n",
    "\\\\[ \n",
    "\\text{K} = \\text{K} - \\eta \\cdot \\frac{\\partial Loss}{\\partial K}\\\\  \n",
    "\\\\] \n",
    "\\\\[ \n",
    "\\text{W} = \\text{W} - \\eta \\cdot \\frac{\\partial Loss}{\\partial W}\\\\  \n",
    "\\\\] \n",
    "\\\\[ \n",
    "\\text{b} = \\text{b} - \\eta \\cdot \\frac{\\partial Loss}{\\partial b}\\\\  \n",
    "\\\\]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "1c179624-513e-4b6e-81f2-0e468523eb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update weights\n",
    "learning_rate=0.001\n",
    "Q -= learning_rate * dLoss_dQ\n",
    "K -= learning_rate * dLoss_dK\n",
    "V -= learning_rate * dLoss_dV\n",
    "linearlayer -= learning_rate * d_linear\n",
    "linear_bias -= learning_rate * d_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5538f436-433c-419c-a253-c0efe256a5e0",
   "metadata": {},
   "source": [
    "Putting together all the steps we finally have the classification model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "df45d5ed-1d29-4f99-a24b-529148fe03f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QKVAttentionClassifier:\n",
    "    def __init__(self, word_len, words_per_phrase, batch_size, dk, dv, num_classes):\n",
    "\n",
    "        self.word_len = word_len\n",
    "        self.batch_size = batch_size\n",
    "        self.dk = dk\n",
    "        self.dv = dv\n",
    "        self.num_classes = num_classes\n",
    "        self.words_per_phrase = words_per_phrase\n",
    "        \n",
    "        # Initialize weights with Xavier/Glorot initialization\n",
    "        self.Q = np.random.randn(self.word_len, self.dk) / np.sqrt(self.word_len)  # * 0.01\n",
    "        self.K = np.random.randn(self.word_len, self.dk) / np.sqrt(self.word_len)  # * 0.01\n",
    "        self.V = np.random.randn(self.word_len, self.dk) / np.sqrt(self.word_len)  # * 0.01\n",
    "\n",
    "        # Initialize linear layer weights\n",
    "        self.linearlayer = np.random.randn(self.dk, self.num_classes) / np.sqrt(self.dk)\n",
    "        self.linear_bias = np.zeros(self.num_classes)\n",
    "\n",
    "    def softmax(self, x, axis=-1):\n",
    "        x = np.clip(x, -1e4, 1e4)  # Clip for numerical stability\n",
    "        e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "        return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "    def cross_entropy_loss(self, predictions, target):\n",
    "        # Cross-entropy loss for a batch of predictions and targets\n",
    "        batch_loss = -np.sum(target * np.log(predictions + 1e-9), axis=1)\n",
    "        return np.mean(batch_loss)\n",
    "\n",
    "    def AttentionHead(self, Inputs):\n",
    "        self.Qval = np.dot(Inputs, self.Q)\n",
    "        self.Kval = np.dot(Inputs, self.K)\n",
    "        self.Vval = np.dot(Inputs, self.V)\n",
    "\n",
    "        QKscaled = np.matmul(self.Qval, np.transpose(self.Kval, (0, 2, 1))) / np.sqrt(self.K.shape[1])\n",
    "        # QKscaled = np.clip(QKscaled, -1e2, 1e2)\n",
    "        self.Attention_weights = self.softmax(QKscaled)\n",
    "\n",
    "        return np.matmul(self.Attention_weights, self.Vval)\n",
    "\n",
    "    def LinearLayer(self):\n",
    "        output = np.matmul(self.phrase_representation, self.linearlayer) + self.linear_bias\n",
    "        return output\n",
    "\n",
    "    def forward(self, Inputs):\n",
    "\n",
    "        Attention = self.AttentionHead(Inputs)\n",
    "\n",
    "        self.phrase_representation = np.mean(Attention, axis=1)\n",
    "\n",
    "        Zout = self.LinearLayer()\n",
    "\n",
    "        Sigma_Zout = self.softmax(Zout)\n",
    "\n",
    "        return Sigma_Zout\n",
    "        \n",
    "    def BackPropagation(self, dLoss_dSigma_Zout, inputs):\n",
    "        \n",
    "        # Gradient for linear layer\n",
    "        dlinear_dW = np.dot(self.phrase_representation.T, dLoss_dSigma_Zout)\n",
    "\n",
    "        # Gradient for bias\n",
    "        d_bias = np.sum(dLoss_dSigma_Zout, axis=0)\n",
    "        \n",
    "         # Gradient for phrase representation\n",
    "        d_phrase_rep = np.dot(dLoss_dSigma_Zout, self.linearlayer.T)\n",
    "\n",
    "        # Gradient for attention\n",
    "        dL_dA = np.array([np.outer(np.ones(self.words_per_phrase), d_phrase_rep[i, :]) for i in range(d_phrase_rep.shape[0])])\n",
    "\n",
    "        # Gradient for V\n",
    "        d_Vval=np.matmul(np.transpose(dL_dA,(0,2,1)), self.Attention_weights) \n",
    "        dLoss_dV = np.mean(np.matmul(d_Vval,inputs),axis=0).T\n",
    "        \n",
    "        #Gradient of softmax\n",
    "        dAttention_dSoftmax=np.matmul(self.Attention_weights,(1-self.Attention_weights))\n",
    "        \n",
    "        #Gradient of Q\n",
    "        dQKscaled_dQ=np.matmul(np.transpose(inputs,(0,2,1)),self.Kval)/np.sqrt(self.dk)\n",
    "        dLoss_dQ=np.mean(np.transpose(np.transpose(np.transpose(dL_dA,(0,2,1))@dAttention_dSoftmax,(0,2,1))@np.transpose( dQKscaled_dQ,(0,2,1)),(0,2,1))@self.Vval,axis=0)\n",
    "        \n",
    "        #Gradient of K\n",
    "        dQKscaled_dK=np.transpose(self.Qval,(0,2,1))@inputs/np.sqrt(dk) \n",
    "        dLoss_dK=np.mean(np.transpose(np.transpose(np.transpose(dL_dA,(0,2,1))@dAttention_dSoftmax,(0,2,1))@dQKscaled_dK,(0,2,1))@self.Vval,axis=0)\n",
    "        \n",
    "        #Gradient clipping\n",
    "        clip_value = 10.0\n",
    "        dLoss_dQ = np.clip(dLoss_dQ, -clip_value, clip_value)\n",
    "        dLoss_dK = np.clip(dLoss_dK, -clip_value, clip_value)\n",
    "        dLoss_dV = np.clip(dLoss_dV, -clip_value, clip_value)\n",
    "        dlinear_dW = np.clip(dlinear_dW, -clip_value, clip_value)\n",
    "        d_bias = np.clip(d_bias, -clip_value, clip_value)\n",
    "\n",
    "        self.UpdateParams(dLoss_dQ,dLoss_dK,dLoss_dV,dlinear_dW,d_bias)\n",
    "\n",
    "    def UpdateParams(self, dLoss_dQ, dLoss_dK, dLoss_dV, dlinear_dW, d_bias):\n",
    "        self.Q -= self.learning_rate * dLoss_dQ\n",
    "        self.K -= self.learning_rate * dLoss_dK\n",
    "        self.V -= self.learning_rate * dLoss_dV\n",
    "        self.linearlayer -= self.learning_rate * dlinear_dW\n",
    "        self.linear_bias -= self.learning_rate * d_bias\n",
    "\n",
    "    def train(self, X_train, y_train, num_epochs, learning_rate=0.01):\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            total_loss = 0\n",
    "\n",
    "            num_batches_per_epoch = len(X_train) // self.batch_size\n",
    "\n",
    "            for i in tqdm(range(num_batches_per_epoch), desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "                start = i * self.batch_size\n",
    "                end = start + self.batch_size\n",
    "                X_batch = X_train[start:end]\n",
    "                y_batch = y_train[start:end]\n",
    "\n",
    "                yi = self.forward(X_batch)\n",
    "\n",
    "                Loss = self.cross_entropy_loss(yi, y_batch)\n",
    "                total_loss += Loss\n",
    "\n",
    "                dLoss_dSigma_Zout = yi - y_batch\n",
    "\n",
    "                self.BackPropagation(dLoss_dSigma_Zout, X_batch)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {(total_loss / num_batches_per_epoch):.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614bc7b6-8229-4c26-b4a7-7bbf02cf12b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45cab89-4a7a-49b2-8446-fa5d45449e82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b50a552-3422-4b37-8713-d4ec13830644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "73d48560-3e6e-4f76-b6fb-ea168a2988ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2898e162-809a-4934-8945-a7bebc9cf396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy \n",
    "# import numpy as np \n",
    "# import pandas as pd\n",
    "# df=pd.read_csv(\"data/bbc-text.csv\")\n",
    "# nlp = spacy.load('en_core_web_lg')\n",
    " \n",
    "# def preprocess_text(text, max_words=70):\n",
    "#     # Process the text using SpaCy\n",
    "#     doc = nlp(text)\n",
    "    \n",
    "#     # Filter out stopwords, punctuation, and spaces\n",
    "#     tokens = [token.text for token in doc if not token.is_stop and not token.is_punct and not token.is_space]\n",
    "    \n",
    "#     # Limit to the top 'max_words' words and pad if necessary\n",
    "#     if len(tokens) > max_words:\n",
    "#         tokens = tokens[:max_words]  # Keep the first max_words words\n",
    "#     else:\n",
    "#         tokens += ['<PAD>'] * (max_words - len(tokens))  # Pad the list with '<PAD>' token\n",
    "    \n",
    "#     # Join the tokens back into a string or return a list\n",
    "#     return tokens\n",
    "# df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "# df['processed_text']\n",
    "\n",
    "\n",
    "# inputs = [] \n",
    "# # Process each phrase in the 'category' column\n",
    "# for phrase in list(df['processed_text']):\n",
    "#     doc = nlp(\" \".join(phrase))  # Process the phrase with SpaCy\n",
    "#     # Extract word vectors\n",
    "#     matrix = np.array([token.vector for token in doc])\n",
    "#     inputs.append(matrix)\n",
    "\n",
    "\n",
    "\n",
    "# def get_train_test_data(data_dir):\n",
    "#     # Get the train data\n",
    "#     train_data = pd.read_json(f\"{data_dir}/train.json\")\n",
    "#     train_data.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "#     # Get the test data\n",
    "#     test_data = pd.read_json(f\"{data_dir}/test.json\")\n",
    "#     test_data.drop(['id'], axis=1, inplace=True)\n",
    "    \n",
    "#     return train_data, test_data\n",
    "\n",
    "# data_dir = \"corpus\"\n",
    "\n",
    "# train_data, test_data = get_train_test_data(data_dir)\n",
    "\n",
    "# # Take one example from the dataset and print it\n",
    "# example_summary, example_dialogue = train_data.iloc[10]\n",
    "# print(f\"Dialogue:\\n{example_dialogue}\")\n",
    "# print(f\"\\nSummary:\\n{example_summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba720f59-28b9-4c2f-b129-677a567505fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1446c00-9370-4bf4-ad93-ae0f79e846f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b036f928-2902-4ece-92f6-abf185e4fe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('c:\\\\python312\\\\lib\\\\site-packages')\n",
    "import pickle\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    # Subtract the max value for numerical stability\n",
    "    x = np.clip(x, -1500, 1500)\n",
    "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "def cross_entropy_loss(predictions, target):\n",
    "    return -np.sum(target * np.log(predictions + 1e-9))  # Adding a small constant to avoid log(0)\n",
    "\n",
    "\n",
    "df=pd.read_csv(\"data/bbc-text.csv\")\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "with open('data/InputProcessed.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "\n",
    "y = np.array(pd.get_dummies(df[\"category\"], dtype=int))\n",
    "tts=0.85\n",
    "X_train,X_test=X[0:round(tts*len(X))],X[round(tts*len(X)):]\n",
    "y_train,y_test=y[0:round(tts*len(X))],y[round(tts*len(X)):]\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "68c3a014-7c52-47d6-b0f0-4aab54cf6c1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|██████████████████████████████████████████████████████████████████████| 59/59 [00:12<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 3.9633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|██████████████████████████████████████████████████████████████████████| 59/59 [00:12<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30, Loss: 2.7801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: 100%|██████████████████████████████████████████████████████████████████████| 59/59 [00:11<00:00,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30, Loss: 0.6913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: 100%|██████████████████████████████████████████████████████████████████████| 59/59 [00:10<00:00,  5.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30, Loss: 0.3028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30: 100%|██████████████████████████████████████████████████████████████████████| 59/59 [00:10<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30, Loss: 0.2236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30: 100%|██████████████████████████████████████████████████████████████████████| 59/59 [00:10<00:00,  5.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30, Loss: 0.1853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30: 100%|██████████████████████████████████████████████████████████████████████| 59/59 [00:11<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30, Loss: 0.1632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30: 100%|██████████████████████████████████████████████████████████████████████| 59/59 [00:10<00:00,  5.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30, Loss: 0.1474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30: 100%|██████████████████████████████████████████████████████████████████████| 59/59 [00:11<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30, Loss: 0.1332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30: 100%|█████████████████████████████████████████████████████████████████████| 59/59 [00:11<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30, Loss: 0.1227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30: 100%|█████████████████████████████████████████████████████████████████████| 59/59 [00:12<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30, Loss: 0.1150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30: 100%|█████████████████████████████████████████████████████████████████████| 59/59 [00:11<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30, Loss: 0.1072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30: 100%|█████████████████████████████████████████████████████████████████████| 59/59 [00:10<00:00,  5.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30, Loss: 0.1001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30: 100%|█████████████████████████████████████████████████████████████████████| 59/59 [00:10<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30, Loss: 0.0946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30: 100%|█████████████████████████████████████████████████████████████████████| 59/59 [00:10<00:00,  5.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30, Loss: 0.0899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30: 100%|█████████████████████████████████████████████████████████████████████| 59/59 [00:10<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30, Loss: 0.0843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30: 100%|█████████████████████████████████████████████████████████████████████| 59/59 [00:10<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30, Loss: 0.0802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30: 100%|█████████████████████████████████████████████████████████████████████| 59/59 [00:10<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30, Loss: 0.0760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30: 100%|█████████████████████████████████████████████████████████████████████| 59/59 [00:09<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30, Loss: 0.0724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30: 100%|█████████████████████████████████████████████████████████████████████| 59/59 [00:10<00:00,  5.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30, Loss: 0.0686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30: 100%|█████████████████████████████████████████████████████████████████████| 59/59 [00:10<00:00,  5.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30, Loss: 0.0652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30: 100%|█████████████████████████████████████████████████████████████████████| 59/59 [00:10<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30, Loss: 0.0620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30: 100%|█████████████████████████████████████████████████████████████████████| 59/59 [00:09<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30, Loss: 0.0588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30: 100%|█████████████████████████████████████████████████████████████████████| 59/59 [00:10<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30, Loss: 0.0563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30: 100%|█████████████████████████████████████████████████████████████████████| 59/59 [00:10<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30, Loss: 0.0540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30: 100%|█████████████████████████████████████████████████████████████████████| 59/59 [00:10<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30, Loss: 0.0515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30: 100%|█████████████████████████████████████████████████████████████████████| 59/59 [00:11<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30, Loss: 0.0488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30: 100%|█████████████████████████████████████████████████████████████████████| 59/59 [00:11<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30, Loss: 0.0465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30: 100%|█████████████████████████████████████████████████████████████████████| 59/59 [00:08<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30, Loss: 0.0443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30: 100%|█████████████████████████████████████████████████████████████████████| 59/59 [00:10<00:00,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30, Loss: 0.0426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class QKVAttentionClassifier:\n",
    "    def __init__(self, word_len, words_per_phrase, batch_size, dk, dv, num_classes):\n",
    "\n",
    "        self.word_len = word_len\n",
    "        self.batch_size = batch_size\n",
    "        self.dk = dk\n",
    "        self.dv = dv\n",
    "        self.num_classes = num_classes\n",
    "        self.words_per_phrase = words_per_phrase\n",
    "        \n",
    "        # Initialize weights with Xavier/Glorot initialization\n",
    "        self.Q = np.random.randn(self.word_len, self.dk) / np.sqrt(self.word_len)  # * 0.01\n",
    "        self.K = np.random.randn(self.word_len, self.dk) / np.sqrt(self.word_len)  # * 0.01\n",
    "        self.V = np.random.randn(self.word_len, self.dk) / np.sqrt(self.word_len)  # * 0.01\n",
    "\n",
    "        # Initialize linear layer weights\n",
    "        self.linearlayer = np.random.randn(self.dk, self.num_classes) / np.sqrt(self.dk)\n",
    "        self.linear_bias = np.zeros(self.num_classes)\n",
    "\n",
    "    def softmax(self, x, axis=-1):\n",
    "        x = np.clip(x, -1e4, 1e4)  # Clip for numerical stability\n",
    "        e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "        return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "    def cross_entropy_loss(self, predictions, target):\n",
    "        # Cross-entropy loss for a batch of predictions and targets\n",
    "        batch_loss = -np.sum(target * np.log(predictions + 1e-9), axis=1)\n",
    "        return np.mean(batch_loss)\n",
    "\n",
    "    def AttentionHead(self, Inputs):\n",
    "        self.Qval = np.dot(Inputs, self.Q)\n",
    "        self.Kval = np.dot(Inputs, self.K)\n",
    "        self.Vval = np.dot(Inputs, self.V)\n",
    "\n",
    "        QKscaled = np.matmul(self.Qval, np.transpose(self.Kval, (0, 2, 1))) / np.sqrt(self.K.shape[1])\n",
    "        # QKscaled = np.clip(QKscaled, -1e2, 1e2)\n",
    "        self.Attention_weights = self.softmax(QKscaled)\n",
    "\n",
    "        return np.matmul(self.Attention_weights, self.Vval)\n",
    "\n",
    "    def LinearLayer(self):\n",
    "        output = np.matmul(self.phrase_representation, self.linearlayer) + self.linear_bias\n",
    "        return output\n",
    "\n",
    "    def forward(self, Inputs):\n",
    "\n",
    "        Attention = self.AttentionHead(Inputs)\n",
    "\n",
    "        self.phrase_representation = np.mean(Attention, axis=1)\n",
    "\n",
    "        Zout = self.LinearLayer()\n",
    "\n",
    "        Sigma_Zout = self.softmax(Zout)\n",
    "\n",
    "        return Sigma_Zout\n",
    "        \n",
    "    def BackPropagation(self, dLoss_dSigma_Zout, inputs):\n",
    "        \n",
    "        # Gradient for linear layer\n",
    "        dlinear_dW = np.dot(self.phrase_representation.T, dLoss_dSigma_Zout)\n",
    "\n",
    "        # Gradient for bias\n",
    "        d_bias = np.sum(dLoss_dSigma_Zout, axis=0)\n",
    "        \n",
    "         # Gradient for phrase representation\n",
    "        d_phrase_rep = np.dot(dLoss_dSigma_Zout, self.linearlayer.T)\n",
    "\n",
    "        # Gradient for attention\n",
    "        dL_dA = np.array([np.outer(np.ones(self.words_per_phrase), d_phrase_rep[i, :]) for i in range(d_phrase_rep.shape[0])])\n",
    "\n",
    "        # Gradient for V\n",
    "        d_Vval=np.matmul(np.transpose(dL_dA,(0,2,1)), self.Attention_weights) \n",
    "        dLoss_dV = np.mean(np.matmul(d_Vval,inputs),axis=0).T\n",
    "        \n",
    "        #Gradient of softmax\n",
    "        dAttention_dSoftmax=np.matmul(self.Attention_weights,(1-self.Attention_weights))\n",
    "        \n",
    "        #Gradient of Q\n",
    "        dQKscaled_dQ=np.matmul(np.transpose(inputs,(0,2,1)),self.Kval)/np.sqrt(self.dk)\n",
    "        dLoss_dQ=np.mean(np.transpose(np.transpose(np.transpose(dL_dA,(0,2,1))@dAttention_dSoftmax,(0,2,1))@np.transpose( dQKscaled_dQ,(0,2,1)),(0,2,1))@self.Vval,axis=0)\n",
    "        \n",
    "        #Gradient of K\n",
    "        dQKscaled_dK=np.transpose(self.Qval,(0,2,1))@inputs/np.sqrt(self.dk) \n",
    "        dLoss_dK=np.mean(np.transpose(np.transpose(np.transpose(dL_dA,(0,2,1))@dAttention_dSoftmax,(0,2,1))@dQKscaled_dK,(0,2,1))@self.Vval,axis=0)\n",
    "        \n",
    "        #Gradient clipping\n",
    "        clip_value = 10.0\n",
    "        dLoss_dQ = np.clip(dLoss_dQ, -clip_value, clip_value)\n",
    "        dLoss_dK = np.clip(dLoss_dK, -clip_value, clip_value)\n",
    "        dLoss_dV = np.clip(dLoss_dV, -clip_value, clip_value)\n",
    "        dlinear_dW = np.clip(dlinear_dW, -clip_value, clip_value)\n",
    "        d_bias = np.clip(d_bias, -clip_value, clip_value)\n",
    "\n",
    "        self.UpdateParams(dLoss_dQ,dLoss_dK,dLoss_dV,dlinear_dW,d_bias)\n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "    def UpdateParams(self, dLoss_dQ, dLoss_dK, dLoss_dV, dlinear_dW, d_bias):\n",
    "        self.Q -= self.learning_rate * dLoss_dQ\n",
    "        self.K -= self.learning_rate * dLoss_dK\n",
    "        self.V -= self.learning_rate * dLoss_dV\n",
    "        self.linearlayer -= self.learning_rate * dlinear_dW\n",
    "        self.linear_bias -= self.learning_rate * d_bias\n",
    "\n",
    "    def train(self, X_train, y_train, num_epochs, learning_rate=0.01):\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            total_loss = 0\n",
    "\n",
    "            num_batches_per_epoch = len(X_train) // self.batch_size\n",
    "\n",
    "            for i in tqdm(range(num_batches_per_epoch), desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "                start = i * self.batch_size\n",
    "                end = start + self.batch_size\n",
    "                X_batch = X_train[start:end]\n",
    "                y_batch = y_train[start:end]\n",
    "\n",
    "                yi = self.forward(X_batch)\n",
    "\n",
    "                Loss = self.cross_entropy_loss(yi, y_batch)\n",
    "                total_loss += Loss\n",
    "\n",
    "                dLoss_dSigma_Zout = yi - y_batch\n",
    "\n",
    "                self.BackPropagation(dLoss_dSigma_Zout, X_batch)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {(total_loss / num_batches_per_epoch):.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    " \n",
    "\n",
    "\n",
    "def pad_sequences(sequences, max_len):\n",
    "    padded_sequences = np.zeros((len(sequences), max_len, sequences[0].shape[1]))\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = min(seq.shape[0], max_len)\n",
    "        padded_sequences[i, :length] = seq[:length]\n",
    "    return padded_sequences\n",
    "\n",
    "word_len = 300\n",
    "word_per_phrase=70\n",
    "dk = 32\n",
    "dv = 150\n",
    "batch_size = 32\n",
    "num_classes = 5\n",
    "max_seq_length = 70\n",
    "\n",
    "# Assuming X_train and y_train are your training data and labels\n",
    "X_train_padded = pad_sequences(X_train, max_seq_length)\n",
    " \n",
    "\n",
    "model = QKVAttentionClassifier(word_len, word_per_phrase, batch_size, dk, dv, num_classes)\n",
    "model.train(X_train_padded, y_train, num_epochs=30, learning_rate=0.001)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "389c7910-755c-4c1e-8add-6bd7ddde5f3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "4c494b7e-bc42-4def-8cec-a5a87318e67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "01641a4a-e67b-49a4-ac2e-a022304ec8e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test= pad_sequences(X_test, max_seq_length)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Convert one-hot encoded true labels to class labels\n",
    "y_true = np.argmax(y_test, axis=1) \n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=np.arange(5))\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.arange(5), yticklabels=np.arange(5))\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e7ceaa2-e90f-4522-a6eb-5c3fabb752cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test= pad_sequences(X_test, max_seq_length)\n",
    "\n",
    "predictions = classifier.predict(X_test)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Convert one-hot encoded true labels to class labels\n",
    "y_true = np.argmax(y_test, axis=1) \n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=np.arange(5))\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.arange(5), yticklabels=np.arange(5))\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a1638f-fa3c-48aa-8314-e5f934c8c0dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
