{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ba3ebc-2725-445f-9b98-bc4740981009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>business</td>\n",
       "      <td>cars pull down us retail figures us retail sal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>politics</td>\n",
       "      <td>kilroy unveils immigration policy ex-chatshow ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>rem announce new glasgow concert us band rem h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>politics</td>\n",
       "      <td>how political squabbles snowball it s become c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>sport</td>\n",
       "      <td>souness delight at euro progress boss graeme s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           category                                               text\n",
       "0              tech  tv future in the hands of viewers with home th...\n",
       "1          business  worldcom boss  left books alone  former worldc...\n",
       "2             sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3             sport  yeading face newcastle in fa cup premiership s...\n",
       "4     entertainment  ocean s twelve raids box office ocean s twelve...\n",
       "...             ...                                                ...\n",
       "2220       business  cars pull down us retail figures us retail sal...\n",
       "2221       politics  kilroy unveils immigration policy ex-chatshow ...\n",
       "2222  entertainment  rem announce new glasgow concert us band rem h...\n",
       "2223       politics  how political squabbles snowball it s become c...\n",
       "2224          sport  souness delight at euro progress boss graeme s...\n",
       "\n",
       "[2225 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from functools import partial\n",
    "import sys\n",
    "sys.path.append('c:\\\\python312\\\\lib\\\\site-packages')\n",
    "# Load the data\n",
    "df = pd.read_csv(\"data/bbc-text.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7a131eb-97b7-4984-9795-bc1aeb0e336b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tv',\n",
       " 'future',\n",
       " 'in',\n",
       " 'the',\n",
       " 'hands',\n",
       " 'of',\n",
       " 'viewers',\n",
       " 'with',\n",
       " 'home',\n",
       " 'theatre',\n",
       " 'systems',\n",
       " 'plasma',\n",
       " 'high',\n",
       " 'definition',\n",
       " 'tvs',\n",
       " 'and',\n",
       " 'digital',\n",
       " 'video',\n",
       " 'recorders',\n",
       " 'moving',\n",
       " 'into',\n",
       " 'the',\n",
       " 'living',\n",
       " 'room',\n",
       " 'the',\n",
       " 'way',\n",
       " 'people',\n",
       " 'watch',\n",
       " 'tv',\n",
       " 'will',\n",
       " 'be',\n",
       " 'radically',\n",
       " 'different',\n",
       " 'in',\n",
       " 'five',\n",
       " 'years',\n",
       " 'time',\n",
       " 'that',\n",
       " 'is',\n",
       " 'according',\n",
       " 'to',\n",
       " 'an',\n",
       " 'expert',\n",
       " 'panel',\n",
       " 'which',\n",
       " 'gathered',\n",
       " 'at',\n",
       " 'the',\n",
       " 'annual',\n",
       " 'consumer',\n",
       " 'electronics',\n",
       " 'show',\n",
       " 'in',\n",
       " 'las',\n",
       " 'vegas',\n",
       " 'to',\n",
       " 'discuss',\n",
       " 'how',\n",
       " 'these',\n",
       " 'new',\n",
       " 'technologies',\n",
       " 'will',\n",
       " 'impact',\n",
       " 'one',\n",
       " 'of',\n",
       " 'our',\n",
       " 'favourite',\n",
       " 'pastimes',\n",
       " 'with',\n",
       " 'the',\n",
       " 'us',\n",
       " 'leading',\n",
       " 'the',\n",
       " 'trend',\n",
       " 'programmes',\n",
       " 'and',\n",
       " 'other',\n",
       " 'content',\n",
       " 'will',\n",
       " 'be',\n",
       " 'delivered',\n",
       " 'to',\n",
       " 'viewers',\n",
       " 'via',\n",
       " 'home',\n",
       " 'networks',\n",
       " 'through',\n",
       " 'cable',\n",
       " 'satellite',\n",
       " 'telecoms',\n",
       " 'companies',\n",
       " 'and',\n",
       " 'broadband',\n",
       " 'service',\n",
       " 'providers',\n",
       " 'to',\n",
       " 'front',\n",
       " 'rooms',\n",
       " 'and',\n",
       " 'portable',\n",
       " 'devices',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'most',\n",
       " 'talked',\n",
       " 'about',\n",
       " 'technologies',\n",
       " 'of',\n",
       " 'ces',\n",
       " 'has',\n",
       " 'been',\n",
       " 'digital',\n",
       " 'and',\n",
       " 'personal',\n",
       " 'video',\n",
       " 'recorders',\n",
       " 'dvr',\n",
       " 'and',\n",
       " 'pvr',\n",
       " 'these',\n",
       " 'set',\n",
       " 'top',\n",
       " 'boxes',\n",
       " 'like',\n",
       " 'the',\n",
       " 'us',\n",
       " 's',\n",
       " 'tivo',\n",
       " 'and',\n",
       " 'the',\n",
       " 'uk',\n",
       " 's',\n",
       " 'sky',\n",
       " 'system',\n",
       " 'allow',\n",
       " 'people',\n",
       " 'to',\n",
       " 'record',\n",
       " 'store',\n",
       " 'play',\n",
       " 'pause',\n",
       " 'and',\n",
       " 'forward',\n",
       " 'wind',\n",
       " 'tv',\n",
       " 'programmes',\n",
       " 'when',\n",
       " 'they',\n",
       " 'want',\n",
       " 'essentially',\n",
       " 'the',\n",
       " 'technology',\n",
       " 'allows',\n",
       " 'for',\n",
       " 'much',\n",
       " 'more',\n",
       " 'personalised',\n",
       " 'tv',\n",
       " 'they',\n",
       " 'are',\n",
       " 'also',\n",
       " 'being',\n",
       " 'built',\n",
       " 'in',\n",
       " 'to',\n",
       " 'high',\n",
       " 'definition',\n",
       " 'tv',\n",
       " 'sets',\n",
       " 'which',\n",
       " 'are',\n",
       " 'big',\n",
       " 'business',\n",
       " 'in',\n",
       " 'japan',\n",
       " 'and',\n",
       " 'the',\n",
       " 'us',\n",
       " 'but',\n",
       " 'slower',\n",
       " 'to',\n",
       " 'take',\n",
       " 'off',\n",
       " 'in',\n",
       " 'europe',\n",
       " 'because',\n",
       " 'of',\n",
       " 'the',\n",
       " 'lack',\n",
       " 'of',\n",
       " 'high',\n",
       " 'definition',\n",
       " 'programming',\n",
       " 'not',\n",
       " 'only',\n",
       " 'can',\n",
       " 'people',\n",
       " 'forward',\n",
       " 'wind',\n",
       " 'through',\n",
       " 'adverts',\n",
       " 'they',\n",
       " 'can',\n",
       " 'also',\n",
       " 'forget',\n",
       " 'about',\n",
       " 'abiding',\n",
       " 'by',\n",
       " 'network',\n",
       " 'and',\n",
       " 'channel',\n",
       " 'schedules',\n",
       " 'putting',\n",
       " 'together',\n",
       " 'their',\n",
       " 'own',\n",
       " 'a',\n",
       " 'la',\n",
       " 'carte',\n",
       " 'entertainment',\n",
       " 'but',\n",
       " 'some',\n",
       " 'us',\n",
       " 'networks',\n",
       " 'and',\n",
       " 'cable',\n",
       " 'and',\n",
       " 'satellite',\n",
       " 'companies',\n",
       " 'are',\n",
       " 'worried',\n",
       " 'about',\n",
       " 'what',\n",
       " 'it',\n",
       " 'means',\n",
       " 'for',\n",
       " 'them',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'advertising',\n",
       " 'revenues',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'brand',\n",
       " 'identity',\n",
       " 'and',\n",
       " 'viewer',\n",
       " 'loyalty',\n",
       " 'to',\n",
       " 'channels',\n",
       " 'although',\n",
       " 'the',\n",
       " 'us',\n",
       " 'leads',\n",
       " 'in',\n",
       " 'this',\n",
       " 'technology',\n",
       " 'at',\n",
       " 'the',\n",
       " 'moment',\n",
       " 'it',\n",
       " 'is',\n",
       " 'also',\n",
       " 'a',\n",
       " 'concern',\n",
       " 'that',\n",
       " 'is',\n",
       " 'being',\n",
       " 'raised',\n",
       " 'in',\n",
       " 'europe',\n",
       " 'particularly',\n",
       " 'with',\n",
       " 'the',\n",
       " 'growing',\n",
       " 'uptake',\n",
       " 'of',\n",
       " 'services',\n",
       " 'like',\n",
       " 'sky',\n",
       " 'what',\n",
       " 'happens',\n",
       " 'here',\n",
       " 'today',\n",
       " 'we',\n",
       " 'will',\n",
       " 'see',\n",
       " 'in',\n",
       " 'nine',\n",
       " 'months',\n",
       " 'to',\n",
       " 'a',\n",
       " 'years',\n",
       " 'time',\n",
       " 'in',\n",
       " 'the',\n",
       " 'uk',\n",
       " 'adam',\n",
       " 'hume',\n",
       " 'the',\n",
       " 'bbc',\n",
       " 'broadcast',\n",
       " 's',\n",
       " 'futurologist',\n",
       " 'told',\n",
       " 'the',\n",
       " 'bbc',\n",
       " 'news',\n",
       " 'website',\n",
       " 'for',\n",
       " 'the',\n",
       " 'likes',\n",
       " 'of',\n",
       " 'the',\n",
       " 'bbc',\n",
       " 'there',\n",
       " 'are',\n",
       " 'no',\n",
       " 'issues',\n",
       " 'of',\n",
       " 'lost',\n",
       " 'advertising',\n",
       " 'revenue',\n",
       " 'yet',\n",
       " 'it',\n",
       " 'is',\n",
       " 'a',\n",
       " 'more',\n",
       " 'pressing',\n",
       " 'issue',\n",
       " 'at',\n",
       " 'the',\n",
       " 'moment',\n",
       " 'for',\n",
       " 'commercial',\n",
       " 'uk',\n",
       " 'broadcasters',\n",
       " 'but',\n",
       " 'brand',\n",
       " 'loyalty',\n",
       " 'is',\n",
       " 'important',\n",
       " 'for',\n",
       " 'everyone',\n",
       " 'we',\n",
       " 'will',\n",
       " 'be',\n",
       " 'talking',\n",
       " 'more',\n",
       " 'about',\n",
       " 'content',\n",
       " 'brands',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'network',\n",
       " 'brands',\n",
       " 'said',\n",
       " 'tim',\n",
       " 'hanlon',\n",
       " 'from',\n",
       " 'brand',\n",
       " 'communications',\n",
       " 'firm',\n",
       " 'starcom',\n",
       " 'mediavest',\n",
       " 'the',\n",
       " 'reality',\n",
       " 'is',\n",
       " 'that',\n",
       " 'with',\n",
       " 'broadband',\n",
       " 'connections',\n",
       " 'anybody',\n",
       " 'can',\n",
       " 'be',\n",
       " 'the',\n",
       " 'producer',\n",
       " 'of',\n",
       " 'content',\n",
       " 'he',\n",
       " 'added',\n",
       " 'the',\n",
       " 'challenge',\n",
       " 'now',\n",
       " 'is',\n",
       " 'that',\n",
       " 'it',\n",
       " 'is',\n",
       " 'hard',\n",
       " 'to',\n",
       " 'promote',\n",
       " 'a',\n",
       " 'programme',\n",
       " 'with',\n",
       " 'so',\n",
       " 'much',\n",
       " 'choice',\n",
       " 'what',\n",
       " 'this',\n",
       " 'means',\n",
       " 'said',\n",
       " 'stacey',\n",
       " 'jolna',\n",
       " 'senior',\n",
       " 'vice',\n",
       " 'president',\n",
       " 'of',\n",
       " 'tv',\n",
       " 'guide',\n",
       " 'tv',\n",
       " 'group',\n",
       " 'is',\n",
       " 'that',\n",
       " 'the',\n",
       " 'way',\n",
       " 'people',\n",
       " 'find',\n",
       " 'the',\n",
       " 'content',\n",
       " 'they',\n",
       " 'want',\n",
       " 'to',\n",
       " 'watch',\n",
       " 'has',\n",
       " 'to',\n",
       " 'be',\n",
       " 'simplified',\n",
       " 'for',\n",
       " 'tv',\n",
       " 'viewers',\n",
       " 'it',\n",
       " 'means',\n",
       " 'that',\n",
       " 'networks',\n",
       " 'in',\n",
       " 'us',\n",
       " 'terms',\n",
       " 'or',\n",
       " 'channels',\n",
       " 'could',\n",
       " 'take',\n",
       " 'a',\n",
       " 'leaf',\n",
       " 'out',\n",
       " 'of',\n",
       " 'google',\n",
       " 's',\n",
       " 'book',\n",
       " 'and',\n",
       " 'be',\n",
       " 'the',\n",
       " 'search',\n",
       " 'engine',\n",
       " 'of',\n",
       " 'the',\n",
       " 'future',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'the',\n",
       " 'scheduler',\n",
       " 'to',\n",
       " 'help',\n",
       " 'people',\n",
       " 'find',\n",
       " 'what',\n",
       " 'they',\n",
       " 'want',\n",
       " 'to',\n",
       " 'watch',\n",
       " 'this',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'channel',\n",
       " 'model',\n",
       " 'might',\n",
       " 'work',\n",
       " 'for',\n",
       " 'the',\n",
       " 'younger',\n",
       " 'ipod',\n",
       " 'generation',\n",
       " 'which',\n",
       " 'is',\n",
       " 'used',\n",
       " 'to',\n",
       " 'taking',\n",
       " 'control',\n",
       " 'of',\n",
       " 'their',\n",
       " 'gadgets',\n",
       " 'and',\n",
       " 'what',\n",
       " 'they',\n",
       " 'play',\n",
       " 'on',\n",
       " 'them',\n",
       " 'but',\n",
       " 'it',\n",
       " 'might',\n",
       " 'not',\n",
       " 'suit',\n",
       " 'everyone',\n",
       " 'the',\n",
       " 'panel',\n",
       " 'recognised',\n",
       " 'older',\n",
       " 'generations',\n",
       " 'are',\n",
       " 'more',\n",
       " 'comfortable',\n",
       " 'with',\n",
       " 'familiar',\n",
       " 'schedules',\n",
       " 'and',\n",
       " 'channel',\n",
       " 'brands',\n",
       " 'because',\n",
       " 'they',\n",
       " 'know',\n",
       " 'what',\n",
       " 'they',\n",
       " 'are',\n",
       " 'getting',\n",
       " 'they',\n",
       " 'perhaps',\n",
       " 'do',\n",
       " 'not',\n",
       " 'want',\n",
       " 'so',\n",
       " 'much',\n",
       " 'of',\n",
       " 'the',\n",
       " 'choice',\n",
       " 'put',\n",
       " 'into',\n",
       " 'their',\n",
       " 'hands',\n",
       " 'mr',\n",
       " 'hanlon',\n",
       " 'suggested',\n",
       " 'on',\n",
       " 'the',\n",
       " 'other',\n",
       " 'end',\n",
       " 'you',\n",
       " 'have',\n",
       " 'the',\n",
       " 'kids',\n",
       " 'just',\n",
       " 'out',\n",
       " 'of',\n",
       " 'diapers',\n",
       " 'who',\n",
       " 'are',\n",
       " 'pushing',\n",
       " 'buttons',\n",
       " 'already',\n",
       " 'everything',\n",
       " 'is',\n",
       " 'possible',\n",
       " 'and',\n",
       " 'available',\n",
       " 'to',\n",
       " 'them',\n",
       " 'said',\n",
       " 'mr',\n",
       " 'hanlon',\n",
       " 'ultimately',\n",
       " 'the',\n",
       " 'consumer',\n",
       " 'will',\n",
       " 'tell',\n",
       " 'the',\n",
       " 'market',\n",
       " 'they',\n",
       " 'want',\n",
       " 'of',\n",
       " 'the',\n",
       " '50',\n",
       " '000',\n",
       " 'new',\n",
       " 'gadgets',\n",
       " 'and',\n",
       " 'technologies',\n",
       " 'being',\n",
       " 'showcased',\n",
       " 'at',\n",
       " 'ces',\n",
       " 'many',\n",
       " 'of',\n",
       " 'them',\n",
       " 'are',\n",
       " 'about',\n",
       " 'enhancing',\n",
       " 'the',\n",
       " 'tv',\n",
       " 'watching',\n",
       " 'experience',\n",
       " 'high',\n",
       " 'definition',\n",
       " 'tv',\n",
       " 'sets',\n",
       " 'are',\n",
       " 'everywhere',\n",
       " 'and',\n",
       " 'many',\n",
       " 'new',\n",
       " 'models',\n",
       " 'of',\n",
       " 'lcd',\n",
       " 'liquid',\n",
       " 'crystal',\n",
       " 'display',\n",
       " 'tvs',\n",
       " 'have',\n",
       " 'been',\n",
       " 'launched',\n",
       " 'with',\n",
       " 'dvr',\n",
       " 'capability',\n",
       " 'built',\n",
       " 'into',\n",
       " 'them',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'being',\n",
       " 'external',\n",
       " 'boxes',\n",
       " 'one',\n",
       " 'such',\n",
       " 'example',\n",
       " 'launched',\n",
       " 'at',\n",
       " 'the',\n",
       " 'show',\n",
       " 'is',\n",
       " 'humax',\n",
       " 's',\n",
       " '26',\n",
       " 'inch',\n",
       " 'lcd',\n",
       " 'tv',\n",
       " 'with',\n",
       " 'an',\n",
       " '80',\n",
       " 'hour',\n",
       " 'tivo',\n",
       " 'dvr',\n",
       " 'and',\n",
       " 'dvd',\n",
       " 'recorder',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'us',\n",
       " 's',\n",
       " 'biggest',\n",
       " 'satellite',\n",
       " 'tv',\n",
       " 'companies',\n",
       " 'directtv',\n",
       " 'has',\n",
       " 'even',\n",
       " 'launched',\n",
       " 'its',\n",
       " 'own',\n",
       " 'branded',\n",
       " 'dvr',\n",
       " 'at',\n",
       " 'the',\n",
       " 'show',\n",
       " 'with',\n",
       " '100',\n",
       " 'hours',\n",
       " 'of',\n",
       " 'recording',\n",
       " 'capability',\n",
       " 'instant',\n",
       " 'replay',\n",
       " 'and',\n",
       " 'a',\n",
       " 'search',\n",
       " 'function',\n",
       " 'the',\n",
       " 'set',\n",
       " 'can',\n",
       " 'pause',\n",
       " 'and',\n",
       " 'rewind',\n",
       " 'tv',\n",
       " 'for',\n",
       " 'up',\n",
       " 'to',\n",
       " '90',\n",
       " 'hours',\n",
       " 'and',\n",
       " 'microsoft',\n",
       " 'chief',\n",
       " 'bill',\n",
       " 'gates',\n",
       " 'announced',\n",
       " 'in',\n",
       " 'his',\n",
       " 'pre',\n",
       " 'show',\n",
       " 'keynote',\n",
       " 'speech',\n",
       " 'a',\n",
       " 'partnership',\n",
       " 'with',\n",
       " 'tivo',\n",
       " 'called',\n",
       " 'tivotogo',\n",
       " 'which',\n",
       " 'means',\n",
       " 'people',\n",
       " 'can',\n",
       " 'play',\n",
       " 'recorded',\n",
       " 'programmes',\n",
       " 'on',\n",
       " 'windows',\n",
       " 'pcs',\n",
       " 'and',\n",
       " 'mobile',\n",
       " 'devices',\n",
       " 'all',\n",
       " 'these',\n",
       " 'reflect',\n",
       " 'the',\n",
       " 'increasing',\n",
       " 'trend',\n",
       " 'of',\n",
       " 'freeing',\n",
       " 'up',\n",
       " 'multimedia',\n",
       " 'so',\n",
       " 'that',\n",
       " 'people',\n",
       " 'can',\n",
       " 'watch',\n",
       " 'what',\n",
       " 'they',\n",
       " 'want',\n",
       " 'when',\n",
       " 'they',\n",
       " 'want',\n",
       " 'worldcom',\n",
       " 'boss',\n",
       " 'left',\n",
       " 'books',\n",
       " 'alone',\n",
       " 'former',\n",
       " 'worldcom',\n",
       " 'boss',\n",
       " 'bernie',\n",
       " 'ebbers',\n",
       " 'who',\n",
       " 'is',\n",
       " 'accused',\n",
       " 'of',\n",
       " 'overseeing',\n",
       " 'an',\n",
       " '11bn',\n",
       " '5',\n",
       " '8bn',\n",
       " 'fraud',\n",
       " 'never',\n",
       " 'made',\n",
       " 'accounting',\n",
       " 'decisions',\n",
       " 'a',\n",
       " 'witness',\n",
       " 'has',\n",
       " 'told',\n",
       " 'jurors',\n",
       " 'david',\n",
       " 'myers',\n",
       " 'made',\n",
       " 'the',\n",
       " 'comments',\n",
       " 'under',\n",
       " 'questioning',\n",
       " 'by',\n",
       " 'defence',\n",
       " 'lawyers',\n",
       " 'who',\n",
       " 'have',\n",
       " 'been',\n",
       " 'arguing',\n",
       " 'that',\n",
       " 'mr',\n",
       " 'ebbers',\n",
       " 'was',\n",
       " 'not',\n",
       " 'responsible',\n",
       " 'for',\n",
       " 'worldcom',\n",
       " 's',\n",
       " 'problems',\n",
       " 'the',\n",
       " 'phone',\n",
       " 'company',\n",
       " 'collapsed',\n",
       " 'in',\n",
       " '2002',\n",
       " 'and',\n",
       " 'prosecutors',\n",
       " 'claim',\n",
       " 'that',\n",
       " 'losses',\n",
       " 'were',\n",
       " 'hidden',\n",
       " 'to',\n",
       " 'protect',\n",
       " 'the',\n",
       " 'firm',\n",
       " 's',\n",
       " 'shares',\n",
       " 'mr',\n",
       " 'myers',\n",
       " 'has',\n",
       " 'already',\n",
       " 'pleaded',\n",
       " 'guilty',\n",
       " 'to',\n",
       " 'fraud',\n",
       " 'and',\n",
       " 'is',\n",
       " 'assisting',\n",
       " 'prosecutors',\n",
       " 'on',\n",
       " 'monday',\n",
       " 'defence',\n",
       " 'lawyer',\n",
       " 'reid',\n",
       " 'weingarten',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'distance',\n",
       " 'his',\n",
       " 'client',\n",
       " 'from',\n",
       " 'the',\n",
       " 'allegations',\n",
       " 'during',\n",
       " 'cross',\n",
       " 'examination',\n",
       " 'he',\n",
       " 'asked',\n",
       " 'mr',\n",
       " 'myers',\n",
       " 'if',\n",
       " 'he',\n",
       " 'ever',\n",
       " 'knew',\n",
       " 'mr',\n",
       " 'ebbers',\n",
       " 'make',\n",
       " 'an',\n",
       " 'accounting',\n",
       " 'decision',\n",
       " 'not',\n",
       " 'that',\n",
       " 'i',\n",
       " 'am',\n",
       " 'aware',\n",
       " 'of',\n",
       " 'mr',\n",
       " 'myers',\n",
       " 'replied',\n",
       " 'did',\n",
       " 'you',\n",
       " 'ever',\n",
       " 'know',\n",
       " 'mr',\n",
       " 'ebbers',\n",
       " 'to',\n",
       " 'make',\n",
       " 'an',\n",
       " 'accounting',\n",
       " 'entry',\n",
       " 'into',\n",
       " 'worldcom',\n",
       " 'books',\n",
       " 'mr',\n",
       " 'weingarten',\n",
       " 'pressed',\n",
       " 'no',\n",
       " 'replied',\n",
       " 'the',\n",
       " 'witness',\n",
       " 'mr',\n",
       " 'myers',\n",
       " 'has',\n",
       " 'admitted',\n",
       " 'that',\n",
       " 'he',\n",
       " 'ordered',\n",
       " 'false',\n",
       " 'accounting',\n",
       " 'entries',\n",
       " 'at',\n",
       " 'the',\n",
       " 'request',\n",
       " 'of',\n",
       " 'former',\n",
       " 'worldcom',\n",
       " 'chief',\n",
       " 'financial',\n",
       " 'officer',\n",
       " 'scott',\n",
       " 'sullivan',\n",
       " 'defence',\n",
       " 'lawyers',\n",
       " 'have',\n",
       " 'been',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'paint',\n",
       " 'mr',\n",
       " 'sullivan',\n",
       " 'who',\n",
       " 'has',\n",
       " 'admitted',\n",
       " 'fraud',\n",
       " 'and',\n",
       " 'will',\n",
       " 'testify',\n",
       " 'later',\n",
       " 'in',\n",
       " 'the',\n",
       " 'trial',\n",
       " 'as',\n",
       " 'the',\n",
       " 'mastermind',\n",
       " 'behind',\n",
       " 'worldcom',\n",
       " 's',\n",
       " 'accounting',\n",
       " 'house',\n",
       " 'of',\n",
       " 'cards',\n",
       " 'mr',\n",
       " 'ebbers',\n",
       " 'team',\n",
       " 'meanwhile',\n",
       " 'are',\n",
       " 'looking',\n",
       " 'to',\n",
       " 'portray',\n",
       " 'him',\n",
       " 'as',\n",
       " 'an',\n",
       " 'affable',\n",
       " 'boss',\n",
       " 'who',\n",
       " 'by',\n",
       " 'his',\n",
       " 'own',\n",
       " 'admission',\n",
       " 'is',\n",
       " 'more',\n",
       " 'pe',\n",
       " 'graduate',\n",
       " 'than',\n",
       " 'economist',\n",
       " 'whatever',\n",
       " 'his',\n",
       " 'abilities',\n",
       " 'mr',\n",
       " 'ebbers',\n",
       " 'transformed',\n",
       " 'worldcom',\n",
       " 'from',\n",
       " 'a',\n",
       " 'relative',\n",
       " 'unknown',\n",
       " 'into',\n",
       " 'a',\n",
       " '160bn',\n",
       " 'telecoms',\n",
       " 'giant',\n",
       " 'and',\n",
       " 'investor',\n",
       " 'darling',\n",
       " 'of',\n",
       " 'the',\n",
       " 'late',\n",
       " '1990s',\n",
       " 'worldcom',\n",
       " 's',\n",
       " 'problems',\n",
       " 'mounted',\n",
       " 'however',\n",
       " 'as',\n",
       " 'competition',\n",
       " ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text = ' '.join(df[\"text\"].tolist()) \n",
    "text = re.sub(r'[^\\w\\s]',' ',text).split()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bb17064-0e6b-47b4-9127-a0e7ea54d850",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['playwrights',\n",
       " 'beamed',\n",
       " 'lothian',\n",
       " 'wirelessly',\n",
       " 'marigny',\n",
       " 'sensationalist',\n",
       " 'extras',\n",
       " 'discreet',\n",
       " 'ankles',\n",
       " '11th',\n",
       " 'describing',\n",
       " 'regions',\n",
       " 'abate',\n",
       " 'ochil',\n",
       " 'resort',\n",
       " 'flies',\n",
       " 'ovation',\n",
       " 'abused',\n",
       " 'buyout',\n",
       " 'gooneratne',\n",
       " 'spybot',\n",
       " 'baker',\n",
       " 'cloud',\n",
       " 'scot',\n",
       " 'senanayake',\n",
       " 'poliakoff',\n",
       " 'sporadic',\n",
       " 'differentiate',\n",
       " 'sticks',\n",
       " 'bush',\n",
       " 'malta',\n",
       " '155',\n",
       " 'eczema',\n",
       " 'rolling',\n",
       " 'chapters',\n",
       " 'investigated',\n",
       " 'wheelock',\n",
       " 'branson',\n",
       " 'proportion',\n",
       " 'eurotunnel',\n",
       " 'attendant',\n",
       " 'concept',\n",
       " 'ancillary',\n",
       " 'functions',\n",
       " 'embedded',\n",
       " 'verify',\n",
       " 'lifan',\n",
       " 'pvrs',\n",
       " 'tyres',\n",
       " 'research',\n",
       " 'partido',\n",
       " 'lin',\n",
       " 'sonny',\n",
       " 'sher',\n",
       " 'atkins',\n",
       " 'puerta',\n",
       " 'mistiming',\n",
       " 'stilianos',\n",
       " 'engagement',\n",
       " 'issue',\n",
       " 'warchus',\n",
       " 'trauma',\n",
       " 'kournikova',\n",
       " 'grim',\n",
       " 'howarth',\n",
       " 'proudest',\n",
       " 'commercialised',\n",
       " 'islamic',\n",
       " 'hindus',\n",
       " 'compromising',\n",
       " 'mandolin',\n",
       " 'carta',\n",
       " 'starfox',\n",
       " 'deservedly',\n",
       " 'livestock',\n",
       " 'mscps',\n",
       " 'mirror',\n",
       " 'wearable',\n",
       " 'ransom',\n",
       " 'guernsey',\n",
       " 'duel',\n",
       " 'swindon',\n",
       " '84th',\n",
       " 'maligned',\n",
       " 'driv3r',\n",
       " 'xda',\n",
       " 'indirectly',\n",
       " 'ushered',\n",
       " 'indignity',\n",
       " 'hate',\n",
       " 'tub',\n",
       " '365m',\n",
       " 'jordanian',\n",
       " 'nfl',\n",
       " 'unwind',\n",
       " 'iggy',\n",
       " 'volcanic',\n",
       " 'snapple',\n",
       " 'victors',\n",
       " 'gibraltar',\n",
       " 'scheming',\n",
       " 'kidnappers',\n",
       " 'spurns',\n",
       " 'jadakiss',\n",
       " 'medically',\n",
       " 'resultant',\n",
       " 'evidence',\n",
       " 'surrender',\n",
       " 'mirage',\n",
       " 'diversifies',\n",
       " 'hevesi',\n",
       " 'sugababes',\n",
       " 'hails',\n",
       " 'cbbc',\n",
       " 'bn',\n",
       " 'lse',\n",
       " 'parmjit',\n",
       " 'recommendation',\n",
       " 'fingerbobs',\n",
       " 'himself',\n",
       " 'tense',\n",
       " 'writers',\n",
       " 'dodgy',\n",
       " 'rib',\n",
       " 'irancell',\n",
       " 'games',\n",
       " 'disciplines',\n",
       " 'unsettle',\n",
       " 'craftily',\n",
       " 'shefrin',\n",
       " 'ioannou',\n",
       " 'advocate',\n",
       " 'jockey',\n",
       " 'childcare',\n",
       " 'seti',\n",
       " 'fireside',\n",
       " 'clumping',\n",
       " 'afternoon',\n",
       " 'president',\n",
       " 'admit',\n",
       " 'putin',\n",
       " 'previews',\n",
       " 'iyad',\n",
       " 'firming',\n",
       " 'mysterious',\n",
       " 'mahmoud',\n",
       " 'itself',\n",
       " '398m',\n",
       " 'delhi',\n",
       " 'superheroes',\n",
       " 'wes',\n",
       " 'decent',\n",
       " 'maniacs',\n",
       " 'metadata',\n",
       " 'facing',\n",
       " 'oligarchs',\n",
       " 'advocacy',\n",
       " 'sizeable',\n",
       " 'philandering',\n",
       " 'generations',\n",
       " 'west',\n",
       " 'demanded',\n",
       " 'prosecuted',\n",
       " 'knifes',\n",
       " 'firmly',\n",
       " 'divorcee',\n",
       " 'cotterills',\n",
       " 'tempted',\n",
       " 'parken',\n",
       " 'marker',\n",
       " 'denial',\n",
       " 'delighted',\n",
       " 'shape',\n",
       " 'maintain',\n",
       " 'mattered',\n",
       " 'beijing',\n",
       " 'stakeholders',\n",
       " 'interoperable',\n",
       " 'parlour',\n",
       " 'propelled',\n",
       " 'dispenser',\n",
       " 'chefs',\n",
       " 'quieter',\n",
       " 'wheelchair',\n",
       " 'redifining',\n",
       " 'endured',\n",
       " 'stun',\n",
       " 'inched',\n",
       " 'sweden',\n",
       " 'sweep',\n",
       " 'integral',\n",
       " 'relinquish',\n",
       " 'calculated',\n",
       " '239',\n",
       " 'packaged',\n",
       " 'rubinsohn',\n",
       " 'bulletin',\n",
       " 'purlie',\n",
       " 'holocaust',\n",
       " 'softer',\n",
       " 'soweto',\n",
       " 'interacted',\n",
       " 'broker',\n",
       " 'boudewijn',\n",
       " 'repayment',\n",
       " 'ulvaeus',\n",
       " 'among',\n",
       " 'abolish',\n",
       " 'alison',\n",
       " 'baffling',\n",
       " 'nagged',\n",
       " 'clouding',\n",
       " 'descendants',\n",
       " 'hardwick',\n",
       " 'entitle',\n",
       " 'wishful',\n",
       " 'filethat',\n",
       " 'knits',\n",
       " 'elephant',\n",
       " 'twee',\n",
       " 'pornographic',\n",
       " 'glimmer',\n",
       " '06bn',\n",
       " '281m',\n",
       " 'kirkcaldy',\n",
       " 'hiromichi',\n",
       " 'ftse',\n",
       " 'containers',\n",
       " '292',\n",
       " 'enforce',\n",
       " 'necklace',\n",
       " 'crippling',\n",
       " 'downloads',\n",
       " 'australia',\n",
       " 'twats',\n",
       " 'browsed',\n",
       " 'negras',\n",
       " 'messy',\n",
       " 'jarvis',\n",
       " 'boundary',\n",
       " 'prestigious',\n",
       " 'sudden',\n",
       " 'miss',\n",
       " 'partizani',\n",
       " 'gonna',\n",
       " 'eyes',\n",
       " 'seafarer',\n",
       " 'showdown',\n",
       " 'solicit',\n",
       " '55th',\n",
       " 'stayed',\n",
       " 'related',\n",
       " 'lively',\n",
       " 'dragonhood',\n",
       " 'deteriorated',\n",
       " 'masse',\n",
       " 'pole',\n",
       " 'ifrs',\n",
       " 'carolyn',\n",
       " 'principle',\n",
       " 'mpaa',\n",
       " 'uncouth',\n",
       " 'parole',\n",
       " 'michaella',\n",
       " 'under',\n",
       " 'usability',\n",
       " 'knvb',\n",
       " 'fingermouse',\n",
       " 'adrenaline',\n",
       " 'allegedly',\n",
       " 'inbev',\n",
       " 'arpey',\n",
       " 'doctored',\n",
       " 'hounslow',\n",
       " '136',\n",
       " 'sightings',\n",
       " 'allan',\n",
       " '801',\n",
       " 'finest',\n",
       " 'psg',\n",
       " 'dales',\n",
       " 'bin',\n",
       " 'noe',\n",
       " 'molyneux',\n",
       " 'wages',\n",
       " 'viewing',\n",
       " 'zenith',\n",
       " 'jana',\n",
       " 'maintains',\n",
       " 'sulimanovic',\n",
       " 'alltel',\n",
       " 'uaf',\n",
       " 'counterproductive',\n",
       " 'prioritising',\n",
       " 'tivo',\n",
       " 'presented',\n",
       " 'hearts',\n",
       " 'fiorina',\n",
       " 'toast',\n",
       " 'odds',\n",
       " 'composers',\n",
       " 'labour',\n",
       " 'excessive',\n",
       " 'spiralling',\n",
       " 'adamu',\n",
       " 'baying',\n",
       " 'shelving',\n",
       " 'insurers',\n",
       " 'blythe',\n",
       " 'stroked',\n",
       " 'occupy',\n",
       " 'indrawati',\n",
       " 'lombard',\n",
       " 'graduate',\n",
       " 'nomination',\n",
       " '275m',\n",
       " 'tarrant',\n",
       " 'sugary',\n",
       " 'car',\n",
       " 'preventable',\n",
       " 'informa',\n",
       " 'sparked',\n",
       " 'working',\n",
       " 'prints',\n",
       " 'lured',\n",
       " 'venus',\n",
       " 'charged',\n",
       " 'gazetta',\n",
       " 'kleinberg',\n",
       " 'rural',\n",
       " 'medicines',\n",
       " 'wi',\n",
       " '2012',\n",
       " 'regulators',\n",
       " 'coach',\n",
       " 'scared',\n",
       " 'beck',\n",
       " 'comedians',\n",
       " 'councillors',\n",
       " 'boys',\n",
       " 'jowell',\n",
       " 'adapters',\n",
       " 'alarming',\n",
       " 'hovers',\n",
       " 'motjaba',\n",
       " 'edna',\n",
       " 'pictures',\n",
       " 'henry',\n",
       " 'poured',\n",
       " 'ale',\n",
       " 'coyne',\n",
       " 'entrenched',\n",
       " 'curse',\n",
       " 'nevis',\n",
       " 'trail',\n",
       " 'golds',\n",
       " 'personified',\n",
       " 'droll',\n",
       " 'banana',\n",
       " 'foolish',\n",
       " 'vieira',\n",
       " 'filmmaking',\n",
       " 'scrappy',\n",
       " 'thornton',\n",
       " 'burnout',\n",
       " 'immerses',\n",
       " 'oh',\n",
       " 'hoarding',\n",
       " 'kingsholm',\n",
       " 'bear',\n",
       " 'cosgrave',\n",
       " 'wilbur',\n",
       " 'encrypted',\n",
       " 'dwyer',\n",
       " 'nominee',\n",
       " 'cadillac',\n",
       " 'munro',\n",
       " 'legality',\n",
       " 'amusing',\n",
       " 'actresses',\n",
       " 'raw',\n",
       " 'serbia',\n",
       " 'boosh',\n",
       " 'nutrition',\n",
       " 'closed',\n",
       " 'seller',\n",
       " 'texts',\n",
       " 'forcible',\n",
       " 'engaging',\n",
       " 'compassionate',\n",
       " '2300',\n",
       " 'biochemistry',\n",
       " 'bonds',\n",
       " 'shouldering',\n",
       " 'havoc',\n",
       " 'hermann',\n",
       " 'javier',\n",
       " 'tunes',\n",
       " 'appears',\n",
       " '1980s',\n",
       " 'criticises',\n",
       " 'hosts',\n",
       " '1949',\n",
       " 'maybe',\n",
       " 'vocal',\n",
       " 'unambitious',\n",
       " 'deuce',\n",
       " 'fixture',\n",
       " 'monopoly',\n",
       " 'michelangelo',\n",
       " 'unlucky',\n",
       " 'underscore',\n",
       " 'macrovision',\n",
       " 'blume',\n",
       " '3500',\n",
       " 'insurgency',\n",
       " 'boateng',\n",
       " 'behold',\n",
       " '269bn',\n",
       " 'gif',\n",
       " 'baltacha',\n",
       " 'eurodisney',\n",
       " 'irregularity',\n",
       " 'carlish',\n",
       " 'cert',\n",
       " 'tio2',\n",
       " 'magna',\n",
       " 'file',\n",
       " 'browse',\n",
       " 'woodhill',\n",
       " 'gosport',\n",
       " 'kluivert',\n",
       " 'beat',\n",
       " 'goal',\n",
       " 'professionalism',\n",
       " 'perpetuated',\n",
       " 'limbs',\n",
       " 'tariff',\n",
       " 'signing',\n",
       " 'coolly',\n",
       " 'tiananmen',\n",
       " 'blushes',\n",
       " 'sykes',\n",
       " 'affiliated',\n",
       " 'elevator',\n",
       " 'editor',\n",
       " 'lineout',\n",
       " 'utah',\n",
       " 'primary',\n",
       " 'pills',\n",
       " 'reminds',\n",
       " 'abdullatif',\n",
       " 'emerge',\n",
       " 'farndon',\n",
       " 'israeli',\n",
       " 'powys',\n",
       " 'subtle',\n",
       " 'glowing',\n",
       " 'negate',\n",
       " 'principally',\n",
       " 'cordoned',\n",
       " 'benitez',\n",
       " 'addressing',\n",
       " 'gladly',\n",
       " '417',\n",
       " 'camus',\n",
       " '274',\n",
       " 'publicist',\n",
       " 'genghis',\n",
       " '53m',\n",
       " 'hayes',\n",
       " 'moblog',\n",
       " 'quashed',\n",
       " 'embracing',\n",
       " 'siding',\n",
       " 'iwata',\n",
       " 'scrutiny',\n",
       " 'holborn',\n",
       " 'rene',\n",
       " 'tightening',\n",
       " 'advise',\n",
       " 'slips',\n",
       " '128',\n",
       " 'town',\n",
       " 'maggs',\n",
       " 'whisker',\n",
       " 'contributed',\n",
       " 'harrier',\n",
       " 'spot',\n",
       " 'twinkle',\n",
       " 'dispose',\n",
       " 'bremen',\n",
       " 'silvers',\n",
       " 'adsl',\n",
       " 'haunted',\n",
       " 'switzerland',\n",
       " 'wynne',\n",
       " 'world',\n",
       " 'pmc',\n",
       " 'draining',\n",
       " 'giuseppe',\n",
       " 'bow',\n",
       " 'proves',\n",
       " 'plumb',\n",
       " 'muddy',\n",
       " 'confidently',\n",
       " '13th',\n",
       " 'bhs',\n",
       " 'sorrow',\n",
       " '09secs',\n",
       " 'packet',\n",
       " 'capital',\n",
       " 'guest',\n",
       " 'governor',\n",
       " 'gypsies',\n",
       " 'overshot',\n",
       " 'ounce',\n",
       " '284m',\n",
       " 'kilmarnock',\n",
       " 'costin',\n",
       " 'a400m',\n",
       " 'theoretical',\n",
       " 'naysmith',\n",
       " 'panoramic',\n",
       " 'commentators',\n",
       " 'lampard',\n",
       " 'bernadette',\n",
       " 'dismiss',\n",
       " 'neglected',\n",
       " 'demonisation',\n",
       " 'insensitivity',\n",
       " 'kerrang',\n",
       " 'dorothy',\n",
       " 'proportions',\n",
       " 'eases',\n",
       " 'rightful',\n",
       " 'case',\n",
       " 'a320',\n",
       " 'pepito',\n",
       " '360',\n",
       " 'justifying',\n",
       " 'responsive',\n",
       " 'revolutionised',\n",
       " 'proponents',\n",
       " 'terminations',\n",
       " '77',\n",
       " 'incompetent',\n",
       " 'cause',\n",
       " 'unicef',\n",
       " 'organise',\n",
       " 'sativex',\n",
       " 'sveta',\n",
       " 'exporter',\n",
       " 'underneath',\n",
       " 'regenerate',\n",
       " 'bombard',\n",
       " 'torvill',\n",
       " 'stunned',\n",
       " 'timeframe',\n",
       " 'abysmal',\n",
       " 'usernames',\n",
       " 'upstage',\n",
       " 'terrible',\n",
       " 'shaftesbury',\n",
       " 'masahiro',\n",
       " 'vestey',\n",
       " 'transgression',\n",
       " 'fromental',\n",
       " 'soderling',\n",
       " 'wishes',\n",
       " 'criss',\n",
       " 'letmathe',\n",
       " 'pitting',\n",
       " 'rejoined',\n",
       " 'debrief',\n",
       " '173m',\n",
       " 'gameboy',\n",
       " 'sped',\n",
       " 'individuals',\n",
       " 'white',\n",
       " 'punished',\n",
       " 'lining',\n",
       " 'galesi',\n",
       " 'razor',\n",
       " 'lamp',\n",
       " 'knockback',\n",
       " 'refuted',\n",
       " 'murder',\n",
       " 'auteuil',\n",
       " 'froze',\n",
       " 'beckinsale',\n",
       " 'collector',\n",
       " 'aramburu',\n",
       " 'combats',\n",
       " 'cursing',\n",
       " 'relation',\n",
       " '125m',\n",
       " 'blast',\n",
       " 'northwest',\n",
       " '160',\n",
       " 'toho',\n",
       " 'detrimental',\n",
       " 'duck',\n",
       " '124bn',\n",
       " '483',\n",
       " 'triggers',\n",
       " 'airy',\n",
       " 'thought',\n",
       " 'assembling',\n",
       " 'circumstances',\n",
       " 'tricking',\n",
       " 'iconic',\n",
       " 'discos',\n",
       " 'othello',\n",
       " 'att',\n",
       " 'chevrontexaco',\n",
       " 'adamind',\n",
       " 'copper',\n",
       " 'easymobile',\n",
       " 'arson',\n",
       " 'malik',\n",
       " 'sapping',\n",
       " 'mp3g',\n",
       " 'progressed',\n",
       " 'kroner',\n",
       " 'stanley',\n",
       " 'predicts',\n",
       " 'technorati',\n",
       " 'everyone',\n",
       " 'prompt',\n",
       " 'unnatural',\n",
       " 'contrasting',\n",
       " '1848',\n",
       " 'exceeded',\n",
       " 'cisse',\n",
       " 'manzerana',\n",
       " 'villalba',\n",
       " 'him',\n",
       " 'museum',\n",
       " 'underline',\n",
       " '222',\n",
       " 'hewlett',\n",
       " 'applying',\n",
       " 'johnny',\n",
       " 'note',\n",
       " 'eyewitness',\n",
       " '402',\n",
       " 'claiming',\n",
       " 'ewood',\n",
       " 'compete',\n",
       " 'townsden',\n",
       " 'scripted',\n",
       " 'householders',\n",
       " 'serena',\n",
       " 'wonderful',\n",
       " 'quiksilver',\n",
       " 'sibierski',\n",
       " 'highlanders',\n",
       " 'dusty',\n",
       " 'transpired',\n",
       " 'sayeed',\n",
       " 'noise',\n",
       " '26',\n",
       " 'loser',\n",
       " 'burrell',\n",
       " 'grimes',\n",
       " 'newey',\n",
       " 'scuppered',\n",
       " 'swung',\n",
       " 'forgetting',\n",
       " 'ailis',\n",
       " '3x',\n",
       " 'testosterone',\n",
       " 'nine',\n",
       " 'company',\n",
       " 'aitken',\n",
       " 'root',\n",
       " 'exerts',\n",
       " 'bortey',\n",
       " 'announced',\n",
       " 'unrecoverable',\n",
       " 'jolene',\n",
       " 'clocking',\n",
       " 'acknowledges',\n",
       " 'flush',\n",
       " 'leblanc',\n",
       " 'offsetting',\n",
       " 'capleton',\n",
       " 'developing',\n",
       " 'valery',\n",
       " 'patterson',\n",
       " 'squashed',\n",
       " 'puerto',\n",
       " 'gunning',\n",
       " 'podcaster',\n",
       " 'quite',\n",
       " 'incredibles',\n",
       " 'uniting',\n",
       " 'serdengecti',\n",
       " 'retrieving',\n",
       " 'reams',\n",
       " 'crosshair',\n",
       " 'helicopters',\n",
       " 'rigoglioso',\n",
       " 'presidential',\n",
       " 'could',\n",
       " 'cueto',\n",
       " 'dedicated',\n",
       " 'multidisciplinary',\n",
       " 'stresses',\n",
       " '962',\n",
       " 'fluid',\n",
       " 'mogul',\n",
       " 'fervent',\n",
       " 'squeezing',\n",
       " 'lily',\n",
       " 'ancic',\n",
       " 'contingency',\n",
       " 'luton',\n",
       " 'pelletier',\n",
       " 'fulfilling',\n",
       " '400m',\n",
       " 'mubarak',\n",
       " 'elements',\n",
       " 'sacchi',\n",
       " 'constraint',\n",
       " 'emil',\n",
       " 'humility',\n",
       " 'fresh',\n",
       " 'policeman',\n",
       " 'kishishev',\n",
       " 'markets',\n",
       " 'cavalcade',\n",
       " 'abbey',\n",
       " 'rabbi',\n",
       " 'successes',\n",
       " 'sees',\n",
       " 'jude',\n",
       " 'brutally',\n",
       " 'counterparts',\n",
       " 'whetting',\n",
       " 'pointed',\n",
       " 'romance',\n",
       " '661',\n",
       " '234m',\n",
       " 'tube',\n",
       " 'flushing',\n",
       " 'tel',\n",
       " 'berlin',\n",
       " 'councillor',\n",
       " 'perreira',\n",
       " 'regional',\n",
       " 'needy',\n",
       " 'monsters',\n",
       " 'yangtze',\n",
       " 'pixar',\n",
       " 'nationalities',\n",
       " 'booming',\n",
       " 'diana',\n",
       " '12cm',\n",
       " '785',\n",
       " 'werner',\n",
       " 'rentaghost',\n",
       " 'effectively',\n",
       " 'referees',\n",
       " 'carnival',\n",
       " 'insubstantial',\n",
       " 'grape',\n",
       " 'occupation',\n",
       " 'mosquito',\n",
       " 'yesteryear',\n",
       " 'gaming',\n",
       " 'kenny',\n",
       " 'reunification',\n",
       " 'inflated',\n",
       " 'visitor',\n",
       " '95bn',\n",
       " 'priorities',\n",
       " 'mulyani',\n",
       " 'toyota',\n",
       " 'yorker',\n",
       " 'shadrin',\n",
       " 'smernicki',\n",
       " 'surfwear',\n",
       " 'selectors',\n",
       " 'tipped',\n",
       " 'rakes',\n",
       " '800m',\n",
       " 'multilaterally',\n",
       " 'chopped',\n",
       " 'webcam',\n",
       " 'bernab',\n",
       " 'pretoria',\n",
       " '975',\n",
       " 'glamorised',\n",
       " 'nicky',\n",
       " 'unsuccessfully',\n",
       " 'blighted',\n",
       " 'gloating',\n",
       " '2940',\n",
       " 'antelope',\n",
       " 'moggi',\n",
       " 'goosens',\n",
       " 'sops',\n",
       " 'frankfurt',\n",
       " 'hogged',\n",
       " '825',\n",
       " 'these',\n",
       " 'bridgewell',\n",
       " 'abroad',\n",
       " '148',\n",
       " 'malnourishment',\n",
       " 'peters',\n",
       " 'nettles',\n",
       " 'discrepancy',\n",
       " 'mikoyan',\n",
       " 'uncompromising',\n",
       " 'likening',\n",
       " 'violently',\n",
       " 'greenwich',\n",
       " 'marseille',\n",
       " 'bay',\n",
       " 'southcorp',\n",
       " 'forehand',\n",
       " 'trustees',\n",
       " 'rks',\n",
       " 'cabinet',\n",
       " 'turnover',\n",
       " '247p',\n",
       " 'goodbody',\n",
       " 'shuttle',\n",
       " 'televised',\n",
       " '1927',\n",
       " 'goodfellas',\n",
       " 'trickle',\n",
       " 'invent',\n",
       " 'ivanovic',\n",
       " 'dissuade',\n",
       " 'sheppey',\n",
       " 'described',\n",
       " 'principal',\n",
       " 'disguising',\n",
       " 'ascii',\n",
       " 'nhs',\n",
       " 'kyrgyz',\n",
       " 'questions',\n",
       " 'guadalcanal',\n",
       " 'bernabeu',\n",
       " 'bishop',\n",
       " 'bargain',\n",
       " 'denies',\n",
       " '24',\n",
       " 'instantaneous',\n",
       " 'feedback',\n",
       " 'raskin',\n",
       " 'violations',\n",
       " 'reeves',\n",
       " 'dividends',\n",
       " 'distribute',\n",
       " 'anders',\n",
       " 'yamanouchi',\n",
       " 'lull',\n",
       " 'youngster',\n",
       " 'katie',\n",
       " 'istanbul',\n",
       " 'sprinting',\n",
       " 'trident',\n",
       " 'fizzy',\n",
       " 'anchor',\n",
       " 'cocaine',\n",
       " 'davenport',\n",
       " 'soap',\n",
       " 'crossing',\n",
       " 'km',\n",
       " 'strip',\n",
       " 'goat',\n",
       " 'destroying',\n",
       " 'situated',\n",
       " 'cynical',\n",
       " 'including',\n",
       " 'passwords',\n",
       " 'mycokemusic',\n",
       " 'rid',\n",
       " 'sputtering',\n",
       " 'respects',\n",
       " 'erich',\n",
       " 'distinctive',\n",
       " 'von',\n",
       " 'continents',\n",
       " 'roughshod',\n",
       " 'clamped',\n",
       " 'hayao',\n",
       " 'supergrass',\n",
       " 'licking',\n",
       " 'skinny',\n",
       " 'latinohiphopradio',\n",
       " 'abortions',\n",
       " 'defensively',\n",
       " 'merit',\n",
       " 'scanlon',\n",
       " 'gretchen',\n",
       " 'beynon',\n",
       " 'oak',\n",
       " 'example',\n",
       " 'ambassadors',\n",
       " 'hike',\n",
       " 'bowen',\n",
       " 'startac',\n",
       " 'ever',\n",
       " 'repressors',\n",
       " 'olivier',\n",
       " 'respectable',\n",
       " 'sexist',\n",
       " 'goin',\n",
       " 'hobbes',\n",
       " 'pony',\n",
       " 'refers',\n",
       " 'lifelong',\n",
       " 'shortened',\n",
       " 'segments',\n",
       " 'impose',\n",
       " 'axed',\n",
       " 'gabrielle',\n",
       " 'disposing',\n",
       " 'depriving',\n",
       " 'technically',\n",
       " 'tupac',\n",
       " 'discussions',\n",
       " 'qadeer',\n",
       " 'needing',\n",
       " 'nod',\n",
       " '04m',\n",
       " 'ruben',\n",
       " 'first',\n",
       " 'mohi',\n",
       " 'shoo',\n",
       " 'yagan',\n",
       " 'websites',\n",
       " 'wrists',\n",
       " 'inception',\n",
       " 'monumental',\n",
       " 'farewell',\n",
       " 'partners',\n",
       " 'tropical',\n",
       " 'frightening',\n",
       " 'spree',\n",
       " 'cotton',\n",
       " 'emergence',\n",
       " 'employer',\n",
       " 'themed',\n",
       " 'beckons',\n",
       " 'lyric',\n",
       " 'memorably',\n",
       " 'throughout',\n",
       " 'immense',\n",
       " 'rival',\n",
       " 'angle',\n",
       " 'azmat',\n",
       " 'biographer',\n",
       " 'pester',\n",
       " 'chittabrata',\n",
       " 'consider',\n",
       " 'unbroken',\n",
       " 'rectitude',\n",
       " 'islwyn',\n",
       " 'telephone',\n",
       " 'creditor',\n",
       " 'chongqing',\n",
       " 'angela',\n",
       " 'undimmed',\n",
       " 'liqueur',\n",
       " 'lash',\n",
       " 'curve',\n",
       " 'expectancies',\n",
       " 'tbwa',\n",
       " 'dunbar',\n",
       " 'oc',\n",
       " 'hecuba',\n",
       " 'sohn',\n",
       " 'darling',\n",
       " 'secure',\n",
       " 'daughter',\n",
       " 'move',\n",
       " 'churlishly',\n",
       " 'wall',\n",
       " 'dbs',\n",
       " 'felony',\n",
       " 'assess',\n",
       " 'intended',\n",
       " 'brawn',\n",
       " 'prudence',\n",
       " 'extensions',\n",
       " 'relief',\n",
       " 'images',\n",
       " 'matsuoka',\n",
       " 'bailiffs',\n",
       " 'superbly',\n",
       " 'carvalho',\n",
       " 'insidious',\n",
       " 'zakuani',\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary=list(set(text)) \n",
    "vocabulary \n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f262c373-ca63-4103-88ff-94cd53140347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29457 ['playwrights', 'beamed', 'lothian', 'wirelessly', 'marigny', 'sensationalist', 'extras', 'discreet', 'ankles', '11th']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(vocabulary),vocabulary[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a16e145c-dc8d-42a9-a7ba-0308a8343c10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tv future in the hands of viewers with home theatre systems  plasma high definition tvs  and digital video recorders moving into the living room  the way people watch tv will be radically different in five years  time   that is according to an expert panel which gathered at the annual consumer electronics show in las vegas to discuss how these new technologies will impact one of our favourite pastimes  with the us leading the trend  programmes and other content will be delivered to viewers via home networks  through cable  satellite  telecoms companies  and broadband service providers to front rooms and portable devices   one of the most talked about technologies of ces has been digital and personal video recorders  dvr and pvr   these set top boxes  like the us s tivo and the uk s sky  system  allow people to record  store  play  pause and forward wind tv programmes when they want   essentially  the technology allows for much more personalised tv  they are also being built in to high definition tv sets  which are big business in japan and the us  but slower to take off in europe because of the lack of high definition programming  not only can people forward wind through adverts  they can also forget about abiding by network and channel schedules  putting together their own a la carte entertainment  but some us networks and cable and satellite companies are worried about what it means for them in terms of advertising revenues as well as  brand identity  and viewer loyalty to channels  although the us leads in this technology at the moment  it is also a concern that is being raised in europe  particularly with the growing uptake of services like sky    what happens here today  we will see in nine months to a years  time in the uk   adam hume  the bbc broadcast s futurologist told the bbc news website  for the likes of the bbc  there are no issues of lost advertising revenue yet  it is a more pressing issue at the moment for commercial uk broadcasters  but brand loyalty is important for everyone   we will be talking more about content brands rather than network brands   said tim hanlon  from brand communications firm starcom mediavest   the reality is that with broadband connections  anybody can be the producer of content   he added   the challenge now is that it is hard to promote a programme with so much choice    what this means  said stacey jolna  senior vice president of tv guide tv group  is that the way people find the content they want to watch has to be simplified for tv viewers  it means that networks  in us terms  or channels could take a leaf out of google s book and be the search engine of the future  instead of the scheduler to help people find what they want to watch  this kind of channel model might work for the younger ipod generation which is used to taking control of their gadgets and what they play on them  but it might not suit everyone  the panel recognised  older generations are more comfortable with familiar schedules and channel brands because they know what they are getting  they perhaps do not want so much of the choice put into their hands  mr hanlon suggested   on the other end  you have the kids just out of diapers who are pushing buttons already   everything is possible and available to them   said mr hanlon   ultimately  the consumer will tell the market they want    of the 50 000 new gadgets and technologies being showcased at ces  many of them are about enhancing the tv watching experience  high definition tv sets are everywhere and many new models of lcd  liquid crystal display  tvs have been launched with dvr capability built into them  instead of being external boxes  one such example launched at the show is humax s 26 inch lcd tv with an 80 hour tivo dvr and dvd recorder  one of the us s biggest satellite tv companies  directtv  has even launched its own branded dvr at the show with 100 hours of recording capability  instant replay  and a search function  the set can pause and rewind tv for up to 90 hours  and microsoft chief bill gates announced in his pre show keynote speech a partnership with tivo  called tivotogo  which means people can play recorded programmes on windows pcs and mobile devices  all these reflect the increasing trend of freeing up multimedia so that people can watch what they want  when they want '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'[^\\w\\s]',' ',df[\"text\"].iloc[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27cd7d54-caf1-436b-8f42-b670233ce7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_one_hot_vectors(words,vocabulary):\n",
    "    #print(len(words))\n",
    "    inputs=np.zeros((len(words),len(vocabulary)),int)\n",
    "    #print(inputs.shape)\n",
    "    for i in range(len(words)): \n",
    "        inputs[i][vocabulary.index(words[i])]=1\n",
    "    return inputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fdff8613-ba48-415b-aa0c-032b4540153c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "744\n",
      "294\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 23\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m training_samples\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df)):\n\u001b[1;32m---> 23\u001b[0m     training_samples\u001b[38;5;241m=\u001b[39m\u001b[43minputs_window_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m[^\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m,\u001b[49m\u001b[43mside_window_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(training_samples))\n",
      "Cell \u001b[1;32mIn[72], line 12\u001b[0m, in \u001b[0;36minputs_window_words\u001b[1;34m(sequence, vocabulary, side_window_size)\u001b[0m\n\u001b[0;32m     10\u001b[0m words_input\u001b[38;5;241m=\u001b[39mwords_before\u001b[38;5;241m+\u001b[39mwords_after\n\u001b[0;32m     11\u001b[0m X_i\u001b[38;5;241m=\u001b[39minput_one_hot_vectors(words_input,vocabulary)\n\u001b[1;32m---> 12\u001b[0m y_i\u001b[38;5;241m=\u001b[39m\u001b[43minput_one_hot_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#print(X_i,y_i)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m training_sample\u001b[38;5;241m=\u001b[39m[X_i,y_i]\n",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m, in \u001b[0;36minput_one_hot_vectors\u001b[1;34m(words, vocabulary)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minput_one_hot_vectors\u001b[39m(words,vocabulary):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m#print(len(words))\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m#print(inputs.shape)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(words)): \n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "side_window_size=3\n",
    "\n",
    "def inputs_window_words(sequence,vocabulary,side_window_size):\n",
    "    training_samples=[]\n",
    "    for i in range(side_window_size,len(sequence)-side_window_size):\n",
    "        words_before=sequence[i-side_window_size:i]\n",
    "        words_after=sequence[i+1:i+1+side_window_size]\n",
    "        #print(words_before)\n",
    "        #print(words_after)\n",
    "        words_input=words_before+words_after\n",
    "        X_i=input_one_hot_vectors(words_input,vocabulary)\n",
    "        y_i=input_one_hot_vectors([sequence[i]],vocabulary)\n",
    "        #print(X_i,y_i)\n",
    "        training_sample=[X_i,y_i]\n",
    "        training_samples.append(training_sample)\n",
    "    return training_samples\n",
    "    \n",
    "\n",
    "\n",
    "                 \n",
    "\n",
    "for i in range(len(df)):\n",
    "    training_samples=inputs_window_words(re.sub(r'[^\\w\\s]',' ',df[\"text\"].iloc[i]).split(),vocabulary,side_window_size)\n",
    "    print(len(training_samples))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e98f037f-7114-4f13-bc2f-bbef192e8a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 29457)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(training_samples[0][1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2acdafdc-bc69-4e77-a9f2-47c4261b1904",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7aa7e116-6531-4419-b88a-2f6308d34d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "Words_len_embedding_layer= np.random.rand(len(vocabulary),embedding_size)   \n",
    "Words_len_embedding_bias = np.random.rand(embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2e7740a4-c20d-45a3-8d00-4890f2d8893e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29457, 300)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Words_len_embedding_layer.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd54508-881b-4314-8929-05dcb546ddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=-1):\n",
    "        x = np.clip(x, -1e4, 1e4)  # Clip for numerical stability\n",
    "        e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "        return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "Sigma_Zout_one=softmax(np.matmul(np.array(training_samples[1][0]), Words_len_embedding_layer) + Words_len_embedding_bias)\n",
    "Sigma_Zout_one.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c89e23-6966-49dc-b43c-2e85b19fc2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma_Zout_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0457a1c-78b6-4970-b65c-88f1c285dafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first approach concat\n",
    "Sigma_Zout_one=Sigma_Zout_one.reshape(1,Sigma_Zout_one.shape[0]*embedding_size)\n",
    "Sigma_Zout_one.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725189f1-4b50-4f96-9e9f-dc60004c2e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#second approach average\n",
    "Sigma_Zout_one=np.mean(Sigma_Zout_one,axis=0).reshape(1,Sigma_Zout_one.shape[1])\n",
    "Sigma_Zout_one.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7659cd6-da6a-4de9-9a7f-648f6ff20bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e46c722-69bc-4bda-b67d-ed11f3faf2f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "71a09186-a080-48c9-bfaa-a0cab758ffd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 29457)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linearlayer_maps_vocab= np.random.rand(Sigma_Zout_one.shape[1],len(vocabulary))   \n",
    "linear_bias_maps_vocab = np.random.rand(len(vocabulary))\n",
    "linearlayer_maps_vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eaf177-a6c5-4a3a-b8ef-a995a5a07371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2dd99f69-bc80-4af9-9d33-68f0976f2d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma_Zout=softmax(np.matmul(Sigma_Zout_one,linearlayer_maps_vocab) + linear_bias_maps_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2dbb13f7-9044-4bab-a8ce-cad49146828e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 29457), (1, 29457))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sigma_Zout.shape,training_samples[1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1a94a4df-5813-4c98-a143-bd39884a798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(predictions, target):\n",
    "        # Cross-entropy loss for a batch of predictions and targets\n",
    "        batch_loss = -np.sum(target * np.log(predictions + 1e-9), axis=1)\n",
    "        return np.mean(batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "63588c07-f211-4aed-a6f6-6dcfe5ca8c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.902769768660638"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_loss(Sigma_Zout,training_samples[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "352e1db7-380c-4bb1-ae70-534c25cb3920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.624056073222354"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_loss(Sigma_Zout,training_samples[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c77e80-494e-4d47-913d-132c67760217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class Word2Vec:\n",
    "    def __init__(self, window_lenght):\n",
    "        self.window_lenght=window_lenght\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "76516ca3-e5c8-458c-bd63-e1910cbe72ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  29457\n",
      "training samples from text 0: 744\n",
      "PHRASE 0 sample 0 Xi=(6, 29457), yi=(1, 29457), Loss=10.537377067921993\n",
      "PHRASE 0 sample 1 Xi=(6, 29457), yi=(1, 29457), Loss=10.058926379828693\n",
      "PHRASE 0 sample 2 Xi=(6, 29457), yi=(1, 29457), Loss=9.850157763090325\n",
      "PHRASE 0 sample 3 Xi=(6, 29457), yi=(1, 29457), Loss=10.030239128079767\n",
      "PHRASE 0 sample 4 Xi=(6, 29457), yi=(1, 29457), Loss=10.777342509598505\n",
      "PHRASE 0 sample 5 Xi=(6, 29457), yi=(1, 29457), Loss=10.32403161095101\n",
      "PHRASE 0 sample 6 Xi=(6, 29457), yi=(1, 29457), Loss=10.717082625862314\n",
      "PHRASE 0 sample 7 Xi=(6, 29457), yi=(1, 29457), Loss=10.168426382412445\n",
      "PHRASE 0 sample 8 Xi=(6, 29457), yi=(1, 29457), Loss=9.914414688497205\n",
      "PHRASE 0 sample 9 Xi=(6, 29457), yi=(1, 29457), Loss=10.725238013253946\n",
      "PHRASE 0 sample 10 Xi=(6, 29457), yi=(1, 29457), Loss=10.175788225809054\n",
      "PHRASE 0 sample 11 Xi=(6, 29457), yi=(1, 29457), Loss=9.905414540911394\n",
      "PHRASE 0 sample 12 Xi=(6, 29457), yi=(1, 29457), Loss=10.30794558859777\n",
      "PHRASE 0 sample 13 Xi=(6, 29457), yi=(1, 29457), Loss=10.798543019224956\n",
      "PHRASE 0 sample 14 Xi=(6, 29457), yi=(1, 29457), Loss=10.11872285522376\n",
      "PHRASE 0 sample 15 Xi=(6, 29457), yi=(1, 29457), Loss=10.253555142823634\n",
      "PHRASE 0 sample 16 Xi=(6, 29457), yi=(1, 29457), Loss=10.38266609717057\n",
      "PHRASE 0 sample 17 Xi=(6, 29457), yi=(1, 29457), Loss=10.541816873208822\n",
      "PHRASE 0 sample 18 Xi=(6, 29457), yi=(1, 29457), Loss=10.53900019466647\n",
      "PHRASE 0 sample 19 Xi=(6, 29457), yi=(1, 29457), Loss=10.768521511387355\n",
      "PHRASE 0 sample 20 Xi=(6, 29457), yi=(1, 29457), Loss=9.956899133733753\n",
      "PHRASE 0 sample 21 Xi=(6, 29457), yi=(1, 29457), Loss=10.555785433046662\n",
      "PHRASE 0 sample 22 Xi=(6, 29457), yi=(1, 29457), Loss=10.819416071512624\n",
      "PHRASE 0 sample 23 Xi=(6, 29457), yi=(1, 29457), Loss=10.216442146412675\n",
      "PHRASE 0 sample 24 Xi=(6, 29457), yi=(1, 29457), Loss=10.537896815553337\n",
      "PHRASE 0 sample 25 Xi=(6, 29457), yi=(1, 29457), Loss=10.110114751729935\n",
      "PHRASE 0 sample 26 Xi=(6, 29457), yi=(1, 29457), Loss=10.376278360883465\n",
      "PHRASE 0 sample 27 Xi=(6, 29457), yi=(1, 29457), Loss=9.997847342804985\n",
      "PHRASE 0 sample 28 Xi=(6, 29457), yi=(1, 29457), Loss=10.568886105069899\n",
      "PHRASE 0 sample 29 Xi=(6, 29457), yi=(1, 29457), Loss=10.519910663967337\n",
      "PHRASE 0 sample 30 Xi=(6, 29457), yi=(1, 29457), Loss=10.274683904543389\n",
      "PHRASE 0 sample 31 Xi=(6, 29457), yi=(1, 29457), Loss=10.789676059732628\n",
      "PHRASE 0 sample 32 Xi=(6, 29457), yi=(1, 29457), Loss=10.892587099040686\n",
      "PHRASE 0 sample 33 Xi=(6, 29457), yi=(1, 29457), Loss=10.030479683020687\n",
      "PHRASE 0 sample 34 Xi=(6, 29457), yi=(1, 29457), Loss=10.329162552809235\n",
      "PHRASE 0 sample 35 Xi=(6, 29457), yi=(1, 29457), Loss=10.81319679491931\n",
      "PHRASE 0 sample 36 Xi=(6, 29457), yi=(1, 29457), Loss=10.660287410635371\n",
      "PHRASE 0 sample 37 Xi=(6, 29457), yi=(1, 29457), Loss=10.25690195367539\n",
      "PHRASE 0 sample 38 Xi=(6, 29457), yi=(1, 29457), Loss=10.269778250502895\n",
      "PHRASE 0 sample 39 Xi=(6, 29457), yi=(1, 29457), Loss=10.143369609101153\n",
      "PHRASE 0 sample 40 Xi=(6, 29457), yi=(1, 29457), Loss=10.324589343222822\n",
      "PHRASE 0 sample 41 Xi=(6, 29457), yi=(1, 29457), Loss=10.099320153736596\n",
      "PHRASE 0 sample 42 Xi=(6, 29457), yi=(1, 29457), Loss=10.090262459094326\n",
      "PHRASE 0 sample 43 Xi=(6, 29457), yi=(1, 29457), Loss=10.094872083576853\n",
      "PHRASE 0 sample 44 Xi=(6, 29457), yi=(1, 29457), Loss=10.539607774874366\n",
      "PHRASE 0 sample 45 Xi=(6, 29457), yi=(1, 29457), Loss=10.305612013924227\n",
      "PHRASE 0 sample 46 Xi=(6, 29457), yi=(1, 29457), Loss=10.587507499512487\n",
      "PHRASE 0 sample 47 Xi=(6, 29457), yi=(1, 29457), Loss=10.82638335822629\n",
      "PHRASE 0 sample 48 Xi=(6, 29457), yi=(1, 29457), Loss=10.16879455656286\n",
      "PHRASE 0 sample 49 Xi=(6, 29457), yi=(1, 29457), Loss=10.261924059639618\n",
      "PHRASE 0 sample 50 Xi=(6, 29457), yi=(1, 29457), Loss=10.665557365287968\n",
      "PHRASE 0 sample 51 Xi=(6, 29457), yi=(1, 29457), Loss=10.44691253949095\n",
      "PHRASE 0 sample 52 Xi=(6, 29457), yi=(1, 29457), Loss=10.24906832346173\n",
      "PHRASE 0 sample 53 Xi=(6, 29457), yi=(1, 29457), Loss=9.886560292577016\n",
      "PHRASE 0 sample 54 Xi=(6, 29457), yi=(1, 29457), Loss=10.812466539646822\n",
      "PHRASE 0 sample 55 Xi=(6, 29457), yi=(1, 29457), Loss=10.724728328073907\n",
      "PHRASE 0 sample 56 Xi=(6, 29457), yi=(1, 29457), Loss=10.291698692739233\n",
      "PHRASE 0 sample 57 Xi=(6, 29457), yi=(1, 29457), Loss=10.289578714767629\n",
      "PHRASE 0 sample 58 Xi=(6, 29457), yi=(1, 29457), Loss=10.365539107245288\n",
      "PHRASE 0 sample 59 Xi=(6, 29457), yi=(1, 29457), Loss=9.923337015341133\n",
      "PHRASE 0 sample 60 Xi=(6, 29457), yi=(1, 29457), Loss=9.911780236564473\n",
      "PHRASE 0 sample 61 Xi=(6, 29457), yi=(1, 29457), Loss=9.881719589313752\n",
      "PHRASE 0 sample 62 Xi=(6, 29457), yi=(1, 29457), Loss=10.40847787554084\n",
      "PHRASE 0 sample 63 Xi=(6, 29457), yi=(1, 29457), Loss=10.13908333947623\n",
      "PHRASE 0 sample 64 Xi=(6, 29457), yi=(1, 29457), Loss=10.519709802731803\n",
      "PHRASE 0 sample 65 Xi=(6, 29457), yi=(1, 29457), Loss=10.787974518294925\n",
      "PHRASE 0 sample 66 Xi=(6, 29457), yi=(1, 29457), Loss=10.535076748008814\n",
      "PHRASE 0 sample 67 Xi=(6, 29457), yi=(1, 29457), Loss=10.30150957815276\n",
      "PHRASE 0 sample 68 Xi=(6, 29457), yi=(1, 29457), Loss=10.47629740795787\n",
      "PHRASE 0 sample 69 Xi=(6, 29457), yi=(1, 29457), Loss=10.570998691568885\n",
      "PHRASE 0 sample 70 Xi=(6, 29457), yi=(1, 29457), Loss=10.502200724265546\n",
      "PHRASE 0 sample 71 Xi=(6, 29457), yi=(1, 29457), Loss=9.999710159559365\n",
      "PHRASE 0 sample 72 Xi=(6, 29457), yi=(1, 29457), Loss=10.274387900460331\n",
      "PHRASE 0 sample 73 Xi=(6, 29457), yi=(1, 29457), Loss=10.50523401583329\n",
      "PHRASE 0 sample 74 Xi=(6, 29457), yi=(1, 29457), Loss=9.921968398992547\n",
      "PHRASE 0 sample 75 Xi=(6, 29457), yi=(1, 29457), Loss=10.360168565811922\n",
      "PHRASE 0 sample 76 Xi=(6, 29457), yi=(1, 29457), Loss=9.968421238102803\n",
      "PHRASE 0 sample 77 Xi=(6, 29457), yi=(1, 29457), Loss=10.28534558558232\n",
      "PHRASE 0 sample 78 Xi=(6, 29457), yi=(1, 29457), Loss=10.241094097091151\n",
      "PHRASE 0 sample 79 Xi=(6, 29457), yi=(1, 29457), Loss=10.033687844183106\n",
      "PHRASE 0 sample 80 Xi=(6, 29457), yi=(1, 29457), Loss=10.541348026717628\n",
      "PHRASE 0 sample 81 Xi=(6, 29457), yi=(1, 29457), Loss=10.320810536843723\n",
      "PHRASE 0 sample 82 Xi=(6, 29457), yi=(1, 29457), Loss=10.255463523833983\n",
      "PHRASE 0 sample 83 Xi=(6, 29457), yi=(1, 29457), Loss=10.370424956349707\n",
      "PHRASE 0 sample 84 Xi=(6, 29457), yi=(1, 29457), Loss=10.094935509855652\n",
      "PHRASE 0 sample 85 Xi=(6, 29457), yi=(1, 29457), Loss=10.769782225870443\n",
      "PHRASE 0 sample 86 Xi=(6, 29457), yi=(1, 29457), Loss=9.933622343289327\n",
      "PHRASE 0 sample 87 Xi=(6, 29457), yi=(1, 29457), Loss=10.04557021414369\n",
      "PHRASE 0 sample 88 Xi=(6, 29457), yi=(1, 29457), Loss=10.303820344322233\n",
      "PHRASE 0 sample 89 Xi=(6, 29457), yi=(1, 29457), Loss=9.949513314277619\n",
      "PHRASE 0 sample 90 Xi=(6, 29457), yi=(1, 29457), Loss=10.738530041378077\n",
      "PHRASE 0 sample 91 Xi=(6, 29457), yi=(1, 29457), Loss=10.342604309358775\n",
      "PHRASE 0 sample 92 Xi=(6, 29457), yi=(1, 29457), Loss=10.221257373438915\n",
      "PHRASE 0 sample 93 Xi=(6, 29457), yi=(1, 29457), Loss=10.737243392613884\n",
      "PHRASE 0 sample 94 Xi=(6, 29457), yi=(1, 29457), Loss=10.082760188806834\n",
      "PHRASE 0 sample 95 Xi=(6, 29457), yi=(1, 29457), Loss=10.296476206654257\n",
      "PHRASE 0 sample 96 Xi=(6, 29457), yi=(1, 29457), Loss=10.622897471397211\n",
      "PHRASE 0 sample 97 Xi=(6, 29457), yi=(1, 29457), Loss=9.929495113348862\n",
      "PHRASE 0 sample 98 Xi=(6, 29457), yi=(1, 29457), Loss=9.889092892589614\n",
      "PHRASE 0 sample 99 Xi=(6, 29457), yi=(1, 29457), Loss=9.862374456154685\n",
      "PHRASE 0 sample 100 Xi=(6, 29457), yi=(1, 29457), Loss=10.558118540845415\n",
      "PHRASE 0 sample 101 Xi=(6, 29457), yi=(1, 29457), Loss=10.044716864235772\n",
      "PHRASE 0 sample 102 Xi=(6, 29457), yi=(1, 29457), Loss=10.286350415155805\n",
      "PHRASE 0 sample 103 Xi=(6, 29457), yi=(1, 29457), Loss=10.588406978836545\n",
      "PHRASE 0 sample 104 Xi=(6, 29457), yi=(1, 29457), Loss=10.305737930757726\n",
      "PHRASE 0 sample 105 Xi=(6, 29457), yi=(1, 29457), Loss=9.874508533863626\n",
      "PHRASE 0 sample 106 Xi=(6, 29457), yi=(1, 29457), Loss=9.818462555940751\n",
      "PHRASE 0 sample 107 Xi=(6, 29457), yi=(1, 29457), Loss=10.701225583463863\n",
      "PHRASE 0 sample 108 Xi=(6, 29457), yi=(1, 29457), Loss=10.20783297943148\n",
      "PHRASE 0 sample 109 Xi=(6, 29457), yi=(1, 29457), Loss=10.816672648055455\n",
      "PHRASE 0 sample 110 Xi=(6, 29457), yi=(1, 29457), Loss=10.2995701060593\n",
      "PHRASE 0 sample 111 Xi=(6, 29457), yi=(1, 29457), Loss=10.396823227885855\n",
      "PHRASE 0 sample 112 Xi=(6, 29457), yi=(1, 29457), Loss=10.126983801046089\n",
      "PHRASE 0 sample 113 Xi=(6, 29457), yi=(1, 29457), Loss=10.253882645812581\n",
      "PHRASE 0 sample 114 Xi=(6, 29457), yi=(1, 29457), Loss=9.98946386109312\n",
      "PHRASE 0 sample 115 Xi=(6, 29457), yi=(1, 29457), Loss=10.309224256917487\n",
      "PHRASE 0 sample 116 Xi=(6, 29457), yi=(1, 29457), Loss=10.247290844730461\n",
      "PHRASE 0 sample 117 Xi=(6, 29457), yi=(1, 29457), Loss=10.716616951734649\n",
      "PHRASE 0 sample 118 Xi=(6, 29457), yi=(1, 29457), Loss=10.698495337478352\n",
      "PHRASE 0 sample 119 Xi=(6, 29457), yi=(1, 29457), Loss=10.54227312576958\n",
      "PHRASE 0 sample 120 Xi=(6, 29457), yi=(1, 29457), Loss=10.003472899689175\n",
      "PHRASE 0 sample 121 Xi=(6, 29457), yi=(1, 29457), Loss=10.04418578023918\n",
      "PHRASE 0 sample 122 Xi=(6, 29457), yi=(1, 29457), Loss=10.552139546079518\n",
      "PHRASE 0 sample 123 Xi=(6, 29457), yi=(1, 29457), Loss=10.326551480848728\n",
      "PHRASE 0 sample 124 Xi=(6, 29457), yi=(1, 29457), Loss=10.46062654983182\n",
      "PHRASE 0 sample 125 Xi=(6, 29457), yi=(1, 29457), Loss=10.257519001972893\n",
      "PHRASE 0 sample 126 Xi=(6, 29457), yi=(1, 29457), Loss=10.295418135526168\n",
      "PHRASE 0 sample 127 Xi=(6, 29457), yi=(1, 29457), Loss=10.559993429633442\n",
      "PHRASE 0 sample 128 Xi=(6, 29457), yi=(1, 29457), Loss=9.951195496092664\n",
      "PHRASE 0 sample 129 Xi=(6, 29457), yi=(1, 29457), Loss=10.474931154900842\n",
      "PHRASE 0 sample 130 Xi=(6, 29457), yi=(1, 29457), Loss=9.920179264978286\n",
      "PHRASE 0 sample 131 Xi=(6, 29457), yi=(1, 29457), Loss=10.436046284840632\n",
      "PHRASE 0 sample 132 Xi=(6, 29457), yi=(1, 29457), Loss=10.1487078829445\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[163], line 99\u001b[0m\n\u001b[0;32m     97\u001b[0m complete_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m     98\u001b[0m words_vectorizer \u001b[38;5;241m=\u001b[39m Word2Vec(embedding_size, semi_context_window, complete_text, flattening_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 99\u001b[0m \u001b[43mwords_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[163], line 88\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[1;34m(self, X_train)\u001b[0m\n\u001b[0;32m     85\u001b[0m Xi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(training_samples[n][\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     86\u001b[0m yi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(training_samples[n][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m---> 88\u001b[0m Z_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m Loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_entropy_loss(Z_out, yi)\n\u001b[0;32m     90\u001b[0m dLoss_dSigma_Zout \u001b[38;5;241m=\u001b[39m Sigma_Zout \u001b[38;5;241m-\u001b[39m yi\n",
      "Cell \u001b[1;32mIn[163], line 62\u001b[0m, in \u001b[0;36mWord2Vec.forward\u001b[1;34m(self, Input)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, Input):\n\u001b[1;32m---> 62\u001b[0m     sigma_zout_one \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mInput\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords_len_embedding_layer\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwords_len_embedding_bias)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflattening_strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcat\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     64\u001b[0m         sigma_zout_one \u001b[38;5;241m=\u001b[39m sigma_zout_one\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, sigma_zout_one\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_size) \n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "class Word2Vec:\n",
    "\n",
    "    def __init__(self, embedding_size, semi_context_window, complete_text, flattening_strategy=\"concat\"):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.side_window_size = semi_context_window\n",
    "        self.complete_text = complete_text\n",
    "        self.vocabulary = self.create_vocabulary()\n",
    "        self.words_len_embedding_layer = np.random.rand(len(self.vocabulary), embedding_size)\n",
    "        self.words_len_embedding_bias = np.random.rand(embedding_size)\n",
    "        self.flattening_strategy = flattening_strategy\n",
    "        self.outlayer_maps_vocab_average = np.random.rand(embedding_size, len(self.vocabulary))\n",
    "        self.out_bias_maps_vocab_average = np.random.rand(len(self.vocabulary))\n",
    "        self.outlayer_maps_vocab_concat = np.random.rand(semi_context_window * 2 * embedding_size, len(self.vocabulary))\n",
    "        self.out_bias_maps_vocab_concat = np.random.rand(len(self.vocabulary))\n",
    "        self.learning_rate = 0.01\n",
    "\n",
    "    def cross_entropy_loss(self, predictions, target):\n",
    "        # Cross-entropy loss for a batch of predictions and targets\n",
    "        batch_loss = -np.sum(target * np.log(predictions + 1e-9), axis=1)\n",
    "        return np.mean(batch_loss)\n",
    "\n",
    "    def softmax(self, x, axis=-1):\n",
    "        x = np.clip(x, -1e4, 1e4)  # Clip for numerical stability\n",
    "        e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "        return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "    def create_vocabulary(self):\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', self.complete_text).split()\n",
    "        vocabulary = list(set(text))\n",
    "        print(\"Vocabulary size: \", len(vocabulary))\n",
    "        return vocabulary\n",
    "\n",
    "    def input_one_hot_vectors(self, words, vocabulary):\n",
    "        # print(len(words))\n",
    "        inputs = np.zeros((len(words), len(vocabulary)), int)\n",
    "        # print(inputs.shape)\n",
    "        for i in range(len(words)):\n",
    "            inputs[i][vocabulary.index(words[i])] = 1\n",
    "        return inputs\n",
    "\n",
    "    def inputs_window_words(self, input_text, vocabulary):\n",
    "        sequence = re.sub(r'[^\\w\\s]', ' ', input_text).split()\n",
    "        training_samples = []\n",
    "        for i in range(self.side_window_size, len(sequence) - self.side_window_size):\n",
    "            words_before = sequence[i - self.side_window_size:i]\n",
    "            words_after = sequence[i + 1:i + 1 + self.side_window_size]\n",
    "            # print(words_before)\n",
    "            # print(words_after)\n",
    "            words_input = words_before + words_after\n",
    "            X_i = self.input_one_hot_vectors(words_input, vocabulary)\n",
    "            y_i = self.input_one_hot_vectors([sequence[i]], vocabulary)\n",
    "            # print(words_input,sequence[i])\n",
    "            training_sample = [X_i, y_i]\n",
    "            training_samples.append(training_sample)\n",
    "        yield training_samples\n",
    "\n",
    "    def forward(self, Input):\n",
    "        sigma_zout_one = self.softmax(np.matmul(np.array(Input), self.words_len_embedding_layer) + self.words_len_embedding_bias)\n",
    "        if self.flattening_strategy == \"concat\":\n",
    "            sigma_zout_one = sigma_zout_one.reshape(1, sigma_zout_one.shape[0] * self.embedding_size) \n",
    "            sigma_zout_output = self.softmax(np.matmul(sigma_zout_one, self.outlayer_maps_vocab_concat) + self.out_bias_maps_vocab_concat)\n",
    "\n",
    "        \n",
    "        elif self.flattening_strategy == \"average\":\n",
    "            sigma_zout_one = np.mean(sigma_zout_one, axis=0).reshape(1, sigma_zout_one.shape[1])\n",
    "            sigma_zout_output = self.softmax(\n",
    "                np.matmul(sigma_zout_one, self.outlayer_maps_vocab_average) + self.out_bias_maps_vocab_average)\n",
    "\n",
    "        return sigma_zout_output\n",
    "\n",
    "    def train(self, X_train):\n",
    "        for i in range(len(X_train)):\n",
    "            gen = self.inputs_window_words(X_train[i], self.vocabulary)\n",
    "            for training_samples in gen:\n",
    "                \n",
    "                Xi = np.array(training_samples[0][0])\n",
    "                yi = np.array(training_samples[0][1])\n",
    "                print(f\"training samples from text {i}: {len(training_samples)}\")\n",
    "                \n",
    "                for n in range(len(training_samples)):\n",
    "                    Xi = np.array(training_samples[n][0])\n",
    "                    yi = np.array(training_samples[n][1])\n",
    "                    \n",
    "                    Z_out = self.forward(Xi)\n",
    "                    Loss = self.cross_entropy_loss(Z_out, yi)\n",
    "                    dLoss_dZout = Sigma_Zout - yi\n",
    "                    if flattening_strategy==\"concat\":\n",
    "                        \n",
    "                        pass\n",
    "                    else:\n",
    "                        \n",
    "                    \n",
    "                    dLoss_dSigma_Zout_one=dLoss_dZout*\n",
    "                    \n",
    "                    print(f\"PHRASE {i} sample {n} Xi={Xi.shape}, yi={yi.shape}, Loss={Loss}\")\n",
    "                    \n",
    "\n",
    "\n",
    "embedding_size = 300\n",
    "semi_context_window = 3\n",
    "complete_text = ' '.join(df[\"text\"].tolist())\n",
    "words_vectorizer = Word2Vec(embedding_size, semi_context_window, complete_text, flattening_strategy=\"concat\")\n",
    "words_vectorizer.train(df[\"text\"].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ffdbd8-4293-477b-a403-7c65dd07679c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deed2ac-9239-4dbd-821f-4464508cd0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49365a2b-a199-4968-b68b-fb92d21e1fbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "embedding_size=50\n",
    "semi_context_window=5\n",
    "complete_text = ' '.join(df[\"text\"].tolist())\n",
    "words_vectorizer = Word2Vec(embedding_size, semi_context_window, complete_text,flattening_strategy=\"average\")\n",
    "words_vectorizer.train(df[\"text\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a4d356e1-ea28-49be-91d8-956b0ea71ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tv future in the hands of viewers with home theatre systems  plasma high-definition tvs  and digital video recorders moving into the living room  the way people watch tv will be radically different in five years  time.  that is according to an expert panel which gathered at the annual consumer electronics show in las vegas to discuss how these new technologies will impact one of our favourite pastimes. with the us leading the trend  programmes and other content will be delivered to viewers via home networks  through cable  satellite  telecoms companies  and broadband service providers to front rooms and portable devices.  one of the most talked-about technologies of ces has been digital and personal video recorders (dvr and pvr). these set-top boxes  like the us s tivo and the uk s sky+ system  allow people to record  store  play  pause and forward wind tv programmes when they want.  essentially  the technology allows for much more personalised tv. they are also being built-in to high-definition tv sets  which are big business in japan and the us  but slower to take off in europe because of the lack of high-definition programming. not only can people forward wind through adverts  they can also forget about abiding by network and channel schedules  putting together their own a-la-carte entertainment. but some us networks and cable and satellite companies are worried about what it means for them in terms of advertising revenues as well as  brand identity  and viewer loyalty to channels. although the us leads in this technology at the moment  it is also a concern that is being raised in europe  particularly with the growing uptake of services like sky+.  what happens here today  we will see in nine months to a years  time in the uk   adam hume  the bbc broadcast s futurologist told the bbc news website. for the likes of the bbc  there are no issues of lost advertising revenue yet. it is a more pressing issue at the moment for commercial uk broadcasters  but brand loyalty is important for everyone.  we will be talking more about content brands rather than network brands   said tim hanlon  from brand communications firm starcom mediavest.  the reality is that with broadband connections  anybody can be the producer of content.  he added:  the challenge now is that it is hard to promote a programme with so much choice.   what this means  said stacey jolna  senior vice president of tv guide tv group  is that the way people find the content they want to watch has to be simplified for tv viewers. it means that networks  in us terms  or channels could take a leaf out of google s book and be the search engine of the future  instead of the scheduler to help people find what they want to watch. this kind of channel model might work for the younger ipod generation which is used to taking control of their gadgets and what they play on them. but it might not suit everyone  the panel recognised. older generations are more comfortable with familiar schedules and channel brands because they know what they are getting. they perhaps do not want so much of the choice put into their hands  mr hanlon suggested.  on the other end  you have the kids just out of diapers who are pushing buttons already - everything is possible and available to them   said mr hanlon.  ultimately  the consumer will tell the market they want.   of the 50 000 new gadgets and technologies being showcased at ces  many of them are about enhancing the tv-watching experience. high-definition tv sets are everywhere and many new models of lcd (liquid crystal display) tvs have been launched with dvr capability built into them  instead of being external boxes. one such example launched at the show is humax s 26-inch lcd tv with an 80-hour tivo dvr and dvd recorder. one of the us s biggest satellite tv companies  directtv  has even launched its own branded dvr at the show with 100-hours of recording capability  instant replay  and a search function. the set can pause and rewind tv for up to 90 hours. and microsoft chief bill gates announced in his pre-show keynote speech a partnership with tivo  called tivotogo  which means people can play recorded programmes on windows pcs and mobile devices. all these reflect the increasing trend of freeing up multimedia so that people can watch what they want  when they want.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "eb9e141b-9b86-4c51-8a26-889087fcdb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "class Word2Vec:\n",
    "\n",
    "    def __init__(self, embedding_size, semi_context_window, complete_text, vocabulary, flattening_strategy=\"concat\"):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.side_window_size = semi_context_window\n",
    "        self.complete_text = complete_text\n",
    "        self.vocabulary = vocabulary\n",
    "\n",
    "        self.words_len_embedding_layer = np.random.rand(len(self.vocabulary), embedding_size)\n",
    "        self.words_len_embedding_bias = np.random.rand(embedding_size)\n",
    "        self.flattening_strategy = flattening_strategy\n",
    "\n",
    "        self.outlayer_maps_vocab_average = np.random.rand(embedding_size, len(self.vocabulary))\n",
    "        self.out_bias_maps_vocab_average = np.random.rand(len(self.vocabulary))\n",
    "\n",
    "        self.outlayer_maps_vocab_concat = np.random.rand(semi_context_window * 2 * embedding_size, len(self.vocabulary))\n",
    "        self.out_bias_maps_vocab_concat = np.random.rand(len(self.vocabulary))\n",
    "        self.learning_rate = 0.005\n",
    "\n",
    "    def cross_entropy_loss(self, predictions, target):\n",
    "        # Cross-entropy loss for a batch of predictions and targets\n",
    "        batch_loss = -np.sum(target * np.log(predictions + 1e-9), axis=1)\n",
    "        return np.mean(batch_loss)\n",
    "\n",
    "    def softmax(self, x, axis=-1):\n",
    "        x = np.clip(x, -1e4, 1e4)  # Clip for numerical stability\n",
    "        e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "        return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "    def input_one_hot_vectors(self, words, vocabulary):\n",
    "        # print(len(words))\n",
    "        inputs = np.zeros((len(words), len(vocabulary)), int)\n",
    "        # print(inputs.shape)\n",
    "        for i in range(len(words)):\n",
    "            inputs[i][vocabulary.index(words[i])] = 1\n",
    "        return inputs\n",
    "\n",
    "    def inputs_window_words(self, input_text, vocabulary):\n",
    "        sequence = re.sub(r'[^\\w\\s]', ' ', input_text).split()\n",
    "        training_samples = []\n",
    "        for i in range(self.side_window_size, len(sequence) - self.side_window_size):\n",
    "            words_before = sequence[i - self.side_window_size:i]\n",
    "            words_after = sequence[i + 1:i + 1 + self.side_window_size]\n",
    "            # print(words_before)\n",
    "            # print(words_after)\n",
    "            words_input = words_before + words_after\n",
    "            X_i = self.input_one_hot_vectors(words_input, vocabulary)\n",
    "            y_i = self.input_one_hot_vectors([sequence[i]], vocabulary)\n",
    "            # print(words_input,sequence[i])\n",
    "            training_sample = [X_i, y_i]\n",
    "            training_samples.append(training_sample)\n",
    "        yield training_samples\n",
    "\n",
    "    def forward(self, Input):\n",
    "        sigma_zout_one = self.softmax(\n",
    "            np.matmul(np.array(Input), self.words_len_embedding_layer) + self.words_len_embedding_bias)\n",
    "        if self.flattening_strategy == \"concat\":\n",
    "            sigma_zout_one = sigma_zout_one.reshape(1, sigma_zout_one.shape[0] * self.embedding_size)\n",
    "            sigma_zout_output = self.softmax(\n",
    "                np.matmul(sigma_zout_one, self.outlayer_maps_vocab_concat) + self.out_bias_maps_vocab_concat)\n",
    "\n",
    "\n",
    "        elif self.flattening_strategy == \"average\":\n",
    "            sigma_zout_one = np.mean(sigma_zout_one, axis=0).reshape(1, sigma_zout_one.shape[1])\n",
    "            sigma_zout_output = self.softmax(\n",
    "                np.matmul(sigma_zout_one, self.outlayer_maps_vocab_average) + self.out_bias_maps_vocab_average)\n",
    "\n",
    "        return [sigma_zout_one, sigma_zout_output]\n",
    "\n",
    "    def backpropagation(self, dLoss_dZ2, sigma_Z_1, sigma_Z_2, Xi):\n",
    "        # print(\"dLoss_dZ2\", dLoss_dZ2.shape)\n",
    "        # print(\"W2\", self.outlayer_maps_vocab_concat.shape)\n",
    "        # print(\"b2\", self.out_bias_maps_vocab_concat.shape)\n",
    "        # print(\"sigma_Z_1\", sigma_Z_1.shape)\n",
    "        # print(\"sigma_Z_2\", sigma_Z_2.shape)\n",
    "        # print(\"W1\", self.words_len_embedding_layer.shape)\n",
    "        # print(\"b1\", self.words_len_embedding_bias.shape)\n",
    "        # print(\"Xi\", Xi.shape)\n",
    "\n",
    "        if self.flattening_strategy == \"concat\":\n",
    "\n",
    "            dLoss_dW2 = np.matmul(sigma_Z_1.T, dLoss_dZ2)  # (1800, 29457)\n",
    "        \n",
    "            # Gradient of loss with respect to b2\n",
    "            dLoss_db2 = np.sum(dLoss_dZ2, axis=0)  # (29457,)\n",
    "            \n",
    "            # Gradient of loss with respect to Z1\n",
    "            dLoss_dZ1 = np.matmul(dLoss_dZ2, self.outlayer_maps_vocab_concat.T)  # (1, 1800)\n",
    "            \n",
    "            # Reshape dLoss_dZ1 to match the original shape of sigma_Z_1\n",
    "            #dLoss_dZ1 = dLoss_dZ1.reshape(-1, self.side_window_size * 2, self.embedding_size)  # (1, 6, 300)\n",
    "            \n",
    "            # Gradient through softmax\n",
    "            # Note: sigma_Z_1 already contains the softmax output\n",
    "            dZ1_dPreSoftmax = sigma_Z_1 * (1 - sigma_Z_1)  # (1, 6, 300)\n",
    "            dLoss_dPreSoftmax = dLoss_dZ1 * dZ1_dPreSoftmax  # (1, 6, 300)\n",
    "            \n",
    "            # Gradient of loss with respect to W1\n",
    "            dLoss_dW1 = np.matmul(Xi.T, dLoss_dPreSoftmax.reshape(Xi.shape[0], -1))  # (29457, 1800)\n",
    "            \n",
    "            # Gradient of loss with respect to b1\n",
    "            dLoss_db1 = np.sum(dLoss_dPreSoftmax, axis=(0, 1))  # (300,)\n",
    "            \n",
    "            # Update weights and biases\n",
    "            self.outlayer_maps_vocab_concat -= self.learning_rate * dLoss_dW2\n",
    "            self.out_bias_maps_vocab_concat -= self.learning_rate * dLoss_db2\n",
    "            self.words_len_embedding_layer -= self.learning_rate * dLoss_dW1\n",
    "            self.words_len_embedding_bias -= self.learning_rate * dLoss_db1\n",
    "\n",
    "            pass\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def update(self, dLoss_dWi, dLoss_dWo):\n",
    "\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        sigma_zout_one = self.softmax(np.matmul(np.array(X), self.words_len_embedding_layer) + self.words_len_embedding_bias)\n",
    "        return sigma_zout_one\n",
    "\n",
    "    \n",
    "    def train(self, X_train):\n",
    "        total_loss=0\n",
    "        for trainingepoch in range(10): \n",
    "            for i in range(len(X_train)):\n",
    "                gen = self.inputs_window_words(X_train[i], self.vocabulary)\n",
    "                for training_samples in gen:\n",
    "    \n",
    "                    \n",
    "                    print(f\"training samples from row {i}: {len(training_samples)}\")\n",
    "    \n",
    "                    for n in range(len(training_samples)):\n",
    "                        Xi = np.array(training_samples[n][0])\n",
    "                        yi = training_samples[n][1]\n",
    "                         \n",
    "                        sigmaz_out_one, sigma_zout_two = self.forward(Xi)\n",
    "                        Loss = self.cross_entropy_loss(sigma_zout_two, yi)\n",
    "                        total_loss+=Loss\n",
    "                        dLoss_dZout = sigma_zout_two - yi\n",
    "                         \n",
    "                        self.backpropagation(dLoss_dZout, sigmaz_out_one, sigma_zout_two, Xi)\n",
    "                        #print(f\"PHRASE {i} sample {n} Xi={Xi.shape}, yi={yi.shape},target={self.vocabulary[np.where(yi[0] != 0)[0][0]]} Loss={Loss}\")\n",
    "                        \n",
    "                    print(f\"Loss sample: {total_loss/len(training_samples)}\")\n",
    "                    total_loss=0\n",
    "\n",
    "\n",
    "def create_vocabulary(complete_text):\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', complete_text).split()\n",
    "    vocabulary = list(set(text))\n",
    "    print(\"Vocabulary size: \", len(vocabulary))\n",
    "    return vocabulary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "5ed66b17-c916-4608-ad61-16065559493c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  29457\n",
      "training samples from row 0: 296\n",
      "PHRASE 0 sample 0 Xi=(4, 29457), yi=(1, 29457),target=left Loss=9.989566388502196\n",
      "PHRASE 0 sample 1 Xi=(4, 29457), yi=(1, 29457),target=books Loss=9.860384177923512\n",
      "PHRASE 0 sample 2 Xi=(4, 29457), yi=(1, 29457),target=alone Loss=10.551344206378214\n",
      "PHRASE 0 sample 3 Xi=(4, 29457), yi=(1, 29457),target=former Loss=9.981538179597873\n",
      "PHRASE 0 sample 4 Xi=(4, 29457), yi=(1, 29457),target=worldcom Loss=10.06708663143755\n",
      "PHRASE 0 sample 5 Xi=(4, 29457), yi=(1, 29457),target=boss Loss=10.111815796032644\n",
      "PHRASE 0 sample 6 Xi=(4, 29457), yi=(1, 29457),target=bernie Loss=10.11394481792694\n",
      "PHRASE 0 sample 7 Xi=(4, 29457), yi=(1, 29457),target=ebbers Loss=10.385186476534608\n",
      "PHRASE 0 sample 8 Xi=(4, 29457), yi=(1, 29457),target=who Loss=10.196947719394757\n",
      "PHRASE 0 sample 9 Xi=(4, 29457), yi=(1, 29457),target=is Loss=10.865645824875665\n",
      "PHRASE 0 sample 10 Xi=(4, 29457), yi=(1, 29457),target=accused Loss=10.343677131603002\n",
      "PHRASE 0 sample 11 Xi=(4, 29457), yi=(1, 29457),target=of Loss=10.15781425199461\n",
      "PHRASE 0 sample 12 Xi=(4, 29457), yi=(1, 29457),target=overseeing Loss=10.57955134667108\n",
      "PHRASE 0 sample 13 Xi=(4, 29457), yi=(1, 29457),target=an Loss=9.805191724377757\n",
      "PHRASE 0 sample 14 Xi=(4, 29457), yi=(1, 29457),target=11bn Loss=10.626829911018797\n",
      "PHRASE 0 sample 15 Xi=(4, 29457), yi=(1, 29457),target=5 Loss=10.26141931709186\n",
      "PHRASE 0 sample 16 Xi=(4, 29457), yi=(1, 29457),target=8bn Loss=10.218932881880688\n",
      "PHRASE 0 sample 17 Xi=(4, 29457), yi=(1, 29457),target=fraud Loss=9.97818048967739\n",
      "PHRASE 0 sample 18 Xi=(4, 29457), yi=(1, 29457),target=never Loss=10.68268860840327\n",
      "PHRASE 0 sample 19 Xi=(4, 29457), yi=(1, 29457),target=made Loss=10.245846481539353\n",
      "PHRASE 0 sample 20 Xi=(4, 29457), yi=(1, 29457),target=accounting Loss=9.843567740750606\n",
      "PHRASE 0 sample 21 Xi=(4, 29457), yi=(1, 29457),target=decisions Loss=10.688025155758183\n",
      "PHRASE 0 sample 22 Xi=(4, 29457), yi=(1, 29457),target=a Loss=10.038827471845073\n",
      "PHRASE 0 sample 23 Xi=(4, 29457), yi=(1, 29457),target=witness Loss=10.2424744801585\n",
      "PHRASE 0 sample 24 Xi=(4, 29457), yi=(1, 29457),target=has Loss=10.033583897423293\n",
      "PHRASE 0 sample 25 Xi=(4, 29457), yi=(1, 29457),target=told Loss=10.40559680424452\n",
      "PHRASE 0 sample 26 Xi=(4, 29457), yi=(1, 29457),target=jurors Loss=9.979205943900736\n",
      "PHRASE 0 sample 27 Xi=(4, 29457), yi=(1, 29457),target=david Loss=10.328033208696738\n",
      "PHRASE 0 sample 28 Xi=(4, 29457), yi=(1, 29457),target=myers Loss=9.913167429397214\n",
      "PHRASE 0 sample 29 Xi=(4, 29457), yi=(1, 29457),target=made Loss=10.253259601270505\n",
      "PHRASE 0 sample 30 Xi=(4, 29457), yi=(1, 29457),target=the Loss=10.261368417591521\n",
      "PHRASE 0 sample 31 Xi=(4, 29457), yi=(1, 29457),target=comments Loss=10.400179964202424\n",
      "PHRASE 0 sample 32 Xi=(4, 29457), yi=(1, 29457),target=under Loss=10.450039969518686\n",
      "PHRASE 0 sample 33 Xi=(4, 29457), yi=(1, 29457),target=questioning Loss=10.798735330538697\n",
      "PHRASE 0 sample 34 Xi=(4, 29457), yi=(1, 29457),target=by Loss=10.296747569762543\n",
      "PHRASE 0 sample 35 Xi=(4, 29457), yi=(1, 29457),target=defence Loss=10.197984125775479\n",
      "PHRASE 0 sample 36 Xi=(4, 29457), yi=(1, 29457),target=lawyers Loss=10.40422843415277\n",
      "PHRASE 0 sample 37 Xi=(4, 29457), yi=(1, 29457),target=who Loss=10.191117719563056\n",
      "PHRASE 0 sample 38 Xi=(4, 29457), yi=(1, 29457),target=have Loss=10.49648702970932\n",
      "PHRASE 0 sample 39 Xi=(4, 29457), yi=(1, 29457),target=been Loss=10.029317135477532\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[358], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m flattening_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcat\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m=\u001b[39mWord2Vec(embedding_size, semi_context_window, complete_text, vocabulary, flattening_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[357], line 148\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[1;34m(self, X_train)\u001b[0m\n\u001b[0;32m    145\u001b[0m     dLoss_dZout \u001b[38;5;241m=\u001b[39m sigma_zout_two \u001b[38;5;241m-\u001b[39m yi\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackpropagation(dLoss_dZout, sigmaz_out_one, sigma_zout_two, Xi)\n\u001b[1;32m--> 148\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPHRASE \u001b[39m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124m sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Xi=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mXi\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, yi=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myi\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,target=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocabulary[np\u001b[38;5;241m.\u001b[39mwhere(yi[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m!=\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLoss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss sample: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(training_samples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    151\u001b[0m total_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "complete_text = ' '.join(df[\"text\"].tolist())\n",
    "vocabulary = create_vocabulary(complete_text)\n",
    "embedding_size=100\n",
    "semi_context_window=2 \n",
    "flattening_strategy=\"concat\"\n",
    "model=Word2Vec(embedding_size, semi_context_window, complete_text, vocabulary, flattening_strategy=\"concat\")\n",
    "model.train(df[\"text\"].iloc[1:].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fc6f95-bfa3-4ca3-bf7c-0cd5f7fbf3be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578b022e-e518-4267-a517-75271a6667af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "770b1867-9a6f-4752-9d87-f1f6280f4914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  29457\n",
      "training samples from row 0: 746\n",
      "Loss sample: 10.339881447509951\n",
      "training samples from row 1: 296\n",
      "Loss sample: 10.303335499291242\n",
      "training samples from row 2: 244\n",
      "Loss sample: 10.278985434263609\n",
      "training samples from row 3: 345\n",
      "Loss sample: 10.289031145067018\n",
      "training samples from row 4: 265\n",
      "Loss sample: 10.305471162116696\n",
      "training samples from row 5: 621\n",
      "Loss sample: 10.255056942862838\n",
      "training samples from row 6: 265\n",
      "Loss sample: 10.218099685164837\n",
      "training samples from row 7: 194\n",
      "Loss sample: 10.240909190597455\n",
      "training samples from row 8: 161\n",
      "Loss sample: 10.278276155293653\n",
      "training samples from row 9: 232\n",
      "Loss sample: 10.180323131398156\n",
      "training samples from row 10: 307\n",
      "Loss sample: 10.211666986607627\n",
      "training samples from row 11: 198\n",
      "Loss sample: 10.26137337517218\n",
      "training samples from row 12: 338\n",
      "Loss sample: 10.17480218428747\n",
      "training samples from row 13: 287\n",
      "Loss sample: 10.16308247236951\n",
      "training samples from row 14: 462\n",
      "Loss sample: 10.171837546340958\n",
      "training samples from row 15: 290\n",
      "Loss sample: 10.175904115988377\n",
      "training samples from row 16: 408\n",
      "Loss sample: 10.162027072798741\n",
      "training samples from row 17: 439\n",
      "Loss sample: 10.059245110719434\n",
      "training samples from row 18: 290\n",
      "Loss sample: 10.08594996658016\n",
      "training samples from row 19: 467\n",
      "Loss sample: 10.112935175332666\n",
      "training samples from row 20: 221\n",
      "Loss sample: 9.967561118495992\n",
      "training samples from row 21: 353\n",
      "Loss sample: 10.059268082267284\n",
      "training samples from row 22: 613\n",
      "Loss sample: 9.98347892280274\n",
      "training samples from row 23: 152\n",
      "Loss sample: 9.964989407242154\n",
      "training samples from row 24: 664\n",
      "Loss sample: 9.985242814014693\n",
      "training samples from row 25: 211\n",
      "Loss sample: 9.957552057615189\n",
      "training samples from row 26: 243\n",
      "Loss sample: 10.031908037365609\n",
      "training samples from row 27: 298\n",
      "Loss sample: 10.014283827391663\n",
      "training samples from row 28: 579\n",
      "Loss sample: 9.994750837394477\n",
      "training samples from row 29: 395\n",
      "Loss sample: 9.936843510626405\n",
      "training samples from row 30: 476\n",
      "Loss sample: 9.901093857977077\n",
      "training samples from row 31: 642\n",
      "Loss sample: 9.855268773587813\n",
      "training samples from row 32: 221\n",
      "Loss sample: 9.799360330266975\n",
      "training samples from row 33: 297\n",
      "Loss sample: 9.835200884567607\n",
      "training samples from row 34: 139\n",
      "Loss sample: 9.96322497417618\n",
      "training samples from row 35: 260\n",
      "Loss sample: 9.836022294050858\n",
      "training samples from row 36: 185\n",
      "Loss sample: 9.784545370498325\n",
      "training samples from row 37: 315\n",
      "Loss sample: 9.749752994068448\n",
      "training samples from row 38: 510\n",
      "Loss sample: 9.774901499159542\n",
      "training samples from row 39: 258\n",
      "Loss sample: 9.704787966580133\n",
      "training samples from row 40: 363\n",
      "Loss sample: 9.890568056530922\n",
      "training samples from row 41: 353\n",
      "Loss sample: 9.75126858313297\n",
      "training samples from row 42: 410\n",
      "Loss sample: 9.694812917970259\n",
      "training samples from row 43: 181\n",
      "Loss sample: 9.814202083717738\n",
      "training samples from row 44: 311\n",
      "Loss sample: 9.523295639506\n",
      "training samples from row 45: 1302\n",
      "Loss sample: 9.72639900838195\n",
      "training samples from row 46: 498\n",
      "Loss sample: 9.577814977678216\n",
      "training samples from row 47: 577\n",
      "Loss sample: 9.592078522417118\n",
      "training samples from row 48: 212\n",
      "Loss sample: 9.620640196780279\n",
      "training samples from row 49: 157\n",
      "Loss sample: 9.626273221655808\n",
      "training samples from row 50: 234\n",
      "Loss sample: 9.523717325888928\n",
      "training samples from row 51: 534\n",
      "Loss sample: 9.58728800659349\n",
      "training samples from row 52: 313\n",
      "Loss sample: 9.666010603680343\n",
      "training samples from row 53: 274\n",
      "Loss sample: 9.687107618940683\n",
      "training samples from row 54: 300\n",
      "Loss sample: 9.576519048223743\n",
      "training samples from row 55: 176\n",
      "Loss sample: 9.513589559639101\n",
      "training samples from row 56: 509\n",
      "Loss sample: 9.545077808917057\n",
      "training samples from row 57: 641\n",
      "Loss sample: 9.686531745294477\n",
      "training samples from row 58: 424\n",
      "Loss sample: 9.706699590902621\n",
      "training samples from row 59: 496\n",
      "Loss sample: 9.467169956521024\n",
      "training samples from row 60: 460\n",
      "Loss sample: 9.39065265405434\n",
      "training samples from row 61: 325\n",
      "Loss sample: 9.579688826653113\n",
      "training samples from row 62: 227\n",
      "Loss sample: 9.310411820584113\n",
      "training samples from row 63: 166\n",
      "Loss sample: 9.610062674879282\n",
      "training samples from row 64: 178\n",
      "Loss sample: 9.279277134047552\n",
      "training samples from row 65: 431\n",
      "Loss sample: 9.513997880840055\n",
      "training samples from row 66: 268\n",
      "Loss sample: 9.382884450265983\n",
      "training samples from row 67: 402\n",
      "Loss sample: 9.145092508859177\n",
      "training samples from row 68: 233\n",
      "Loss sample: 9.451891483999656\n",
      "training samples from row 69: 352\n",
      "Loss sample: 9.459827730065728\n",
      "training samples from row 70: 621\n",
      "Loss sample: 9.316772144652672\n",
      "training samples from row 71: 213\n",
      "Loss sample: 9.310863611618627\n",
      "training samples from row 72: 457\n",
      "Loss sample: 9.32154488393497\n",
      "training samples from row 73: 203\n",
      "Loss sample: 9.389731917757457\n",
      "training samples from row 74: 189\n",
      "Loss sample: 9.09792320656061\n",
      "training samples from row 75: 604\n",
      "Loss sample: 9.398886930286325\n",
      "training samples from row 76: 419\n",
      "Loss sample: 9.400186535510919\n",
      "training samples from row 77: 520\n",
      "Loss sample: 9.184411206002924\n",
      "training samples from row 78: 188\n",
      "Loss sample: 9.663236944180568\n",
      "training samples from row 79: 270\n",
      "Loss sample: 9.15173795145413\n",
      "training samples from row 80: 231\n",
      "Loss sample: 9.331882211133346\n",
      "training samples from row 81: 175\n",
      "Loss sample: 9.185851293265328\n",
      "training samples from row 82: 311\n",
      "Loss sample: 9.085310895521173\n",
      "training samples from row 83: 235\n",
      "Loss sample: 9.204876887157328\n",
      "training samples from row 84: 232\n",
      "Loss sample: 9.186477451213616\n",
      "training samples from row 85: 287\n",
      "Loss sample: 9.325626336043232\n",
      "training samples from row 86: 319\n",
      "Loss sample: 9.391575077358839\n",
      "training samples from row 87: 326\n",
      "Loss sample: 9.1632924209306\n",
      "training samples from row 88: 540\n",
      "Loss sample: 9.313201084738088\n",
      "training samples from row 89: 221\n",
      "Loss sample: 9.290002014709156\n",
      "training samples from row 90: 257\n",
      "Loss sample: 9.23996978990739\n",
      "training samples from row 91: 125\n",
      "Loss sample: 9.380386750415175\n",
      "training samples from row 92: 643\n",
      "Loss sample: 9.179644753526818\n",
      "training samples from row 93: 221\n",
      "Loss sample: 9.316215195461185\n",
      "training samples from row 94: 494\n",
      "Loss sample: 9.136024919615483\n",
      "training samples from row 95: 131\n",
      "Loss sample: 9.474505332397593\n",
      "training samples from row 96: 236\n",
      "Loss sample: 9.155473350939753\n",
      "training samples from row 97: 653\n",
      "Loss sample: 9.260287724571466\n",
      "training samples from row 98: 186\n",
      "Loss sample: 9.098877830313068\n",
      "training samples from row 99: 591\n",
      "Loss sample: 9.335468695309885\n",
      "training samples from row 100: 199\n",
      "Loss sample: 9.200512023336369\n",
      "training samples from row 101: 406\n",
      "Loss sample: 9.398035457732647\n",
      "training samples from row 102: 897\n",
      "Loss sample: 8.998124710707893\n",
      "training samples from row 103: 577\n",
      "Loss sample: 9.18006607699259\n",
      "training samples from row 104: 374\n",
      "Loss sample: 9.195048091083434\n",
      "training samples from row 105: 370\n",
      "Loss sample: 9.364597442408149\n",
      "training samples from row 106: 151\n",
      "Loss sample: 9.06819544865574\n",
      "training samples from row 107: 301\n",
      "Loss sample: 9.018573015795871\n",
      "training samples from row 108: 266\n",
      "Loss sample: 8.927547164529635\n",
      "training samples from row 109: 732\n",
      "Loss sample: 9.090862816443668\n",
      "training samples from row 110: 342\n",
      "Loss sample: 9.122781334468046\n",
      "training samples from row 111: 309\n",
      "Loss sample: 9.33300720466882\n",
      "training samples from row 112: 169\n",
      "Loss sample: 9.272158947600772\n",
      "training samples from row 113: 232\n",
      "Loss sample: 9.340952088969667\n",
      "training samples from row 114: 145\n",
      "Loss sample: 9.2671973755461\n",
      "training samples from row 115: 232\n",
      "Loss sample: 9.23047744744798\n",
      "training samples from row 116: 167\n",
      "Loss sample: 9.228263375965874\n",
      "training samples from row 117: 400\n",
      "Loss sample: 8.87284937575616\n",
      "training samples from row 118: 524\n",
      "Loss sample: 9.17744139158034\n",
      "training samples from row 119: 455\n",
      "Loss sample: 9.065714944035161\n",
      "training samples from row 120: 152\n",
      "Loss sample: 9.128896591244185\n",
      "training samples from row 121: 162\n",
      "Loss sample: 8.834553458722269\n",
      "training samples from row 122: 299\n",
      "Loss sample: 9.120183222434367\n",
      "training samples from row 123: 540\n",
      "Loss sample: 9.086948021689961\n",
      "training samples from row 124: 275\n",
      "Loss sample: 9.003231836201008\n",
      "training samples from row 125: 315\n",
      "Loss sample: 8.975308639820751\n",
      "training samples from row 126: 567\n",
      "Loss sample: 9.014656586183113\n",
      "training samples from row 127: 192\n",
      "Loss sample: 9.328926086284756\n",
      "training samples from row 128: 513\n",
      "Loss sample: 8.953320608196243\n",
      "training samples from row 129: 389\n",
      "Loss sample: 9.041902241537185\n",
      "training samples from row 130: 391\n",
      "Loss sample: 8.7717835267455\n",
      "training samples from row 131: 432\n",
      "Loss sample: 8.770482242079932\n",
      "training samples from row 132: 294\n",
      "Loss sample: 9.236754408640524\n",
      "training samples from row 133: 299\n",
      "Loss sample: 9.191364284488813\n",
      "training samples from row 134: 268\n",
      "Loss sample: 8.939606697704884\n",
      "training samples from row 135: 446\n",
      "Loss sample: 8.79354594741252\n",
      "training samples from row 136: 162\n",
      "Loss sample: 8.77049506076051\n",
      "training samples from row 137: 321\n",
      "Loss sample: 9.027355092220597\n",
      "training samples from row 138: 306\n",
      "Loss sample: 8.978732658474597\n",
      "training samples from row 139: 228\n",
      "Loss sample: 9.011137242448598\n",
      "training samples from row 140: 668\n",
      "Loss sample: 8.980933625981661\n",
      "training samples from row 141: 373\n",
      "Loss sample: 8.83711587798037\n",
      "training samples from row 142: 866\n",
      "Loss sample: 8.940507843350918\n",
      "training samples from row 143: 392\n",
      "Loss sample: 8.863159019195264\n",
      "training samples from row 144: 390\n",
      "Loss sample: 8.67807162296128\n",
      "training samples from row 145: 301\n",
      "Loss sample: 8.929496847174612\n",
      "training samples from row 146: 511\n",
      "Loss sample: 8.936131333241837\n",
      "training samples from row 147: 232\n",
      "Loss sample: 9.205199396812942\n",
      "training samples from row 148: 462\n",
      "Loss sample: 8.965975449591712\n",
      "training samples from row 149: 784\n",
      "Loss sample: 8.964324819689368\n",
      "training samples from row 150: 255\n",
      "Loss sample: 8.76031832459107\n",
      "training samples from row 151: 450\n",
      "Loss sample: 8.95596748935195\n",
      "training samples from row 152: 252\n",
      "Loss sample: 9.142393073662305\n",
      "training samples from row 153: 326\n",
      "Loss sample: 8.955981443962807\n",
      "training samples from row 154: 513\n",
      "Loss sample: 9.003660727322245\n",
      "training samples from row 155: 727\n",
      "Loss sample: 8.998870475226992\n",
      "training samples from row 156: 381\n",
      "Loss sample: 8.960378644360635\n",
      "training samples from row 157: 437\n",
      "Loss sample: 9.026069718072458\n",
      "training samples from row 158: 307\n",
      "Loss sample: 8.93657970126442\n",
      "training samples from row 159: 271\n",
      "Loss sample: 8.817948372213428\n",
      "training samples from row 160: 229\n",
      "Loss sample: 9.138963924539725\n",
      "training samples from row 161: 876\n",
      "Loss sample: 8.824788459876064\n",
      "training samples from row 162: 488\n",
      "Loss sample: 8.797897555948364\n",
      "training samples from row 163: 194\n",
      "Loss sample: 8.580666707472755\n",
      "training samples from row 164: 383\n",
      "Loss sample: 8.6880292541238\n",
      "training samples from row 165: 296\n",
      "Loss sample: 8.77104750627941\n",
      "training samples from row 166: 285\n",
      "Loss sample: 8.825349808561423\n",
      "training samples from row 167: 530\n",
      "Loss sample: 8.757012219639913\n",
      "training samples from row 168: 632\n",
      "Loss sample: 8.82075268383456\n",
      "training samples from row 169: 322\n",
      "Loss sample: 8.973299271553863\n",
      "training samples from row 170: 523\n",
      "Loss sample: 8.682434759639156\n",
      "training samples from row 171: 444\n",
      "Loss sample: 9.424227794439869\n",
      "training samples from row 172: 364\n",
      "Loss sample: 8.663624863350934\n",
      "training samples from row 173: 536\n",
      "Loss sample: 8.719050062614516\n",
      "training samples from row 174: 545\n",
      "Loss sample: 9.081623282168108\n",
      "training samples from row 175: 314\n",
      "Loss sample: 8.616084976426896\n",
      "training samples from row 176: 329\n",
      "Loss sample: 8.956900974721089\n",
      "training samples from row 177: 194\n",
      "Loss sample: 8.863884833400613\n",
      "training samples from row 178: 485\n",
      "Loss sample: 8.65278463533985\n",
      "training samples from row 179: 473\n",
      "Loss sample: 8.842987976019298\n",
      "training samples from row 180: 318\n",
      "Loss sample: 8.751781884744126\n",
      "training samples from row 181: 759\n",
      "Loss sample: 8.969910283410918\n",
      "training samples from row 182: 226\n",
      "Loss sample: 8.908027394408238\n",
      "training samples from row 183: 290\n",
      "Loss sample: 8.730022271523893\n",
      "training samples from row 184: 306\n",
      "Loss sample: 8.78634630440117\n",
      "training samples from row 185: 139\n",
      "Loss sample: 8.958776870449265\n",
      "training samples from row 186: 369\n",
      "Loss sample: 8.639965896287324\n",
      "training samples from row 187: 140\n",
      "Loss sample: 8.973674319413417\n",
      "training samples from row 188: 516\n",
      "Loss sample: 8.693232681837452\n",
      "training samples from row 189: 586\n",
      "Loss sample: 9.039276246568898\n",
      "training samples from row 190: 252\n",
      "Loss sample: 8.375962839216971\n",
      "training samples from row 191: 814\n",
      "Loss sample: 8.737894038371248\n",
      "training samples from row 192: 567\n",
      "Loss sample: 8.715476989069483\n",
      "training samples from row 193: 398\n",
      "Loss sample: 8.885453232924903\n",
      "training samples from row 194: 417\n",
      "Loss sample: 8.600228341478656\n",
      "training samples from row 195: 333\n",
      "Loss sample: 8.92961817653533\n",
      "training samples from row 196: 240\n",
      "Loss sample: 8.783929042165703\n",
      "training samples from row 197: 153\n",
      "Loss sample: 8.716860974090695\n",
      "training samples from row 198: 345\n",
      "Loss sample: 8.769890432737718\n",
      "training samples from row 199: 402\n",
      "Loss sample: 8.793331486995179\n",
      "training samples from row 200: 168\n",
      "Loss sample: 8.516219819546853\n",
      "training samples from row 201: 299\n",
      "Loss sample: 8.691787216092022\n",
      "training samples from row 202: 461\n",
      "Loss sample: 8.737491686552174\n",
      "training samples from row 203: 435\n",
      "Loss sample: 8.657106489599315\n",
      "training samples from row 204: 220\n",
      "Loss sample: 8.770147088115024\n",
      "training samples from row 205: 262\n",
      "Loss sample: 8.79732641885038\n",
      "training samples from row 206: 436\n",
      "Loss sample: 8.68254707840031\n",
      "training samples from row 207: 284\n",
      "Loss sample: 8.634584763718339\n",
      "training samples from row 208: 276\n",
      "Loss sample: 8.422078211600475\n",
      "training samples from row 209: 382\n",
      "Loss sample: 8.91078280571279\n",
      "training samples from row 210: 219\n",
      "Loss sample: 8.531072686488017\n",
      "training samples from row 211: 495\n",
      "Loss sample: 8.576743575962618\n",
      "training samples from row 212: 167\n",
      "Loss sample: 8.806805803670928\n",
      "training samples from row 213: 233\n",
      "Loss sample: 8.212246396133361\n",
      "training samples from row 214: 364\n",
      "Loss sample: 9.138699540887513\n",
      "training samples from row 215: 204\n",
      "Loss sample: 8.849861836297935\n",
      "training samples from row 216: 289\n",
      "Loss sample: 8.782506969104809\n",
      "training samples from row 217: 243\n",
      "Loss sample: 8.675165188122765\n",
      "training samples from row 218: 692\n",
      "Loss sample: 8.574627377432355\n",
      "training samples from row 219: 196\n",
      "Loss sample: 8.471511926184112\n",
      "training samples from row 220: 210\n",
      "Loss sample: 8.925814622871492\n",
      "training samples from row 221: 408\n",
      "Loss sample: 8.50853304063899\n",
      "training samples from row 222: 935\n",
      "Loss sample: 8.569094746730988\n",
      "training samples from row 223: 425\n",
      "Loss sample: 8.813310689514616\n",
      "training samples from row 224: 355\n",
      "Loss sample: 8.739300355124604\n",
      "training samples from row 225: 297\n",
      "Loss sample: 8.689859784364998\n",
      "training samples from row 226: 812\n",
      "Loss sample: 8.622068917150289\n",
      "training samples from row 227: 1726\n",
      "Loss sample: 8.549725145950198\n",
      "training samples from row 228: 836\n",
      "Loss sample: 8.552057679305452\n",
      "training samples from row 229: 402\n",
      "Loss sample: 9.204194048357158\n",
      "training samples from row 230: 590\n",
      "Loss sample: 8.578323496625837\n",
      "training samples from row 231: 288\n",
      "Loss sample: 8.782214571981555\n",
      "training samples from row 232: 306\n",
      "Loss sample: 8.587104927888822\n",
      "training samples from row 233: 178\n",
      "Loss sample: 8.827325490186148\n",
      "training samples from row 234: 361\n",
      "Loss sample: 8.72612870129007\n",
      "training samples from row 235: 372\n",
      "Loss sample: 8.59910080795613\n",
      "training samples from row 236: 520\n",
      "Loss sample: 8.471925704417647\n",
      "training samples from row 237: 374\n",
      "Loss sample: 8.781052405007461\n",
      "training samples from row 238: 292\n",
      "Loss sample: 8.473674081108108\n",
      "training samples from row 239: 693\n",
      "Loss sample: 8.634329486176767\n",
      "training samples from row 240: 198\n",
      "Loss sample: 8.858779615293605\n",
      "training samples from row 241: 712\n",
      "Loss sample: 8.53049095865514\n",
      "training samples from row 242: 271\n",
      "Loss sample: 8.684076056056128\n",
      "training samples from row 243: 192\n",
      "Loss sample: 8.66646367403032\n",
      "training samples from row 244: 260\n",
      "Loss sample: 8.737121353265293\n",
      "training samples from row 245: 388\n",
      "Loss sample: 8.808125538784982\n",
      "training samples from row 246: 430\n",
      "Loss sample: 8.622107055070055\n",
      "training samples from row 247: 386\n",
      "Loss sample: 8.400081128087121\n",
      "training samples from row 248: 552\n",
      "Loss sample: 8.600851329663488\n",
      "training samples from row 249: 228\n",
      "Loss sample: 8.368022122223152\n",
      "training samples from row 250: 149\n",
      "Loss sample: 8.757820853408328\n",
      "training samples from row 251: 699\n",
      "Loss sample: 8.463820721514342\n",
      "training samples from row 252: 385\n",
      "Loss sample: 8.355733578254876\n",
      "training samples from row 253: 329\n",
      "Loss sample: 8.52782634224402\n",
      "training samples from row 254: 195\n",
      "Loss sample: 8.2002217295289\n",
      "training samples from row 255: 380\n",
      "Loss sample: 8.443950352884494\n",
      "training samples from row 256: 173\n",
      "Loss sample: 8.313499833114502\n",
      "training samples from row 257: 601\n",
      "Loss sample: 8.682183176169962\n",
      "training samples from row 258: 742\n",
      "Loss sample: 8.869995590312083\n",
      "training samples from row 259: 1014\n",
      "Loss sample: 8.584391521772018\n",
      "training samples from row 260: 158\n",
      "Loss sample: 8.541156476972107\n",
      "training samples from row 261: 138\n",
      "Loss sample: 8.831961015518766\n",
      "training samples from row 262: 493\n",
      "Loss sample: 8.689562971978685\n",
      "training samples from row 263: 668\n",
      "Loss sample: 8.60582713771758\n",
      "training samples from row 264: 628\n",
      "Loss sample: 8.508953738209755\n",
      "training samples from row 265: 253\n",
      "Loss sample: 8.593205064175939\n",
      "training samples from row 266: 453\n",
      "Loss sample: 8.526180965516165\n",
      "training samples from row 267: 176\n",
      "Loss sample: 8.365278526490437\n",
      "training samples from row 268: 371\n",
      "Loss sample: 8.972867750881859\n",
      "training samples from row 269: 547\n",
      "Loss sample: 8.680083763922319\n",
      "training samples from row 270: 336\n",
      "Loss sample: 8.53483794653082\n",
      "training samples from row 271: 438\n",
      "Loss sample: 8.59459261360078\n",
      "training samples from row 272: 146\n",
      "Loss sample: 8.610705616034942\n",
      "training samples from row 273: 224\n",
      "Loss sample: 8.835071035505509\n",
      "training samples from row 274: 164\n",
      "Loss sample: 8.832496001842737\n",
      "training samples from row 275: 403\n",
      "Loss sample: 8.48153591785883\n",
      "training samples from row 276: 334\n",
      "Loss sample: 8.62906550180015\n",
      "training samples from row 277: 256\n",
      "Loss sample: 8.809600927736474\n",
      "training samples from row 278: 437\n",
      "Loss sample: 8.404000877728755\n",
      "training samples from row 279: 673\n",
      "Loss sample: 8.47812279302849\n",
      "training samples from row 280: 489\n",
      "Loss sample: 8.492482392062048\n",
      "training samples from row 281: 299\n",
      "Loss sample: 8.57872097026411\n",
      "training samples from row 282: 357\n",
      "Loss sample: 8.62107996359003\n",
      "training samples from row 283: 522\n",
      "Loss sample: 8.704438389828908\n",
      "training samples from row 284: 498\n",
      "Loss sample: 8.548705954343028\n",
      "training samples from row 285: 366\n",
      "Loss sample: 8.600172738317001\n",
      "training samples from row 286: 145\n",
      "Loss sample: 8.57737465574164\n",
      "training samples from row 287: 324\n",
      "Loss sample: 8.040258008629062\n",
      "training samples from row 288: 731\n",
      "Loss sample: 8.465428360967936\n",
      "training samples from row 289: 242\n",
      "Loss sample: 8.661150153089954\n",
      "training samples from row 290: 396\n",
      "Loss sample: 8.476979008087493\n",
      "training samples from row 291: 411\n",
      "Loss sample: 8.395335306379854\n",
      "training samples from row 292: 238\n",
      "Loss sample: 8.633177534672567\n",
      "training samples from row 293: 199\n",
      "Loss sample: 8.604251863298654\n",
      "training samples from row 294: 384\n",
      "Loss sample: 8.553642665551301\n",
      "training samples from row 295: 484\n",
      "Loss sample: 8.380014225337495\n",
      "training samples from row 296: 167\n",
      "Loss sample: 8.98358150996789\n",
      "training samples from row 297: 124\n",
      "Loss sample: 8.700621487118886\n",
      "training samples from row 298: 530\n",
      "Loss sample: 8.374198902274994\n",
      "training samples from row 299: 636\n",
      "Loss sample: 8.820625879346478\n",
      "training samples from row 300: 379\n",
      "Loss sample: 8.562067625160086\n",
      "training samples from row 301: 523\n",
      "Loss sample: 8.321353279782635\n",
      "training samples from row 302: 170\n",
      "Loss sample: 8.784215255293423\n",
      "training samples from row 303: 222\n",
      "Loss sample: 8.772036353394922\n",
      "training samples from row 304: 182\n",
      "Loss sample: 8.49037021908735\n",
      "training samples from row 305: 419\n",
      "Loss sample: 8.532184180116188\n",
      "training samples from row 306: 334\n",
      "Loss sample: 8.813848124495834\n",
      "training samples from row 307: 216\n",
      "Loss sample: 8.60917285758275\n",
      "training samples from row 308: 385\n",
      "Loss sample: 8.284019638590323\n",
      "training samples from row 309: 614\n",
      "Loss sample: 8.448098385721618\n",
      "training samples from row 310: 160\n",
      "Loss sample: 8.44623242080478\n",
      "training samples from row 311: 152\n",
      "Loss sample: 8.636161188400747\n",
      "training samples from row 312: 176\n",
      "Loss sample: 8.810360749179384\n",
      "training samples from row 313: 515\n",
      "Loss sample: 8.325059910340656\n",
      "training samples from row 314: 157\n",
      "Loss sample: 8.608074418291569\n",
      "training samples from row 315: 629\n",
      "Loss sample: 8.115106542266927\n",
      "training samples from row 316: 338\n",
      "Loss sample: 8.331197239331722\n",
      "training samples from row 317: 289\n",
      "Loss sample: 8.463682163275493\n",
      "training samples from row 318: 672\n",
      "Loss sample: 8.5165678333366\n",
      "training samples from row 319: 822\n",
      "Loss sample: 8.362974125443488\n",
      "training samples from row 320: 161\n",
      "Loss sample: 8.218016586869144\n",
      "training samples from row 321: 467\n",
      "Loss sample: 8.400452736940586\n",
      "training samples from row 322: 458\n",
      "Loss sample: 8.512621031494907\n",
      "training samples from row 323: 87\n",
      "Loss sample: 8.518095557388019\n",
      "training samples from row 324: 557\n",
      "Loss sample: 8.625930849251672\n",
      "training samples from row 325: 257\n",
      "Loss sample: 8.170424093347556\n",
      "training samples from row 326: 314\n",
      "Loss sample: 8.50194247520199\n",
      "training samples from row 327: 225\n",
      "Loss sample: 9.17919532872519\n",
      "training samples from row 328: 261\n",
      "Loss sample: 8.61385536113636\n",
      "training samples from row 329: 551\n",
      "Loss sample: 8.180283039554483\n",
      "training samples from row 330: 279\n",
      "Loss sample: 8.412508170806891\n",
      "training samples from row 331: 863\n",
      "Loss sample: 8.255466262640967\n",
      "training samples from row 332: 257\n",
      "Loss sample: 8.424443580996822\n",
      "training samples from row 333: 296\n",
      "Loss sample: 8.39243737833302\n",
      "training samples from row 334: 221\n",
      "Loss sample: 8.690627437626732\n",
      "training samples from row 335: 283\n",
      "Loss sample: 8.375427706070028\n",
      "training samples from row 336: 229\n",
      "Loss sample: 8.719214616503143\n",
      "training samples from row 337: 366\n",
      "Loss sample: 8.373791845757916\n",
      "training samples from row 338: 453\n",
      "Loss sample: 8.462921278431132\n",
      "training samples from row 339: 575\n",
      "Loss sample: 8.325472853567996\n",
      "training samples from row 340: 664\n",
      "Loss sample: 8.4214151813153\n",
      "training samples from row 341: 847\n",
      "Loss sample: 8.384369595700047\n",
      "training samples from row 342: 456\n",
      "Loss sample: 8.782247202770266\n",
      "training samples from row 343: 413\n",
      "Loss sample: 8.249376791570914\n",
      "training samples from row 344: 393\n",
      "Loss sample: 8.570464270054918\n",
      "training samples from row 345: 232\n",
      "Loss sample: 8.559950913737536\n",
      "training samples from row 346: 282\n",
      "Loss sample: 8.611409403794658\n",
      "training samples from row 347: 224\n",
      "Loss sample: 8.428004618399465\n",
      "training samples from row 348: 372\n",
      "Loss sample: 8.29885980320151\n",
      "training samples from row 349: 400\n",
      "Loss sample: 8.470428162591544\n",
      "training samples from row 350: 287\n",
      "Loss sample: 8.167036111078094\n",
      "training samples from row 351: 1024\n",
      "Loss sample: 8.571244208705016\n",
      "training samples from row 352: 575\n",
      "Loss sample: 8.395169801114774\n",
      "training samples from row 353: 253\n",
      "Loss sample: 8.28823300770833\n",
      "training samples from row 354: 380\n",
      "Loss sample: 8.132849000496837\n",
      "training samples from row 355: 248\n",
      "Loss sample: 8.189044545350795\n",
      "training samples from row 356: 300\n",
      "Loss sample: 8.075619183097869\n",
      "training samples from row 357: 512\n",
      "Loss sample: 8.240369447796825\n",
      "training samples from row 358: 164\n",
      "Loss sample: 8.26129992278742\n",
      "training samples from row 359: 366\n",
      "Loss sample: 8.40272877553338\n",
      "training samples from row 360: 252\n",
      "Loss sample: 8.224781998988792\n",
      "training samples from row 361: 265\n",
      "Loss sample: 8.134870705275084\n",
      "training samples from row 362: 268\n",
      "Loss sample: 8.524503618093267\n",
      "training samples from row 363: 236\n",
      "Loss sample: 8.489246917608881\n",
      "training samples from row 364: 605\n",
      "Loss sample: 8.49807772128141\n",
      "training samples from row 365: 253\n",
      "Loss sample: 8.622881533699342\n",
      "training samples from row 366: 649\n",
      "Loss sample: 8.392042395966104\n",
      "training samples from row 367: 163\n",
      "Loss sample: 8.62992486892048\n",
      "training samples from row 368: 347\n",
      "Loss sample: 8.188347683905917\n",
      "training samples from row 369: 204\n",
      "Loss sample: 8.435455639891856\n",
      "training samples from row 370: 825\n",
      "Loss sample: 8.332457971283446\n",
      "training samples from row 371: 279\n",
      "Loss sample: 8.455935846945584\n",
      "training samples from row 372: 190\n",
      "Loss sample: 8.563164084001144\n",
      "training samples from row 373: 383\n",
      "Loss sample: 8.229339942153691\n",
      "training samples from row 374: 205\n",
      "Loss sample: 8.508545497271475\n",
      "training samples from row 375: 617\n",
      "Loss sample: 8.313940547494978\n",
      "training samples from row 376: 339\n",
      "Loss sample: 8.364160891941463\n",
      "training samples from row 377: 294\n",
      "Loss sample: 8.39560332461837\n",
      "training samples from row 378: 439\n",
      "Loss sample: 8.34544190146961\n",
      "training samples from row 379: 350\n",
      "Loss sample: 8.496079653600148\n",
      "training samples from row 380: 293\n",
      "Loss sample: 8.509365197067666\n",
      "training samples from row 381: 329\n",
      "Loss sample: 8.36211609056776\n",
      "training samples from row 382: 196\n",
      "Loss sample: 8.723109230584257\n",
      "training samples from row 383: 329\n",
      "Loss sample: 8.211618237654767\n",
      "training samples from row 384: 260\n",
      "Loss sample: 8.54395105161889\n",
      "training samples from row 385: 865\n",
      "Loss sample: 8.028702390699758\n",
      "training samples from row 386: 249\n",
      "Loss sample: 8.343344616534763\n",
      "training samples from row 387: 276\n",
      "Loss sample: 8.362634345635234\n",
      "training samples from row 388: 215\n",
      "Loss sample: 7.964781060638756\n",
      "training samples from row 389: 592\n",
      "Loss sample: 8.241463010557029\n",
      "training samples from row 390: 338\n",
      "Loss sample: 8.230551200831703\n",
      "training samples from row 391: 288\n",
      "Loss sample: 8.49744853585825\n",
      "training samples from row 392: 297\n",
      "Loss sample: 8.352382833164393\n",
      "training samples from row 393: 388\n",
      "Loss sample: 8.472242579467322\n",
      "training samples from row 394: 273\n",
      "Loss sample: 8.444522237071054\n",
      "training samples from row 395: 260\n",
      "Loss sample: 8.381258244902517\n",
      "training samples from row 396: 222\n",
      "Loss sample: 8.469405115187175\n",
      "training samples from row 397: 520\n",
      "Loss sample: 8.39397053161619\n",
      "training samples from row 398: 476\n",
      "Loss sample: 8.44363885145418\n",
      "training samples from row 399: 498\n",
      "Loss sample: 8.288010301177895\n",
      "training samples from row 400: 242\n",
      "Loss sample: 8.618538720930275\n",
      "training samples from row 401: 337\n",
      "Loss sample: 8.222681483284303\n",
      "training samples from row 402: 453\n",
      "Loss sample: 8.565720173410378\n",
      "training samples from row 403: 452\n",
      "Loss sample: 8.44447559037168\n",
      "training samples from row 404: 334\n",
      "Loss sample: 8.381452975082501\n",
      "training samples from row 405: 461\n",
      "Loss sample: 8.338898921850843\n",
      "training samples from row 406: 196\n",
      "Loss sample: 8.59387028254765\n",
      "training samples from row 407: 238\n",
      "Loss sample: 7.791535594544856\n",
      "training samples from row 408: 4487\n",
      "Loss sample: 8.272068955550349\n",
      "training samples from row 409: 357\n",
      "Loss sample: 8.110169764464523\n",
      "training samples from row 410: 325\n",
      "Loss sample: 8.343763473582275\n",
      "training samples from row 411: 288\n",
      "Loss sample: 8.306988516206552\n",
      "training samples from row 412: 347\n",
      "Loss sample: 8.204362980823507\n",
      "training samples from row 413: 215\n",
      "Loss sample: 8.370211296231117\n",
      "training samples from row 414: 763\n",
      "Loss sample: 7.970524182202638\n",
      "training samples from row 415: 246\n",
      "Loss sample: 8.352166436000967\n",
      "training samples from row 416: 274\n",
      "Loss sample: 8.76957665045341\n",
      "training samples from row 417: 315\n",
      "Loss sample: 8.559354986616734\n",
      "training samples from row 418: 327\n",
      "Loss sample: 8.190603084138537\n",
      "training samples from row 419: 208\n",
      "Loss sample: 8.363422997500619\n",
      "training samples from row 420: 229\n",
      "Loss sample: 8.263872276000619\n",
      "training samples from row 421: 133\n",
      "Loss sample: 8.413904499424863\n",
      "training samples from row 422: 257\n",
      "Loss sample: 8.604293391838944\n",
      "training samples from row 423: 248\n",
      "Loss sample: 8.925939922547263\n",
      "training samples from row 424: 254\n",
      "Loss sample: 8.387549691998723\n",
      "training samples from row 425: 254\n",
      "Loss sample: 8.256804482832901\n",
      "training samples from row 426: 219\n",
      "Loss sample: 8.429480417150675\n",
      "training samples from row 427: 233\n",
      "Loss sample: 8.589946309763453\n",
      "training samples from row 428: 479\n",
      "Loss sample: 8.330950469321097\n",
      "training samples from row 429: 282\n",
      "Loss sample: 8.347094937168034\n",
      "training samples from row 430: 435\n",
      "Loss sample: 8.119459081424893\n",
      "training samples from row 431: 492\n",
      "Loss sample: 8.255610518347439\n",
      "training samples from row 432: 692\n",
      "Loss sample: 8.773583637533099\n",
      "training samples from row 433: 157\n",
      "Loss sample: 8.366998472138322\n",
      "training samples from row 434: 365\n",
      "Loss sample: 8.527535582522926\n",
      "training samples from row 435: 805\n",
      "Loss sample: 8.280636279780548\n",
      "training samples from row 436: 172\n",
      "Loss sample: 8.204856448392952\n",
      "training samples from row 437: 434\n",
      "Loss sample: 8.286190702556732\n",
      "training samples from row 438: 269\n",
      "Loss sample: 8.234381960701477\n",
      "training samples from row 439: 256\n",
      "Loss sample: 8.344898521752645\n",
      "training samples from row 440: 424\n",
      "Loss sample: 8.095977436907164\n",
      "training samples from row 441: 226\n",
      "Loss sample: 8.505707985799132\n",
      "training samples from row 442: 478\n",
      "Loss sample: 8.813172584896618\n",
      "training samples from row 443: 363\n",
      "Loss sample: 8.265353771253427\n",
      "training samples from row 444: 306\n",
      "Loss sample: 8.406040855791952\n",
      "training samples from row 445: 175\n",
      "Loss sample: 8.483680031312366\n",
      "training samples from row 446: 408\n",
      "Loss sample: 8.254664315240507\n",
      "training samples from row 447: 256\n",
      "Loss sample: 8.446275339608825\n",
      "training samples from row 448: 146\n",
      "Loss sample: 8.333868185868536\n",
      "training samples from row 449: 191\n",
      "Loss sample: 8.355297315280112\n",
      "training samples from row 450: 300\n",
      "Loss sample: 8.254812237810278\n",
      "training samples from row 451: 282\n",
      "Loss sample: 8.18026758323587\n",
      "training samples from row 452: 183\n",
      "Loss sample: 8.479399135812853\n",
      "training samples from row 453: 534\n",
      "Loss sample: 8.547601692930153\n",
      "training samples from row 454: 302\n",
      "Loss sample: 8.319029170364772\n",
      "training samples from row 455: 576\n",
      "Loss sample: 8.295381169879628\n",
      "training samples from row 456: 274\n",
      "Loss sample: 7.658903127612376\n",
      "training samples from row 457: 1047\n",
      "Loss sample: 8.30790614632815\n",
      "training samples from row 458: 304\n",
      "Loss sample: 8.182095644257954\n",
      "training samples from row 459: 398\n",
      "Loss sample: 8.338187264272195\n",
      "training samples from row 460: 326\n",
      "Loss sample: 8.316940817843141\n",
      "training samples from row 461: 185\n",
      "Loss sample: 8.452060073215561\n",
      "training samples from row 462: 934\n",
      "Loss sample: 8.190733546600002\n",
      "training samples from row 463: 472\n",
      "Loss sample: 8.472504368002873\n",
      "training samples from row 464: 196\n",
      "Loss sample: 8.302896288337548\n",
      "training samples from row 465: 276\n",
      "Loss sample: 8.192340787266389\n",
      "training samples from row 466: 557\n",
      "Loss sample: 8.048761694651498\n",
      "training samples from row 467: 266\n",
      "Loss sample: 8.553876499391023\n",
      "training samples from row 468: 575\n",
      "Loss sample: 8.309823638382737\n",
      "training samples from row 469: 498\n",
      "Loss sample: 8.297834772144332\n",
      "training samples from row 470: 493\n",
      "Loss sample: 8.324229010475962\n",
      "training samples from row 471: 1380\n",
      "Loss sample: 8.002162390530376\n",
      "training samples from row 472: 575\n",
      "Loss sample: 8.208700782845046\n",
      "training samples from row 473: 219\n",
      "Loss sample: 8.209381065217912\n",
      "training samples from row 474: 299\n",
      "Loss sample: 7.891007177247087\n",
      "training samples from row 475: 246\n",
      "Loss sample: 8.91335841712464\n",
      "training samples from row 476: 421\n",
      "Loss sample: 8.15293610673583\n",
      "training samples from row 477: 341\n",
      "Loss sample: 8.337063702069665\n",
      "training samples from row 478: 472\n",
      "Loss sample: 8.177398313286112\n",
      "training samples from row 479: 188\n",
      "Loss sample: 8.072472480546676\n",
      "training samples from row 480: 228\n",
      "Loss sample: 8.232693365848167\n",
      "training samples from row 481: 357\n",
      "Loss sample: 8.191538669793644\n",
      "training samples from row 482: 2992\n",
      "Loss sample: 8.26909083708984\n",
      "training samples from row 483: 489\n",
      "Loss sample: 8.160739887823212\n",
      "training samples from row 484: 183\n",
      "Loss sample: 8.164284963433552\n",
      "training samples from row 485: 283\n",
      "Loss sample: 8.813404619381306\n",
      "training samples from row 486: 545\n",
      "Loss sample: 8.085073187440827\n",
      "training samples from row 487: 242\n",
      "Loss sample: 8.074661150672508\n",
      "training samples from row 488: 135\n",
      "Loss sample: 8.33038326348662\n",
      "training samples from row 489: 452\n",
      "Loss sample: 8.232544571729811\n",
      "training samples from row 490: 664\n",
      "Loss sample: 8.00144144105484\n",
      "training samples from row 491: 571\n",
      "Loss sample: 8.400630497717856\n",
      "training samples from row 492: 261\n",
      "Loss sample: 8.39298550414923\n",
      "training samples from row 493: 366\n",
      "Loss sample: 7.968538806702904\n",
      "training samples from row 494: 404\n",
      "Loss sample: 8.02719143665791\n",
      "training samples from row 495: 514\n",
      "Loss sample: 8.218678289647942\n",
      "training samples from row 496: 233\n",
      "Loss sample: 8.25588745546205\n",
      "training samples from row 497: 560\n",
      "Loss sample: 8.19341562432084\n",
      "training samples from row 498: 784\n",
      "Loss sample: 8.062397868008762\n",
      "training samples from row 499: 324\n",
      "Loss sample: 8.098240836542228\n",
      "training samples from row 500: 347\n",
      "Loss sample: 7.8582237529636565\n",
      "training samples from row 501: 209\n",
      "Loss sample: 8.49615156612325\n",
      "training samples from row 502: 204\n",
      "Loss sample: 8.471990642789851\n",
      "training samples from row 503: 391\n",
      "Loss sample: 7.969719984953707\n",
      "training samples from row 504: 343\n",
      "Loss sample: 8.041383647941759\n",
      "training samples from row 505: 338\n",
      "Loss sample: 7.92932338066978\n",
      "training samples from row 506: 615\n",
      "Loss sample: 8.769306749258822\n",
      "training samples from row 507: 366\n",
      "Loss sample: 8.198188995604136\n",
      "training samples from row 508: 917\n",
      "Loss sample: 8.160339014966103\n",
      "training samples from row 509: 251\n",
      "Loss sample: 8.416349316651873\n",
      "training samples from row 510: 253\n",
      "Loss sample: 8.126175254496783\n",
      "training samples from row 511: 255\n",
      "Loss sample: 8.459937578647994\n",
      "training samples from row 512: 466\n",
      "Loss sample: 8.047604595750979\n",
      "training samples from row 513: 227\n",
      "Loss sample: 8.542901455790618\n",
      "training samples from row 514: 646\n",
      "Loss sample: 8.096583001853096\n",
      "training samples from row 515: 246\n",
      "Loss sample: 8.41243608388342\n",
      "training samples from row 516: 255\n",
      "Loss sample: 8.058718915623874\n",
      "training samples from row 517: 678\n",
      "Loss sample: 8.732498203817476\n",
      "training samples from row 518: 246\n",
      "Loss sample: 8.293329459409946\n",
      "training samples from row 519: 385\n",
      "Loss sample: 7.981872793009411\n",
      "training samples from row 520: 424\n",
      "Loss sample: 8.122457812827472\n",
      "training samples from row 521: 323\n",
      "Loss sample: 8.22243887697664\n",
      "training samples from row 522: 267\n",
      "Loss sample: 8.12612546158234\n",
      "training samples from row 523: 286\n",
      "Loss sample: 7.97088801658521\n",
      "training samples from row 524: 334\n",
      "Loss sample: 8.492589594799549\n",
      "training samples from row 525: 148\n",
      "Loss sample: 7.927432723334078\n",
      "training samples from row 526: 265\n",
      "Loss sample: 8.088580615695335\n",
      "training samples from row 527: 309\n",
      "Loss sample: 8.172391286326503\n",
      "training samples from row 528: 148\n",
      "Loss sample: 8.086786308659573\n",
      "training samples from row 529: 129\n",
      "Loss sample: 7.831256717296866\n",
      "training samples from row 530: 825\n",
      "Loss sample: 8.023627688874761\n",
      "training samples from row 531: 341\n",
      "Loss sample: 8.14217676578338\n",
      "training samples from row 532: 228\n",
      "Loss sample: 8.249244354570513\n",
      "training samples from row 533: 334\n",
      "Loss sample: 8.119297606103169\n",
      "training samples from row 534: 688\n",
      "Loss sample: 8.15871069684004\n",
      "training samples from row 535: 246\n",
      "Loss sample: 8.290405748322888\n",
      "training samples from row 536: 204\n",
      "Loss sample: 8.173973730058288\n",
      "training samples from row 537: 293\n",
      "Loss sample: 8.341121905748905\n",
      "training samples from row 538: 195\n",
      "Loss sample: 8.313133889661836\n",
      "training samples from row 539: 502\n",
      "Loss sample: 8.404539582999325\n",
      "training samples from row 540: 506\n",
      "Loss sample: 8.183676040598163\n",
      "training samples from row 541: 298\n",
      "Loss sample: 8.031030347322075\n",
      "training samples from row 542: 501\n",
      "Loss sample: 7.773550908622464\n",
      "training samples from row 543: 296\n",
      "Loss sample: 8.167229452363504\n",
      "training samples from row 544: 458\n",
      "Loss sample: 8.409719607003785\n",
      "training samples from row 545: 135\n",
      "Loss sample: 8.531461967516094\n",
      "training samples from row 546: 229\n",
      "Loss sample: 8.000686155837032\n",
      "training samples from row 547: 671\n",
      "Loss sample: 8.051522265460225\n",
      "training samples from row 548: 588\n",
      "Loss sample: 8.15093778655465\n",
      "training samples from row 549: 232\n",
      "Loss sample: 8.343440238229439\n",
      "training samples from row 550: 469\n",
      "Loss sample: 8.362994508970326\n",
      "training samples from row 551: 386\n",
      "Loss sample: 8.3123718180458\n",
      "training samples from row 552: 389\n",
      "Loss sample: 8.053255863020695\n",
      "training samples from row 553: 772\n",
      "Loss sample: 8.394222173334603\n",
      "training samples from row 554: 202\n",
      "Loss sample: 8.324876503942352\n",
      "training samples from row 555: 240\n",
      "Loss sample: 8.056124767933365\n",
      "training samples from row 556: 439\n",
      "Loss sample: 8.35759972671837\n",
      "training samples from row 557: 427\n",
      "Loss sample: 8.011472278143156\n",
      "training samples from row 558: 210\n",
      "Loss sample: 8.385748553788503\n",
      "training samples from row 559: 343\n",
      "Loss sample: 8.351135312920903\n",
      "training samples from row 560: 238\n",
      "Loss sample: 8.113455966351669\n",
      "training samples from row 561: 205\n",
      "Loss sample: 7.914933716741246\n",
      "training samples from row 562: 581\n",
      "Loss sample: 8.24344698351003\n",
      "training samples from row 563: 608\n",
      "Loss sample: 8.094567904458819\n",
      "training samples from row 564: 181\n",
      "Loss sample: 8.29173773206129\n",
      "training samples from row 565: 422\n",
      "Loss sample: 7.877112619940261\n",
      "training samples from row 566: 248\n",
      "Loss sample: 8.782256218519965\n",
      "training samples from row 567: 453\n",
      "Loss sample: 7.92943248976346\n",
      "training samples from row 568: 279\n",
      "Loss sample: 8.411947045416628\n",
      "training samples from row 569: 197\n",
      "Loss sample: 8.686758806436433\n",
      "training samples from row 570: 256\n",
      "Loss sample: 8.173759766115463\n",
      "training samples from row 571: 277\n",
      "Loss sample: 8.132679480632731\n",
      "training samples from row 572: 216\n",
      "Loss sample: 8.227010216264834\n",
      "training samples from row 573: 300\n",
      "Loss sample: 8.39491713643873\n",
      "training samples from row 574: 602\n",
      "Loss sample: 8.190360389407005\n",
      "training samples from row 575: 343\n",
      "Loss sample: 8.410594163572764\n",
      "training samples from row 576: 368\n",
      "Loss sample: 8.29097535510657\n",
      "training samples from row 577: 288\n",
      "Loss sample: 8.181627941656224\n",
      "training samples from row 578: 139\n",
      "Loss sample: 8.364285880563246\n",
      "training samples from row 579: 374\n",
      "Loss sample: 7.933026722159106\n",
      "training samples from row 580: 438\n",
      "Loss sample: 8.306194416564528\n",
      "training samples from row 581: 597\n",
      "Loss sample: 8.39640220844311\n",
      "training samples from row 582: 210\n",
      "Loss sample: 8.471854594069582\n",
      "training samples from row 583: 565\n",
      "Loss sample: 8.45364322515712\n",
      "training samples from row 584: 254\n",
      "Loss sample: 8.447001647700459\n",
      "training samples from row 585: 814\n",
      "Loss sample: 7.986845902465314\n",
      "training samples from row 586: 229\n",
      "Loss sample: 8.367887229489586\n",
      "training samples from row 587: 140\n",
      "Loss sample: 8.195633360346362\n",
      "training samples from row 588: 193\n",
      "Loss sample: 7.999158758169888\n",
      "training samples from row 589: 340\n",
      "Loss sample: 8.098391769176983\n",
      "training samples from row 590: 182\n",
      "Loss sample: 8.228566316204118\n",
      "training samples from row 591: 293\n",
      "Loss sample: 7.986505037947675\n",
      "training samples from row 592: 597\n",
      "Loss sample: 8.525379451479523\n",
      "training samples from row 593: 151\n",
      "Loss sample: 8.082000002507629\n",
      "training samples from row 594: 188\n",
      "Loss sample: 8.484808177954893\n",
      "training samples from row 595: 242\n",
      "Loss sample: 8.02248738898421\n",
      "training samples from row 596: 307\n",
      "Loss sample: 7.889675270264722\n",
      "training samples from row 597: 334\n",
      "Loss sample: 7.987799505648448\n",
      "training samples from row 598: 354\n",
      "Loss sample: 7.856480238638395\n",
      "training samples from row 599: 379\n",
      "Loss sample: 8.097011942005771\n",
      "training samples from row 600: 244\n",
      "Loss sample: 8.611071206277252\n",
      "training samples from row 601: 310\n",
      "Loss sample: 8.217869283893977\n",
      "training samples from row 602: 209\n",
      "Loss sample: 8.485193309927974\n",
      "training samples from row 603: 416\n",
      "Loss sample: 8.440109930049731\n",
      "training samples from row 604: 335\n",
      "Loss sample: 8.032397700101992\n",
      "training samples from row 605: 1379\n",
      "Loss sample: 7.832909271745755\n",
      "training samples from row 606: 651\n",
      "Loss sample: 8.089760042474667\n",
      "training samples from row 607: 351\n",
      "Loss sample: 8.067275094037687\n",
      "training samples from row 608: 339\n",
      "Loss sample: 7.855648081375748\n",
      "training samples from row 609: 310\n",
      "Loss sample: 8.307004354933257\n",
      "training samples from row 610: 340\n",
      "Loss sample: 8.081952534422426\n",
      "training samples from row 611: 156\n",
      "Loss sample: 7.823716267248904\n",
      "training samples from row 612: 981\n",
      "Loss sample: 8.036343929736493\n",
      "training samples from row 613: 516\n",
      "Loss sample: 7.987886339247794\n",
      "training samples from row 614: 239\n",
      "Loss sample: 8.380916559711796\n",
      "training samples from row 615: 911\n",
      "Loss sample: 8.055145555727876\n",
      "training samples from row 616: 506\n",
      "Loss sample: 8.139027473826152\n",
      "training samples from row 617: 1076\n",
      "Loss sample: 8.298013618981784\n",
      "training samples from row 618: 620\n",
      "Loss sample: 8.20867445991777\n",
      "training samples from row 619: 270\n",
      "Loss sample: 8.474064327495057\n",
      "training samples from row 620: 253\n",
      "Loss sample: 8.158270333888279\n",
      "training samples from row 621: 373\n",
      "Loss sample: 8.351179137936386\n",
      "training samples from row 622: 489\n",
      "Loss sample: 8.05355054896767\n",
      "training samples from row 623: 364\n",
      "Loss sample: 8.213634906660428\n",
      "training samples from row 624: 700\n",
      "Loss sample: 8.026206909823133\n",
      "training samples from row 625: 310\n",
      "Loss sample: 8.054321498980265\n",
      "training samples from row 626: 512\n",
      "Loss sample: 8.285945863474044\n",
      "training samples from row 627: 265\n",
      "Loss sample: 8.709736140975055\n",
      "training samples from row 628: 478\n",
      "Loss sample: 8.277683740782143\n",
      "training samples from row 629: 281\n",
      "Loss sample: 8.137445556334997\n",
      "training samples from row 630: 554\n",
      "Loss sample: 8.004669118398628\n",
      "training samples from row 631: 394\n",
      "Loss sample: 7.975403582474242\n",
      "training samples from row 632: 266\n",
      "Loss sample: 7.927036052495945\n",
      "training samples from row 633: 189\n",
      "Loss sample: 8.091346450198742\n",
      "training samples from row 634: 122\n",
      "Loss sample: 8.316151886803485\n",
      "training samples from row 635: 668\n",
      "Loss sample: 8.109145869411574\n",
      "training samples from row 636: 463\n",
      "Loss sample: 7.9559381632746025\n",
      "training samples from row 637: 406\n",
      "Loss sample: 8.050485059035985\n",
      "training samples from row 638: 752\n",
      "Loss sample: 8.007289948157817\n",
      "training samples from row 639: 560\n",
      "Loss sample: 8.144152737034432\n",
      "training samples from row 640: 341\n",
      "Loss sample: 7.978842912747395\n",
      "training samples from row 641: 247\n",
      "Loss sample: 8.089668468147186\n",
      "training samples from row 642: 302\n",
      "Loss sample: 8.333547432295452\n",
      "training samples from row 643: 310\n",
      "Loss sample: 7.814784850622483\n",
      "training samples from row 644: 738\n",
      "Loss sample: 8.31913948610851\n",
      "training samples from row 645: 149\n",
      "Loss sample: 8.400890380135408\n",
      "training samples from row 646: 532\n",
      "Loss sample: 8.055166331145594\n",
      "training samples from row 647: 471\n",
      "Loss sample: 8.107415236981812\n",
      "training samples from row 648: 286\n",
      "Loss sample: 8.385446449002407\n",
      "training samples from row 649: 836\n",
      "Loss sample: 8.453693910457451\n",
      "training samples from row 650: 366\n",
      "Loss sample: 8.04378378490536\n",
      "training samples from row 651: 235\n",
      "Loss sample: 7.97325017103311\n",
      "training samples from row 652: 741\n",
      "Loss sample: 8.26708845590389\n",
      "training samples from row 653: 214\n",
      "Loss sample: 7.640315019930033\n",
      "training samples from row 654: 651\n",
      "Loss sample: 7.8702738697792975\n",
      "training samples from row 655: 291\n",
      "Loss sample: 8.031835720683187\n",
      "training samples from row 656: 158\n",
      "Loss sample: 8.064997311262903\n",
      "training samples from row 657: 411\n",
      "Loss sample: 8.063143693671973\n",
      "training samples from row 658: 158\n",
      "Loss sample: 8.239555248018751\n",
      "training samples from row 659: 251\n",
      "Loss sample: 7.994222096901313\n",
      "training samples from row 660: 405\n",
      "Loss sample: 7.994202217878758\n",
      "training samples from row 661: 215\n",
      "Loss sample: 8.054798191475127\n",
      "training samples from row 662: 267\n",
      "Loss sample: 8.036882258822182\n",
      "training samples from row 663: 539\n",
      "Loss sample: 8.103144245799708\n",
      "training samples from row 664: 375\n",
      "Loss sample: 8.053375079250262\n",
      "training samples from row 665: 439\n",
      "Loss sample: 7.960947809727005\n",
      "training samples from row 666: 386\n",
      "Loss sample: 7.9137856072703165\n",
      "training samples from row 667: 310\n",
      "Loss sample: 8.337580166288063\n",
      "training samples from row 668: 521\n",
      "Loss sample: 8.255357294622174\n",
      "training samples from row 669: 167\n",
      "Loss sample: 8.472017577686334\n",
      "training samples from row 670: 344\n",
      "Loss sample: 8.368112119691304\n",
      "training samples from row 671: 754\n",
      "Loss sample: 8.12355488484921\n",
      "training samples from row 672: 379\n",
      "Loss sample: 8.363236901652833\n",
      "training samples from row 673: 263\n",
      "Loss sample: 8.291559384974416\n",
      "training samples from row 674: 868\n",
      "Loss sample: 8.133795636072003\n",
      "training samples from row 675: 949\n",
      "Loss sample: 8.063821588920707\n",
      "training samples from row 676: 277\n",
      "Loss sample: 8.004142680220736\n",
      "training samples from row 677: 3352\n",
      "Loss sample: 7.992331287216417\n",
      "training samples from row 678: 242\n",
      "Loss sample: 8.20991467773355\n",
      "training samples from row 679: 184\n",
      "Loss sample: 8.071160404610689\n",
      "training samples from row 680: 179\n",
      "Loss sample: 8.480003982379145\n",
      "training samples from row 681: 322\n",
      "Loss sample: 8.23821954750719\n",
      "training samples from row 682: 379\n",
      "Loss sample: 7.741303545187839\n",
      "training samples from row 683: 181\n",
      "Loss sample: 8.228702519407028\n",
      "training samples from row 684: 381\n",
      "Loss sample: 7.88264118787785\n",
      "training samples from row 685: 267\n",
      "Loss sample: 8.228624423891395\n",
      "training samples from row 686: 194\n",
      "Loss sample: 7.863642904985109\n",
      "training samples from row 687: 340\n",
      "Loss sample: 8.220458928384375\n",
      "training samples from row 688: 512\n",
      "Loss sample: 7.8864276186002025\n",
      "training samples from row 689: 321\n",
      "Loss sample: 8.44704188207762\n",
      "training samples from row 690: 219\n",
      "Loss sample: 7.825196175342633\n",
      "training samples from row 691: 443\n",
      "Loss sample: 7.99064313390655\n",
      "training samples from row 692: 224\n",
      "Loss sample: 8.32873731829744\n",
      "training samples from row 693: 226\n",
      "Loss sample: 7.94283552866261\n",
      "training samples from row 694: 283\n",
      "Loss sample: 7.994842845438111\n",
      "training samples from row 695: 149\n",
      "Loss sample: 8.214386663043543\n",
      "training samples from row 696: 405\n",
      "Loss sample: 8.23436337642581\n",
      "training samples from row 697: 401\n",
      "Loss sample: 7.881442053233549\n",
      "training samples from row 698: 823\n",
      "Loss sample: 8.362779089621394\n",
      "training samples from row 699: 405\n",
      "Loss sample: 7.64898198065064\n",
      "training samples from row 700: 591\n",
      "Loss sample: 8.079241792011874\n",
      "training samples from row 701: 825\n",
      "Loss sample: 8.24263319438732\n",
      "training samples from row 702: 676\n",
      "Loss sample: 7.899521369426916\n",
      "training samples from row 703: 305\n",
      "Loss sample: 7.902188396918105\n",
      "training samples from row 704: 426\n",
      "Loss sample: 8.005496814520237\n",
      "training samples from row 705: 177\n",
      "Loss sample: 8.08071206863599\n",
      "training samples from row 706: 415\n",
      "Loss sample: 8.332474864727272\n",
      "training samples from row 707: 361\n",
      "Loss sample: 7.868658643069394\n",
      "training samples from row 708: 261\n",
      "Loss sample: 8.029154178963454\n",
      "training samples from row 709: 310\n",
      "Loss sample: 8.11335640837209\n",
      "training samples from row 710: 283\n",
      "Loss sample: 8.14546552683938\n",
      "training samples from row 711: 265\n",
      "Loss sample: 7.9487066612796795\n",
      "training samples from row 712: 199\n",
      "Loss sample: 8.512268323159775\n",
      "training samples from row 713: 835\n",
      "Loss sample: 7.765791835235063\n",
      "training samples from row 714: 171\n",
      "Loss sample: 8.36390341800998\n",
      "training samples from row 715: 228\n",
      "Loss sample: 8.094246956383676\n",
      "training samples from row 716: 250\n",
      "Loss sample: 7.964378454296049\n",
      "training samples from row 717: 571\n",
      "Loss sample: 8.230685736295905\n",
      "training samples from row 718: 330\n",
      "Loss sample: 8.295021303587184\n",
      "training samples from row 719: 325\n",
      "Loss sample: 7.892275649575082\n",
      "training samples from row 720: 234\n",
      "Loss sample: 7.785446442953529\n",
      "training samples from row 721: 377\n",
      "Loss sample: 7.865728621209656\n",
      "training samples from row 722: 168\n",
      "Loss sample: 8.564525670511662\n",
      "training samples from row 723: 140\n",
      "Loss sample: 7.942675237226784\n",
      "training samples from row 724: 158\n",
      "Loss sample: 7.908907946637682\n",
      "training samples from row 725: 145\n",
      "Loss sample: 8.119958614857657\n",
      "training samples from row 726: 254\n",
      "Loss sample: 8.640937104516423\n",
      "training samples from row 727: 340\n",
      "Loss sample: 8.440071418494218\n",
      "training samples from row 728: 161\n",
      "Loss sample: 8.531549164368363\n",
      "training samples from row 729: 128\n",
      "Loss sample: 9.026819275128531\n",
      "training samples from row 730: 263\n",
      "Loss sample: 7.865835118216253\n",
      "training samples from row 731: 315\n",
      "Loss sample: 7.94579801844623\n",
      "training samples from row 732: 657\n",
      "Loss sample: 8.77299372040368\n",
      "training samples from row 733: 254\n",
      "Loss sample: 8.01592773048405\n",
      "training samples from row 734: 335\n",
      "Loss sample: 7.844472488544066\n",
      "training samples from row 735: 310\n",
      "Loss sample: 8.471584031979248\n",
      "training samples from row 736: 523\n",
      "Loss sample: 7.830676936547783\n",
      "training samples from row 737: 244\n",
      "Loss sample: 8.4119212544307\n",
      "training samples from row 738: 283\n",
      "Loss sample: 8.361699785330094\n",
      "training samples from row 739: 569\n",
      "Loss sample: 7.955443555812718\n",
      "training samples from row 740: 361\n",
      "Loss sample: 8.415218375923262\n",
      "training samples from row 741: 198\n",
      "Loss sample: 8.135983341247913\n",
      "training samples from row 742: 180\n",
      "Loss sample: 8.043926600491611\n",
      "training samples from row 743: 355\n",
      "Loss sample: 7.869227770561837\n",
      "training samples from row 744: 500\n",
      "Loss sample: 8.30774734619278\n",
      "training samples from row 745: 379\n",
      "Loss sample: 8.066956818895791\n",
      "training samples from row 746: 428\n",
      "Loss sample: 8.402755330264277\n",
      "training samples from row 747: 383\n",
      "Loss sample: 8.375955552518016\n",
      "training samples from row 748: 488\n",
      "Loss sample: 7.902181760386701\n",
      "training samples from row 749: 296\n",
      "Loss sample: 8.032597949083161\n",
      "training samples from row 750: 282\n",
      "Loss sample: 8.206607290246781\n",
      "training samples from row 751: 564\n",
      "Loss sample: 8.214811510450899\n",
      "training samples from row 752: 294\n",
      "Loss sample: 8.192656415661643\n",
      "training samples from row 753: 391\n",
      "Loss sample: 7.975934247978855\n",
      "training samples from row 754: 618\n",
      "Loss sample: 7.761814254103661\n",
      "training samples from row 755: 490\n",
      "Loss sample: 7.97743947145181\n",
      "training samples from row 756: 263\n",
      "Loss sample: 8.37174806329992\n",
      "training samples from row 757: 411\n",
      "Loss sample: 8.338749141081236\n",
      "training samples from row 758: 410\n",
      "Loss sample: 7.931406169376942\n",
      "training samples from row 759: 393\n",
      "Loss sample: 8.175901698342411\n",
      "training samples from row 760: 531\n",
      "Loss sample: 7.959655441490442\n",
      "training samples from row 761: 669\n",
      "Loss sample: 7.988228054658463\n",
      "training samples from row 762: 198\n",
      "Loss sample: 8.220039170566551\n",
      "training samples from row 763: 463\n",
      "Loss sample: 7.932509439794591\n",
      "training samples from row 764: 394\n",
      "Loss sample: 7.804103096071825\n",
      "training samples from row 765: 148\n",
      "Loss sample: 8.106614857768914\n",
      "training samples from row 766: 176\n",
      "Loss sample: 8.228705808715015\n",
      "training samples from row 767: 449\n",
      "Loss sample: 8.170634012756322\n",
      "training samples from row 768: 489\n",
      "Loss sample: 8.057006662028577\n",
      "training samples from row 769: 218\n",
      "Loss sample: 8.301495110545385\n",
      "training samples from row 770: 354\n",
      "Loss sample: 7.780288304486741\n",
      "training samples from row 771: 526\n",
      "Loss sample: 8.044824598317417\n",
      "training samples from row 772: 521\n",
      "Loss sample: 7.983024460760518\n",
      "training samples from row 773: 646\n",
      "Loss sample: 8.221099415881548\n",
      "training samples from row 774: 241\n",
      "Loss sample: 8.176424117461076\n",
      "training samples from row 775: 139\n",
      "Loss sample: 7.965710825625804\n",
      "training samples from row 776: 379\n",
      "Loss sample: 8.222256497928184\n",
      "training samples from row 777: 409\n",
      "Loss sample: 7.990559385314786\n",
      "training samples from row 778: 435\n",
      "Loss sample: 7.900959887596159\n",
      "training samples from row 779: 655\n",
      "Loss sample: 7.913760289210314\n",
      "training samples from row 780: 397\n",
      "Loss sample: 8.721696671770347\n",
      "training samples from row 781: 446\n",
      "Loss sample: 7.915117050260852\n",
      "training samples from row 782: 506\n",
      "Loss sample: 8.390199511378775\n",
      "training samples from row 783: 210\n",
      "Loss sample: 8.95269982695963\n",
      "training samples from row 784: 243\n",
      "Loss sample: 8.526623593115183\n",
      "training samples from row 785: 874\n",
      "Loss sample: 7.868581900061216\n",
      "training samples from row 786: 323\n",
      "Loss sample: 8.21897490055145\n",
      "training samples from row 787: 379\n",
      "Loss sample: 8.167143670778575\n",
      "training samples from row 788: 296\n",
      "Loss sample: 8.194063194957211\n",
      "training samples from row 789: 417\n",
      "Loss sample: 8.608730318742186\n",
      "training samples from row 790: 328\n",
      "Loss sample: 8.368902112225022\n",
      "training samples from row 791: 485\n",
      "Loss sample: 7.755312387908694\n",
      "training samples from row 792: 337\n",
      "Loss sample: 8.213633354843697\n",
      "training samples from row 793: 696\n",
      "Loss sample: 8.065140079461\n",
      "training samples from row 794: 323\n",
      "Loss sample: 8.09557308545577\n",
      "training samples from row 795: 520\n",
      "Loss sample: 7.891460633290911\n",
      "training samples from row 796: 392\n",
      "Loss sample: 8.135031031444019\n",
      "training samples from row 797: 520\n",
      "Loss sample: 7.906458866041156\n",
      "training samples from row 798: 247\n",
      "Loss sample: 7.984385806438204\n",
      "training samples from row 799: 226\n",
      "Loss sample: 8.126945524507631\n",
      "training samples from row 800: 622\n",
      "Loss sample: 8.192156867526519\n",
      "training samples from row 801: 767\n",
      "Loss sample: 8.488341704740575\n",
      "training samples from row 802: 225\n",
      "Loss sample: 8.066479716126667\n",
      "training samples from row 803: 437\n",
      "Loss sample: 7.997446215999261\n",
      "training samples from row 804: 253\n",
      "Loss sample: 8.183856471689266\n",
      "training samples from row 805: 439\n",
      "Loss sample: 7.841626678004803\n",
      "training samples from row 806: 248\n",
      "Loss sample: 7.92939438023945\n",
      "training samples from row 807: 404\n",
      "Loss sample: 7.994010068514861\n",
      "training samples from row 808: 342\n",
      "Loss sample: 8.320691131406731\n",
      "training samples from row 809: 503\n",
      "Loss sample: 8.389435770914472\n",
      "training samples from row 810: 146\n",
      "Loss sample: 7.736668175126651\n",
      "training samples from row 811: 331\n",
      "Loss sample: 8.100400590147709\n",
      "training samples from row 812: 321\n",
      "Loss sample: 7.811025171561182\n",
      "training samples from row 813: 313\n",
      "Loss sample: 8.008582557896036\n",
      "training samples from row 814: 373\n",
      "Loss sample: 7.956851660352398\n",
      "training samples from row 815: 483\n",
      "Loss sample: 8.122498334757275\n",
      "training samples from row 816: 650\n",
      "Loss sample: 8.03151526415822\n",
      "training samples from row 817: 537\n",
      "Loss sample: 7.907851793928521\n",
      "training samples from row 818: 217\n",
      "Loss sample: 8.122941380716751\n",
      "training samples from row 819: 382\n",
      "Loss sample: 8.10743530527592\n",
      "training samples from row 820: 283\n",
      "Loss sample: 8.611025650740029\n",
      "training samples from row 821: 364\n",
      "Loss sample: 8.237278442486982\n",
      "training samples from row 822: 536\n",
      "Loss sample: 8.299742646362427\n",
      "training samples from row 823: 350\n",
      "Loss sample: 8.13174472082684\n",
      "training samples from row 824: 419\n",
      "Loss sample: 7.766430792106328\n",
      "training samples from row 825: 213\n",
      "Loss sample: 7.896176535838912\n",
      "training samples from row 826: 369\n",
      "Loss sample: 8.20176696823054\n",
      "training samples from row 827: 366\n",
      "Loss sample: 7.717136633852387\n",
      "training samples from row 828: 385\n",
      "Loss sample: 8.012850396362502\n",
      "training samples from row 829: 184\n",
      "Loss sample: 8.37979056824034\n",
      "training samples from row 830: 560\n",
      "Loss sample: 8.668442989628534\n",
      "training samples from row 831: 583\n",
      "Loss sample: 7.937161969497942\n",
      "training samples from row 832: 256\n",
      "Loss sample: 7.915370539893303\n",
      "training samples from row 833: 284\n",
      "Loss sample: 7.965884696081264\n",
      "training samples from row 834: 300\n",
      "Loss sample: 7.779792895593241\n",
      "training samples from row 835: 223\n",
      "Loss sample: 8.23667636598004\n",
      "training samples from row 836: 265\n",
      "Loss sample: 8.084710564479163\n",
      "training samples from row 837: 521\n",
      "Loss sample: 8.170290820759616\n",
      "training samples from row 838: 258\n",
      "Loss sample: 8.176640213874508\n",
      "training samples from row 839: 735\n",
      "Loss sample: 8.06245515600448\n",
      "training samples from row 840: 627\n",
      "Loss sample: 7.997397775786196\n",
      "training samples from row 841: 317\n",
      "Loss sample: 8.12852078514177\n",
      "training samples from row 842: 230\n",
      "Loss sample: 8.104412834756793\n",
      "training samples from row 843: 651\n",
      "Loss sample: 8.19852690454067\n",
      "training samples from row 844: 824\n",
      "Loss sample: 7.7970323259182654\n",
      "training samples from row 845: 318\n",
      "Loss sample: 8.265245992453776\n",
      "training samples from row 846: 283\n",
      "Loss sample: 8.131625984536386\n",
      "training samples from row 847: 412\n",
      "Loss sample: 7.75409008918437\n",
      "training samples from row 848: 450\n",
      "Loss sample: 8.18074409086352\n",
      "training samples from row 849: 389\n",
      "Loss sample: 8.643978119040348\n",
      "training samples from row 850: 545\n",
      "Loss sample: 8.015485815591571\n",
      "training samples from row 851: 912\n",
      "Loss sample: 8.055485387137864\n",
      "training samples from row 852: 246\n",
      "Loss sample: 8.725527153673013\n",
      "training samples from row 853: 476\n",
      "Loss sample: 7.856938278850116\n",
      "training samples from row 854: 630\n",
      "Loss sample: 7.991449533645128\n",
      "training samples from row 855: 243\n",
      "Loss sample: 8.004543421495114\n",
      "training samples from row 856: 226\n",
      "Loss sample: 8.10220938023503\n",
      "training samples from row 857: 452\n",
      "Loss sample: 7.8480949295238265\n",
      "training samples from row 858: 475\n",
      "Loss sample: 8.188548592067853\n",
      "training samples from row 859: 533\n",
      "Loss sample: 7.895772420032645\n",
      "training samples from row 860: 376\n",
      "Loss sample: 7.9040649729517805\n",
      "training samples from row 861: 518\n",
      "Loss sample: 7.987864463717485\n",
      "training samples from row 862: 234\n",
      "Loss sample: 7.928872357721881\n",
      "training samples from row 863: 609\n",
      "Loss sample: 8.172292828022762\n",
      "training samples from row 864: 555\n",
      "Loss sample: 8.155027620889099\n",
      "training samples from row 865: 2429\n",
      "Loss sample: 7.988242261337309\n",
      "training samples from row 866: 253\n",
      "Loss sample: 8.382188074619124\n",
      "training samples from row 867: 281\n",
      "Loss sample: 7.72432145619513\n",
      "training samples from row 868: 480\n",
      "Loss sample: 7.690909528869846\n",
      "training samples from row 869: 315\n",
      "Loss sample: 7.749275321502163\n",
      "training samples from row 870: 281\n",
      "Loss sample: 8.110656233411126\n",
      "training samples from row 871: 346\n",
      "Loss sample: 7.932310858482186\n",
      "training samples from row 872: 534\n",
      "Loss sample: 8.478754676143327\n",
      "training samples from row 873: 341\n",
      "Loss sample: 7.776012701097113\n",
      "training samples from row 874: 390\n",
      "Loss sample: 7.741879706942574\n",
      "training samples from row 875: 311\n",
      "Loss sample: 8.034842733142899\n",
      "training samples from row 876: 525\n",
      "Loss sample: 8.042592465996902\n",
      "training samples from row 877: 521\n",
      "Loss sample: 7.798685697998408\n",
      "training samples from row 878: 272\n",
      "Loss sample: 7.689312581171216\n",
      "training samples from row 879: 213\n",
      "Loss sample: 8.145449378924985\n",
      "training samples from row 880: 2463\n",
      "Loss sample: 7.898981389924866\n",
      "training samples from row 881: 235\n",
      "Loss sample: 8.067462929127915\n",
      "training samples from row 882: 696\n",
      "Loss sample: 8.140935113546234\n",
      "training samples from row 883: 632\n",
      "Loss sample: 7.730015354639326\n",
      "training samples from row 884: 182\n",
      "Loss sample: 7.809689540724316\n",
      "training samples from row 885: 224\n",
      "Loss sample: 7.7945154796987355\n",
      "training samples from row 886: 492\n",
      "Loss sample: 7.896842334957638\n",
      "training samples from row 887: 490\n",
      "Loss sample: 7.982158716695651\n",
      "training samples from row 888: 198\n",
      "Loss sample: 7.912126914661791\n",
      "training samples from row 889: 451\n",
      "Loss sample: 7.85385941184764\n",
      "training samples from row 890: 742\n",
      "Loss sample: 8.479561196357727\n",
      "training samples from row 891: 505\n",
      "Loss sample: 8.102595338658157\n",
      "training samples from row 892: 454\n",
      "Loss sample: 8.69737149929719\n",
      "training samples from row 893: 249\n",
      "Loss sample: 7.919986891679762\n",
      "training samples from row 894: 511\n",
      "Loss sample: 7.845731055829953\n",
      "training samples from row 895: 250\n",
      "Loss sample: 7.991112855720589\n",
      "training samples from row 896: 298\n",
      "Loss sample: 7.974189589991178\n",
      "training samples from row 897: 583\n",
      "Loss sample: 8.274881676574266\n",
      "training samples from row 898: 167\n",
      "Loss sample: 7.858423068069828\n",
      "training samples from row 899: 361\n",
      "Loss sample: 7.680219163842322\n",
      "training samples from row 900: 288\n",
      "Loss sample: 7.852913714710202\n",
      "training samples from row 901: 644\n",
      "Loss sample: 8.192019188068423\n",
      "training samples from row 902: 642\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[362], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m flattening_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcat\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m=\u001b[39mWord2Vec(embedding_size, semi_context_window, complete_text, vocabulary, flattening_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[359], line 147\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[1;34m(self, X_train)\u001b[0m\n\u001b[0;32m    144\u001b[0m     total_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mLoss\n\u001b[0;32m    145\u001b[0m     dLoss_dZout \u001b[38;5;241m=\u001b[39m sigma_zout_two \u001b[38;5;241m-\u001b[39m yi\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackpropagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdLoss_dZout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigmaz_out_one\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_zout_two\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;66;03m#print(f\"PHRASE {i} sample {n} Xi={Xi.shape}, yi={yi.shape},target={self.vocabulary[np.where(yi[0] != 0)[0][0]]} Loss={Loss}\")\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss sample: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(training_samples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[359], line 86\u001b[0m, in \u001b[0;36mWord2Vec.backpropagation\u001b[1;34m(self, dLoss_dZ2, sigma_Z_1, sigma_Z_2, Xi)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackpropagation\u001b[39m(\u001b[38;5;28mself\u001b[39m, dLoss_dZ2, sigma_Z_1, sigma_Z_2, Xi):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;66;03m# print(\"dLoss_dZ2\", dLoss_dZ2.shape)\u001b[39;00m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;66;03m# print(\"W2\", self.outlayer_maps_vocab_concat.shape)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# print(\"b1\", self.words_len_embedding_bias.shape)\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;66;03m# print(\"Xi\", Xi.shape)\u001b[39;00m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflattening_strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcat\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 86\u001b[0m         dLoss_dW2 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43msigma_Z_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdLoss_dZ2\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (1800, 29457)\u001b[39;00m\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;66;03m# Gradient of loss with respect to b2\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         dLoss_db2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dLoss_dZ2, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# (29457,)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "complete_text = ' '.join(df[\"text\"].tolist())\n",
    "vocabulary = create_vocabulary(complete_text)\n",
    "embedding_size=100\n",
    "semi_context_window=2 \n",
    "flattening_strategy=\"concat\"\n",
    "model=Word2Vec(embedding_size, semi_context_window, complete_text, vocabulary, flattening_strategy=\"concat\")\n",
    "model.train(df[\"text\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f67fd3-e289-4ab1-809b-f4da58e5e975",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.embedding_size = embedding_size\n",
    "        self.side_window_size = semi_context_window\n",
    "        self.complete_text = complete_text\n",
    "        self.vocabulary = vocabulary\n",
    "\n",
    "        self.words_len_embedding_layer = np.random.rand(len(self.vocabulary), embedding_size)\n",
    "        self.words_len_embedding_bias = np.random.rand(embedding_size)\n",
    "        self.flattening_strategy = flattening_strategy\n",
    "\n",
    "        self.outlayer_maps_vocab_average = np.random.rand(embedding_size, len(self.vocabulary))\n",
    "        self.out_bias_maps_vocab_average = np.random.rand(len(self.vocabulary))\n",
    "\n",
    "        self.outlayer_maps_vocab_concat = np.random.rand(semi_context_window * 2 * embedding_size, len(self.vocabulary))\n",
    "        self.out_bias_maps_vocab_concat = np.random.rand(len(self.vocabulary))\n",
    "        self.learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "bac2f95b-f664-4ad2-8019-5c509131596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    " \n",
    "with open('data/words_len_embedding_layer.pkl', 'wb') as handle:\n",
    "    pickle.dump(model.words_len_embedding_layer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('data/words_len_embedding_bias.pkl', 'wb') as handle:\n",
    "    pickle.dump(model.words_len_embedding_bias, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('data/outlayer_maps_vocab_average.pkl', 'wb') as handle:\n",
    "    pickle.dump(model.outlayer_maps_vocab_average, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('data/out_bias_maps_vocab_average.pkl', 'wb') as handle:\n",
    "    pickle.dump(model.out_bias_maps_vocab_average, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('data/outlayer_maps_vocab_concat.pkl', 'wb') as handle:\n",
    "    pickle.dump(model.outlayer_maps_vocab_concat, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('data/out_bias_maps_vocab_concat.pkl', 'wb') as handle:\n",
    "    pickle.dump(model.out_bias_maps_vocab_concat, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a737c1d-3452-409f-9440-ca5164d4d6b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "d97da8a2-8b93-47d6-ab04-72ded6713289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.61558769, 0.42641241, 0.33079602, ..., 0.92085622, 0.99881068,\n",
       "        0.89552219],\n",
       "       [0.87261605, 0.53257311, 0.03031227, ..., 0.20816584, 0.85581632,\n",
       "        0.98724434],\n",
       "       [0.28798672, 0.23416618, 0.1491817 , ..., 0.27298054, 0.9225899 ,\n",
       "        0.13177087],\n",
       "       ...,\n",
       "       [0.05761968, 0.17018932, 0.56018847, ..., 0.00129951, 0.80647558,\n",
       "        0.79852319],\n",
       "       [0.26102676, 0.37119619, 0.96895095, ..., 0.14781618, 0.78808541,\n",
       "        0.11557316],\n",
       "       [0.35304612, 0.08419512, 0.09100647, ..., 0.58819109, 0.64675526,\n",
       "        0.97931644]])"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.words_len_embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea13a86e-8d7e-46f1-b39e-5b82d18c26b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gen2 = model.inputs_window_words(df[\"text\"].iloc[0],vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "397e43c3-65f8-4dac-82ae-b8da64ba76af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a=[x for x in gen2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "c194472a-a977-49f6-975c-e15c85e58faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(a[0][1][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "6bf0bc12-9ad1-4263-ad5b-3edcab5d8b38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model.predict(a[0][1][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46404d1-4f34-46ac-94f2-f07c99718092",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891b7e37-10f7-430a-bb1b-93c5d53d6ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(self,dLoss_dZ2,sigma_Z_1,sigma_Z_2,Xi):\n",
    "        print(\"dLoss_dZ2\",dLoss_dZ2.shape)\n",
    "        print(\"W2\",self.outlayer_maps_vocab_concat.shape)\n",
    "        print(\"b2\",self.out_bias_maps_vocab_concat.shape)\n",
    "        print(\"sigma_Z_1\",sigma_Z_1.shape)\n",
    "        print(\"sigma_Z_2\",sigma_Z_2.shape)\n",
    "        print(\"W1\",self.words_len_embedding_layer.shape)\n",
    "        print(\"b1\",self.words_len_embedding_bias.shape)\n",
    "        \n",
    "        if self.flattening_strategy==\"concat\":\n",
    "             \n",
    "            dLoss_dSigma_Z1 = np.matmul(dLoss_dZ2,self.outlayer_maps_vocab_concat.T) # (1, 1800)\n",
    "            print(\"dLoss_dSigma_Zout_one\",dLoss_dSigma_Zout_one.shape)\n",
    "            dSigma1_dZ1=sigma_Z_1*(1-sigma_Z_1) \n",
    "            print(\"dSigma1_dZ1\",dSigma1_dZ1.shape)\n",
    "            dLoss_dW1=dLoss_dSigma_Z1.T@dSigma1_dZ1 #(1800,1800)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            # Gradient of loss with respect to W2\n",
    "            dLoss_dW2 = np.matmul(sigma_Z_1.T, dLoss_dZ2)  # (1800, 29457) \n",
    "            # Gradient of loss with respect to b2\n",
    "            dLoss_db2 = np.sum(dLoss_dZ2, axis=0) \n",
    "            self.outlayer_maps_vocab_concat -= self.learning_rate * dLoss_dW2\n",
    "            self.out_bias_maps_vocab_concat -= self.learning_rate * dLoss_db2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060ae09c-2f35-47ec-86cc-c10063b699aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if self.flattening_strategy == \"concat\":\n",
    "        # Gradient of loss with respect to W2\n",
    "        dLoss_dW2 = np.matmul(sigma_Z_1.T, dLoss_dZ2)  # (1800, 29457)\n",
    "        \n",
    "        # Gradient of loss with respect to b2\n",
    "        dLoss_db2 = np.sum(dLoss_dZ2, axis=0)  # (29457,)\n",
    "        \n",
    "        # Gradient of loss with respect to Z1\n",
    "        dLoss_dZ1 = np.matmul(dLoss_dZ2, self.outlayer_maps_vocab_concat.T)  # (1, 1800)\n",
    "        \n",
    "        # Reshape dLoss_dZ1 to match the original shape of sigma_Z_1\n",
    "        dLoss_dZ1 = dLoss_dZ1.reshape(-1, self.side_window_size * 2, self.embedding_size)  # (1, 6, 300)\n",
    "        \n",
    "        # Gradient of loss with respect to W1\n",
    "        dLoss_dW1 = np.matmul(Xi.T, dLoss_dZ1.reshape(Xi.shape[0], -1))  # (29457, 1800)\n",
    "        \n",
    "        # Gradient of loss with respect to b1\n",
    "        dLoss_db1 = np.sum(dLoss_dZ1, axis=(0, 1))  # (300,)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.outlayer_maps_vocab_concat -= self.learning_rate * dLoss_dW2\n",
    "        self.out_bias_maps_vocab_concat -= self.learning_rate * dLoss_db2\n",
    "        self.words_len_embedding_layer -= self.learning_rate * dLoss_dW1\n",
    "        self.words_len_embedding_bias -= self.learning_rate * dLoss_db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f755c0ee-d777-4ce3-a2c4-b8daba4b4fab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
